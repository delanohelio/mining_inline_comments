{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA0MDcxNTMx", "number": 4699, "title": "Issue 4676: (PDP-34) Part 2 of 4 - Implementation of ChunkStorage for HDFS, FileSystem and ExtendedS3.", "bodyText": "NOTE  This PR depends on #4686 to be completed first.\nIssue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - ChunkStorage for FileSystem, HDFS and ExtendedS3.\nSigned-off-by: Sachin Joshi sachin.joshi@emc.com\nChange log description\n\nInitial implementation of PDP 34.\nChunkStorage for FileSystem, HDFS and ExtendedS3. Minor code reuse and refactoring.\n\nPurpose of the change\nFixes #4676 - Initial implementation of PDP -34\nWhat the code does\nDesign :\n\nThis PR implements core parts of following design.  https://github.com/pravega/pravega/wiki/PDP-34:-Simplified-Tier-2\n\nGeneral Comments:\n\nThe code is added in a manner so that older AsyncStorage+Sync based code continues to work. Reusing and refactoring code wherever possible.\nThe ChunkStorage interface is synchronous for simplicity of implementation. We'll change it to Async if required at later date.\n\nPart 1 of 4 PRs :\n\nCore interfaces and core functionality of ChunkedSegmentStorage.\nThis PR _ ChunkStorage implementation for HDFS, ECS and FileSystem_\nTable segment based metadata store,  container bootstrap logic. end to end integration tests.\nChanges for configuration, tests and other incidental changes\n\nHow to verify it\n\n New or old unit tests should pass.\n System tests should pass.", "createdAt": "2020-04-16T01:47:36Z", "url": "https://github.com/pravega/pravega/pull/4699", "merged": true, "mergeCommit": {"oid": "87aba53a6db55bc8593dd6042c12e87d6d7ad316"}, "closed": true, "closedAt": "2020-07-17T21:33:15Z", "author": {"login": "sachin-j-joshi"}, "timelineItems": {"totalCount": 46, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcYQCO1gBqjMyNDA5NzAxOTk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc13-OmgH2gAyNDA0MDcxNTMxOjc2ZDRjNTUzN2RlNTM1M2RjZGVhZjVkZDk3YWZkZDVhYWIzMTMzNDQ=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA4Mjk1MjM5", "url": "https://github.com/pravega/pravega/pull/4699#pullrequestreview-408295239", "createdAt": "2020-05-08T14:51:27Z", "commit": null, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "21221c46235598cd3ea78f0255cfeb7457fad145", "author": {"user": {"login": "sachin-j-joshi", "name": "Sachin Jayant Joshi"}}, "url": "https://github.com/pravega/pravega/commit/21221c46235598cd3ea78f0255cfeb7457fad145", "committedDate": "2020-06-04T23:18:00Z", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Implementation of ChunkStorageProvider for HDFS, FileSystem and ExtendedS3.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "21221c46235598cd3ea78f0255cfeb7457fad145", "author": {"user": {"login": "sachin-j-joshi", "name": "Sachin Jayant Joshi"}}, "url": "https://github.com/pravega/pravega/commit/21221c46235598cd3ea78f0255cfeb7457fad145", "committedDate": "2020-06-04T23:18:00Z", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Implementation of ChunkStorageProvider for HDFS, FileSystem and ExtendedS3.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>"}, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "931c0f24ae41c1e3d21e609b9270dc5fb45e0b47", "author": {"user": {"login": "sachin-j-joshi", "name": "Sachin Jayant Joshi"}}, "url": "https://github.com/pravega/pravega/commit/931c0f24ae41c1e3d21e609b9270dc5fb45e0b47", "committedDate": "2020-06-12T02:40:50Z", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Implementation of ChunkStorageProvider for HDFS, FileSystem and ExtendedS3.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "931c0f24ae41c1e3d21e609b9270dc5fb45e0b47", "author": {"user": {"login": "sachin-j-joshi", "name": "Sachin Jayant Joshi"}}, "url": "https://github.com/pravega/pravega/commit/931c0f24ae41c1e3d21e609b9270dc5fb45e0b47", "committedDate": "2020-06-12T02:40:50Z", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Implementation of ChunkStorageProvider for HDFS, FileSystem and ExtendedS3.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>"}, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "5b0e178c83b487d6bf5b13c23e55d69ed0c4ce4b", "author": {"user": {"login": "sachin-j-joshi", "name": "Sachin Jayant Joshi"}}, "url": "https://github.com/pravega/pravega/commit/5b0e178c83b487d6bf5b13c23e55d69ed0c4ce4b", "committedDate": "2020-06-24T21:48:59Z", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Implementation of ChunkStorageProvider for HDFS, FileSystem and ExtendedS3.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDM5MjMxNDg1", "url": "https://github.com/pravega/pravega/pull/4699#pullrequestreview-439231485", "createdAt": "2020-06-29T14:59:21Z", "commit": null, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxNDo1OToyMlrOGqVANA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yOVQxNToyNjo0OVrOGqWNzw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAzNzQ5Mg==", "bodyText": "Remind me whether this is support for write at offset or append only at the end.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r447037492", "createdAt": "2020-06-29T14:59:22Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,440 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsAppend() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA0MzI3OA==", "bodyText": "Is spotbugs really complaining about this? We don't do it in other parts of the code and it doesn't complain, why does it complain here, do you know?", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r447043278", "createdAt": "2020-06-29T15:07:16Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,440 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    /**\n+     * Opens storage object for Read.\n+     *\n+     * @param chunkName String name of the storage object to read from.\n+     * @return ChunkHandle A readable handle for the given chunk.\n+     * @throws ChunkStorageException    Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    /**\n+     * Opens storage object for Write (or modifications).\n+     *\n+     * @param chunkName String name of the storage object to write to or modify.\n+     * @return ChunkHandle A writable handle for the given chunk.\n+     * @throws ChunkStorageException    Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    /**\n+     * Reads a range of bytes from the underlying storage object.\n+     *\n+     * @param handle       ChunkHandle of the storage object to read from.\n+     * @param fromOffset   Offset in the file from which to start reading.\n+     * @param length       Number of bytes to read.\n+     * @param buffer       Byte buffer to which data is copied.\n+     * @param bufferOffset Offset in the buffer at which to start copying read data.\n+     * @return int Number of bytes read.\n+     * @throws ChunkStorageException     Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException  If argument is invalid.\n+     * @throws NullPointerException      If the parameter is null.\n+     * @throws IndexOutOfBoundsException If the index is out of bounds.\n+     */\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    /**\n+     * Writes the given data to the underlying storage object.\n+     *\n+     * @param handle ChunkHandle of the storage object to write to.\n+     * @param offset Offset in the file to start writing.\n+     * @param length Number of bytes to write.\n+     * @param data   An InputStream representing the data to write.\n+     * @return int Number of bytes written.\n+     * @throws ChunkStorageException     Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws IndexOutOfBoundsException Throws IndexOutOfBoundsException in case of invalid index.\n+     */\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+            client.putObject(this.config.getBucket(), this.config.getPrefix() + handle.getChunkName(),\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return 0;\n+    }\n+\n+    /**\n+     * Concatenates two or more chunks using storage native  functionality. (Eg. Multipart upload.)\n+     *\n+     * @param chunks Array of ConcatArgument objects containing info about existing chunks to be appended together.\n+     *               The chunks are appended in the same sequence the names are provided.\n+     * @return int Number of bytes concatenated.\n+     * @throws ChunkStorageException         Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws UnsupportedOperationException If this operation is not supported by this provider.\n+     */\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        int totalBytesConcated = 0;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            String targetPath = config.getPrefix() + chunks[0].getName();\n+            String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new FileNotFoundException(chunks[0].getName());\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName());\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName(),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                client.deleteObject(config.getBucket(), config.getPrefix() + chunks[i].getName());\n+            }\n+        } catch (RuntimeException e) {\n+            throw e; // make spotbugs happy", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 256}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA0NTkxMA==", "bodyText": "Doesn't FileChannel have a truncation operation?", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r447045910", "createdAt": "2020-06-29T15:10:56Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,431 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Timer;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA0ODU2OA==", "bodyText": "In general, the value I see in javadocs for overridden methods is the additional information about the specifics of the implementation. Otherwise, just copying from parent class or interface doesn't add much. I'm in favor of adding comments to reflect what is relevant for the particular implementation.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r447048568", "createdAt": "2020-06-29T15:14:41Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,431 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Timer;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        try {\n+            return FileChannel.open(path, openOption);\n+        } catch (NoSuchFileException e) {\n+            throw new FileNotFoundException(path.toString());\n+        }\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        try {\n+            return Files.size(path);\n+        } catch (NoSuchFileException e) {\n+            throw new FileNotFoundException(path.toString());\n+        }\n+    }\n+\n+    /**\n+     * Retrieves the ChunkInfo for given name.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1MDQ5OA==", "bodyText": "I also thought that HDFS had a truncate operation, is there a reason for us to say that it doesn't support truncate?\nhttps://hadoop.apache.org/docs/current/hadoop-project-dist/hadoop-common/FileSystemShell.html#truncate", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r447050498", "createdAt": "2020-06-29T15:17:17Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,473 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1MTIxOA==", "bodyText": "Can we import rather than have the full reference?", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r447051218", "createdAt": "2020-06-29T15:18:17Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,473 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem: {}.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Retrieves the ChunkInfo for given name.\n+     *\n+     * @param chunkName String name of the storage object to read from.\n+     * @return ChunkInfo Information about the given chunk.\n+     * @throws ChunkStorageException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus last = fileSystem.getFileStatus(getFilePath(chunkName));\n+            ChunkInfo result = ChunkInfo.builder().name(chunkName).length(last.getLen()).build();\n+            return result;\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * Creates a new file.\n+     *\n+     * @param chunkName String name of the storage object to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * @throws ChunkStorageException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (org.apache.hadoop.fs.FileAlreadyExistsException e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1MTczNg==", "bodyText": "Remove new line.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r447051736", "createdAt": "2020-06-29T15:19:00Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,473 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem: {}.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Retrieves the ChunkInfo for given name.\n+     *\n+     * @param chunkName String name of the storage object to read from.\n+     * @return ChunkInfo Information about the given chunk.\n+     * @throws ChunkStorageException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus last = fileSystem.getFileStatus(getFilePath(chunkName));\n+            ChunkInfo result = ChunkInfo.builder().name(chunkName).length(last.getLen()).build();\n+            return result;\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * Creates a new file.\n+     *\n+     * @param chunkName String name of the storage object to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * @throws ChunkStorageException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (org.apache.hadoop.fs.FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doCreate\", e);\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the storage object to check.\n+     * @return True if the object exists, false otherwise.\n+     * @throws ChunkStorageException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        FileStatus status = null;\n+        try {\n+            status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            // HDFS could not find the file. Returning false.\n+            log.warn(\"Got exception checking if file exists\", e);\n+        }\n+        boolean exists = status != null;\n+        return exists;\n+    }\n+\n+    /**\n+     * Deletes a file.\n+     *\n+     * @param handle ChunkHandle of the storage object to delete.\n+     * @return True if the object was deleted, false otherwise.\n+     * @throws ChunkStorageException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            boolean retValue = this.fileSystem.delete(getFilePath(handle.getChunkName()), true);\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    private void throwException(String chunkName, String message, IOException e) throws ChunkStorageException {\n+        if (e instanceof FileNotFoundException) {\n+            throw new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        throw new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    /**\n+     * Opens storage object for Read.\n+     *\n+     * @param chunkName String name of the storage object to read from.\n+     * @return ChunkHandle A readable handle for the given chunk.\n+     * @throws ChunkStorageException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doOpenRead\", e);\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    /**\n+     * Opens storage object for Write (or modifications).\n+     *\n+     * @param chunkName String name of the storage object to write to or modify.\n+     * @return ChunkHandle A writable handle for the given chunk.\n+     * @throws ChunkStorageException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doOpenWrite\", e);\n+        }\n+        return null;\n+    }\n+\n+    /**\n+     * Reads a range of bytes from the underlying storage object.\n+     *\n+     * @param handle       ChunkHandle of the storage object to read from.\n+     * @param fromOffset   Offset in the file from which to start reading.\n+     * @param length       Number of bytes to read.\n+     * @param buffer       Byte buffer to which data is copied.\n+     * @param bufferOffset Offset in the buffer at which to start copying read data.\n+     * @return int Number of bytes read.\n+     * @throws ChunkStorageException Throws IOException in case of I/O related exceptions.\n+     * @throws IllegalArgumentException  If argument is invalid.\n+     * @throws NullPointerException      If the parameter is null.\n+     * @throws IndexOutOfBoundsException If the index is out of bounds.\n+     */\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 283}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1NTQxMg==", "bodyText": "This comment needs to change similarly to the comment in the previous PR, it should be 2^61 - 1 instead of 2^62. I'm actually wondering why I'm seeing this here, is it because this PR includes changes of Part 1?", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r447055412", "createdAt": "2020-06-29T15:24:11Z", "author": {"login": "fpj"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/SegmentRollingPolicy.java", "diffHunk": "@@ -16,7 +16,10 @@\n  * A generic rolling policy that can be applied to any Storage unit.\n  */\n public final class SegmentRollingPolicy {\n-    public static final SegmentRollingPolicy NO_ROLLING = new SegmentRollingPolicy(Long.MAX_VALUE);\n+    /**\n+     * Max rolling length is 2^62 so that we can use CompactLong in serialization everywhere.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1NzM1OQ==", "bodyText": "I actually confirmed that this is the case, that the changes of Part 1 are also here. What's the plan exactly? Do you expect git to automatically sort this out when Part 1 is merged?", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r447057359", "createdAt": "2020-06-29T15:26:49Z", "author": {"login": "fpj"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/SegmentRollingPolicy.java", "diffHunk": "@@ -16,7 +16,10 @@\n  * A generic rolling policy that can be applied to any Storage unit.\n  */\n public final class SegmentRollingPolicy {\n-    public static final SegmentRollingPolicy NO_ROLLING = new SegmentRollingPolicy(Long.MAX_VALUE);\n+    /**\n+     * Max rolling length is 2^62 so that we can use CompactLong in serialization everywhere.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1NTQxMg=="}, "originalCommit": null, "originalPosition": 6}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": null}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "000dba2d0d433b1ec4f45c6f8dc52517a2e6e614", "author": {"user": {"login": "sachin-j-joshi", "name": "Sachin Jayant Joshi"}}, "url": "https://github.com/pravega/pravega/commit/000dba2d0d433b1ec4f45c6f8dc52517a2e6e614", "committedDate": "2020-07-02T15:06:31Z", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 1 of 4) - Fix review comments.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "2915a6f096ffbf738c471619c5495ec5b98716a8", "author": {"user": {"login": "sachin-j-joshi", "name": "Sachin Jayant Joshi"}}, "url": "https://github.com/pravega/pravega/commit/2915a6f096ffbf738c471619c5495ec5b98716a8", "committedDate": "2020-07-08T17:05:59Z", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Rename ChunkManager to ChunkedSegmentStorage.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ3MTI4NDIx", "url": "https://github.com/pravega/pravega/pull/4699#pullrequestreview-447128421", "createdAt": "2020-07-13T10:29:15Z", "commit": null, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 21, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxMDoyOToxNlrOGwirlw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxMTozNTo0N1rOGwkv1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU1MzA0Nw==", "bodyText": "Should we be using preconditions here instead? Also, don't we need to check the fromOffset with respect to the bufferOffset? Is fromOffset relative to buggerOffset?", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453553047", "createdAt": "2020-07-13T10:29:16Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU1NTE4OQ==", "bodyText": "Why is this throwing UnsupportedOperationException, where is this coming from?", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453555189", "createdAt": "2020-07-13T10:33:20Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+            client.putObject(this.config.getBucket(), this.config.getPrefix() + handle.getChunkName(),\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU1NjUzOA==", "bodyText": "It is a bit odd to throw FileNotFoundException as this is not about files, maybe have an exception that derives from IOException or ChunkStorageException. Would it be a problem to have both ObjectNotFoundException and FileNotFoundException from a catch perspective? Maybe they can both derive from a NotFoundException class?", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453556538", "createdAt": "2020-07-13T10:35:40Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+            client.putObject(this.config.getBucket(), this.config.getPrefix() + handle.getChunkName(),\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        int totalBytesConcated = 0;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            String targetPath = config.getPrefix() + chunks[0].getName();\n+            String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new FileNotFoundException(chunks[0].getName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU1OTcxNQ==", "bodyText": "We recently had some issues with MPUs in ECS, and here I see that we are always performing concatenation with MPU, could you remind me why this is the right approach?", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453559715", "createdAt": "2020-07-13T10:41:50Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+            client.putObject(this.config.getBucket(), this.config.getPrefix() + handle.getChunkName(),\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        int totalBytesConcated = 0;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            String targetPath = config.getPrefix() + chunks[0].getName();\n+            String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 155}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU2MTg3NA==", "bodyText": "I'm also wondering if this should be a pre-condition.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453561874", "createdAt": "2020-07-13T10:45:48Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+            client.putObject(this.config.getBucket(), this.config.getPrefix() + handle.getChunkName(),\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        int totalBytesConcated = 0;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            String targetPath = config.getPrefix() + chunks[0].getName();\n+            String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new FileNotFoundException(chunks[0].getName());\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName());\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName(),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                client.deleteObject(config.getBucket(), config.getPrefix() + chunks[i].getName());\n+            }\n+        } catch (RuntimeException e) {\n+            // Make spotbugs happy. Wants us to catch RuntimeException in a separate catch block.\n+            // Error message is REC_CATCH_EXCEPTION: Exception is caught when Exception is not thrown\n+            throwException(chunks[0].getName(), \"doConcat\", e);\n+        } catch (Exception e) {\n+            throwException(chunks[0].getName(), \"doConcat\", e);\n+        }\n+        return totalBytesConcated;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws ChunkStorageException, UnsupportedOperationException {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException, UnsupportedOperationException {\n+        try {\n+            setPermission(handle, isReadOnly ? Permission.READ : Permission.FULL_CONTROL);\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+        return true;\n+    }\n+\n+    private void setPermission(ChunkHandle handle, Permission permission) {\n+        AccessControlList acl = client.getObjectAcl(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+        acl.getGrants().clear();\n+        acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), permission));\n+\n+        client.setObjectAcl(\n+                new SetObjectAclRequest(config.getBucket(), config.getPrefix() + handle.getChunkName()).withAcl(acl));\n+    }\n+\n+    private <T> T throwException(String chunkName, String message, Exception e) throws ChunkStorageException {\n+        if (e instanceof S3Exception) {\n+            S3Exception s3Exception = (S3Exception) e;\n+            String errorCode = Strings.nullToEmpty(s3Exception.getErrorCode());\n+\n+            if (errorCode.equals(\"NoSuchKey\")) {\n+                throw new ChunkNotFoundException(chunkName, message);\n+            }\n+\n+            if (errorCode.equals(\"PreconditionFailed\")) {\n+                throw new ChunkAlreadyExistsException(chunkName, message);\n+            }\n+\n+            if (errorCode.equals(\"InvalidRange\")\n+                    || errorCode.equals(\"InvalidArgument\")\n+                    || errorCode.equals(\"MethodNotAllowed\")\n+                    || s3Exception.getHttpCode() == HttpStatus.SC_REQUESTED_RANGE_NOT_SATISFIABLE) {\n+                throw new IllegalArgumentException(chunkName, e);\n+            }\n+\n+            if (errorCode.equals(\"AccessDenied\")) {\n+                throw new ChunkStorageException(chunkName, String.format(\"Access denied for chunk %s - %s.\", chunkName, message));\n+            }\n+        }\n+\n+        if (e instanceof IndexOutOfBoundsException) {\n+            throw new ArrayIndexOutOfBoundsException(e.getMessage());\n+        }\n+\n+        throw Exceptions.sneakyThrow(e);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(),\n+                    config.getPrefix() + chunkName);\n+\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(result.getContentLength())\n+                    .build();\n+\n+            return information;\n+        } catch (Exception e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            if (!client.listObjects(config.getBucket(), config.getPrefix() + chunkName).getObjects().isEmpty()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 277}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU2MjUwOQ==", "bodyText": "This one is breaking the pattern of catching exception and call throwException. Could we do it differently to follow the same pattern?", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453562509", "createdAt": "2020-07-13T10:46:54Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+            client.putObject(this.config.getBucket(), this.config.getPrefix() + handle.getChunkName(),\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        int totalBytesConcated = 0;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            String targetPath = config.getPrefix() + chunks[0].getName();\n+            String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new FileNotFoundException(chunks[0].getName());\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName());\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName(),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                client.deleteObject(config.getBucket(), config.getPrefix() + chunks[i].getName());\n+            }\n+        } catch (RuntimeException e) {\n+            // Make spotbugs happy. Wants us to catch RuntimeException in a separate catch block.\n+            // Error message is REC_CATCH_EXCEPTION: Exception is caught when Exception is not thrown\n+            throwException(chunks[0].getName(), \"doConcat\", e);\n+        } catch (Exception e) {\n+            throwException(chunks[0].getName(), \"doConcat\", e);\n+        }\n+        return totalBytesConcated;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws ChunkStorageException, UnsupportedOperationException {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException, UnsupportedOperationException {\n+        try {\n+            setPermission(handle, isReadOnly ? Permission.READ : Permission.FULL_CONTROL);\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+        return true;\n+    }\n+\n+    private void setPermission(ChunkHandle handle, Permission permission) {\n+        AccessControlList acl = client.getObjectAcl(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+        acl.getGrants().clear();\n+        acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), permission));\n+\n+        client.setObjectAcl(\n+                new SetObjectAclRequest(config.getBucket(), config.getPrefix() + handle.getChunkName()).withAcl(acl));\n+    }\n+\n+    private <T> T throwException(String chunkName, String message, Exception e) throws ChunkStorageException {\n+        if (e instanceof S3Exception) {\n+            S3Exception s3Exception = (S3Exception) e;\n+            String errorCode = Strings.nullToEmpty(s3Exception.getErrorCode());\n+\n+            if (errorCode.equals(\"NoSuchKey\")) {\n+                throw new ChunkNotFoundException(chunkName, message);\n+            }\n+\n+            if (errorCode.equals(\"PreconditionFailed\")) {\n+                throw new ChunkAlreadyExistsException(chunkName, message);\n+            }\n+\n+            if (errorCode.equals(\"InvalidRange\")\n+                    || errorCode.equals(\"InvalidArgument\")\n+                    || errorCode.equals(\"MethodNotAllowed\")\n+                    || s3Exception.getHttpCode() == HttpStatus.SC_REQUESTED_RANGE_NOT_SATISFIABLE) {\n+                throw new IllegalArgumentException(chunkName, e);\n+            }\n+\n+            if (errorCode.equals(\"AccessDenied\")) {\n+                throw new ChunkStorageException(chunkName, String.format(\"Access denied for chunk %s - %s.\", chunkName, message));\n+            }\n+        }\n+\n+        if (e instanceof IndexOutOfBoundsException) {\n+            throw new ArrayIndexOutOfBoundsException(e.getMessage());\n+        }\n+\n+        throw Exceptions.sneakyThrow(e);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(),\n+                    config.getPrefix() + chunkName);\n+\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(result.getContentLength())\n+                    .build();\n+\n+            return information;\n+        } catch (Exception e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            if (!client.listObjects(config.getBucket(), config.getPrefix() + chunkName).getObjects().isEmpty()) {\n+                throw new ChunkAlreadyExistsException(chunkName, \"Chunk already exists\");\n+            }\n+\n+            S3ObjectMetadata metadata = new S3ObjectMetadata();\n+            metadata.setContentLength((long) 0);\n+\n+            PutObjectRequest request = new PutObjectRequest(config.getBucket(), config.getPrefix() + chunkName, null);\n+\n+            AccessControlList acl = new AccessControlList();\n+            acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), Permission.FULL_CONTROL));\n+            request.setAcl(acl);\n+\n+            if (config.isUseNoneMatch()) {\n+                request.setIfNoneMatch(\"*\");\n+            }\n+            client.putObject(request);\n+\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (Exception e) {\n+            throwException(chunkName, \"doCreate\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(),\n+                    config.getPrefix() + chunkName);\n+            return true;\n+        } catch (S3Exception e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 308}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU2NDg4Nw==", "bodyText": "I see that you are trying to make this uniform across chunk storage implementations. One way maybe is to have a \"NotFoundException\" and derive one subtype for each chunk storage implementation?", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453564887", "createdAt": "2020-07-13T10:51:31Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,317 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Timer;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        try {\n+            return FileChannel.open(path, openOption);\n+        } catch (NoSuchFileException e) {\n+            throw new FileNotFoundException(path.toString());", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU2ODAxNw==", "bodyText": "Should these two be  in the inverted order? The same question holds for other before() and after() calls below.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453568017", "createdAt": "2020-07-13T10:57:51Z", "author": {"login": "fpj"}, "path": "bindings/src/test/java/io/pravega/storage/hdfs/HDFSSimpleStorageTest.java", "diffHunk": "@@ -0,0 +1,177 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkedRollingStorageTests;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageTests;\n+import io.pravega.segmentstore.storage.chunklayer.SimpleStorageTests;\n+import io.pravega.segmentstore.storage.chunklayer.SystemJournalTests;\n+import lombok.Getter;\n+import org.apache.hadoop.hdfs.MiniDFSCluster;\n+import org.junit.After;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.util.concurrent.Executor;\n+\n+import static org.junit.Assert.assertEquals;\n+\n+/***\n+ * Unit tests for {@link HDFSChunkStorage} based {@link io.pravega.segmentstore.storage.Storage}.\n+ */\n+public class HDFSSimpleStorageTest extends SimpleStorageTests {\n+    @Rule\n+    public Timeout globalTimeout = Timeout.seconds(TIMEOUT.getSeconds());\n+    private TestContext testContext = new TestContext();\n+\n+    @Before\n+    public void before() throws Exception {\n+        super.before();\n+        testContext.setUp();\n+    }\n+\n+    @After\n+    public void after() throws Exception {\n+        testContext.tearDown();\n+        super.after();\n+    }\n+\n+    protected ChunkStorage getChunkStorage()  throws Exception {\n+        return testContext.getChunkStorage();\n+    }\n+\n+    /**\n+     * {@link ChunkedRollingStorageTests} tests for {@link HDFSChunkStorage} based {@link io.pravega.segmentstore.storage.Storage}.\n+     */\n+    public static class HDFSRollingTests extends ChunkedRollingStorageTests {\n+        @Rule\n+        public Timeout globalTimeout = Timeout.seconds(TIMEOUT.getSeconds());\n+        private TestContext testContext = new TestContext();\n+\n+        @Before\n+        public void before() throws Exception {\n+            super.before();\n+            testContext.setUp();\n+        }\n+\n+        @After\n+        public void after() throws Exception {\n+            testContext.tearDown();\n+            super.after();\n+        }\n+\n+        protected ChunkStorage getChunkStorage(Executor executor)  throws Exception {\n+            return testContext.getChunkStorage();\n+        }\n+    }\n+\n+    /**\n+     * {@link ChunkStorageTests} tests for {@link HDFSChunkStorage} based {@link io.pravega.segmentstore.storage.Storage}.\n+     */\n+    public static class HDFSChunkStorageTests extends ChunkStorageTests {\n+        @Rule\n+        public Timeout globalTimeout = Timeout.seconds(TIMEOUT.getSeconds());\n+        private TestContext testContext = new TestContext();\n+\n+        @Before\n+        public void before() throws Exception {\n+            testContext.setUp();\n+            super.before();", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU2OTE4NQ==", "bodyText": "I'm not sure I understand the purpose of this mock, there is only a single test case it is serving, doReadTest.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453569185", "createdAt": "2020-07-13T11:00:16Z", "author": {"login": "fpj"}, "path": "bindings/src/test/java/io/pravega/storage/filesystem/FileSystemChunkStorageMockTest.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import lombok.Getter;\n+import lombok.Setter;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+import org.mockito.ArgumentCaptor;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.lang.reflect.Field;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.spi.AbstractInterruptibleChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.time.Duration;\n+import java.util.List;\n+import java.util.concurrent.Executor;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.*;\n+import static org.mockito.Mockito.mock;\n+\n+/**\n+ * Unit tests for {@link FileSystemChunkStorage} that uses mocks.\n+ */\n+public class FileSystemChunkStorageMockTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU3NDMzMQ==", "bodyText": "Why is this necessary?", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453574331", "createdAt": "2020-07-13T11:10:47Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,317 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Timer;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        try {\n+            return FileChannel.open(path, openOption);\n+        } catch (NoSuchFileException e) {\n+            throw new FileNotFoundException(path.toString());\n+        }\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        try {\n+            return Files.size(path);\n+        } catch (NoSuchFileException e) {\n+            throw new FileNotFoundException(path.toString());\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())\n+                    .build();\n+\n+            return information;\n+        } catch (IOException e) {\n+            throwExeption(chunkName, \"doGetInfo\", e);\n+        }\n+        throw new ChunkStorageException(chunkName, \"Unreachable code\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 127}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU3NDk4Mw==", "bodyText": "This doesn't really throw ChunkStorageException, does it?", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453574983", "createdAt": "2020-07-13T11:12:06Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,317 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Timer;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        try {\n+            return FileChannel.open(path, openOption);\n+        } catch (NoSuchFileException e) {\n+            throw new FileNotFoundException(path.toString());\n+        }\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        try {\n+            return Files.size(path);\n+        } catch (NoSuchFileException e) {\n+            throw new FileNotFoundException(path.toString());\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())\n+                    .build();\n+\n+            return information;\n+        } catch (IOException e) {\n+            throwExeption(chunkName, \"doGetInfo\", e);\n+        }\n+        throw new ChunkStorageException(chunkName, \"Unreachable code\");\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            FileAttribute<Set<PosixFilePermission>> fileAttributes = PosixFilePermissions.asFileAttribute(FileSystemUtils.READ_WRITE_PERMISSION);\n+\n+            Path path = Paths.get(config.getRoot(), chunkName);\n+            Path parent = path.getParent();\n+            assert parent != null;\n+            Files.createDirectories(parent);\n+            Files.createFile(path, fileAttributes);\n+\n+        } catch (IOException e) {\n+            throwExeption(chunkName, \"doCreate\", e);\n+        }\n+\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    public void throwExeption(String chunkName, String message, Exception e) throws ChunkStorageException {\n+        if (e instanceof FileNotFoundException || e instanceof NoSuchFileException) {\n+            throw new ChunkNotFoundException(chunkName, message);\n+        }\n+        if (e instanceof FileAlreadyExistsException) {\n+            throw new ChunkAlreadyExistsException(chunkName, message);\n+        }\n+        throw new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        return Files.exists(Paths.get(config.getRoot(), chunkName));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU3ODQ0Ng==", "bodyText": "The false parameter here indicates that the file metadata change is not flushed. Given that we are changing the size of the file, I'd think that the metadata flush is necessary. What do you think?", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453578446", "createdAt": "2020-07-13T11:18:55Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,317 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Timer;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        try {\n+            return FileChannel.open(path, openOption);\n+        } catch (NoSuchFileException e) {\n+            throw new FileNotFoundException(path.toString());\n+        }\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        try {\n+            return Files.size(path);\n+        } catch (NoSuchFileException e) {\n+            throw new FileNotFoundException(path.toString());\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())\n+                    .build();\n+\n+            return information;\n+        } catch (IOException e) {\n+            throwExeption(chunkName, \"doGetInfo\", e);\n+        }\n+        throw new ChunkStorageException(chunkName, \"Unreachable code\");\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            FileAttribute<Set<PosixFilePermission>> fileAttributes = PosixFilePermissions.asFileAttribute(FileSystemUtils.READ_WRITE_PERMISSION);\n+\n+            Path path = Paths.get(config.getRoot(), chunkName);\n+            Path parent = path.getParent();\n+            assert parent != null;\n+            Files.createDirectories(parent);\n+            Files.createFile(path, fileAttributes);\n+\n+        } catch (IOException e) {\n+            throwExeption(chunkName, \"doCreate\", e);\n+        }\n+\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    public void throwExeption(String chunkName, String message, Exception e) throws ChunkStorageException {\n+        if (e instanceof FileNotFoundException || e instanceof NoSuchFileException) {\n+            throw new ChunkNotFoundException(chunkName, message);\n+        }\n+        if (e instanceof FileAlreadyExistsException) {\n+            throw new ChunkAlreadyExistsException(chunkName, message);\n+        }\n+        throw new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        return Files.exists(Paths.get(config.getRoot(), chunkName));\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            Files.delete(Paths.get(config.getRoot(), handle.getChunkName()));\n+        } catch (IOException e) {\n+            throwExeption(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenRead\");\n+        }\n+\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenWrite\");\n+        } else if (Files.isWritable(path)) {\n+            return ChunkHandle.writeHandle(chunkName);\n+        } else {\n+            return ChunkHandle.readHandle(chunkName);\n+        }\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        Timer timer = new Timer();\n+\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+        try {\n+            long fileSize = getFileSize(path);\n+            if (fileSize < fromOffset) {\n+                throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the \" +\n+                        \"current size of chunk (%d).\", fromOffset, fileSize));\n+            }\n+        } catch (IOException e) {\n+            throwExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.READ)) {\n+            int totalBytesRead = 0;\n+            long readOffset = fromOffset;\n+            do {\n+                ByteBuffer readBuffer = ByteBuffer.wrap(buffer, bufferOffset, length);\n+                int bytesRead = channel.read(readBuffer, readOffset);\n+                bufferOffset += bytesRead;\n+                totalBytesRead += bytesRead;\n+                length -= bytesRead;\n+                readOffset += bytesRead;\n+            } while (length != 0);\n+            return totalBytesRead;\n+        } catch (IOException e) {\n+            throwExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException {\n+        Timer timer = new Timer();\n+\n+        if (handle.isReadOnly()) {\n+            throw new IllegalArgumentException(\"Write called on a readonly handle of chunk \" + handle.getChunkName());\n+        }\n+\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+\n+        long totalBytesWritten = 0;\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.WRITE)) {\n+            long fileSize = channel.size();\n+            if (fileSize != offset) {\n+                throw new IndexOutOfBoundsException(String.format(\"fileSize (%d) did not match offset (%d) for chunk %s\", fileSize, offset, handle.getChunkName()));\n+            }\n+\n+            // Wrap the input data into a ReadableByteChannel, but do not close it. Doing so will result in closing\n+            // the underlying InputStream, which is not desirable if it is to be reused.\n+            ReadableByteChannel sourceChannel = Channels.newChannel(data);\n+            while (length != 0) {\n+                long bytesWritten = channel.transferFrom(sourceChannel, offset, length);\n+                assert bytesWritten > 0 : \"Unable to make any progress transferring data.\";\n+                offset += bytesWritten;\n+                totalBytesWritten += bytesWritten;\n+                length -= bytesWritten;\n+            }\n+            channel.force(false);\n+        } catch (IOException e) {\n+            throwExeption(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return (int) totalBytesWritten;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        try {\n+            int totalBytesConcated = 0;\n+            Path targetPath = Paths.get(config.getRoot(), chunks[0].getName());\n+            long offset = chunks[0].getLength();\n+\n+            for (int i = 1; i < chunks.length; i++) {\n+                val source = chunks[i];\n+                Preconditions.checkArgument(!chunks[0].getName().equals(source.getName()), \"target and source can not be same.\");\n+                Path sourcePath = Paths.get(config.getRoot(), source.getName());\n+                long length = chunks[i].getLength();\n+                Preconditions.checkState(offset <= getFileSize(targetPath));\n+                Preconditions.checkState(length <= getFileSize(sourcePath));\n+                try (FileChannel targetChannel = getFileChannel(targetPath, StandardOpenOption.WRITE);\n+                     RandomAccessFile sourceFile = new RandomAccessFile(String.valueOf(sourcePath), \"r\")) {\n+                    while (length > 0) {\n+                        long bytesTransferred = targetChannel.transferFrom(sourceFile.getChannel(), offset, length);\n+                        offset += bytesTransferred;\n+                        length -= bytesTransferred;\n+                    }\n+                    targetChannel.force(false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 284}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU3OTMxNQ==", "bodyText": "Why is it swallowing the exception? Maybe the file is there and the exception is due to something else, in which case the return value is incorrect.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453579315", "createdAt": "2020-07-13T11:20:31Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem: {}.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus last = fileSystem.getFileStatus(getFilePath(chunkName));\n+            ChunkInfo result = ChunkInfo.builder().name(chunkName).length(last.getLen()).build();\n+            return result;\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doCreate\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        FileStatus status = null;\n+        try {\n+            status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            // HDFS could not find the file. Returning false.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 155}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU4MDM1OA==", "bodyText": "Same comment about preconditions and checking fromOffset and bufferOffset. make sure that we are comprehensively checking everything we need to check.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453580358", "createdAt": "2020-07-13T11:22:43Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem: {}.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus last = fileSystem.getFileStatus(getFilePath(chunkName));\n+            ChunkInfo result = ChunkInfo.builder().name(chunkName).length(last.getLen()).build();\n+            return result;\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doCreate\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        FileStatus status = null;\n+        try {\n+            status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            // HDFS could not find the file. Returning false.\n+            log.warn(\"Got exception checking if file exists\", e);\n+        }\n+        boolean exists = status != null;\n+        return exists;\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            boolean retValue = this.fileSystem.delete(getFilePath(handle.getChunkName()), true);\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    private void throwException(String chunkName, String message, IOException e) throws ChunkStorageException {\n+        if (e instanceof FileNotFoundException) {\n+            throw new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        throw new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doOpenRead\", e);\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doOpenWrite\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0 || buffer.length < bufferOffset + length) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 207}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU4MTE2Nw==", "bodyText": "The {} is not needed.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453581167", "createdAt": "2020-07-13T11:24:25Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem: {}.\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU4MjE5Mg==", "bodyText": "it doesn't look like we need this boolean retValue declaration.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453582192", "createdAt": "2020-07-13T11:26:29Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem: {}.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus last = fileSystem.getFileStatus(getFilePath(chunkName));\n+            ChunkInfo result = ChunkInfo.builder().name(chunkName).length(last.getLen()).build();\n+            return result;\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doCreate\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        FileStatus status = null;\n+        try {\n+            status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            // HDFS could not find the file. Returning false.\n+            log.warn(\"Got exception checking if file exists\", e);\n+        }\n+        boolean exists = status != null;\n+        return exists;\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            boolean retValue = this.fileSystem.delete(getFilePath(handle.getChunkName()), true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU4MjQ5Mg==", "bodyText": "This status is not used anywhere.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453582492", "createdAt": "2020-07-13T11:27:09Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem: {}.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus last = fileSystem.getFileStatus(getFilePath(chunkName));\n+            ChunkInfo result = ChunkInfo.builder().name(chunkName).length(last.getLen()).build();\n+            return result;\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doCreate\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        FileStatus status = null;\n+        try {\n+            status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            // HDFS could not find the file. Returning false.\n+            log.warn(\"Got exception checking if file exists\", e);\n+        }\n+        boolean exists = status != null;\n+        return exists;\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            boolean retValue = this.fileSystem.delete(getFilePath(handle.getChunkName()), true);\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    private void throwException(String chunkName, String message, IOException e) throws ChunkStorageException {\n+        if (e instanceof FileNotFoundException) {\n+            throw new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        throw new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU4MjY1Nw==", "bodyText": "This status isn't used anywhere.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453582657", "createdAt": "2020-07-13T11:27:31Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem: {}.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus last = fileSystem.getFileStatus(getFilePath(chunkName));\n+            ChunkInfo result = ChunkInfo.builder().name(chunkName).length(last.getLen()).build();\n+            return result;\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doCreate\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        FileStatus status = null;\n+        try {\n+            status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            // HDFS could not find the file. Returning false.\n+            log.warn(\"Got exception checking if file exists\", e);\n+        }\n+        boolean exists = status != null;\n+        return exists;\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            boolean retValue = this.fileSystem.delete(getFilePath(handle.getChunkName()), true);\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    private void throwException(String chunkName, String message, IOException e) throws ChunkStorageException {\n+        if (e instanceof FileNotFoundException) {\n+            throw new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        throw new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doOpenRead\", e);\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 194}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU4NDI5Mw==", "bodyText": "Fix this comment as the last part does not read correctly.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453584293", "createdAt": "2020-07-13T11:30:42Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem: {}.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus last = fileSystem.getFileStatus(getFilePath(chunkName));\n+            ChunkInfo result = ChunkInfo.builder().name(chunkName).length(last.getLen()).build();\n+            return result;\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doCreate\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        FileStatus status = null;\n+        try {\n+            status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            // HDFS could not find the file. Returning false.\n+            log.warn(\"Got exception checking if file exists\", e);\n+        }\n+        boolean exists = status != null;\n+        return exists;\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            boolean retValue = this.fileSystem.delete(getFilePath(handle.getChunkName()), true);\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    private void throwException(String chunkName, String message, IOException e) throws ChunkStorageException {\n+        if (e instanceof FileNotFoundException) {\n+            throw new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        throw new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doOpenRead\", e);\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doOpenWrite\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0 || buffer.length < bufferOffset + length) {\n+                throw new ArrayIndexOutOfBoundsException(String.format(\n+                        \"Offset (%s) must be non-negative, and bufferOffset (%s) and length (%s) must be valid indices into buffer of size %s.\",\n+                        fromOffset, bufferOffset, length, buffer.length));\n+            }\n+\n+            int totalBytesRead = readInternal(handle, buffer, fromOffset, bufferOffset, length);\n+            return totalBytesRead;\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try (FSDataOutputStream stream = this.fileSystem.append(getFilePath(handle.getChunkName()))) {\n+            if (stream.getPos() != offset) {\n+                // Looks like the filesystem changed from underneath us. This could be our bug, but it could be something else.\n+                throw new IndexOutOfBoundsException();\n+            }\n+\n+            if (length == 0) {\n+                // Note: IOUtils.copyBytes with length == 0 will enter an infinite loop, hence the need for this check.\n+                return 0;\n+            }\n+\n+            // We need to be very careful with IOUtils.copyBytes. There are many overloads with very similar signatures.\n+            // There is a difference between (InputStream, OutputStream, int, boolean) and (InputStream, OutputStream, long, boolean),\n+            // in that the one with \"int\" uses the third arg as a buffer size, and the one with \"long\" uses it as the number\n+            // of bytes to copy.\n+            IOUtils.copyBytes(data, stream, (long) length, false);\n+\n+            stream.flush();\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        ensureInitializedAndNotClosed();\n+        int length = 0;\n+        try {\n+            val sources = new Path[chunks.length - 1];\n+            for (int i = 1; i < chunks.length; i++) {\n+                val chunkLength = chunks[i].getLength();\n+                this.fileSystem.truncate(getFilePath(chunks[i].getName()), chunkLength);\n+                sources[i - 1] = getFilePath(chunks[i].getName());\n+                length += chunkLength;\n+            }\n+            // Concat source file into target.\n+            this.fileSystem.concat(getFilePath(chunks[0].getName()), sources);\n+        } catch (IOException e) {\n+            throwException(chunks[0].getName(), \"doConcat\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws ChunkStorageException, UnsupportedOperationException {\n+        throw new UnsupportedOperationException(getClass().getName() + \" does not support chunk truncation.\");\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException, UnsupportedOperationException {\n+        try {\n+            this.fileSystem.setPermission(getFilePath(handle.getChunkName()), isReadOnly ? READONLY_PERMISSION : READWRITE_PERMISSION);\n+            return true;\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region Storage Implementation\n+\n+    @SneakyThrows(IOException.class)\n+    public void initialize() {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        Preconditions.checkState(this.fileSystem == null, \"HDFSStorage has already been initialized.\");\n+        Configuration conf = new Configuration();\n+        conf.set(\"fs.default.name\", this.config.getHdfsHostURL());\n+        conf.set(\"fs.default.fs\", this.config.getHdfsHostURL());\n+        conf.set(\"fs.hdfs.impl\", \"org.apache.hadoop.hdfs.DistributedFileSystem\");\n+\n+        // FileSystem has a bad habit of caching Clients/Instances based on target URI. We do not like this, since we", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 297}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU4NjAzNg==", "bodyText": "Maybe point to documentation about this so that anyone checking this code understands the impact of this choice. it would be particularly important to document the potential impact of setting this to true. A quick search reveals comments going both ways.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453586036", "createdAt": "2020-07-13T11:34:12Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem: {}.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus last = fileSystem.getFileStatus(getFilePath(chunkName));\n+            ChunkInfo result = ChunkInfo.builder().name(chunkName).length(last.getLen()).build();\n+            return result;\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doCreate\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        FileStatus status = null;\n+        try {\n+            status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            // HDFS could not find the file. Returning false.\n+            log.warn(\"Got exception checking if file exists\", e);\n+        }\n+        boolean exists = status != null;\n+        return exists;\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            boolean retValue = this.fileSystem.delete(getFilePath(handle.getChunkName()), true);\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    private void throwException(String chunkName, String message, IOException e) throws ChunkStorageException {\n+        if (e instanceof FileNotFoundException) {\n+            throw new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        throw new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doOpenRead\", e);\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doOpenWrite\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0 || buffer.length < bufferOffset + length) {\n+                throw new ArrayIndexOutOfBoundsException(String.format(\n+                        \"Offset (%s) must be non-negative, and bufferOffset (%s) and length (%s) must be valid indices into buffer of size %s.\",\n+                        fromOffset, bufferOffset, length, buffer.length));\n+            }\n+\n+            int totalBytesRead = readInternal(handle, buffer, fromOffset, bufferOffset, length);\n+            return totalBytesRead;\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try (FSDataOutputStream stream = this.fileSystem.append(getFilePath(handle.getChunkName()))) {\n+            if (stream.getPos() != offset) {\n+                // Looks like the filesystem changed from underneath us. This could be our bug, but it could be something else.\n+                throw new IndexOutOfBoundsException();\n+            }\n+\n+            if (length == 0) {\n+                // Note: IOUtils.copyBytes with length == 0 will enter an infinite loop, hence the need for this check.\n+                return 0;\n+            }\n+\n+            // We need to be very careful with IOUtils.copyBytes. There are many overloads with very similar signatures.\n+            // There is a difference between (InputStream, OutputStream, int, boolean) and (InputStream, OutputStream, long, boolean),\n+            // in that the one with \"int\" uses the third arg as a buffer size, and the one with \"long\" uses it as the number\n+            // of bytes to copy.\n+            IOUtils.copyBytes(data, stream, (long) length, false);\n+\n+            stream.flush();\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        ensureInitializedAndNotClosed();\n+        int length = 0;\n+        try {\n+            val sources = new Path[chunks.length - 1];\n+            for (int i = 1; i < chunks.length; i++) {\n+                val chunkLength = chunks[i].getLength();\n+                this.fileSystem.truncate(getFilePath(chunks[i].getName()), chunkLength);\n+                sources[i - 1] = getFilePath(chunks[i].getName());\n+                length += chunkLength;\n+            }\n+            // Concat source file into target.\n+            this.fileSystem.concat(getFilePath(chunks[0].getName()), sources);\n+        } catch (IOException e) {\n+            throwException(chunks[0].getName(), \"doConcat\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws ChunkStorageException, UnsupportedOperationException {\n+        throw new UnsupportedOperationException(getClass().getName() + \" does not support chunk truncation.\");\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException, UnsupportedOperationException {\n+        try {\n+            this.fileSystem.setPermission(getFilePath(handle.getChunkName()), isReadOnly ? READONLY_PERMISSION : READWRITE_PERMISSION);\n+            return true;\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region Storage Implementation\n+\n+    @SneakyThrows(IOException.class)\n+    public void initialize() {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        Preconditions.checkState(this.fileSystem == null, \"HDFSStorage has already been initialized.\");\n+        Configuration conf = new Configuration();\n+        conf.set(\"fs.default.name\", this.config.getHdfsHostURL());\n+        conf.set(\"fs.default.fs\", this.config.getHdfsHostURL());\n+        conf.set(\"fs.hdfs.impl\", \"org.apache.hadoop.hdfs.DistributedFileSystem\");\n+\n+        // FileSystem has a bad habit of caching Clients/Instances based on target URI. We do not like this, since we\n+        // want to own our implementation so that when we close it, we don't interfere with others.\n+        conf.set(\"fs.hdfs.impl.disable.cache\", \"true\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 299}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU4NjkwMA==", "bodyText": "Could we check this as a precondition?", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453586900", "createdAt": "2020-07-13T11:35:47Z", "author": {"login": "fpj"}, "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem: {}.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus last = fileSystem.getFileStatus(getFilePath(chunkName));\n+            ChunkInfo result = ChunkInfo.builder().name(chunkName).length(last.getLen()).build();\n+            return result;\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doCreate\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        FileStatus status = null;\n+        try {\n+            status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            // HDFS could not find the file. Returning false.\n+            log.warn(\"Got exception checking if file exists\", e);\n+        }\n+        boolean exists = status != null;\n+        return exists;\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            boolean retValue = this.fileSystem.delete(getFilePath(handle.getChunkName()), true);\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    private void throwException(String chunkName, String message, IOException e) throws ChunkStorageException {\n+        if (e instanceof FileNotFoundException) {\n+            throw new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        throw new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doOpenRead\", e);\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doOpenWrite\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0 || buffer.length < bufferOffset + length) {\n+                throw new ArrayIndexOutOfBoundsException(String.format(\n+                        \"Offset (%s) must be non-negative, and bufferOffset (%s) and length (%s) must be valid indices into buffer of size %s.\",\n+                        fromOffset, bufferOffset, length, buffer.length));\n+            }\n+\n+            int totalBytesRead = readInternal(handle, buffer, fromOffset, bufferOffset, length);\n+            return totalBytesRead;\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try (FSDataOutputStream stream = this.fileSystem.append(getFilePath(handle.getChunkName()))) {\n+            if (stream.getPos() != offset) {\n+                // Looks like the filesystem changed from underneath us. This could be our bug, but it could be something else.\n+                throw new IndexOutOfBoundsException();\n+            }\n+\n+            if (length == 0) {\n+                // Note: IOUtils.copyBytes with length == 0 will enter an infinite loop, hence the need for this check.\n+                return 0;\n+            }\n+\n+            // We need to be very careful with IOUtils.copyBytes. There are many overloads with very similar signatures.\n+            // There is a difference between (InputStream, OutputStream, int, boolean) and (InputStream, OutputStream, long, boolean),\n+            // in that the one with \"int\" uses the third arg as a buffer size, and the one with \"long\" uses it as the number\n+            // of bytes to copy.\n+            IOUtils.copyBytes(data, stream, (long) length, false);\n+\n+            stream.flush();\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        ensureInitializedAndNotClosed();\n+        int length = 0;\n+        try {\n+            val sources = new Path[chunks.length - 1];\n+            for (int i = 1; i < chunks.length; i++) {\n+                val chunkLength = chunks[i].getLength();\n+                this.fileSystem.truncate(getFilePath(chunks[i].getName()), chunkLength);\n+                sources[i - 1] = getFilePath(chunks[i].getName());\n+                length += chunkLength;\n+            }\n+            // Concat source file into target.\n+            this.fileSystem.concat(getFilePath(chunks[0].getName()), sources);\n+        } catch (IOException e) {\n+            throwException(chunks[0].getName(), \"doConcat\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws ChunkStorageException, UnsupportedOperationException {\n+        throw new UnsupportedOperationException(getClass().getName() + \" does not support chunk truncation.\");\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException, UnsupportedOperationException {\n+        try {\n+            this.fileSystem.setPermission(getFilePath(handle.getChunkName()), isReadOnly ? READONLY_PERMISSION : READWRITE_PERMISSION);\n+            return true;\n+        } catch (IOException e) {\n+            throwException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region Storage Implementation\n+\n+    @SneakyThrows(IOException.class)\n+    public void initialize() {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        Preconditions.checkState(this.fileSystem == null, \"HDFSStorage has already been initialized.\");\n+        Configuration conf = new Configuration();\n+        conf.set(\"fs.default.name\", this.config.getHdfsHostURL());\n+        conf.set(\"fs.default.fs\", this.config.getHdfsHostURL());\n+        conf.set(\"fs.hdfs.impl\", \"org.apache.hadoop.hdfs.DistributedFileSystem\");\n+\n+        // FileSystem has a bad habit of caching Clients/Instances based on target URI. We do not like this, since we\n+        // want to own our implementation so that when we close it, we don't interfere with others.\n+        conf.set(\"fs.hdfs.impl.disable.cache\", \"true\");\n+        if (!this.config.isReplaceDataNodesOnFailure()) {\n+            // Default is DEFAULT, so we only set this if we want it disabled.\n+            conf.set(\"dfs.client.block.write.replace-datanode-on-failure.policy\", \"NEVER\");\n+        }\n+\n+        this.fileSystem = openFileSystem(conf);\n+        log.info(\"Initialized (HDFSHost = '{}'\", this.config.getHdfsHostURL());\n+    }\n+\n+    private FileSystem openFileSystem(Configuration conf) throws IOException {\n+        return FileSystem.get(conf);\n+    }\n+    //endregion\n+\n+    //region Helpers\n+    private void ensureInitializedAndNotClosed() {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        Preconditions.checkState(this.fileSystem != null, \"HDFSStorage is not initialized.\");\n+    }\n+\n+    //endregion\n+\n+    //Region HDFS helper methods.\n+\n+    /**\n+     * Gets an HDFS-friendly path prefix for the given chunk name by pre-pending the HDFS root from the config.\n+     */\n+    private String getPathPrefix(String chunkName) {\n+        return this.config.getHdfsRoot() + Path.SEPARATOR + chunkName;\n+    }\n+\n+    /**\n+     * Gets the full HDFS Path to a file for the given chunk, startOffset and epoch.\n+     */\n+    private Path getFilePath(String chunkName) {\n+        Preconditions.checkState(chunkName != null && chunkName.length() > 0, \"chunkName must be non-null and non-empty\");\n+        return new Path(getPathPrefix(chunkName));\n+    }\n+\n+    /**\n+     * Determines whether the given FileStatus indicates the file is read-only.\n+     *\n+     * @param fs The FileStatus to check.\n+     * @return True or false.\n+     */\n+    private boolean isReadOnly(FileStatus fs) {\n+        return fs.getPermission().getUserAction() == FsAction.READ;\n+    }\n+\n+    private int readInternal(ChunkHandle handle, byte[] buffer, long offset, int bufferOffset, int length) throws IOException {\n+        //There is only one file per chunkName.\n+        try (FSDataInputStream stream = this.fileSystem.open(getFilePath(handle.getChunkName()))) {\n+            stream.readFully(offset, buffer, bufferOffset, length);\n+        } catch (EOFException e) {\n+            throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the current size of chunk.\", offset));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 354}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ3NDI1NTE3", "url": "https://github.com/pravega/pravega/pull/4699#pullrequestreview-447425517", "createdAt": "2020-07-13T16:48:47Z", "commit": null, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxNjo0ODo0OFrOGwxCIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QxNzowMzozMVrOGwxkig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc4ODE5Mw==", "bodyText": "It should be append only at end", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453788193", "createdAt": "2020-07-13T16:48:48Z", "author": {"login": "andreipaduroiu"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,440 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsAppend() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzAzNzQ5Mg=="}, "originalCommit": null, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc4OTA5Mw==", "bodyText": "You need to check bufferOffset and length and validate against buffer as well", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453789093", "createdAt": "2020-07-13T16:50:21Z", "author": {"login": "andreipaduroiu"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU1MzA0Nw=="}, "originalCommit": null, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5MDMxMg==", "bodyText": "This stealthy way of throwing exceptions can lead to bugs down the line. For example, the return 0 line below is unreachable, but the compiler does not know that.\nPlease rework this to include the throw keyword.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453790312", "createdAt": "2020-07-13T16:52:23Z", "author": {"login": "andreipaduroiu"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5MDM3Nw==", "bodyText": "Same below too.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453790377", "createdAt": "2020-07-13T16:52:30Z", "author": {"login": "andreipaduroiu"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5MDMxMg=="}, "originalCommit": null, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5MTExNQ==", "bodyText": "\"Concatenated\"", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453791115", "createdAt": "2020-07-13T16:53:39Z", "author": {"login": "andreipaduroiu"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+            client.putObject(this.config.getBucket(), this.config.getPrefix() + handle.getChunkName(),\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        int totalBytesConcated = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5MjE1Mw==", "bodyText": "I don't understand where this is coming from. Why are we not seeing this in other parts of the code? What specifically is triggering it?", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453792153", "createdAt": "2020-07-13T16:55:21Z", "author": {"login": "andreipaduroiu"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+            client.putObject(this.config.getBucket(), this.config.getPrefix() + handle.getChunkName(),\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        int totalBytesConcated = 0;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            String targetPath = config.getPrefix() + chunks[0].getName();\n+            String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new FileNotFoundException(chunks[0].getName());\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName());\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName(),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                client.deleteObject(config.getBucket(), config.getPrefix() + chunks[i].getName());\n+            }\n+        } catch (RuntimeException e) {\n+            // Make spotbugs happy. Wants us to catch RuntimeException in a separate catch block.", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 191}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5Mjc2Ng==", "bodyText": "I want to understand how we recover from a partially completed concat. What happens if the process crashes:\n\nIn the middle of the above loop (when we build the MPU).\nBetween the above loop and the below loop (cleanup).", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453792766", "createdAt": "2020-07-13T16:56:21Z", "author": {"login": "andreipaduroiu"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+            client.putObject(this.config.getBucket(), this.config.getPrefix() + handle.getChunkName(),\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        int totalBytesConcated = 0;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            String targetPath = config.getPrefix() + chunks[0].getName();\n+            String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new FileNotFoundException(chunks[0].getName());\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName());\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName(),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 184}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5MzMyOA==", "bodyText": "I think this can be done by rewriting this as return throwException(...)", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453793328", "createdAt": "2020-07-13T16:57:17Z", "author": {"login": "andreipaduroiu"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5MDMxMg=="}, "originalCommit": null, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5NDAwMw==", "bodyText": "This method is supposed to return a boolean, so I think that's why we have it that way.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453794003", "createdAt": "2020-07-13T16:58:27Z", "author": {"login": "andreipaduroiu"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+            client.putObject(this.config.getBucket(), this.config.getPrefix() + handle.getChunkName(),\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        int totalBytesConcated = 0;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            String targetPath = config.getPrefix() + chunks[0].getName();\n+            String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new FileNotFoundException(chunks[0].getName());\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName());\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName(),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                client.deleteObject(config.getBucket(), config.getPrefix() + chunks[i].getName());\n+            }\n+        } catch (RuntimeException e) {\n+            // Make spotbugs happy. Wants us to catch RuntimeException in a separate catch block.\n+            // Error message is REC_CATCH_EXCEPTION: Exception is caught when Exception is not thrown\n+            throwException(chunks[0].getName(), \"doConcat\", e);\n+        } catch (Exception e) {\n+            throwException(chunks[0].getName(), \"doConcat\", e);\n+        }\n+        return totalBytesConcated;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws ChunkStorageException, UnsupportedOperationException {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException, UnsupportedOperationException {\n+        try {\n+            setPermission(handle, isReadOnly ? Permission.READ : Permission.FULL_CONTROL);\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+        return true;\n+    }\n+\n+    private void setPermission(ChunkHandle handle, Permission permission) {\n+        AccessControlList acl = client.getObjectAcl(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+        acl.getGrants().clear();\n+        acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), permission));\n+\n+        client.setObjectAcl(\n+                new SetObjectAclRequest(config.getBucket(), config.getPrefix() + handle.getChunkName()).withAcl(acl));\n+    }\n+\n+    private <T> T throwException(String chunkName, String message, Exception e) throws ChunkStorageException {\n+        if (e instanceof S3Exception) {\n+            S3Exception s3Exception = (S3Exception) e;\n+            String errorCode = Strings.nullToEmpty(s3Exception.getErrorCode());\n+\n+            if (errorCode.equals(\"NoSuchKey\")) {\n+                throw new ChunkNotFoundException(chunkName, message);\n+            }\n+\n+            if (errorCode.equals(\"PreconditionFailed\")) {\n+                throw new ChunkAlreadyExistsException(chunkName, message);\n+            }\n+\n+            if (errorCode.equals(\"InvalidRange\")\n+                    || errorCode.equals(\"InvalidArgument\")\n+                    || errorCode.equals(\"MethodNotAllowed\")\n+                    || s3Exception.getHttpCode() == HttpStatus.SC_REQUESTED_RANGE_NOT_SATISFIABLE) {\n+                throw new IllegalArgumentException(chunkName, e);\n+            }\n+\n+            if (errorCode.equals(\"AccessDenied\")) {\n+                throw new ChunkStorageException(chunkName, String.format(\"Access denied for chunk %s - %s.\", chunkName, message));\n+            }\n+        }\n+\n+        if (e instanceof IndexOutOfBoundsException) {\n+            throw new ArrayIndexOutOfBoundsException(e.getMessage());\n+        }\n+\n+        throw Exceptions.sneakyThrow(e);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(),\n+                    config.getPrefix() + chunkName);\n+\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(result.getContentLength())\n+                    .build();\n+\n+            return information;\n+        } catch (Exception e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            if (!client.listObjects(config.getBucket(), config.getPrefix() + chunkName).getObjects().isEmpty()) {\n+                throw new ChunkAlreadyExistsException(chunkName, \"Chunk already exists\");\n+            }\n+\n+            S3ObjectMetadata metadata = new S3ObjectMetadata();\n+            metadata.setContentLength((long) 0);\n+\n+            PutObjectRequest request = new PutObjectRequest(config.getBucket(), config.getPrefix() + chunkName, null);\n+\n+            AccessControlList acl = new AccessControlList();\n+            acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), Permission.FULL_CONTROL));\n+            request.setAcl(acl);\n+\n+            if (config.isUseNoneMatch()) {\n+                request.setIfNoneMatch(\"*\");\n+            }\n+            client.putObject(request);\n+\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (Exception e) {\n+            throwException(chunkName, \"doCreate\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(),\n+                    config.getPrefix() + chunkName);\n+            return true;\n+        } catch (S3Exception e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU2MjUwOQ=="}, "originalCommit": null, "originalPosition": 308}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5NDI0Mw==", "bodyText": "Revert this and all the other files that have no-op changes.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453794243", "createdAt": "2020-07-13T16:58:49Z", "author": {"login": "andreipaduroiu"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3SegmentHandle.java", "diffHunk": "@@ -17,7 +17,6 @@\n     private final String segmentName;\n     private final boolean isReadOnly;\n \n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5NDM5MQ==", "bodyText": "Revert this file.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453794391", "createdAt": "2020-07-13T16:59:06Z", "author": {"login": "andreipaduroiu"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3Storage.java", "diffHunk": "@@ -107,7 +107,6 @@ public ExtendedS3Storage(S3Client client, ExtendedS3StorageConfig config) {\n \n     //endregion\n \n-", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5NDg4OA==", "bodyText": "Please see my comment in ExtendedS3ChunkStorage.java", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453794888", "createdAt": "2020-07-13T16:59:56Z", "author": {"login": "andreipaduroiu"}, "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,317 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Timer;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        try {\n+            return FileChannel.open(path, openOption);\n+        } catch (NoSuchFileException e) {\n+            throw new FileNotFoundException(path.toString());\n+        }\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        try {\n+            return Files.size(path);\n+        } catch (NoSuchFileException e) {\n+            throw new FileNotFoundException(path.toString());\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())\n+                    .build();\n+\n+            return information;\n+        } catch (IOException e) {\n+            throwExeption(chunkName, \"doGetInfo\", e);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5NTE1NA==", "bodyText": "It doesn't have to", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453795154", "createdAt": "2020-07-13T17:00:20Z", "author": {"login": "andreipaduroiu"}, "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,317 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Timer;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        try {\n+            return FileChannel.open(path, openOption);\n+        } catch (NoSuchFileException e) {\n+            throw new FileNotFoundException(path.toString());\n+        }\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        try {\n+            return Files.size(path);\n+        } catch (NoSuchFileException e) {\n+            throw new FileNotFoundException(path.toString());\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())\n+                    .build();\n+\n+            return information;\n+        } catch (IOException e) {\n+            throwExeption(chunkName, \"doGetInfo\", e);\n+        }\n+        throw new ChunkStorageException(chunkName, \"Unreachable code\");\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            FileAttribute<Set<PosixFilePermission>> fileAttributes = PosixFilePermissions.asFileAttribute(FileSystemUtils.READ_WRITE_PERMISSION);\n+\n+            Path path = Paths.get(config.getRoot(), chunkName);\n+            Path parent = path.getParent();\n+            assert parent != null;\n+            Files.createDirectories(parent);\n+            Files.createFile(path, fileAttributes);\n+\n+        } catch (IOException e) {\n+            throwExeption(chunkName, \"doCreate\", e);\n+        }\n+\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    public void throwExeption(String chunkName, String message, Exception e) throws ChunkStorageException {\n+        if (e instanceof FileNotFoundException || e instanceof NoSuchFileException) {\n+            throw new ChunkNotFoundException(chunkName, message);\n+        }\n+        if (e instanceof FileAlreadyExistsException) {\n+            throw new ChunkAlreadyExistsException(chunkName, message);\n+        }\n+        throw new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        return Files.exists(Paths.get(config.getRoot(), chunkName));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU3NDk4Mw=="}, "originalCommit": null, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5NTk2Ng==", "bodyText": "Javadoc for all these fields.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453795966", "createdAt": "2020-07-13T17:01:38Z", "author": {"login": "andreipaduroiu"}, "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemUtils.java", "diffHunk": "@@ -0,0 +1,31 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.collect.ImmutableSet;\n+\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.util.Set;\n+\n+/**\n+ * Utils for File System.\n+ */\n+public class FileSystemUtils {\n+    public static final Set<PosixFilePermission> READ_ONLY_PERMISSION = ImmutableSet.of(", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5NzAwMg==", "bodyText": "This code is copied from the existing HDFSStorage.java. However the code within the try block is also very different. Can we check the contracts for that method to see if it warrants this whole try-catch block in here too?", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r453797002", "createdAt": "2020-07-13T17:03:31Z", "author": {"login": "andreipaduroiu"}, "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,360 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem: {}.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus last = fileSystem.getFileStatus(getFilePath(chunkName));\n+            ChunkInfo result = ChunkInfo.builder().name(chunkName).length(last.getLen()).build();\n+            return result;\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doGetInfo\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throwException(chunkName, \"doCreate\", e);\n+        }\n+        return null;\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        FileStatus status = null;\n+        try {\n+            status = fileSystem.getFileStatus(getFilePath(chunkName));\n+        } catch (IOException e) {\n+            // HDFS could not find the file. Returning false.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU3OTMxNQ=="}, "originalCommit": null, "originalPosition": 155}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8e2a57e4543698481347fec8f5cc603c5e4e3777", "author": {"user": {"login": "sachin-j-joshi", "name": "Sachin Jayant Joshi"}}, "url": "https://github.com/pravega/pravega/commit/8e2a57e4543698481347fec8f5cc603c5e4e3777", "committedDate": "2020-07-13T23:13:16Z", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Implementation of ChunkStorage for HDFS, FileSystem and ExtendedS3.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": null, "afterCommit": {"oid": "8e2a57e4543698481347fec8f5cc603c5e4e3777", "author": {"user": {"login": "sachin-j-joshi", "name": "Sachin Jayant Joshi"}}, "url": "https://github.com/pravega/pravega/commit/8e2a57e4543698481347fec8f5cc603c5e4e3777", "committedDate": "2020-07-13T23:13:16Z", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Implementation of ChunkStorage for HDFS, FileSystem and ExtendedS3.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ3ODkzNDI3", "url": "https://github.com/pravega/pravega/pull/4699#pullrequestreview-447893427", "createdAt": "2020-07-14T08:23:21Z", "commit": {"oid": "8e2a57e4543698481347fec8f5cc603c5e4e3777"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQwODoyMzoyMlrOGxJdDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNFQwODoyMzoyMlrOGxJdDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDE4ODMwMQ==", "bodyText": "I see that we are concatenating very often config.getPrefix() + handle.getChunkName():\n\nit is better to have a common function to compose the final object name\nwe should validate the chunkName and check for invalid values for S3, probably sanitize it by replacing invalid characters\nwhat about adding some know separator between the prefix and the chunkName ? this way it won't be possible to mix data from two separate Pravega clusters that share the same S3 bucket. I know this is a remote possibility because \"prefix\" should be unique and checkName will be some kind of UUID, but adding a check does not cost very much and will make this safer", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454188301", "createdAt": "2020-07-14T08:23:22Z", "author": {"login": "eolivelli"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,325 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+            client.putObject(this.config.getBucket(), this.config.getPrefix() + handle.getChunkName(),\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        int totalBytesConcatenated = 0;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            String targetPath = config.getPrefix() + chunks[0].getName();\n+            String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new ChunkNotFoundException(chunks[0].getName(), \"Target segment does not exist\");\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName());\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName(),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcatenated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                client.deleteObject(config.getBucket(), config.getPrefix() + chunks[i].getName());\n+            }\n+        } catch (RuntimeException e) {\n+            // Make spotbugs happy. Wants us to catch RuntimeException in a separate catch block.\n+            // Error message is REC_CATCH_EXCEPTION: Exception is caught when Exception is not thrown\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } catch (Exception e) {\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        }\n+        return totalBytesConcatenated;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws UnsupportedOperationException {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException {\n+        try {\n+            setPermission(handle, isReadOnly ? Permission.READ : Permission.FULL_CONTROL);\n+            return true;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+    }\n+\n+    private void setPermission(ChunkHandle handle, Permission permission) {\n+        AccessControlList acl = client.getObjectAcl(config.getBucket(), config.getPrefix() + handle.getChunkName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8e2a57e4543698481347fec8f5cc603c5e4e3777"}, "originalPosition": 208}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dd27958f57c16d871c7bb1eb390817d5787a7c17", "author": {"user": {"login": "sachin-j-joshi", "name": "Sachin Jayant Joshi"}}, "url": "https://github.com/pravega/pravega/commit/dd27958f57c16d871c7bb1eb390817d5787a7c17", "committedDate": "2020-07-14T19:45:12Z", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Code cleanup.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7070fa7fd2549b9bcf3577e4260be3882218524d", "author": {"user": {"login": "sachin-j-joshi", "name": "Sachin Jayant Joshi"}}, "url": "https://github.com/pravega/pravega/commit/7070fa7fd2549b9bcf3577e4260be3882218524d", "committedDate": "2020-07-14T20:31:30Z", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Code cleanup.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ4NzcwMzcw", "url": "https://github.com/pravega/pravega/pull/4699#pullrequestreview-448770370", "createdAt": "2020-07-15T09:10:58Z", "commit": null, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwOToxMDo1OFrOGx1WVw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwOToxMDo1OFrOGx1WVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkwNzQ3OQ==", "bodyText": "Ok, this name confused me. if this is about testing the read logic in a given way, then name it accordingly. I thought you're defining a mock to be used across test cases, while it sounds like there are aspects of the read logic that require mocking, so the test is about read, not mocking.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r454907479", "createdAt": "2020-07-15T09:10:58Z", "author": {"login": "fpj"}, "path": "bindings/src/test/java/io/pravega/storage/filesystem/FileSystemChunkStorageMockTest.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import lombok.Getter;\n+import lombok.Setter;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+import org.mockito.ArgumentCaptor;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.lang.reflect.Field;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.spi.AbstractInterruptibleChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.time.Duration;\n+import java.util.List;\n+import java.util.concurrent.Executor;\n+\n+import static org.junit.Assert.assertEquals;\n+import static org.mockito.ArgumentMatchers.any;\n+import static org.mockito.Mockito.*;\n+import static org.mockito.Mockito.mock;\n+\n+/**\n+ * Unit tests for {@link FileSystemChunkStorage} that uses mocks.\n+ */\n+public class FileSystemChunkStorageMockTest {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzU2OTE4NQ=="}, "originalCommit": null, "originalPosition": 41}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9cb3101676aff678eb0bf589ddfbdcaa933a6847", "author": {"user": {"login": "sachin-j-joshi", "name": "Sachin Jayant Joshi"}}, "url": "https://github.com/pravega/pravega/commit/9cb3101676aff678eb0bf589ddfbdcaa933a6847", "committedDate": "2020-07-15T15:12:01Z", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Update comment.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ4OTk1ODgw", "url": "https://github.com/pravega/pravega/pull/4699#pullrequestreview-448995880", "createdAt": "2020-07-15T14:13:33Z", "commit": {"oid": "7070fa7fd2549b9bcf3577e4260be3882218524d"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxNDoxMzozM1rOGyAHfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxNDo1NDo1OVrOGyCHlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4MzkwMw==", "bodyText": "This can be a long process for a single operation. Irrespective of the time this operation can take, my question is: what happens if this operation fails within this loop? Do we leave chunks half concatenated? How do we complete this operation if it is interrupted in the middle of the processing (some chunks are concatenated, some others not yet)?", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455083903", "createdAt": "2020-07-15T14:13:33Z", "author": {"login": "RaulGracia"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,339 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.AbortMultipartUploadRequest;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    getObjectPath(handle.getChunkName()), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            val objectPath = getObjectPath(handle.getChunkName());\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), objectPath);\n+            client.putObject(this.config.getBucket(), objectPath,\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        int totalBytesConcatenated = 0;\n+        String targetPath = getObjectPath(chunks[0].getName());\n+        String uploadId = null;\n+        boolean isCompleted = false;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new ChunkNotFoundException(chunks[0].getName(), \"Target segment does not exist\");\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7070fa7fd2549b9bcf3577e4260be3882218524d"}, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4NzA4OA==", "bodyText": "Are we expecting false here if this process cannot complete? Otherwise, why to return anything at all? It can either execute normally or throw and do not return anything.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455087088", "createdAt": "2020-07-15T14:17:45Z", "author": {"login": "RaulGracia"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,339 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.AbortMultipartUploadRequest;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    getObjectPath(handle.getChunkName()), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            val objectPath = getObjectPath(handle.getChunkName());\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), objectPath);\n+            client.putObject(this.config.getBucket(), objectPath,\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        int totalBytesConcatenated = 0;\n+        String targetPath = getObjectPath(chunks[0].getName());\n+        String uploadId = null;\n+        boolean isCompleted = false;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new ChunkNotFoundException(chunks[0].getName(), \"Target segment does not exist\");\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            getObjectPath(sourceHandle.getName()));\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            getObjectPath(sourceHandle.getName()),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcatenated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            isCompleted = true;\n+\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                try {\n+                    client.deleteObject(config.getBucket(), getObjectPath(chunks[i].getName()));\n+                } catch (Exception e) {\n+                    log.warn(\"Could not delete source chunk - chunk={}.\", chunks[i].getName(), e);\n+                }\n+            }\n+        } catch (RuntimeException e) {\n+            // Make spotbugs happy. Wants us to catch RuntimeException in a separate catch block.\n+            // Error message is REC_CATCH_EXCEPTION: Exception is caught when Exception is not thrown\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } catch (Exception e) {\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } finally {\n+            if (!isCompleted && null != uploadId) {\n+                client.abortMultipartUpload(new AbortMultipartUploadRequest(config.getBucket(), targetPath, uploadId));\n+            }\n+        }\n+        return totalBytesConcatenated;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws UnsupportedOperationException {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException {\n+        try {\n+            setPermission(handle, isReadOnly ? Permission.READ : Permission.FULL_CONTROL);\n+            return true;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7070fa7fd2549b9bcf3577e4260be3882218524d"}, "originalPosition": 218}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4ODcyMA==", "bodyText": "If this \"NoSuchKey\" is something of the protocol, put it at least as a constant for readability", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455088720", "createdAt": "2020-07-15T14:19:55Z", "author": {"login": "RaulGracia"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,339 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.AbortMultipartUploadRequest;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    getObjectPath(handle.getChunkName()), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            val objectPath = getObjectPath(handle.getChunkName());\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), objectPath);\n+            client.putObject(this.config.getBucket(), objectPath,\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        int totalBytesConcatenated = 0;\n+        String targetPath = getObjectPath(chunks[0].getName());\n+        String uploadId = null;\n+        boolean isCompleted = false;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new ChunkNotFoundException(chunks[0].getName(), \"Target segment does not exist\");\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            getObjectPath(sourceHandle.getName()));\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            getObjectPath(sourceHandle.getName()),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcatenated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            isCompleted = true;\n+\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                try {\n+                    client.deleteObject(config.getBucket(), getObjectPath(chunks[i].getName()));\n+                } catch (Exception e) {\n+                    log.warn(\"Could not delete source chunk - chunk={}.\", chunks[i].getName(), e);\n+                }\n+            }\n+        } catch (RuntimeException e) {\n+            // Make spotbugs happy. Wants us to catch RuntimeException in a separate catch block.\n+            // Error message is REC_CATCH_EXCEPTION: Exception is caught when Exception is not thrown\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } catch (Exception e) {\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } finally {\n+            if (!isCompleted && null != uploadId) {\n+                client.abortMultipartUpload(new AbortMultipartUploadRequest(config.getBucket(), targetPath, uploadId));\n+            }\n+        }\n+        return totalBytesConcatenated;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws UnsupportedOperationException {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException {\n+        try {\n+            setPermission(handle, isReadOnly ? Permission.READ : Permission.FULL_CONTROL);\n+            return true;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+    }\n+\n+    private void setPermission(ChunkHandle handle, Permission permission) {\n+        AccessControlList acl = client.getObjectAcl(config.getBucket(), getObjectPath(handle.getChunkName()));\n+        acl.getGrants().clear();\n+        acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), permission));\n+\n+        client.setObjectAcl(\n+                new SetObjectAclRequest(config.getBucket(), getObjectPath(handle.getChunkName())).withAcl(acl));\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(),\n+                    getObjectPath(chunkName));\n+\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(result.getContentLength())\n+                    .build();\n+\n+            return information;\n+        } catch (Exception e) {\n+            throw convertException(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            if (!client.listObjects(config.getBucket(), getObjectPath(chunkName)).getObjects().isEmpty()) {\n+                throw new ChunkAlreadyExistsException(chunkName, \"Chunk already exists\");\n+            }\n+\n+            S3ObjectMetadata metadata = new S3ObjectMetadata();\n+            metadata.setContentLength((long) 0);\n+\n+            PutObjectRequest request = new PutObjectRequest(config.getBucket(), getObjectPath(chunkName), null);\n+\n+            AccessControlList acl = new AccessControlList();\n+            acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), Permission.FULL_CONTROL));\n+            request.setAcl(acl);\n+\n+            if (config.isUseNoneMatch()) {\n+                request.setIfNoneMatch(\"*\");\n+            }\n+            client.putObject(request);\n+\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (Exception e) {\n+            throw convertException(chunkName, \"doCreate\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            client.getObjectMetadata(config.getBucket(), getObjectPath(chunkName));\n+            return true;\n+        } catch (S3Exception e) {\n+            if (e.getErrorCode().equals(\"NoSuchKey\")) {\n+                return false;\n+            } else {\n+                throw convertException(chunkName, \"checkExists\", e);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            client.deleteObject(config.getBucket(), getObjectPath(handle.getChunkName()));\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    private ChunkStorageException convertException(String chunkName, String message, Exception e)  {\n+        ChunkStorageException retValue = null;\n+        if (e instanceof ChunkStorageException) {\n+            return (ChunkStorageException) e;\n+        }\n+        if (e instanceof S3Exception) {\n+            S3Exception s3Exception = (S3Exception) e;\n+            String errorCode = Strings.nullToEmpty(s3Exception.getErrorCode());\n+\n+            if (errorCode.equals(\"NoSuchKey\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7070fa7fd2549b9bcf3577e4260be3882218524d"}, "originalPosition": 306}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA4OTE4Mg==", "bodyText": "I don't know if we would need a place to set all these hardcoded strings for better readability.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455089182", "createdAt": "2020-07-15T14:20:32Z", "author": {"login": "RaulGracia"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,339 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.AbortMultipartUploadRequest;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    getObjectPath(handle.getChunkName()), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            val objectPath = getObjectPath(handle.getChunkName());\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), objectPath);\n+            client.putObject(this.config.getBucket(), objectPath,\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        int totalBytesConcatenated = 0;\n+        String targetPath = getObjectPath(chunks[0].getName());\n+        String uploadId = null;\n+        boolean isCompleted = false;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new ChunkNotFoundException(chunks[0].getName(), \"Target segment does not exist\");\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            getObjectPath(sourceHandle.getName()));\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            getObjectPath(sourceHandle.getName()),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcatenated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            isCompleted = true;\n+\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                try {\n+                    client.deleteObject(config.getBucket(), getObjectPath(chunks[i].getName()));\n+                } catch (Exception e) {\n+                    log.warn(\"Could not delete source chunk - chunk={}.\", chunks[i].getName(), e);\n+                }\n+            }\n+        } catch (RuntimeException e) {\n+            // Make spotbugs happy. Wants us to catch RuntimeException in a separate catch block.\n+            // Error message is REC_CATCH_EXCEPTION: Exception is caught when Exception is not thrown\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } catch (Exception e) {\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } finally {\n+            if (!isCompleted && null != uploadId) {\n+                client.abortMultipartUpload(new AbortMultipartUploadRequest(config.getBucket(), targetPath, uploadId));\n+            }\n+        }\n+        return totalBytesConcatenated;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws UnsupportedOperationException {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException {\n+        try {\n+            setPermission(handle, isReadOnly ? Permission.READ : Permission.FULL_CONTROL);\n+            return true;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+    }\n+\n+    private void setPermission(ChunkHandle handle, Permission permission) {\n+        AccessControlList acl = client.getObjectAcl(config.getBucket(), getObjectPath(handle.getChunkName()));\n+        acl.getGrants().clear();\n+        acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), permission));\n+\n+        client.setObjectAcl(\n+                new SetObjectAclRequest(config.getBucket(), getObjectPath(handle.getChunkName())).withAcl(acl));\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(),\n+                    getObjectPath(chunkName));\n+\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(result.getContentLength())\n+                    .build();\n+\n+            return information;\n+        } catch (Exception e) {\n+            throw convertException(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            if (!client.listObjects(config.getBucket(), getObjectPath(chunkName)).getObjects().isEmpty()) {\n+                throw new ChunkAlreadyExistsException(chunkName, \"Chunk already exists\");\n+            }\n+\n+            S3ObjectMetadata metadata = new S3ObjectMetadata();\n+            metadata.setContentLength((long) 0);\n+\n+            PutObjectRequest request = new PutObjectRequest(config.getBucket(), getObjectPath(chunkName), null);\n+\n+            AccessControlList acl = new AccessControlList();\n+            acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), Permission.FULL_CONTROL));\n+            request.setAcl(acl);\n+\n+            if (config.isUseNoneMatch()) {\n+                request.setIfNoneMatch(\"*\");\n+            }\n+            client.putObject(request);\n+\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (Exception e) {\n+            throw convertException(chunkName, \"doCreate\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            client.getObjectMetadata(config.getBucket(), getObjectPath(chunkName));\n+            return true;\n+        } catch (S3Exception e) {\n+            if (e.getErrorCode().equals(\"NoSuchKey\")) {\n+                return false;\n+            } else {\n+                throw convertException(chunkName, \"checkExists\", e);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            client.deleteObject(config.getBucket(), getObjectPath(handle.getChunkName()));\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    private ChunkStorageException convertException(String chunkName, String message, Exception e)  {\n+        ChunkStorageException retValue = null;\n+        if (e instanceof ChunkStorageException) {\n+            return (ChunkStorageException) e;\n+        }\n+        if (e instanceof S3Exception) {\n+            S3Exception s3Exception = (S3Exception) e;\n+            String errorCode = Strings.nullToEmpty(s3Exception.getErrorCode());\n+\n+            if (errorCode.equals(\"NoSuchKey\")) {\n+                retValue =  new ChunkNotFoundException(chunkName, message, e);\n+            }\n+\n+            if (errorCode.equals(\"PreconditionFailed\")) {\n+                retValue =  new ChunkAlreadyExistsException(chunkName, message, e);\n+            }\n+\n+            if (errorCode.equals(\"InvalidRange\")\n+                    || errorCode.equals(\"InvalidArgument\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7070fa7fd2549b9bcf3577e4260be3882218524d"}, "originalPosition": 315}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTA5NDQ3OA==", "bodyText": "This kind of string concatenations may be expensive if executed too often.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455094478", "createdAt": "2020-07-15T14:27:22Z", "author": {"login": "RaulGracia"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,339 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.AbortMultipartUploadRequest;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    getObjectPath(handle.getChunkName()), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            val objectPath = getObjectPath(handle.getChunkName());\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), objectPath);\n+            client.putObject(this.config.getBucket(), objectPath,\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        int totalBytesConcatenated = 0;\n+        String targetPath = getObjectPath(chunks[0].getName());\n+        String uploadId = null;\n+        boolean isCompleted = false;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new ChunkNotFoundException(chunks[0].getName(), \"Target segment does not exist\");\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            getObjectPath(sourceHandle.getName()));\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            getObjectPath(sourceHandle.getName()),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcatenated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            isCompleted = true;\n+\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                try {\n+                    client.deleteObject(config.getBucket(), getObjectPath(chunks[i].getName()));\n+                } catch (Exception e) {\n+                    log.warn(\"Could not delete source chunk - chunk={}.\", chunks[i].getName(), e);\n+                }\n+            }\n+        } catch (RuntimeException e) {\n+            // Make spotbugs happy. Wants us to catch RuntimeException in a separate catch block.\n+            // Error message is REC_CATCH_EXCEPTION: Exception is caught when Exception is not thrown\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } catch (Exception e) {\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        } finally {\n+            if (!isCompleted && null != uploadId) {\n+                client.abortMultipartUpload(new AbortMultipartUploadRequest(config.getBucket(), targetPath, uploadId));\n+            }\n+        }\n+        return totalBytesConcatenated;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws UnsupportedOperationException {\n+        throw new UnsupportedOperationException();\n+    }\n+\n+    @Override\n+    protected boolean doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException {\n+        try {\n+            setPermission(handle, isReadOnly ? Permission.READ : Permission.FULL_CONTROL);\n+            return true;\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+    }\n+\n+    private void setPermission(ChunkHandle handle, Permission permission) {\n+        AccessControlList acl = client.getObjectAcl(config.getBucket(), getObjectPath(handle.getChunkName()));\n+        acl.getGrants().clear();\n+        acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), permission));\n+\n+        client.setObjectAcl(\n+                new SetObjectAclRequest(config.getBucket(), getObjectPath(handle.getChunkName())).withAcl(acl));\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(),\n+                    getObjectPath(chunkName));\n+\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(result.getContentLength())\n+                    .build();\n+\n+            return information;\n+        } catch (Exception e) {\n+            throw convertException(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            if (!client.listObjects(config.getBucket(), getObjectPath(chunkName)).getObjects().isEmpty()) {\n+                throw new ChunkAlreadyExistsException(chunkName, \"Chunk already exists\");\n+            }\n+\n+            S3ObjectMetadata metadata = new S3ObjectMetadata();\n+            metadata.setContentLength((long) 0);\n+\n+            PutObjectRequest request = new PutObjectRequest(config.getBucket(), getObjectPath(chunkName), null);\n+\n+            AccessControlList acl = new AccessControlList();\n+            acl.addGrants(new Grant(new CanonicalUser(config.getAccessKey(), config.getAccessKey()), Permission.FULL_CONTROL));\n+            request.setAcl(acl);\n+\n+            if (config.isUseNoneMatch()) {\n+                request.setIfNoneMatch(\"*\");\n+            }\n+            client.putObject(request);\n+\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (Exception e) {\n+            throw convertException(chunkName, \"doCreate\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            client.getObjectMetadata(config.getBucket(), getObjectPath(chunkName));\n+            return true;\n+        } catch (S3Exception e) {\n+            if (e.getErrorCode().equals(\"NoSuchKey\")) {\n+                return false;\n+            } else {\n+                throw convertException(chunkName, \"checkExists\", e);\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            client.deleteObject(config.getBucket(), getObjectPath(handle.getChunkName()));\n+        } catch (Exception e) {\n+            throw convertException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    private ChunkStorageException convertException(String chunkName, String message, Exception e)  {\n+        ChunkStorageException retValue = null;\n+        if (e instanceof ChunkStorageException) {\n+            return (ChunkStorageException) e;\n+        }\n+        if (e instanceof S3Exception) {\n+            S3Exception s3Exception = (S3Exception) e;\n+            String errorCode = Strings.nullToEmpty(s3Exception.getErrorCode());\n+\n+            if (errorCode.equals(\"NoSuchKey\")) {\n+                retValue =  new ChunkNotFoundException(chunkName, message, e);\n+            }\n+\n+            if (errorCode.equals(\"PreconditionFailed\")) {\n+                retValue =  new ChunkAlreadyExistsException(chunkName, message, e);\n+            }\n+\n+            if (errorCode.equals(\"InvalidRange\")\n+                    || errorCode.equals(\"InvalidArgument\")\n+                    || errorCode.equals(\"MethodNotAllowed\")\n+                    || s3Exception.getHttpCode() == HttpStatus.SC_REQUESTED_RANGE_NOT_SATISFIABLE) {\n+                retValue =  new ChunkStorageException(chunkName, String.format(\"IllegalArgumentException\", e));\n+            }\n+\n+            if (errorCode.equals(\"AccessDenied\")) {\n+                retValue =  new ChunkStorageException(chunkName, String.format(\"Access denied for chunk %s - %s.\", chunkName, message), e);\n+            }\n+        }\n+\n+        if (retValue == null) {\n+            retValue = new ChunkStorageException(chunkName, message, e);\n+        }\n+\n+        return retValue;\n+    }\n+\n+    private String getObjectPath(String objectName) {\n+        return config.getPrefix() + objectName;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7070fa7fd2549b9bcf3577e4260be3882218524d"}, "originalPosition": 334}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTExNjY5Mw==", "bodyText": "Could it be possible that by some error we end up with a negative length that prevent this loop from finishing? Would a condition like length > 0 be safer in this case?", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455116693", "createdAt": "2020-07-15T14:54:59Z", "author": {"login": "RaulGracia"}, "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,305 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Timer;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        return FileChannel.open(path, openOption);\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        return Files.size(path);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())\n+                    .build();\n+\n+            return information;\n+        } catch (IOException e) {\n+            throw  convertExeption(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            FileAttribute<Set<PosixFilePermission>> fileAttributes = PosixFilePermissions.asFileAttribute(FileSystemUtils.READ_WRITE_PERMISSION);\n+\n+            Path path = Paths.get(config.getRoot(), chunkName);\n+            Path parent = path.getParent();\n+            assert parent != null;\n+            Files.createDirectories(parent);\n+            Files.createFile(path, fileAttributes);\n+\n+        } catch (IOException e) {\n+            throw convertExeption(chunkName, \"doCreate\", e);\n+        }\n+\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    public ChunkStorageException convertExeption(String chunkName, String message, Exception e) throws ChunkStorageException {\n+        if (e instanceof FileNotFoundException || e instanceof NoSuchFileException) {\n+            return new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        if (e instanceof FileAlreadyExistsException) {\n+            return  new ChunkAlreadyExistsException(chunkName, message, e);\n+        }\n+        return new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws IllegalArgumentException {\n+        return Files.exists(Paths.get(config.getRoot(), chunkName));\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            Files.delete(Paths.get(config.getRoot(), handle.getChunkName()));\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenRead\");\n+        }\n+\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenWrite\");\n+        } else if (Files.isWritable(path)) {\n+            return ChunkHandle.writeHandle(chunkName);\n+        } else {\n+            return ChunkHandle.readHandle(chunkName);\n+        }\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        Timer timer = new Timer();\n+\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+        try {\n+            long fileSize = getFileSize(path);\n+            if (fileSize < fromOffset) {\n+                throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the \" +\n+                        \"current size of chunk (%d).\", fromOffset, fileSize));\n+            }\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.READ)) {\n+            int totalBytesRead = 0;\n+            long readOffset = fromOffset;\n+            do {\n+                ByteBuffer readBuffer = ByteBuffer.wrap(buffer, bufferOffset, length);\n+                int bytesRead = channel.read(readBuffer, readOffset);\n+                bufferOffset += bytesRead;\n+                totalBytesRead += bytesRead;\n+                length -= bytesRead;\n+                readOffset += bytesRead;\n+            } while (length != 0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7070fa7fd2549b9bcf3577e4260be3882218524d"}, "originalPosition": 212}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ5MDcxNDY4", "url": "https://github.com/pravega/pravega/pull/4699#pullrequestreview-449071468", "createdAt": "2020-07-15T15:27:02Z", "commit": null, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxNToyNzowMlrOGyDjzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxNTozODoyNVrOGyEAfg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0MDMwMA==", "bodyText": "I don't understand how this is fixed. How is the garbage cleaned up? Is there garbage? If it's already handled, let's write it in a comment.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455140300", "createdAt": "2020-07-15T15:27:02Z", "author": {"login": "andreipaduroiu"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+            client.putObject(this.config.getBucket(), this.config.getPrefix() + handle.getChunkName(),\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        int totalBytesConcated = 0;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            String targetPath = config.getPrefix() + chunks[0].getName();\n+            String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new FileNotFoundException(chunks[0].getName());\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName());\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName(),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5Mjc2Ng=="}, "originalCommit": null, "originalPosition": 184}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0MTc2NA==", "bodyText": "So what is the Spotbugs actual message? What method in the try block above has such a weird declaration that spotbugs isn't happy?", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455141764", "createdAt": "2020-07-15T15:29:14Z", "author": {"login": "andreipaduroiu"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+            client.putObject(this.config.getBucket(), this.config.getPrefix() + handle.getChunkName(),\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        int totalBytesConcated = 0;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            String targetPath = config.getPrefix() + chunks[0].getName();\n+            String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new FileNotFoundException(chunks[0].getName());\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName());\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName(),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                client.deleteObject(config.getBucket(), config.getPrefix() + chunks[i].getName());\n+            }\n+        } catch (RuntimeException e) {\n+            // Make spotbugs happy. Wants us to catch RuntimeException in a separate catch block.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5MjE1Mw=="}, "originalCommit": null, "originalPosition": 191}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0MzQ5Ng==", "bodyText": "I think one way to disambiguate this is to name this operation HeadTruncation. We may want to rename these methods to supportsHeadTruncation. We can do it in a future issue.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455143496", "createdAt": "2020-07-15T15:31:53Z", "author": {"login": "andreipaduroiu"}, "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,473 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on underlying storage object.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzA1MDQ5OA=="}, "originalCommit": null, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0NzE5NQ==", "bodyText": "Why would client be null? you set it in the constructor", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455147195", "createdAt": "2020-07-15T15:37:41Z", "author": {"login": "andreipaduroiu"}, "path": "bindings/src/test/java/io/pravega/storage/extendeds3/ExtendedS3TestContext.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.s3.S3Config;\n+import com.emc.object.s3.bean.ObjectKey;\n+import com.emc.object.s3.jersey.S3JerseyClient;\n+import com.emc.object.s3.request.DeleteObjectsRequest;\n+import com.emc.object.util.ConfigUri;\n+import io.pravega.test.common.TestUtils;\n+\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Test context Extended S3 tests.\n+ */\n+public class ExtendedS3TestContext {\n+    public static final String BUCKET_NAME_PREFIX = \"pravegatest-\";\n+    public final ExtendedS3StorageConfig adapterConfig;\n+    public final S3JerseyClient client;\n+    public final S3ImplBase s3Proxy;\n+    public final int port = TestUtils.getAvailableListenPort();\n+    public final String configUri = \"http://127.0.0.1:\" + port + \"?identity=x&secretKey=x\";\n+    public final S3Config s3Config;\n+\n+    public ExtendedS3TestContext() throws Exception {\n+        String bucketName = BUCKET_NAME_PREFIX + UUID.randomUUID().toString();\n+        this.adapterConfig = ExtendedS3StorageConfig.builder()\n+                .with(ExtendedS3StorageConfig.CONFIGURI, configUri)\n+                .with(ExtendedS3StorageConfig.BUCKET, bucketName)\n+                .with(ExtendedS3StorageConfig.PREFIX, \"samplePrefix\")\n+                .build();\n+        s3Config = new ConfigUri<>(S3Config.class).parseUri(configUri);\n+        s3Proxy = new S3ProxyImpl(configUri, s3Config);\n+        s3Proxy.start();\n+        client = new S3JerseyClientWrapper(s3Config, s3Proxy);\n+        client.createBucket(bucketName);\n+        List<ObjectKey> keys = client.listObjects(bucketName).getObjects().stream()\n+                .map(object -> new ObjectKey(object.getKey()))\n+                .collect(Collectors.toList());\n+\n+        if (!keys.isEmpty()) {\n+            client.deleteObjects(new DeleteObjectsRequest(bucketName).withKeys(keys));\n+        }\n+    }\n+\n+    public void close() throws Exception {\n+        if (client != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7070fa7fd2549b9bcf3577e4260be3882218524d"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE0NzY0Ng==", "bodyText": "There is a chance that something in the constructor fails. We want to put a try-catch block here to shut down the s3proxy if we fail before we exit the constructor.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455147646", "createdAt": "2020-07-15T15:38:25Z", "author": {"login": "andreipaduroiu"}, "path": "bindings/src/test/java/io/pravega/storage/extendeds3/ExtendedS3TestContext.java", "diffHunk": "@@ -0,0 +1,62 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.s3.S3Config;\n+import com.emc.object.s3.bean.ObjectKey;\n+import com.emc.object.s3.jersey.S3JerseyClient;\n+import com.emc.object.s3.request.DeleteObjectsRequest;\n+import com.emc.object.util.ConfigUri;\n+import io.pravega.test.common.TestUtils;\n+\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.stream.Collectors;\n+\n+/**\n+ * Test context Extended S3 tests.\n+ */\n+public class ExtendedS3TestContext {\n+    public static final String BUCKET_NAME_PREFIX = \"pravegatest-\";\n+    public final ExtendedS3StorageConfig adapterConfig;\n+    public final S3JerseyClient client;\n+    public final S3ImplBase s3Proxy;\n+    public final int port = TestUtils.getAvailableListenPort();\n+    public final String configUri = \"http://127.0.0.1:\" + port + \"?identity=x&secretKey=x\";\n+    public final S3Config s3Config;\n+\n+    public ExtendedS3TestContext() throws Exception {\n+        String bucketName = BUCKET_NAME_PREFIX + UUID.randomUUID().toString();\n+        this.adapterConfig = ExtendedS3StorageConfig.builder()\n+                .with(ExtendedS3StorageConfig.CONFIGURI, configUri)\n+                .with(ExtendedS3StorageConfig.BUCKET, bucketName)\n+                .with(ExtendedS3StorageConfig.PREFIX, \"samplePrefix\")\n+                .build();\n+        s3Config = new ConfigUri<>(S3Config.class).parseUri(configUri);\n+        s3Proxy = new S3ProxyImpl(configUri, s3Config);\n+        s3Proxy.start();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7070fa7fd2549b9bcf3577e4260be3882218524d"}, "originalPosition": 44}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "76093576b69ac136b985a396a397ca56089069f1", "author": {"user": {"login": "sachin-j-joshi", "name": "Sachin Jayant Joshi"}}, "url": "https://github.com/pravega/pravega/commit/76093576b69ac136b985a396a397ca56089069f1", "committedDate": "2020-07-15T17:44:09Z", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Address review comments.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6b364af35c7dbda36d481b27a3d70501523c64c5", "author": {"user": {"login": "sachin-j-joshi", "name": "Sachin Jayant Joshi"}}, "url": "https://github.com/pravega/pravega/commit/6b364af35c7dbda36d481b27a3d70501523c64c5", "committedDate": "2020-07-15T18:30:49Z", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - make return value of doSetReadOnly as void.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d7bdf6a6254ac1ae1e6227a80851fa22031d142a", "author": {"user": {"login": "sachin-j-joshi", "name": "Sachin Jayant Joshi"}}, "url": "https://github.com/pravega/pravega/commit/d7bdf6a6254ac1ae1e6227a80851fa22031d142a", "committedDate": "2020-07-15T20:02:12Z", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - After concat delete chunks properly in ChunkedSegmentStorage instead of ChunkStorage.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "52bdaef9b6ee8aa9ba1bcf5e9bff3b577e25c1f5", "author": {"user": {"login": "sachin-j-joshi", "name": "Sachin Jayant Joshi"}}, "url": "https://github.com/pravega/pravega/commit/52bdaef9b6ee8aa9ba1bcf5e9bff3b577e25c1f5", "committedDate": "2020-07-15T20:34:20Z", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - Better parameter comment for defrag.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "30225a38a4b6dde90ff3154ef14651097df8dd63", "author": {"user": {"login": "sachin-j-joshi", "name": "Sachin Jayant Joshi"}}, "url": "https://github.com/pravega/pravega/commit/30225a38a4b6dde90ff3154ef14651097df8dd63", "committedDate": "2020-07-16T01:55:08Z", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - More code coverage.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ5NzEzOTU0", "url": "https://github.com/pravega/pravega/pull/4699#pullrequestreview-449713954", "createdAt": "2020-07-16T10:19:04Z", "commit": {"oid": "30225a38a4b6dde90ff3154ef14651097df8dd63"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ5NzgwNzIw", "url": "https://github.com/pravega/pravega/pull/4699#pullrequestreview-449780720", "createdAt": "2020-07-16T12:03:09Z", "commit": {"oid": "30225a38a4b6dde90ff3154ef14651097df8dd63"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxMjowMzowOVrOGyn12w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQxMjoxNTo1M1rOGyoP0A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNDc0Nw==", "bodyText": "as for S3 storage, we should have a common method to perform Paths.get(config.getRoot(), chunkName)\nit can also be a security issue to let chunkName be anystring.\nwe should have some validation that at least blocks:\n\nnull chars\nrelative path sequences (\"..\", \"/\",\"\",\".\")\nanche chunkname cannot be empty\n\nwe cannot assume that the caller is a good guy, even if this is our own code", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455734747", "createdAt": "2020-07-16T12:03:09Z", "author": {"login": "eolivelli"}, "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        return FileChannel.open(path, openOption);\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        return Files.size(path);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "30225a38a4b6dde90ff3154ef14651097df8dd63"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNTU3Nw==", "bodyText": "isn't Files.size(path)  enough ?\nwhy are we passing thru PosixFileAttributes ?", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455735577", "createdAt": "2020-07-16T12:04:54Z", "author": {"login": "eolivelli"}, "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        return FileChannel.open(path, openOption);\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        return Files.size(path);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "30225a38a4b6dde90ff3154ef14651097df8dd63"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNjEwNg==", "bodyText": "please also check that this is a RegularFile, not a directory or a link (for instance)", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455736106", "createdAt": "2020-07-16T12:05:55Z", "author": {"login": "eolivelli"}, "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        return FileChannel.open(path, openOption);\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        return Files.size(path);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())\n+                    .build();\n+\n+            return information;\n+        } catch (IOException e) {\n+            throw  convertExeption(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            FileAttribute<Set<PosixFilePermission>> fileAttributes = PosixFilePermissions.asFileAttribute(FileSystemUtils.READ_WRITE_PERMISSION);\n+\n+            Path path = Paths.get(config.getRoot(), chunkName);\n+            Path parent = path.getParent();\n+            assert parent != null;\n+            Files.createDirectories(parent);\n+            Files.createFile(path, fileAttributes);\n+\n+        } catch (IOException e) {\n+            throw convertExeption(chunkName, \"doCreate\", e);\n+        }\n+\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    private ChunkStorageException convertExeption(String chunkName, String message, Exception e) {\n+        if (e instanceof FileNotFoundException || e instanceof NoSuchFileException) {\n+            return new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        if (e instanceof FileAlreadyExistsException) {\n+            return  new ChunkAlreadyExistsException(chunkName, message, e);\n+        }\n+        return new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws IllegalArgumentException {\n+        return Files.exists(Paths.get(config.getRoot(), chunkName));\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            Files.delete(Paths.get(config.getRoot(), handle.getChunkName()));\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenRead\");\n+        }\n+\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+        if (!Files.exists(path)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "30225a38a4b6dde90ff3154ef14651097df8dd63"}, "originalPosition": 176}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNjI3Ng==", "bodyText": "you also have this getFileSize\nhttps://github.com/pravega/pravega/pull/4699/files#diff-17a1fa6c3b4ce4b99b95fde90189ccc5R190", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455736276", "createdAt": "2020-07-16T12:06:17Z", "author": {"login": "eolivelli"}, "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        return FileChannel.open(path, openOption);\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        return Files.size(path);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNTU3Nw=="}, "originalCommit": {"oid": "30225a38a4b6dde90ff3154ef14651097df8dd63"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNzMxNQ==", "bodyText": "this is bad from a security perspective\nString.valueOf(sourcePath)\nyou can use sourcePath.toFile()", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455737315", "createdAt": "2020-07-16T12:08:19Z", "author": {"login": "eolivelli"}, "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        return FileChannel.open(path, openOption);\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        return Files.size(path);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())\n+                    .build();\n+\n+            return information;\n+        } catch (IOException e) {\n+            throw  convertExeption(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            FileAttribute<Set<PosixFilePermission>> fileAttributes = PosixFilePermissions.asFileAttribute(FileSystemUtils.READ_WRITE_PERMISSION);\n+\n+            Path path = Paths.get(config.getRoot(), chunkName);\n+            Path parent = path.getParent();\n+            assert parent != null;\n+            Files.createDirectories(parent);\n+            Files.createFile(path, fileAttributes);\n+\n+        } catch (IOException e) {\n+            throw convertExeption(chunkName, \"doCreate\", e);\n+        }\n+\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    private ChunkStorageException convertExeption(String chunkName, String message, Exception e) {\n+        if (e instanceof FileNotFoundException || e instanceof NoSuchFileException) {\n+            return new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        if (e instanceof FileAlreadyExistsException) {\n+            return  new ChunkAlreadyExistsException(chunkName, message, e);\n+        }\n+        return new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws IllegalArgumentException {\n+        return Files.exists(Paths.get(config.getRoot(), chunkName));\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            Files.delete(Paths.get(config.getRoot(), handle.getChunkName()));\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenRead\");\n+        }\n+\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenWrite\");\n+        } else if (Files.isWritable(path)) {\n+            return ChunkHandle.writeHandle(chunkName);\n+        } else {\n+            return ChunkHandle.readHandle(chunkName);\n+        }\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+        try {\n+            long fileSize = getFileSize(path);\n+            if (fileSize < fromOffset) {\n+                throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the \" +\n+                        \"current size of chunk (%d).\", fromOffset, fileSize));\n+            }\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.READ)) {\n+            int totalBytesRead = 0;\n+            long readOffset = fromOffset;\n+            do {\n+                ByteBuffer readBuffer = ByteBuffer.wrap(buffer, bufferOffset, length);\n+                int bytesRead = channel.read(readBuffer, readOffset);\n+                bufferOffset += bytesRead;\n+                totalBytesRead += bytesRead;\n+                length -= bytesRead;\n+                readOffset += bytesRead;\n+            } while (length > 0);\n+            return totalBytesRead;\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException {\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+\n+        long totalBytesWritten = 0;\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.WRITE)) {\n+            long fileSize = channel.size();\n+            if (fileSize != offset) {\n+                throw new IndexOutOfBoundsException(String.format(\"fileSize (%d) did not match offset (%d) for chunk %s\", fileSize, offset, handle.getChunkName()));\n+            }\n+\n+            // Wrap the input data into a ReadableByteChannel, but do not close it. Doing so will result in closing\n+            // the underlying InputStream, which is not desirable if it is to be reused.\n+            ReadableByteChannel sourceChannel = Channels.newChannel(data);\n+            while (length > 0) {\n+                long bytesWritten = channel.transferFrom(sourceChannel, offset, length);\n+                assert bytesWritten > 0 : \"Unable to make any progress transferring data.\";\n+                offset += bytesWritten;\n+                totalBytesWritten += bytesWritten;\n+                length -= bytesWritten;\n+            }\n+            channel.force(true);\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return (int) totalBytesWritten;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        try {\n+            int totalBytesConcated = 0;\n+            Path targetPath = Paths.get(config.getRoot(), chunks[0].getName());\n+            long offset = chunks[0].getLength();\n+\n+            for (int i = 1; i < chunks.length; i++) {\n+                val source = chunks[i];\n+                Preconditions.checkArgument(!chunks[0].getName().equals(source.getName()), \"target and source can not be same.\");\n+                Path sourcePath = Paths.get(config.getRoot(), source.getName());\n+                long length = chunks[i].getLength();\n+                Preconditions.checkState(offset <= getFileSize(targetPath));\n+                Preconditions.checkState(length <= getFileSize(sourcePath));\n+                try (FileChannel targetChannel = getFileChannel(targetPath, StandardOpenOption.WRITE);\n+                     RandomAccessFile sourceFile = new RandomAccessFile(String.valueOf(sourcePath), \"r\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "30225a38a4b6dde90ff3154ef14651097df8dd63"}, "originalPosition": 259}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNzgwMw==", "bodyText": "why are you using RandomAccessFile ?", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455737803", "createdAt": "2020-07-16T12:09:16Z", "author": {"login": "eolivelli"}, "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        return FileChannel.open(path, openOption);\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        return Files.size(path);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())\n+                    .build();\n+\n+            return information;\n+        } catch (IOException e) {\n+            throw  convertExeption(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            FileAttribute<Set<PosixFilePermission>> fileAttributes = PosixFilePermissions.asFileAttribute(FileSystemUtils.READ_WRITE_PERMISSION);\n+\n+            Path path = Paths.get(config.getRoot(), chunkName);\n+            Path parent = path.getParent();\n+            assert parent != null;\n+            Files.createDirectories(parent);\n+            Files.createFile(path, fileAttributes);\n+\n+        } catch (IOException e) {\n+            throw convertExeption(chunkName, \"doCreate\", e);\n+        }\n+\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    private ChunkStorageException convertExeption(String chunkName, String message, Exception e) {\n+        if (e instanceof FileNotFoundException || e instanceof NoSuchFileException) {\n+            return new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        if (e instanceof FileAlreadyExistsException) {\n+            return  new ChunkAlreadyExistsException(chunkName, message, e);\n+        }\n+        return new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws IllegalArgumentException {\n+        return Files.exists(Paths.get(config.getRoot(), chunkName));\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            Files.delete(Paths.get(config.getRoot(), handle.getChunkName()));\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenRead\");\n+        }\n+\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenWrite\");\n+        } else if (Files.isWritable(path)) {\n+            return ChunkHandle.writeHandle(chunkName);\n+        } else {\n+            return ChunkHandle.readHandle(chunkName);\n+        }\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+        try {\n+            long fileSize = getFileSize(path);\n+            if (fileSize < fromOffset) {\n+                throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the \" +\n+                        \"current size of chunk (%d).\", fromOffset, fileSize));\n+            }\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.READ)) {\n+            int totalBytesRead = 0;\n+            long readOffset = fromOffset;\n+            do {\n+                ByteBuffer readBuffer = ByteBuffer.wrap(buffer, bufferOffset, length);\n+                int bytesRead = channel.read(readBuffer, readOffset);\n+                bufferOffset += bytesRead;\n+                totalBytesRead += bytesRead;\n+                length -= bytesRead;\n+                readOffset += bytesRead;\n+            } while (length > 0);\n+            return totalBytesRead;\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException {\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+\n+        long totalBytesWritten = 0;\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.WRITE)) {\n+            long fileSize = channel.size();\n+            if (fileSize != offset) {\n+                throw new IndexOutOfBoundsException(String.format(\"fileSize (%d) did not match offset (%d) for chunk %s\", fileSize, offset, handle.getChunkName()));\n+            }\n+\n+            // Wrap the input data into a ReadableByteChannel, but do not close it. Doing so will result in closing\n+            // the underlying InputStream, which is not desirable if it is to be reused.\n+            ReadableByteChannel sourceChannel = Channels.newChannel(data);\n+            while (length > 0) {\n+                long bytesWritten = channel.transferFrom(sourceChannel, offset, length);\n+                assert bytesWritten > 0 : \"Unable to make any progress transferring data.\";\n+                offset += bytesWritten;\n+                totalBytesWritten += bytesWritten;\n+                length -= bytesWritten;\n+            }\n+            channel.force(true);\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return (int) totalBytesWritten;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        try {\n+            int totalBytesConcated = 0;\n+            Path targetPath = Paths.get(config.getRoot(), chunks[0].getName());\n+            long offset = chunks[0].getLength();\n+\n+            for (int i = 1; i < chunks.length; i++) {\n+                val source = chunks[i];\n+                Preconditions.checkArgument(!chunks[0].getName().equals(source.getName()), \"target and source can not be same.\");\n+                Path sourcePath = Paths.get(config.getRoot(), source.getName());\n+                long length = chunks[i].getLength();\n+                Preconditions.checkState(offset <= getFileSize(targetPath));\n+                Preconditions.checkState(length <= getFileSize(sourcePath));\n+                try (FileChannel targetChannel = getFileChannel(targetPath, StandardOpenOption.WRITE);\n+                     RandomAccessFile sourceFile = new RandomAccessFile(String.valueOf(sourcePath), \"r\")) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczNzMxNQ=="}, "originalCommit": {"oid": "30225a38a4b6dde90ff3154ef14651097df8dd63"}, "originalPosition": 259}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczODcxNA==", "bodyText": "why are you opening the same file more times ?\nit is better to open the file for write only once.\nI suggest you also to:\n\ncreate a temporary file\nfill it\nrename new the file with ATOMIC_MOVE flag to the targetPath\nThis way we are protected from partial writes", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455738714", "createdAt": "2020-07-16T12:10:53Z", "author": {"login": "eolivelli"}, "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        return FileChannel.open(path, openOption);\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        return Files.size(path);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())\n+                    .build();\n+\n+            return information;\n+        } catch (IOException e) {\n+            throw  convertExeption(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            FileAttribute<Set<PosixFilePermission>> fileAttributes = PosixFilePermissions.asFileAttribute(FileSystemUtils.READ_WRITE_PERMISSION);\n+\n+            Path path = Paths.get(config.getRoot(), chunkName);\n+            Path parent = path.getParent();\n+            assert parent != null;\n+            Files.createDirectories(parent);\n+            Files.createFile(path, fileAttributes);\n+\n+        } catch (IOException e) {\n+            throw convertExeption(chunkName, \"doCreate\", e);\n+        }\n+\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    private ChunkStorageException convertExeption(String chunkName, String message, Exception e) {\n+        if (e instanceof FileNotFoundException || e instanceof NoSuchFileException) {\n+            return new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        if (e instanceof FileAlreadyExistsException) {\n+            return  new ChunkAlreadyExistsException(chunkName, message, e);\n+        }\n+        return new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws IllegalArgumentException {\n+        return Files.exists(Paths.get(config.getRoot(), chunkName));\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            Files.delete(Paths.get(config.getRoot(), handle.getChunkName()));\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenRead\");\n+        }\n+\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenWrite\");\n+        } else if (Files.isWritable(path)) {\n+            return ChunkHandle.writeHandle(chunkName);\n+        } else {\n+            return ChunkHandle.readHandle(chunkName);\n+        }\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+        try {\n+            long fileSize = getFileSize(path);\n+            if (fileSize < fromOffset) {\n+                throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the \" +\n+                        \"current size of chunk (%d).\", fromOffset, fileSize));\n+            }\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.READ)) {\n+            int totalBytesRead = 0;\n+            long readOffset = fromOffset;\n+            do {\n+                ByteBuffer readBuffer = ByteBuffer.wrap(buffer, bufferOffset, length);\n+                int bytesRead = channel.read(readBuffer, readOffset);\n+                bufferOffset += bytesRead;\n+                totalBytesRead += bytesRead;\n+                length -= bytesRead;\n+                readOffset += bytesRead;\n+            } while (length > 0);\n+            return totalBytesRead;\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException {\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+\n+        long totalBytesWritten = 0;\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.WRITE)) {\n+            long fileSize = channel.size();\n+            if (fileSize != offset) {\n+                throw new IndexOutOfBoundsException(String.format(\"fileSize (%d) did not match offset (%d) for chunk %s\", fileSize, offset, handle.getChunkName()));\n+            }\n+\n+            // Wrap the input data into a ReadableByteChannel, but do not close it. Doing so will result in closing\n+            // the underlying InputStream, which is not desirable if it is to be reused.\n+            ReadableByteChannel sourceChannel = Channels.newChannel(data);\n+            while (length > 0) {\n+                long bytesWritten = channel.transferFrom(sourceChannel, offset, length);\n+                assert bytesWritten > 0 : \"Unable to make any progress transferring data.\";\n+                offset += bytesWritten;\n+                totalBytesWritten += bytesWritten;\n+                length -= bytesWritten;\n+            }\n+            channel.force(true);\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return (int) totalBytesWritten;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        try {\n+            int totalBytesConcated = 0;\n+            Path targetPath = Paths.get(config.getRoot(), chunks[0].getName());\n+            long offset = chunks[0].getLength();\n+\n+            for (int i = 1; i < chunks.length; i++) {\n+                val source = chunks[i];\n+                Preconditions.checkArgument(!chunks[0].getName().equals(source.getName()), \"target and source can not be same.\");\n+                Path sourcePath = Paths.get(config.getRoot(), source.getName());\n+                long length = chunks[i].getLength();\n+                Preconditions.checkState(offset <= getFileSize(targetPath));\n+                Preconditions.checkState(length <= getFileSize(sourcePath));\n+                try (FileChannel targetChannel = getFileChannel(targetPath, StandardOpenOption.WRITE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "30225a38a4b6dde90ff3154ef14651097df8dd63"}, "originalPosition": 258}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczOTA4NQ==", "bodyText": "Maybe this throw new UnsupportedOperationException()  should be the default implementation in the base class", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455739085", "createdAt": "2020-07-16T12:11:35Z", "author": {"login": "eolivelli"}, "path": "bindings/src/main/java/io/pravega/storage/filesystem/FileSystemChunkStorage.java", "diffHunk": "@@ -0,0 +1,294 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.filesystem;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.io.RandomAccessFile;\n+import java.nio.ByteBuffer;\n+import java.nio.channels.Channels;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.FileAlreadyExistsException;\n+import java.nio.file.Files;\n+import java.nio.file.NoSuchFileException;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.nio.file.attribute.FileAttribute;\n+import java.nio.file.attribute.PosixFileAttributes;\n+import java.nio.file.attribute.PosixFilePermission;\n+import java.nio.file.attribute.PosixFilePermissions;\n+import java.util.Set;\n+\n+/**\n+ * {@link ChunkStorage} for file system based storage.\n+ *\n+ * Each Chunk is represented as a single file on the underlying storage.\n+ * The concat operation is implemented as append.\n+ */\n+\n+@Slf4j\n+public class FileSystemChunkStorage extends BaseChunkStorage {\n+    //region members\n+\n+    private final FileSystemStorageConfig config;\n+\n+    //endregion\n+\n+    //region constructor\n+\n+    /**\n+     * Creates a new instance of the FileSystemChunkStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    public FileSystemChunkStorage(FileSystemStorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region\n+\n+    @VisibleForTesting\n+    protected FileChannel getFileChannel(Path path, StandardOpenOption openOption) throws IOException {\n+        return FileChannel.open(path, openOption);\n+    }\n+\n+    @VisibleForTesting\n+    protected long getFileSize(Path path) throws IOException {\n+        return Files.size(path);\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            PosixFileAttributes attrs = Files.readAttributes(Paths.get(config.getRoot(), chunkName),\n+                    PosixFileAttributes.class);\n+            ChunkInfo information = ChunkInfo.builder()\n+                    .name(chunkName)\n+                    .length(attrs.size())\n+                    .build();\n+\n+            return information;\n+        } catch (IOException e) {\n+            throw  convertExeption(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            FileAttribute<Set<PosixFilePermission>> fileAttributes = PosixFilePermissions.asFileAttribute(FileSystemUtils.READ_WRITE_PERMISSION);\n+\n+            Path path = Paths.get(config.getRoot(), chunkName);\n+            Path parent = path.getParent();\n+            assert parent != null;\n+            Files.createDirectories(parent);\n+            Files.createFile(path, fileAttributes);\n+\n+        } catch (IOException e) {\n+            throw convertExeption(chunkName, \"doCreate\", e);\n+        }\n+\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    private ChunkStorageException convertExeption(String chunkName, String message, Exception e) {\n+        if (e instanceof FileNotFoundException || e instanceof NoSuchFileException) {\n+            return new ChunkNotFoundException(chunkName, message, e);\n+        }\n+        if (e instanceof FileAlreadyExistsException) {\n+            return  new ChunkAlreadyExistsException(chunkName, message, e);\n+        }\n+        return new ChunkStorageException(chunkName, message, e);\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws IllegalArgumentException {\n+        return Files.exists(Paths.get(config.getRoot(), chunkName));\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        try {\n+            Files.delete(Paths.get(config.getRoot(), handle.getChunkName()));\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenRead\");\n+        }\n+\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        Path path = Paths.get(config.getRoot(), chunkName);\n+        if (!Files.exists(path)) {\n+            throw new ChunkNotFoundException(chunkName, \"FileSystemChunkStorage::doOpenWrite\");\n+        } else if (Files.isWritable(path)) {\n+            return ChunkHandle.writeHandle(chunkName);\n+        } else {\n+            return ChunkHandle.readHandle(chunkName);\n+        }\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+        try {\n+            long fileSize = getFileSize(path);\n+            if (fileSize < fromOffset) {\n+                throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the \" +\n+                        \"current size of chunk (%d).\", fromOffset, fileSize));\n+            }\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.READ)) {\n+            int totalBytesRead = 0;\n+            long readOffset = fromOffset;\n+            do {\n+                ByteBuffer readBuffer = ByteBuffer.wrap(buffer, bufferOffset, length);\n+                int bytesRead = channel.read(readBuffer, readOffset);\n+                bufferOffset += bytesRead;\n+                totalBytesRead += bytesRead;\n+                length -= bytesRead;\n+                readOffset += bytesRead;\n+            } while (length > 0);\n+            return totalBytesRead;\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doRead\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException {\n+        Path path = Paths.get(config.getRoot(), handle.getChunkName());\n+\n+        long totalBytesWritten = 0;\n+        try (FileChannel channel = getFileChannel(path, StandardOpenOption.WRITE)) {\n+            long fileSize = channel.size();\n+            if (fileSize != offset) {\n+                throw new IndexOutOfBoundsException(String.format(\"fileSize (%d) did not match offset (%d) for chunk %s\", fileSize, offset, handle.getChunkName()));\n+            }\n+\n+            // Wrap the input data into a ReadableByteChannel, but do not close it. Doing so will result in closing\n+            // the underlying InputStream, which is not desirable if it is to be reused.\n+            ReadableByteChannel sourceChannel = Channels.newChannel(data);\n+            while (length > 0) {\n+                long bytesWritten = channel.transferFrom(sourceChannel, offset, length);\n+                assert bytesWritten > 0 : \"Unable to make any progress transferring data.\";\n+                offset += bytesWritten;\n+                totalBytesWritten += bytesWritten;\n+                length -= bytesWritten;\n+            }\n+            channel.force(true);\n+        } catch (IOException e) {\n+            throw convertExeption(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return (int) totalBytesWritten;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        try {\n+            int totalBytesConcated = 0;\n+            Path targetPath = Paths.get(config.getRoot(), chunks[0].getName());\n+            long offset = chunks[0].getLength();\n+\n+            for (int i = 1; i < chunks.length; i++) {\n+                val source = chunks[i];\n+                Preconditions.checkArgument(!chunks[0].getName().equals(source.getName()), \"target and source can not be same.\");\n+                Path sourcePath = Paths.get(config.getRoot(), source.getName());\n+                long length = chunks[i].getLength();\n+                Preconditions.checkState(offset <= getFileSize(targetPath));\n+                Preconditions.checkState(length <= getFileSize(sourcePath));\n+                try (FileChannel targetChannel = getFileChannel(targetPath, StandardOpenOption.WRITE);\n+                     RandomAccessFile sourceFile = new RandomAccessFile(String.valueOf(sourcePath), \"r\")) {\n+                    while (length > 0) {\n+                        long bytesTransferred = targetChannel.transferFrom(sourceFile.getChannel(), offset, length);\n+                        offset += bytesTransferred;\n+                        length -= bytesTransferred;\n+                    }\n+                    targetChannel.force(true);\n+                    totalBytesConcated += length;\n+                    offset += length;\n+                }\n+\n+            }\n+            return totalBytesConcated;\n+        } catch (IOException e) {\n+            throw convertExeption(chunks[0].getName(), \"doConcat\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws UnsupportedOperationException {\n+        throw new UnsupportedOperationException();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "30225a38a4b6dde90ff3154ef14651097df8dd63"}, "originalPosition": 279}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTczOTc2Nw==", "bodyText": "what happens in case of failure during the execution of this method ?\nwe are losing the contents of the file IMHO", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455739767", "createdAt": "2020-07-16T12:12:56Z", "author": {"login": "eolivelli"}, "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,350 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+import org.apache.hadoop.ipc.RemoteException;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            return ChunkInfo.builder().name(chunkName).length(status.getLen()).build();\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doCreate\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            // Try accessing file.\n+            fileSystem.getFileStatus(getFilePath(chunkName));\n+            return true;\n+        } catch (FileNotFoundException e) {\n+            return false;\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"checkExists\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            val path = getFilePath(handle.getChunkName());\n+            if (!this.fileSystem.delete(path, true)) {\n+                // File was not deleted. Check if exists.\n+                checkFileExists(handle.getChunkName(), \"doDelete\");\n+            }\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        checkFileExists(chunkName, \"doOpenRead\");\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            val status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            if (status.getPermission().getUserAction() == FsAction.READ) {\n+                return ChunkHandle.readHandle(chunkName);\n+            } else {\n+                return ChunkHandle.writeHandle(chunkName);\n+            }\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doOpenWrite\", e);\n+        }\n+\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try (FSDataInputStream stream = this.fileSystem.open(getFilePath(handle.getChunkName()))) {\n+            stream.readFully(fromOffset, buffer, bufferOffset, length);\n+        } catch (EOFException e) {\n+            throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the current size of chunk.\", fromOffset));\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try (FSDataOutputStream stream = this.fileSystem.append(getFilePath(handle.getChunkName()))) {\n+            if (stream.getPos() != offset) {\n+                // Looks like the filesystem changed from underneath us. This could be our bug, but it could be something else.\n+                throw new IndexOutOfBoundsException();\n+            }\n+\n+            if (length == 0) {\n+                // Note: IOUtils.copyBytes with length == 0 will enter an infinite loop, hence the need for this check.\n+                return 0;\n+            }\n+\n+            // We need to be very careful with IOUtils.copyBytes. There are many overloads with very similar signatures.\n+            // There is a difference between (InputStream, OutputStream, int, boolean) and (InputStream, OutputStream, long, boolean),\n+            // in that the one with \"int\" uses the third arg as a buffer size, and the one with \"long\" uses it as the number\n+            // of bytes to copy.\n+            IOUtils.copyBytes(data, stream, (long) length, false);\n+\n+            stream.flush();\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        ensureInitializedAndNotClosed();\n+        int length = 0;\n+        try {\n+            val sources = new Path[chunks.length - 1];\n+            this.fileSystem.truncate(getFilePath(chunks[0].getName()), chunks[0].getLength());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "30225a38a4b6dde90ff3154ef14651097df8dd63"}, "originalPosition": 244}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTc0MDEzMw==", "bodyText": "what about org.apache.hadoop.hdfs.DistributedFileSystem.class.getName() ?\nthis way we will have a compile time guarantee that the name is good", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455740133", "createdAt": "2020-07-16T12:13:34Z", "author": {"login": "eolivelli"}, "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,350 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+import org.apache.hadoop.ipc.RemoteException;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            return ChunkInfo.builder().name(chunkName).length(status.getLen()).build();\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doCreate\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            // Try accessing file.\n+            fileSystem.getFileStatus(getFilePath(chunkName));\n+            return true;\n+        } catch (FileNotFoundException e) {\n+            return false;\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"checkExists\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            val path = getFilePath(handle.getChunkName());\n+            if (!this.fileSystem.delete(path, true)) {\n+                // File was not deleted. Check if exists.\n+                checkFileExists(handle.getChunkName(), \"doDelete\");\n+            }\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        checkFileExists(chunkName, \"doOpenRead\");\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            val status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            if (status.getPermission().getUserAction() == FsAction.READ) {\n+                return ChunkHandle.readHandle(chunkName);\n+            } else {\n+                return ChunkHandle.writeHandle(chunkName);\n+            }\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doOpenWrite\", e);\n+        }\n+\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try (FSDataInputStream stream = this.fileSystem.open(getFilePath(handle.getChunkName()))) {\n+            stream.readFully(fromOffset, buffer, bufferOffset, length);\n+        } catch (EOFException e) {\n+            throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the current size of chunk.\", fromOffset));\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try (FSDataOutputStream stream = this.fileSystem.append(getFilePath(handle.getChunkName()))) {\n+            if (stream.getPos() != offset) {\n+                // Looks like the filesystem changed from underneath us. This could be our bug, but it could be something else.\n+                throw new IndexOutOfBoundsException();\n+            }\n+\n+            if (length == 0) {\n+                // Note: IOUtils.copyBytes with length == 0 will enter an infinite loop, hence the need for this check.\n+                return 0;\n+            }\n+\n+            // We need to be very careful with IOUtils.copyBytes. There are many overloads with very similar signatures.\n+            // There is a difference between (InputStream, OutputStream, int, boolean) and (InputStream, OutputStream, long, boolean),\n+            // in that the one with \"int\" uses the third arg as a buffer size, and the one with \"long\" uses it as the number\n+            // of bytes to copy.\n+            IOUtils.copyBytes(data, stream, (long) length, false);\n+\n+            stream.flush();\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        ensureInitializedAndNotClosed();\n+        int length = 0;\n+        try {\n+            val sources = new Path[chunks.length - 1];\n+            this.fileSystem.truncate(getFilePath(chunks[0].getName()), chunks[0].getLength());\n+            for (int i = 1; i < chunks.length; i++) {\n+                val chunkLength = chunks[i].getLength();\n+                this.fileSystem.truncate(getFilePath(chunks[i].getName()), chunkLength);\n+                sources[i - 1] = getFilePath(chunks[i].getName());\n+                length += chunkLength;\n+            }\n+            // Concat source file into target.\n+            this.fileSystem.concat(getFilePath(chunks[0].getName()), sources);\n+        } catch (IOException e) {\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws UnsupportedOperationException {\n+        throw new UnsupportedOperationException(getClass().getName() + \" does not support chunk truncation.\");\n+    }\n+\n+    @Override\n+    protected void doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException {\n+        try {\n+            this.fileSystem.setPermission(getFilePath(handle.getChunkName()), isReadOnly ? READONLY_PERMISSION : READWRITE_PERMISSION);\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+    }\n+\n+    //endregion\n+\n+    //region Storage Implementation\n+\n+    @SneakyThrows(IOException.class)\n+    public void initialize() {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        Preconditions.checkState(this.fileSystem == null, \"HDFSStorage has already been initialized.\");\n+        Configuration conf = new Configuration();\n+        conf.set(\"fs.default.name\", this.config.getHdfsHostURL());\n+        conf.set(\"fs.default.fs\", this.config.getHdfsHostURL());\n+        conf.set(\"fs.hdfs.impl\", \"org.apache.hadoop.hdfs.DistributedFileSystem\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "30225a38a4b6dde90ff3154ef14651097df8dd63"}, "originalPosition": 284}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTc0MTA1MA==", "bodyText": "do we have a way to pass additional configuration parameters to this client ?\nI mean, probably users would like to add additional configurations, in segment store configuration file\nhdfs.customconfig.xxxx=yyyyy", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455741050", "createdAt": "2020-07-16T12:15:16Z", "author": {"login": "eolivelli"}, "path": "bindings/src/main/java/io/pravega/storage/hdfs/HDFSChunkStorage.java", "diffHunk": "@@ -0,0 +1,350 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.hdfs;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+\n+import java.io.EOFException;\n+import java.io.FileNotFoundException;\n+import java.io.IOException;\n+import java.io.InputStream;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.SneakyThrows;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.hadoop.conf.Configuration;\n+import org.apache.hadoop.fs.FSDataInputStream;\n+import org.apache.hadoop.fs.FSDataOutputStream;\n+import org.apache.hadoop.fs.FileAlreadyExistsException;\n+import org.apache.hadoop.fs.FileStatus;\n+import org.apache.hadoop.fs.FileSystem;\n+import org.apache.hadoop.fs.Path;\n+import org.apache.hadoop.fs.permission.FsAction;\n+import org.apache.hadoop.fs.permission.FsPermission;\n+import org.apache.hadoop.io.IOUtils;\n+import org.apache.hadoop.ipc.RemoteException;\n+\n+/***\n+ *  {@link ChunkStorage} for HDFS based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented using HDFS native concat operation.\n+ */\n+\n+@Slf4j\n+class HDFSChunkStorage extends BaseChunkStorage {\n+    private static final FsPermission READWRITE_PERMISSION = new FsPermission(FsAction.READ_WRITE, FsAction.NONE, FsAction.NONE);\n+    private static final FsPermission READONLY_PERMISSION = new FsPermission(FsAction.READ, FsAction.READ, FsAction.READ);\n+\n+    //region Members\n+\n+    private final HDFSStorageConfig config;\n+    private FileSystem fileSystem;\n+    private final AtomicBoolean closed;\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the HDFSStorage class.\n+     *\n+     * @param config The configuration to use.\n+     */\n+    HDFSChunkStorage(HDFSStorageConfig config) {\n+        Preconditions.checkNotNull(config, \"config\");\n+        this.config = config;\n+        this.closed = new AtomicBoolean(false);\n+        initialize();\n+    }\n+\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region AutoCloseable Implementation\n+\n+    @Override\n+    public void close() {\n+        if (!this.closed.getAndSet(true)) {\n+            if (this.fileSystem != null) {\n+                try {\n+                    this.fileSystem.close();\n+                    this.fileSystem = null;\n+                } catch (IOException e) {\n+                    log.warn(\"Could not close the HDFS filesystem.\", e);\n+                }\n+            }\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkInfo doGetInfo(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            FileStatus status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            return ChunkInfo.builder().name(chunkName).length(status.getLen()).build();\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doGetInfo\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doCreate(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            Path fullPath = getFilePath(chunkName);\n+            // Create the file, and then immediately close the returned OutputStream, so that HDFS may properly create the file.\n+            this.fileSystem.create(fullPath, READWRITE_PERMISSION, false, 0, this.config.getReplication(),\n+                    this.config.getBlockSize(), null).close();\n+            log.debug(\"Created '{}'.\", fullPath);\n+\n+            // return handle\n+            return ChunkHandle.writeHandle(chunkName);\n+        } catch (FileAlreadyExistsException e) {\n+            throw new ChunkAlreadyExistsException(chunkName, \"HDFSChunkStorage::doCreate\");\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doCreate\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected boolean checkExists(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            // Try accessing file.\n+            fileSystem.getFileStatus(getFilePath(chunkName));\n+            return true;\n+        } catch (FileNotFoundException e) {\n+            return false;\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"checkExists\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected void doDelete(ChunkHandle handle) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            val path = getFilePath(handle.getChunkName());\n+            if (!this.fileSystem.delete(path, true)) {\n+                // File was not deleted. Check if exists.\n+                checkFileExists(handle.getChunkName(), \"doDelete\");\n+            }\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doDelete\", e);\n+        }\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        checkFileExists(chunkName, \"doOpenRead\");\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        ensureInitializedAndNotClosed();\n+        try {\n+            val status = fileSystem.getFileStatus(getFilePath(chunkName));\n+            if (status.getPermission().getUserAction() == FsAction.READ) {\n+                return ChunkHandle.readHandle(chunkName);\n+            } else {\n+                return ChunkHandle.writeHandle(chunkName);\n+            }\n+        } catch (IOException e) {\n+            throw convertException(chunkName, \"doOpenWrite\", e);\n+        }\n+\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset)\n+            throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try (FSDataInputStream stream = this.fileSystem.open(getFilePath(handle.getChunkName()))) {\n+            stream.readFully(fromOffset, buffer, bufferOffset, length);\n+        } catch (EOFException e) {\n+            throw new IllegalArgumentException(String.format(\"Reading at offset (%d) which is beyond the current size of chunk.\", fromOffset));\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        ensureInitializedAndNotClosed();\n+        try (FSDataOutputStream stream = this.fileSystem.append(getFilePath(handle.getChunkName()))) {\n+            if (stream.getPos() != offset) {\n+                // Looks like the filesystem changed from underneath us. This could be our bug, but it could be something else.\n+                throw new IndexOutOfBoundsException();\n+            }\n+\n+            if (length == 0) {\n+                // Note: IOUtils.copyBytes with length == 0 will enter an infinite loop, hence the need for this check.\n+                return 0;\n+            }\n+\n+            // We need to be very careful with IOUtils.copyBytes. There are many overloads with very similar signatures.\n+            // There is a difference between (InputStream, OutputStream, int, boolean) and (InputStream, OutputStream, long, boolean),\n+            // in that the one with \"int\" uses the third arg as a buffer size, and the one with \"long\" uses it as the number\n+            // of bytes to copy.\n+            IOUtils.copyBytes(data, stream, (long) length, false);\n+\n+            stream.flush();\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException {\n+        ensureInitializedAndNotClosed();\n+        int length = 0;\n+        try {\n+            val sources = new Path[chunks.length - 1];\n+            this.fileSystem.truncate(getFilePath(chunks[0].getName()), chunks[0].getLength());\n+            for (int i = 1; i < chunks.length; i++) {\n+                val chunkLength = chunks[i].getLength();\n+                this.fileSystem.truncate(getFilePath(chunks[i].getName()), chunkLength);\n+                sources[i - 1] = getFilePath(chunks[i].getName());\n+                length += chunkLength;\n+            }\n+            // Concat source file into target.\n+            this.fileSystem.concat(getFilePath(chunks[0].getName()), sources);\n+        } catch (IOException e) {\n+            throw convertException(chunks[0].getName(), \"doConcat\", e);\n+        }\n+        return length;\n+    }\n+\n+    @Override\n+    protected boolean doTruncate(ChunkHandle handle, long offset) throws UnsupportedOperationException {\n+        throw new UnsupportedOperationException(getClass().getName() + \" does not support chunk truncation.\");\n+    }\n+\n+    @Override\n+    protected void doSetReadOnly(ChunkHandle handle, boolean isReadOnly) throws ChunkStorageException {\n+        try {\n+            this.fileSystem.setPermission(getFilePath(handle.getChunkName()), isReadOnly ? READONLY_PERMISSION : READWRITE_PERMISSION);\n+        } catch (IOException e) {\n+            throw convertException(handle.getChunkName(), \"doSetReadOnly\", e);\n+        }\n+    }\n+\n+    //endregion\n+\n+    //region Storage Implementation\n+\n+    @SneakyThrows(IOException.class)\n+    public void initialize() {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        Preconditions.checkState(this.fileSystem == null, \"HDFSStorage has already been initialized.\");\n+        Configuration conf = new Configuration();\n+        conf.set(\"fs.default.name\", this.config.getHdfsHostURL());\n+        conf.set(\"fs.default.fs\", this.config.getHdfsHostURL());\n+        conf.set(\"fs.hdfs.impl\", \"org.apache.hadoop.hdfs.DistributedFileSystem\");\n+\n+        // We do not want FileSystem to cache clients/instances based on target URI.\n+        // This allows us to close instances without affecting other clients/instances. This should not affect performance.\n+        conf.set(\"fs.hdfs.impl.disable.cache\", \"true\");\n+        if (!this.config.isReplaceDataNodesOnFailure()) {\n+            // Default is DEFAULT, so we only set this if we want it disabled.\n+            conf.set(\"dfs.client.block.write.replace-datanode-on-failure.policy\", \"NEVER\");\n+        }\n+\n+        this.fileSystem = openFileSystem(conf);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "30225a38a4b6dde90ff3154ef14651097df8dd63"}, "originalPosition": 294}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTc0MTM5Mg==", "bodyText": "what happens in case of failure ? where is recovery handled ?", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r455741392", "createdAt": "2020-07-16T12:15:53Z", "author": {"login": "eolivelli"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -847,6 +857,11 @@ private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata, St\n                     concatUsingAppend(concatArgs);\n                 }\n \n+                // Delete chunks.\n+                for (int i = 1; i < chunksToConcat.size(); i++) {\n+                    chunksToDelete.add(chunksToConcat.get(i).getName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "30225a38a4b6dde90ff3154ef14651097df8dd63"}, "originalPosition": 45}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUwMjU3NDMx", "url": "https://github.com/pravega/pravega/pull/4699#pullrequestreview-450257431", "createdAt": "2020-07-16T22:24:49Z", "commit": {"oid": "30225a38a4b6dde90ff3154ef14651097df8dd63"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQyMjoyNDo0OVrOGy-4qA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNlQyMjoyNDo0OVrOGy-4qA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjExMjI5Ng==", "bodyText": "Ok. I see now.", "url": "https://github.com/pravega/pravega/pull/4699#discussion_r456112296", "createdAt": "2020-07-16T22:24:49Z", "author": {"login": "andreipaduroiu"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -0,0 +1,328 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.storage.extendeds3;\n+\n+import com.emc.object.Range;\n+import com.emc.object.s3.S3Client;\n+import com.emc.object.s3.S3Exception;\n+import com.emc.object.s3.S3ObjectMetadata;\n+import com.emc.object.s3.bean.AccessControlList;\n+import com.emc.object.s3.bean.CanonicalUser;\n+import com.emc.object.s3.bean.CopyPartResult;\n+import com.emc.object.s3.bean.Grant;\n+import com.emc.object.s3.bean.MultipartPartETag;\n+import com.emc.object.s3.bean.Permission;\n+import com.emc.object.s3.request.CompleteMultipartUploadRequest;\n+import com.emc.object.s3.request.CopyPartRequest;\n+import com.emc.object.s3.request.PutObjectRequest;\n+import com.emc.object.s3.request.SetObjectAclRequest;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.io.StreamHelpers;\n+import io.pravega.segmentstore.storage.chunklayer.BaseChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkAlreadyExistsException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkHandle;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkInfo;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkNotFoundException;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorage;\n+import io.pravega.segmentstore.storage.chunklayer.ChunkStorageException;\n+import io.pravega.segmentstore.storage.chunklayer.ConcatArgument;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.http.HttpStatus;\n+\n+import java.io.FileNotFoundException;\n+import java.io.InputStream;\n+\n+import java.util.SortedSet;\n+import java.util.TreeSet;\n+\n+/**\n+ * {@link ChunkStorage} for extended S3 based storage.\n+ *\n+ * Each chunk is represented as a single Object on the underlying storage.\n+ *\n+ * This implementation works under the assumption that data is only appended and never modified.\n+ * The concat operation is implemented as multi part copy.\n+ */\n+\n+@Slf4j\n+public class ExtendedS3ChunkStorage extends BaseChunkStorage {\n+\n+    //region members\n+    private final ExtendedS3StorageConfig config;\n+    private final S3Client client;\n+\n+    //endregion\n+\n+    //region constructor\n+    public ExtendedS3ChunkStorage(S3Client client, ExtendedS3StorageConfig config) {\n+        this.config = Preconditions.checkNotNull(config, \"config\");\n+        this.client = Preconditions.checkNotNull(client, \"client\");\n+    }\n+    //endregion\n+\n+    //region capabilities\n+\n+    @Override\n+    public boolean supportsConcat() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsAppend() {\n+        return true;\n+    }\n+\n+    @Override\n+    public boolean supportsTruncation() {\n+        return false;\n+    }\n+\n+    //endregion\n+\n+    //region implementation\n+\n+    @Override\n+    protected ChunkHandle doOpenRead(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.readHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected ChunkHandle doOpenWrite(String chunkName) throws ChunkStorageException, IllegalArgumentException {\n+        if (!checkExists(chunkName)) {\n+            throw new ChunkNotFoundException(chunkName, \"Chunk not found\");\n+        }\n+        return ChunkHandle.writeHandle(chunkName);\n+    }\n+\n+    @Override\n+    protected int doRead(ChunkHandle handle, long fromOffset, int length, byte[] buffer, int bufferOffset) throws ChunkStorageException, NullPointerException, IndexOutOfBoundsException {\n+        try {\n+            if (fromOffset < 0 || bufferOffset < 0 || length < 0) {\n+                throw new ArrayIndexOutOfBoundsException();\n+            }\n+\n+            try (InputStream reader = client.readObjectStream(config.getBucket(),\n+                    config.getPrefix() + handle.getChunkName(), Range.fromOffsetLength(fromOffset, length))) {\n+                if (reader == null) {\n+                    throw new ChunkNotFoundException(handle.getChunkName(), \"Chunk not found\");\n+                }\n+\n+                int bytesRead = StreamHelpers.readAll(reader, buffer, bufferOffset, length);\n+\n+                return bytesRead;\n+            }\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doRead\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    protected int doWrite(ChunkHandle handle, long offset, int length, InputStream data) throws ChunkStorageException, IndexOutOfBoundsException {\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be read-only.\");\n+        try {\n+            S3ObjectMetadata result = client.getObjectMetadata(config.getBucket(), config.getPrefix() + handle.getChunkName());\n+            client.putObject(this.config.getBucket(), this.config.getPrefix() + handle.getChunkName(),\n+                    Range.fromOffsetLength(offset, length), data);\n+            return length;\n+        } catch (Exception e) {\n+            throwException(handle.getChunkName(), \"doWrite\", e);\n+        }\n+        return 0;\n+    }\n+\n+    @Override\n+    public int doConcat(ConcatArgument[] chunks) throws ChunkStorageException, UnsupportedOperationException {\n+        int totalBytesConcated = 0;\n+        try {\n+            int partNumber = 1;\n+\n+            SortedSet<MultipartPartETag> partEtags = new TreeSet<>();\n+            String targetPath = config.getPrefix() + chunks[0].getName();\n+            String uploadId = client.initiateMultipartUpload(config.getBucket(), targetPath);\n+\n+            // check whether the target exists\n+            if (!checkExists(chunks[0].getName())) {\n+                throw new FileNotFoundException(chunks[0].getName());\n+            }\n+\n+            //Copy the parts\n+            for (int i = 0; i < chunks.length; i++) {\n+                if (0 != chunks[i].getLength()) {\n+                    val sourceHandle = chunks[i];\n+                    S3ObjectMetadata metadataResult = client.getObjectMetadata(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName());\n+                    long objectSize = metadataResult.getContentLength(); // in bytes\n+                    Preconditions.checkState(objectSize >= chunks[i].getLength());\n+                    CopyPartRequest copyRequest = new CopyPartRequest(config.getBucket(),\n+                            config.getPrefix() + sourceHandle.getName(),\n+                            config.getBucket(),\n+                            targetPath,\n+                            uploadId,\n+                            partNumber++).withSourceRange(Range.fromOffsetLength(0, chunks[i].getLength()));\n+\n+                    CopyPartResult copyResult = client.copyPart(copyRequest);\n+                    partEtags.add(new MultipartPartETag(copyResult.getPartNumber(), copyResult.getETag()));\n+                    totalBytesConcated += chunks[i].getLength();\n+                }\n+            }\n+\n+            //Close the upload\n+            client.completeMultipartUpload(new CompleteMultipartUploadRequest(config.getBucket(),\n+                    targetPath, uploadId).withParts(partEtags));\n+            // Delete all source objects.\n+            for (int i = 1; i < chunks.length; i++) {\n+                client.deleteObject(config.getBucket(), config.getPrefix() + chunks[i].getName());\n+            }\n+        } catch (RuntimeException e) {\n+            // Make spotbugs happy. Wants us to catch RuntimeException in a separate catch block.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzc5MjE1Mw=="}, "originalCommit": null, "originalPosition": 191}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "76d4c5537de5353dcdeaf5dd97afdd5aab313344", "author": {"user": {"login": "sachin-j-joshi", "name": "Sachin Jayant Joshi"}}, "url": "https://github.com/pravega/pravega/commit/76d4c5537de5353dcdeaf5dd97afdd5aab313344", "committedDate": "2020-07-17T18:16:49Z", "message": "Issue 4676: (PDP-34) Initial Implementation (Part 2 of 4) - More code coverage.\n\nSigned-off-by: Sachin Joshi <sachin.joshi@emc.com>"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3407, "cost": 1, "resetAt": "2021-11-01T16:37:27Z"}}}