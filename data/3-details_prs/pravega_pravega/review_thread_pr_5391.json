{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTMwNjcyMTQ5", "number": 5391, "reviewThreads": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QyMjoyMToyMFrOFAhe5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwOTowNzoxMVrOFDXafg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM2MDkyOTAzOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QyMjoyMToyMFrOH-3sRQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMTozOTo0NlrOH-89SA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY4NjIxMw==", "bodyText": "Why v and not vv?", "url": "https://github.com/pravega/pravega/pull/5391#discussion_r535686213", "createdAt": "2020-12-03T22:21:20Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -144,11 +147,29 @@\n                             CompletableFuture<Void> f;\n                             if (chunksToConcat.size() > 1) {\n                                 // Concat\n-                                f = concatChunks();\n+                                f = concatChunks()\n+                                .handleAsync((vv, ex) -> {\n+                                    if (null != ex) {\n+                                        ex = Exceptions.unwrap(ex);\n+                                        if (ex instanceof InvalidOffsetException) {\n+                                            // Skip ahead by 1 chunk.\n+                                            targetChunkName = chunksToConcat.get(1).getName();\n+                                            chunksToConcat.clear();\n+                                            skipFailed.set(true);\n+                                            return null;\n+                                        }\n+                                        throw new CompletionException(ex);\n+                                    }\n+                                    return v;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dd184163d1bd8bf67b0029a98a72e6fde751de94"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc3MjQ4OA==", "bodyText": "fixed", "url": "https://github.com/pravega/pravega/pull/5391#discussion_r535772488", "createdAt": "2020-12-04T01:39:46Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -144,11 +147,29 @@\n                             CompletableFuture<Void> f;\n                             if (chunksToConcat.size() > 1) {\n                                 // Concat\n-                                f = concatChunks();\n+                                f = concatChunks()\n+                                .handleAsync((vv, ex) -> {\n+                                    if (null != ex) {\n+                                        ex = Exceptions.unwrap(ex);\n+                                        if (ex instanceof InvalidOffsetException) {\n+                                            // Skip ahead by 1 chunk.\n+                                            targetChunkName = chunksToConcat.get(1).getName();\n+                                            chunksToConcat.clear();\n+                                            skipFailed.set(true);\n+                                            return null;\n+                                        }\n+                                        throw new CompletionException(ex);\n+                                    }\n+                                    return v;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY4NjIxMw=="}, "originalCommit": {"oid": "dd184163d1bd8bf67b0029a98a72e6fde751de94"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM2MDkzMDQ1OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QyMjoyMTo0MFrOH-3tCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMTo0MDoyM1rOH-8-Ow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY4NjQxMQ==", "bodyText": "You do not need Async here", "url": "https://github.com/pravega/pravega/pull/5391#discussion_r535686411", "createdAt": "2020-12-03T22:21:40Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -144,11 +147,29 @@\n                             CompletableFuture<Void> f;\n                             if (chunksToConcat.size() > 1) {\n                                 // Concat\n-                                f = concatChunks();\n+                                f = concatChunks()\n+                                .handleAsync((vv, ex) -> {\n+                                    if (null != ex) {\n+                                        ex = Exceptions.unwrap(ex);\n+                                        if (ex instanceof InvalidOffsetException) {\n+                                            // Skip ahead by 1 chunk.\n+                                            targetChunkName = chunksToConcat.get(1).getName();\n+                                            chunksToConcat.clear();\n+                                            skipFailed.set(true);\n+                                            return null;\n+                                        }\n+                                        throw new CompletionException(ex);\n+                                    }\n+                                    return v;\n+                                }, chunkedSegmentStorage.getExecutor());\n                             } else {\n                                 f = CompletableFuture.completedFuture(null);\n                             }\n                             return f.thenRunAsync(() -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dd184163d1bd8bf67b0029a98a72e6fde751de94"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc3MjczMQ==", "bodyText": "leaving it as it is - keeping the old pattern.", "url": "https://github.com/pravega/pravega/pull/5391#discussion_r535772731", "createdAt": "2020-12-04T01:40:23Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -144,11 +147,29 @@\n                             CompletableFuture<Void> f;\n                             if (chunksToConcat.size() > 1) {\n                                 // Concat\n-                                f = concatChunks();\n+                                f = concatChunks()\n+                                .handleAsync((vv, ex) -> {\n+                                    if (null != ex) {\n+                                        ex = Exceptions.unwrap(ex);\n+                                        if (ex instanceof InvalidOffsetException) {\n+                                            // Skip ahead by 1 chunk.\n+                                            targetChunkName = chunksToConcat.get(1).getName();\n+                                            chunksToConcat.clear();\n+                                            skipFailed.set(true);\n+                                            return null;\n+                                        }\n+                                        throw new CompletionException(ex);\n+                                    }\n+                                    return v;\n+                                }, chunkedSegmentStorage.getExecutor());\n                             } else {\n                                 f = CompletableFuture.completedFuture(null);\n                             }\n                             return f.thenRunAsync(() -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY4NjQxMQ=="}, "originalCommit": {"oid": "dd184163d1bd8bf67b0029a98a72e6fde751de94"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM2MDkzMTI0OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QyMjoyMTo1NVrOH-3tiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wNFQwMTo0MDozM1rOH-8-bA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY4NjUzNg==", "bodyText": "skipFailed.compareAndSet(true, false)", "url": "https://github.com/pravega/pravega/pull/5391#discussion_r535686536", "createdAt": "2020-12-03T22:21:55Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -144,11 +147,29 @@\n                             CompletableFuture<Void> f;\n                             if (chunksToConcat.size() > 1) {\n                                 // Concat\n-                                f = concatChunks();\n+                                f = concatChunks()\n+                                .handleAsync((vv, ex) -> {\n+                                    if (null != ex) {\n+                                        ex = Exceptions.unwrap(ex);\n+                                        if (ex instanceof InvalidOffsetException) {\n+                                            // Skip ahead by 1 chunk.\n+                                            targetChunkName = chunksToConcat.get(1).getName();\n+                                            chunksToConcat.clear();\n+                                            skipFailed.set(true);\n+                                            return null;\n+                                        }\n+                                        throw new CompletionException(ex);\n+                                    }\n+                                    return v;\n+                                }, chunkedSegmentStorage.getExecutor());\n                             } else {\n                                 f = CompletableFuture.completedFuture(null);\n                             }\n                             return f.thenRunAsync(() -> {\n+                                if (skipFailed.get()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dd184163d1bd8bf67b0029a98a72e6fde751de94"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc3Mjc4MA==", "bodyText": "fixed.", "url": "https://github.com/pravega/pravega/pull/5391#discussion_r535772780", "createdAt": "2020-12-04T01:40:33Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -144,11 +147,29 @@\n                             CompletableFuture<Void> f;\n                             if (chunksToConcat.size() > 1) {\n                                 // Concat\n-                                f = concatChunks();\n+                                f = concatChunks()\n+                                .handleAsync((vv, ex) -> {\n+                                    if (null != ex) {\n+                                        ex = Exceptions.unwrap(ex);\n+                                        if (ex instanceof InvalidOffsetException) {\n+                                            // Skip ahead by 1 chunk.\n+                                            targetChunkName = chunksToConcat.get(1).getName();\n+                                            chunksToConcat.clear();\n+                                            skipFailed.set(true);\n+                                            return null;\n+                                        }\n+                                        throw new CompletionException(ex);\n+                                    }\n+                                    return v;\n+                                }, chunkedSegmentStorage.getExecutor());\n                             } else {\n                                 f = CompletableFuture.completedFuture(null);\n                             }\n                             return f.thenRunAsync(() -> {\n+                                if (skipFailed.get()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTY4NjUzNg=="}, "originalCommit": {"oid": "dd184163d1bd8bf67b0029a98a72e6fde751de94"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4NzA4MTc5OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/test/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorageTests.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxNjozMDoyNVrOICdyXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwNDozMTozNlrOIC1DiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ1NjA5Mw==", "bodyText": "Please modify TestContext to implement AutoCloseable, and close any resources it uses in the close method. Then update all usages of this class to clean up (decorate with @Cleanup).", "url": "https://github.com/pravega/pravega/pull/5391#discussion_r539456093", "createdAt": "2020-12-09T16:30:25Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/test/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorageTests.java", "diffHunk": "@@ -1317,6 +1317,101 @@ public void testBaseConcatWithDefragWithMinMaxLimits() throws Exception {\n                 new long[]{10, 15});\n     }\n \n+    /**\n+     * Test Concat after repeated failure when concat using append mode is on.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    @Test\n+    public void testConcatUsingAppendsAfterWriteFailure() throws Exception {\n+        long maxRollingSize = 20;\n+        // Last chunk of target in these tests always has garbage at end which can not be overwritten.\n+        testConcatUsingAppendsAfterWriteFailure(maxRollingSize,\n+                new long[] {5},\n+                new long[] {1, 2, 3, 4},\n+                new int[] {},\n+                new long[] {5, 10},\n+                15);\n+\n+        // First chunk in source has garbage at the end.\n+        testConcatUsingAppendsAfterWriteFailure(maxRollingSize,\n+                new long[] {5},\n+                new long[] {1, 2, 3, 4},\n+                new int[]  {0}, // add garbage to these chunks\n+                new long[] {5, 1, 9},\n+                15);\n+\n+        // First two chunk in source has garbage at the end.\n+        testConcatUsingAppendsAfterWriteFailure(maxRollingSize,\n+                new long[] {5},\n+                new long[] {1, 2, 3, 4},\n+                new int[]  {0, 1},  // add garbage to these chunks\n+                new long[] {5, 1, 2, 7},\n+                15);\n+\n+        // First three chunks in source has garbage at the end.\n+        testConcatUsingAppendsAfterWriteFailure(maxRollingSize,\n+                new long[] {5},\n+                new long[] {1, 2, 3, 4},\n+                new int[]  {0, 1, 2},  // add garbage to these chunks\n+                new long[] {5, 1, 2, 3, 4},\n+                15);\n+\n+        // All chunks in source has garbage at the end.\n+        testConcatUsingAppendsAfterWriteFailure(maxRollingSize,\n+                new long[] {5},\n+                new long[] {1, 2, 3, 4},\n+                new int[]  {0, 1, 2, 3},  // add garbage to these chunks\n+                new long[] {5, 1, 2, 3, 4},\n+                15);\n+    }\n+\n+    private void testConcatUsingAppendsAfterWriteFailure(long maxRollingSize,\n+                                                         long[] targetLayoutBefore,\n+                                                         long[] sourceLayout,\n+                                                         int[] chunksWithGarbageIndex,\n+                                                         long[] targetLayoutAfter,\n+                                                         long expectedLength) throws Exception {\n+        String targetSegmentName = \"target\";\n+        String sourceSegmentName = \"source\";\n+        SegmentRollingPolicy policy = new SegmentRollingPolicy(maxRollingSize); // Force rollover after every 20 byte.\n+        ChunkedSegmentStorageConfig config = ChunkedSegmentStorageConfig.DEFAULT_CONFIG.toBuilder()\n+                .maxSizeLimitForConcat(100)\n+                .minSizeLimitForConcat(100)\n+                .build();\n+\n+        TestContext testContext = getTestContext(config);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a4d0a3615323b5edb0dd98b493157b3c36e5d5ba"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTcyMTg2OQ==", "bodyText": "I made these changes in other PR, once that PR is merged and pulled into this PR I'll make this change.\nThanks", "url": "https://github.com/pravega/pravega/pull/5391#discussion_r539721869", "createdAt": "2020-12-09T23:20:49Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/test/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorageTests.java", "diffHunk": "@@ -1317,6 +1317,101 @@ public void testBaseConcatWithDefragWithMinMaxLimits() throws Exception {\n                 new long[]{10, 15});\n     }\n \n+    /**\n+     * Test Concat after repeated failure when concat using append mode is on.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    @Test\n+    public void testConcatUsingAppendsAfterWriteFailure() throws Exception {\n+        long maxRollingSize = 20;\n+        // Last chunk of target in these tests always has garbage at end which can not be overwritten.\n+        testConcatUsingAppendsAfterWriteFailure(maxRollingSize,\n+                new long[] {5},\n+                new long[] {1, 2, 3, 4},\n+                new int[] {},\n+                new long[] {5, 10},\n+                15);\n+\n+        // First chunk in source has garbage at the end.\n+        testConcatUsingAppendsAfterWriteFailure(maxRollingSize,\n+                new long[] {5},\n+                new long[] {1, 2, 3, 4},\n+                new int[]  {0}, // add garbage to these chunks\n+                new long[] {5, 1, 9},\n+                15);\n+\n+        // First two chunk in source has garbage at the end.\n+        testConcatUsingAppendsAfterWriteFailure(maxRollingSize,\n+                new long[] {5},\n+                new long[] {1, 2, 3, 4},\n+                new int[]  {0, 1},  // add garbage to these chunks\n+                new long[] {5, 1, 2, 7},\n+                15);\n+\n+        // First three chunks in source has garbage at the end.\n+        testConcatUsingAppendsAfterWriteFailure(maxRollingSize,\n+                new long[] {5},\n+                new long[] {1, 2, 3, 4},\n+                new int[]  {0, 1, 2},  // add garbage to these chunks\n+                new long[] {5, 1, 2, 3, 4},\n+                15);\n+\n+        // All chunks in source has garbage at the end.\n+        testConcatUsingAppendsAfterWriteFailure(maxRollingSize,\n+                new long[] {5},\n+                new long[] {1, 2, 3, 4},\n+                new int[]  {0, 1, 2, 3},  // add garbage to these chunks\n+                new long[] {5, 1, 2, 3, 4},\n+                15);\n+    }\n+\n+    private void testConcatUsingAppendsAfterWriteFailure(long maxRollingSize,\n+                                                         long[] targetLayoutBefore,\n+                                                         long[] sourceLayout,\n+                                                         int[] chunksWithGarbageIndex,\n+                                                         long[] targetLayoutAfter,\n+                                                         long expectedLength) throws Exception {\n+        String targetSegmentName = \"target\";\n+        String sourceSegmentName = \"source\";\n+        SegmentRollingPolicy policy = new SegmentRollingPolicy(maxRollingSize); // Force rollover after every 20 byte.\n+        ChunkedSegmentStorageConfig config = ChunkedSegmentStorageConfig.DEFAULT_CONFIG.toBuilder()\n+                .maxSizeLimitForConcat(100)\n+                .minSizeLimitForConcat(100)\n+                .build();\n+\n+        TestContext testContext = getTestContext(config);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ1NjA5Mw=="}, "originalCommit": {"oid": "a4d0a3615323b5edb0dd98b493157b3c36e5d5ba"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTgzNzMyMQ==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/5391#discussion_r539837321", "createdAt": "2020-12-10T04:31:36Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/test/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorageTests.java", "diffHunk": "@@ -1317,6 +1317,101 @@ public void testBaseConcatWithDefragWithMinMaxLimits() throws Exception {\n                 new long[]{10, 15});\n     }\n \n+    /**\n+     * Test Concat after repeated failure when concat using append mode is on.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    @Test\n+    public void testConcatUsingAppendsAfterWriteFailure() throws Exception {\n+        long maxRollingSize = 20;\n+        // Last chunk of target in these tests always has garbage at end which can not be overwritten.\n+        testConcatUsingAppendsAfterWriteFailure(maxRollingSize,\n+                new long[] {5},\n+                new long[] {1, 2, 3, 4},\n+                new int[] {},\n+                new long[] {5, 10},\n+                15);\n+\n+        // First chunk in source has garbage at the end.\n+        testConcatUsingAppendsAfterWriteFailure(maxRollingSize,\n+                new long[] {5},\n+                new long[] {1, 2, 3, 4},\n+                new int[]  {0}, // add garbage to these chunks\n+                new long[] {5, 1, 9},\n+                15);\n+\n+        // First two chunk in source has garbage at the end.\n+        testConcatUsingAppendsAfterWriteFailure(maxRollingSize,\n+                new long[] {5},\n+                new long[] {1, 2, 3, 4},\n+                new int[]  {0, 1},  // add garbage to these chunks\n+                new long[] {5, 1, 2, 7},\n+                15);\n+\n+        // First three chunks in source has garbage at the end.\n+        testConcatUsingAppendsAfterWriteFailure(maxRollingSize,\n+                new long[] {5},\n+                new long[] {1, 2, 3, 4},\n+                new int[]  {0, 1, 2},  // add garbage to these chunks\n+                new long[] {5, 1, 2, 3, 4},\n+                15);\n+\n+        // All chunks in source has garbage at the end.\n+        testConcatUsingAppendsAfterWriteFailure(maxRollingSize,\n+                new long[] {5},\n+                new long[] {1, 2, 3, 4},\n+                new int[]  {0, 1, 2, 3},  // add garbage to these chunks\n+                new long[] {5, 1, 2, 3, 4},\n+                15);\n+    }\n+\n+    private void testConcatUsingAppendsAfterWriteFailure(long maxRollingSize,\n+                                                         long[] targetLayoutBefore,\n+                                                         long[] sourceLayout,\n+                                                         int[] chunksWithGarbageIndex,\n+                                                         long[] targetLayoutAfter,\n+                                                         long expectedLength) throws Exception {\n+        String targetSegmentName = \"target\";\n+        String sourceSegmentName = \"source\";\n+        SegmentRollingPolicy policy = new SegmentRollingPolicy(maxRollingSize); // Force rollover after every 20 byte.\n+        ChunkedSegmentStorageConfig config = ChunkedSegmentStorageConfig.DEFAULT_CONFIG.toBuilder()\n+                .maxSizeLimitForConcat(100)\n+                .minSizeLimitForConcat(100)\n+                .build();\n+\n+        TestContext testContext = getTestContext(config);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ1NjA5Mw=="}, "originalCommit": {"oid": "a4d0a3615323b5edb0dd98b493157b3c36e5d5ba"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5MDY5OTcwOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwODo1OToxMFrOIC-TAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxMzowNDowOVrOIDIQHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTk4ODczOA==", "bodyText": "I would like to get a bit more of context about this exception handling and what it really implies for the system. So, in which circumstances this InvalidOffsetException can be thrown? As this defragmentation process is revisiting the chunks written some time back, would it be a synonym of a DataCorruptionException in Tier 2? For instance, related to chunks partially written? Because if this InvalidOffsetException is a symptom of a real problem with LTS data, maybe we could use this offline process to alert that something is wrong with Tier 2 data and stop IO in Pravega until the problem is fixed. Without this kind of audit or verification, could we get InvalidOffsetException (or other exceptions of the sort) from LTS chunks and Pravega would continue processing data, right?", "url": "https://github.com/pravega/pravega/pull/5391#discussion_r539988738", "createdAt": "2020-12-10T08:59:10Z", "author": {"login": "RaulGracia"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -144,11 +147,28 @@\n                             CompletableFuture<Void> f;\n                             if (chunksToConcat.size() > 1) {\n                                 // Concat\n-                                f = concatChunks();\n+                                f = concatChunks()\n+                                .handleAsync((vv, ex) -> {\n+                                    if (null != ex) {\n+                                        ex = Exceptions.unwrap(ex);\n+                                        if (ex instanceof InvalidOffsetException) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3b6419bad8b6c6b49c688e848029f4a8120bc74"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDE1MTgzOQ==", "bodyText": "We accidently encountered this issue while running longevity runs with wrong config.\n\nIn SLTS concat operation is 3 step process - first we merge the chunk metadata list ,  then we defragment the chunks in that list and then finally commit all metadata changes.\nIn this particular instance, the defragment operation was successful but the transaction to commit metadata failed. (because of internal error with table segment code).\nThis resulted in last chunk of target segment having all data from chunks of source segment but metadata still had old information.\nWhen storage writer attempted the operation again, we found that for the last chunk of target segment the length on LTS did not match metadata and it correctly threw InvalidOffsetException\nThis results in unrecoverable error. SW can not make any progress and cache becomes full and everything comes to halt.\n\n\n\nNote that we DO NOT delete source chunks unless commit was successful.\n\n\nHowever this is a more generic failure mode and can happen for any of following reason\n\nprocess crashed while doing defrag\nsome I/O error happened while doing defrag\nthe segment store failed over and the zombie instance continued to write to last chunk. (perfectly valid scenario for SLTS)\nso on.\n\n\n\nSuch kind of error is not data corruption - this is a \"normal\" and valid circumstance .\n\nThe appropriate handling is to skip over the chunk with garbage data at the end.\nThis kind of situation should never block StorageWriter from making progress.", "url": "https://github.com/pravega/pravega/pull/5391#discussion_r540151839", "createdAt": "2020-12-10T13:04:09Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -144,11 +147,28 @@\n                             CompletableFuture<Void> f;\n                             if (chunksToConcat.size() > 1) {\n                                 // Concat\n-                                f = concatChunks();\n+                                f = concatChunks()\n+                                .handleAsync((vv, ex) -> {\n+                                    if (null != ex) {\n+                                        ex = Exceptions.unwrap(ex);\n+                                        if (ex instanceof InvalidOffsetException) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTk4ODczOA=="}, "originalCommit": {"oid": "e3b6419bad8b6c6b49c688e848029f4a8120bc74"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5MDcxOTI3OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/test/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorageTests.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwOTowMzoyOFrOIC-ejg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxMzowODowMVrOIDIZZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTk5MTY5NA==", "bodyText": "Is this validating the InvalidOffsetException, right? As it is not validating that such exception is thrown, it means that the  defragmentation will continue working as usual, skipping any number of such failures.", "url": "https://github.com/pravega/pravega/pull/5391#discussion_r539991694", "createdAt": "2020-12-10T09:03:28Z", "author": {"login": "RaulGracia"}, "path": "segmentstore/storage/src/test/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorageTests.java", "diffHunk": "@@ -1371,6 +1371,102 @@ public void testBaseConcatWithDefragWithMinMaxLimits() throws Exception {\n                 new long[]{10, 15});\n     }\n \n+    /**\n+     * Test Concat after repeated failure when concat using append mode is on.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    @Test\n+    public void testConcatUsingAppendsAfterWriteFailure() throws Exception {\n+        long maxRollingSize = 20;\n+        // Last chunk of target in these tests always has garbage at end which can not be overwritten.\n+        testConcatUsingAppendsAfterWriteFailure(maxRollingSize,\n+                new long[] {5},\n+                new long[] {1, 2, 3, 4},\n+                new int[] {},\n+                new long[] {5, 10},\n+                15);\n+\n+        // First chunk in source has garbage at the end.\n+        testConcatUsingAppendsAfterWriteFailure(maxRollingSize,\n+                new long[] {5},\n+                new long[] {1, 2, 3, 4},\n+                new int[]  {0}, // add garbage to these chunks\n+                new long[] {5, 1, 9},\n+                15);\n+\n+        // First two chunk in source has garbage at the end.\n+        testConcatUsingAppendsAfterWriteFailure(maxRollingSize,\n+                new long[] {5},\n+                new long[] {1, 2, 3, 4},\n+                new int[]  {0, 1},  // add garbage to these chunks\n+                new long[] {5, 1, 2, 7},\n+                15);\n+\n+        // First three chunks in source has garbage at the end.\n+        testConcatUsingAppendsAfterWriteFailure(maxRollingSize,\n+                new long[] {5},\n+                new long[] {1, 2, 3, 4},\n+                new int[]  {0, 1, 2},  // add garbage to these chunks\n+                new long[] {5, 1, 2, 3, 4},\n+                15);\n+\n+        // All chunks in source has garbage at the end.\n+        testConcatUsingAppendsAfterWriteFailure(maxRollingSize,\n+                new long[] {5},\n+                new long[] {1, 2, 3, 4},\n+                new int[]  {0, 1, 2, 3},  // add garbage to these chunks\n+                new long[] {5, 1, 2, 3, 4},\n+                15);\n+    }\n+\n+    private void testConcatUsingAppendsAfterWriteFailure(long maxRollingSize,\n+                                                         long[] targetLayoutBefore,\n+                                                         long[] sourceLayout,\n+                                                         int[] chunksWithGarbageIndex,\n+                                                         long[] targetLayoutAfter,\n+                                                         long expectedLength) throws Exception {\n+        String targetSegmentName = \"target\";\n+        String sourceSegmentName = \"source\";\n+        SegmentRollingPolicy policy = new SegmentRollingPolicy(maxRollingSize); // Force rollover after every 20 byte.\n+        ChunkedSegmentStorageConfig config = ChunkedSegmentStorageConfig.DEFAULT_CONFIG.toBuilder()\n+                .maxSizeLimitForConcat(100)\n+                .minSizeLimitForConcat(100)\n+                .build();\n+\n+        @Cleanup\n+        TestContext testContext = getTestContext(config);\n+        ((AbstractInMemoryChunkStorage) testContext.chunkStorage).setShouldSupportConcat(true);\n+\n+        // Create target\n+        testContext.insertMetadata(targetSegmentName, maxRollingSize, 1, targetLayoutBefore);\n+\n+        // Create source\n+        testContext.insertMetadata(sourceSegmentName, maxRollingSize, 1, sourceLayout);\n+        val hSource = testContext.chunkedSegmentStorage.openWrite(sourceSegmentName).get();\n+        testContext.chunkedSegmentStorage.seal(hSource, null).get();\n+\n+        // Add some garbage data at the end of last chunk\n+        val lastChunkMetadata = TestUtils.getChunkMetadata(testContext.metadataStore,\n+                TestUtils.getSegmentMetadata(testContext.metadataStore, targetSegmentName).getLastChunk());\n+        testContext.chunkStorage.write(ChunkHandle.writeHandle(lastChunkMetadata.getName()), lastChunkMetadata.getLength(), 1, new ByteArrayInputStream(new byte[1]));\n+\n+        // Write some garbage at the end.\n+        val sourceList = TestUtils.getChunkList(testContext.metadataStore, sourceSegmentName);\n+        for (int i : chunksWithGarbageIndex) {\n+            // Append some data to the last chunk to simulate partial write during failure\n+            val chunkMetadata = TestUtils.getChunkMetadata(testContext.metadataStore, sourceList.get(i).getName());\n+            testContext.chunkStorage.write(ChunkHandle.writeHandle(chunkMetadata.getName()), chunkMetadata.getLength(), 1, new ByteArrayInputStream(new byte[1]));\n+        }\n+        val hTarget = testContext.chunkedSegmentStorage.openWrite(targetSegmentName).get();\n+        testContext.chunkedSegmentStorage.concat(hTarget, 5, sourceSegmentName, null).join();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3b6419bad8b6c6b49c688e848029f4a8120bc74"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDE1NDIxNQ==", "bodyText": "Yes. Here we are writing additional garbage data to chunk directly via ChunkStorage without SLTS knowing about it. And therefore SLTS tries to write at the wrong offset.\nIn this case the ChunkStorage contract requires it to throw InvalidOffsetException when write operation is called by SLTS with wrong offset - which it throws.\nThis way the test recreates the exact situation we are trying to fix.", "url": "https://github.com/pravega/pravega/pull/5391#discussion_r540154215", "createdAt": "2020-12-10T13:08:01Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/test/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorageTests.java", "diffHunk": "@@ -1371,6 +1371,102 @@ public void testBaseConcatWithDefragWithMinMaxLimits() throws Exception {\n                 new long[]{10, 15});\n     }\n \n+    /**\n+     * Test Concat after repeated failure when concat using append mode is on.\n+     *\n+     * @throws Exception Exception if any.\n+     */\n+    @Test\n+    public void testConcatUsingAppendsAfterWriteFailure() throws Exception {\n+        long maxRollingSize = 20;\n+        // Last chunk of target in these tests always has garbage at end which can not be overwritten.\n+        testConcatUsingAppendsAfterWriteFailure(maxRollingSize,\n+                new long[] {5},\n+                new long[] {1, 2, 3, 4},\n+                new int[] {},\n+                new long[] {5, 10},\n+                15);\n+\n+        // First chunk in source has garbage at the end.\n+        testConcatUsingAppendsAfterWriteFailure(maxRollingSize,\n+                new long[] {5},\n+                new long[] {1, 2, 3, 4},\n+                new int[]  {0}, // add garbage to these chunks\n+                new long[] {5, 1, 9},\n+                15);\n+\n+        // First two chunk in source has garbage at the end.\n+        testConcatUsingAppendsAfterWriteFailure(maxRollingSize,\n+                new long[] {5},\n+                new long[] {1, 2, 3, 4},\n+                new int[]  {0, 1},  // add garbage to these chunks\n+                new long[] {5, 1, 2, 7},\n+                15);\n+\n+        // First three chunks in source has garbage at the end.\n+        testConcatUsingAppendsAfterWriteFailure(maxRollingSize,\n+                new long[] {5},\n+                new long[] {1, 2, 3, 4},\n+                new int[]  {0, 1, 2},  // add garbage to these chunks\n+                new long[] {5, 1, 2, 3, 4},\n+                15);\n+\n+        // All chunks in source has garbage at the end.\n+        testConcatUsingAppendsAfterWriteFailure(maxRollingSize,\n+                new long[] {5},\n+                new long[] {1, 2, 3, 4},\n+                new int[]  {0, 1, 2, 3},  // add garbage to these chunks\n+                new long[] {5, 1, 2, 3, 4},\n+                15);\n+    }\n+\n+    private void testConcatUsingAppendsAfterWriteFailure(long maxRollingSize,\n+                                                         long[] targetLayoutBefore,\n+                                                         long[] sourceLayout,\n+                                                         int[] chunksWithGarbageIndex,\n+                                                         long[] targetLayoutAfter,\n+                                                         long expectedLength) throws Exception {\n+        String targetSegmentName = \"target\";\n+        String sourceSegmentName = \"source\";\n+        SegmentRollingPolicy policy = new SegmentRollingPolicy(maxRollingSize); // Force rollover after every 20 byte.\n+        ChunkedSegmentStorageConfig config = ChunkedSegmentStorageConfig.DEFAULT_CONFIG.toBuilder()\n+                .maxSizeLimitForConcat(100)\n+                .minSizeLimitForConcat(100)\n+                .build();\n+\n+        @Cleanup\n+        TestContext testContext = getTestContext(config);\n+        ((AbstractInMemoryChunkStorage) testContext.chunkStorage).setShouldSupportConcat(true);\n+\n+        // Create target\n+        testContext.insertMetadata(targetSegmentName, maxRollingSize, 1, targetLayoutBefore);\n+\n+        // Create source\n+        testContext.insertMetadata(sourceSegmentName, maxRollingSize, 1, sourceLayout);\n+        val hSource = testContext.chunkedSegmentStorage.openWrite(sourceSegmentName).get();\n+        testContext.chunkedSegmentStorage.seal(hSource, null).get();\n+\n+        // Add some garbage data at the end of last chunk\n+        val lastChunkMetadata = TestUtils.getChunkMetadata(testContext.metadataStore,\n+                TestUtils.getSegmentMetadata(testContext.metadataStore, targetSegmentName).getLastChunk());\n+        testContext.chunkStorage.write(ChunkHandle.writeHandle(lastChunkMetadata.getName()), lastChunkMetadata.getLength(), 1, new ByteArrayInputStream(new byte[1]));\n+\n+        // Write some garbage at the end.\n+        val sourceList = TestUtils.getChunkList(testContext.metadataStore, sourceSegmentName);\n+        for (int i : chunksWithGarbageIndex) {\n+            // Append some data to the last chunk to simulate partial write during failure\n+            val chunkMetadata = TestUtils.getChunkMetadata(testContext.metadataStore, sourceList.get(i).getName());\n+            testContext.chunkStorage.write(ChunkHandle.writeHandle(chunkMetadata.getName()), chunkMetadata.getLength(), 1, new ByteArrayInputStream(new byte[1]));\n+        }\n+        val hTarget = testContext.chunkedSegmentStorage.openWrite(targetSegmentName).get();\n+        testContext.chunkedSegmentStorage.concat(hTarget, 5, sourceSegmentName, null).join();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTk5MTY5NA=="}, "originalCommit": {"oid": "e3b6419bad8b6c6b49c688e848029f4a8120bc74"}, "originalPosition": 92}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5MDcyNjgzOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwOTowNToxMFrOIC-jDw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxMzowOTozOVrOIDIdpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTk5Mjg0Nw==", "bodyText": "What other types of exceptions are expected here and why they are bubbled up (conversely to InvalidOffsetException)?", "url": "https://github.com/pravega/pravega/pull/5391#discussion_r539992847", "createdAt": "2020-12-10T09:05:10Z", "author": {"login": "RaulGracia"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -144,11 +147,28 @@\n                             CompletableFuture<Void> f;\n                             if (chunksToConcat.size() > 1) {\n                                 // Concat\n-                                f = concatChunks();\n+                                f = concatChunks()\n+                                .handleAsync((vv, ex) -> {\n+                                    if (null != ex) {\n+                                        ex = Exceptions.unwrap(ex);\n+                                        if (ex instanceof InvalidOffsetException) {\n+                                            // Skip ahead by 1 chunk.\n+                                            targetChunkName = chunksToConcat.get(1).getName();\n+                                            chunksToConcat.clear();\n+                                            skipFailed.set(true);\n+                                            return null;\n+                                        }\n+                                        throw new CompletionException(ex);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3b6419bad8b6c6b49c688e848029f4a8120bc74"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDE1NTMwMw==", "bodyText": "We are specifically handling only the InvalidOffsetException and nothing else.\nFor others we just bubble them up.", "url": "https://github.com/pravega/pravega/pull/5391#discussion_r540155303", "createdAt": "2020-12-10T13:09:39Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -144,11 +147,28 @@\n                             CompletableFuture<Void> f;\n                             if (chunksToConcat.size() > 1) {\n                                 // Concat\n-                                f = concatChunks();\n+                                f = concatChunks()\n+                                .handleAsync((vv, ex) -> {\n+                                    if (null != ex) {\n+                                        ex = Exceptions.unwrap(ex);\n+                                        if (ex instanceof InvalidOffsetException) {\n+                                            // Skip ahead by 1 chunk.\n+                                            targetChunkName = chunksToConcat.get(1).getName();\n+                                            chunksToConcat.clear();\n+                                            skipFailed.set(true);\n+                                            return null;\n+                                        }\n+                                        throw new CompletionException(ex);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTk5Mjg0Nw=="}, "originalCommit": {"oid": "e3b6419bad8b6c6b49c688e848029f4a8120bc74"}, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM5MDczNjYyOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQwOTowNzoxMlrOIC-orQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMFQxMzo1NDozM1rOIDKXLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTk5NDI4NQ==", "bodyText": "Do we need to add logs reporting that this exception has been captured? Otherwise, how are we going to realize that the defragmentation process is getting InvalidOffsetException?", "url": "https://github.com/pravega/pravega/pull/5391#discussion_r539994285", "createdAt": "2020-12-10T09:07:12Z", "author": {"login": "RaulGracia"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -144,11 +147,28 @@\n                             CompletableFuture<Void> f;\n                             if (chunksToConcat.size() > 1) {\n                                 // Concat\n-                                f = concatChunks();\n+                                f = concatChunks()\n+                                .handleAsync((vv, ex) -> {\n+                                    if (null != ex) {\n+                                        ex = Exceptions.unwrap(ex);\n+                                        if (ex instanceof InvalidOffsetException) {\n+                                            // Skip ahead by 1 chunk.\n+                                            targetChunkName = chunksToConcat.get(1).getName();\n+                                            chunksToConcat.clear();\n+                                            skipFailed.set(true);\n+                                            return null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3b6419bad8b6c6b49c688e848029f4a8120bc74"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDE1NTQ5Mw==", "bodyText": "Good suggestion. I'll add logs.", "url": "https://github.com/pravega/pravega/pull/5391#discussion_r540155493", "createdAt": "2020-12-10T13:10:00Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -144,11 +147,28 @@\n                             CompletableFuture<Void> f;\n                             if (chunksToConcat.size() > 1) {\n                                 // Concat\n-                                f = concatChunks();\n+                                f = concatChunks()\n+                                .handleAsync((vv, ex) -> {\n+                                    if (null != ex) {\n+                                        ex = Exceptions.unwrap(ex);\n+                                        if (ex instanceof InvalidOffsetException) {\n+                                            // Skip ahead by 1 chunk.\n+                                            targetChunkName = chunksToConcat.get(1).getName();\n+                                            chunksToConcat.clear();\n+                                            skipFailed.set(true);\n+                                            return null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTk5NDI4NQ=="}, "originalCommit": {"oid": "e3b6419bad8b6c6b49c688e848029f4a8120bc74"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDE4NjQxNQ==", "bodyText": "added", "url": "https://github.com/pravega/pravega/pull/5391#discussion_r540186415", "createdAt": "2020-12-10T13:54:33Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -144,11 +147,28 @@\n                             CompletableFuture<Void> f;\n                             if (chunksToConcat.size() > 1) {\n                                 // Concat\n-                                f = concatChunks();\n+                                f = concatChunks()\n+                                .handleAsync((vv, ex) -> {\n+                                    if (null != ex) {\n+                                        ex = Exceptions.unwrap(ex);\n+                                        if (ex instanceof InvalidOffsetException) {\n+                                            // Skip ahead by 1 chunk.\n+                                            targetChunkName = chunksToConcat.get(1).getName();\n+                                            chunksToConcat.clear();\n+                                            skipFailed.set(true);\n+                                            return null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTk5NDI4NQ=="}, "originalCommit": {"oid": "e3b6419bad8b6c6b49c688e848029f4a8120bc74"}, "originalPosition": 38}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4578, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}