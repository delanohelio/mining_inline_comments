{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDg4OTg1NjYy", "number": 5200, "reviewThreads": {"totalCount": 79, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQxNToxNDozOFrOElAw1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQyMDo0NzowMVrOE33j_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3MjQ1MjcxOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/AsyncBaseChunkStorage.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOFQxNToxNDozOFrOHURYSw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMVQyMjo1NzozN1rOHVlOIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTAxODMxNQ==", "bodyText": "why do you need to use \"executeAsync\" here ?\nwe are only performing validations/preconditions\nthe real important code to be async is doCreateAsync, there is no need to add an additional layer", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r491018315", "createdAt": "2020-09-18T15:14:38Z", "author": {"login": "eolivelli"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/AsyncBaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,676 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ *\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ *\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkedSegmentStorage will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ *\n+ * The implementations in this repository are tested using following test suites.\n+ * <ul>\n+ * <li>SimpleStorageTests</li>\n+ * <li>ChunkedRollingStorageTests</li>\n+ * <li>ChunkStorageProviderTests</li>\n+ * <li>SystemJournalTests</li>\n+ * </ul>\n+ */\n+@Slf4j\n+@Beta\n+public abstract class AsyncBaseChunkStorage implements ChunkStorage {\n+\n+    private final AtomicBoolean closed;\n+\n+    private final Executor executor;\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param executor  An Executor for async operations.\n+     */\n+    public AsyncBaseChunkStorage(Executor executor) {\n+        this.closed = new AtomicBoolean(false);\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public CompletableFuture<Boolean> exists(String chunkName) {\n+        return executeAsync(() -> {\n+            Exceptions.checkNotClosed(this.closed.get(), this);\n+            // Validate parameters\n+            checkChunkName(chunkName);\n+\n+            long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+            // Call concrete implementation.\n+            val returnFuture =  checkExistsAsync(chunkName);\n+            returnFuture.thenApplyAsync(retValue -> {\n+\n+                LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName);\n+\n+                return retValue;\n+            });\n+            return returnFuture;\n+        });\n+    }\n+\n+    /**\n+     * Creates a new chunk.\n+     *\n+     * @param chunkName Name of the chunk to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public CompletableFuture<ChunkHandle> create(String chunkName) {\n+        return executeAsync(() -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 140}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjM5MTk2OQ==", "bodyText": "Good catch. The original reason is no more valid. I removed the additional indirection.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r492391969", "createdAt": "2020-09-21T22:57:37Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/AsyncBaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,676 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ *\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ *\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkedSegmentStorage will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ *\n+ * The implementations in this repository are tested using following test suites.\n+ * <ul>\n+ * <li>SimpleStorageTests</li>\n+ * <li>ChunkedRollingStorageTests</li>\n+ * <li>ChunkStorageProviderTests</li>\n+ * <li>SystemJournalTests</li>\n+ * </ul>\n+ */\n+@Slf4j\n+@Beta\n+public abstract class AsyncBaseChunkStorage implements ChunkStorage {\n+\n+    private final AtomicBoolean closed;\n+\n+    private final Executor executor;\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param executor  An Executor for async operations.\n+     */\n+    public AsyncBaseChunkStorage(Executor executor) {\n+        this.closed = new AtomicBoolean(false);\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public CompletableFuture<Boolean> exists(String chunkName) {\n+        return executeAsync(() -> {\n+            Exceptions.checkNotClosed(this.closed.get(), this);\n+            // Validate parameters\n+            checkChunkName(chunkName);\n+\n+            long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+            // Call concrete implementation.\n+            val returnFuture =  checkExistsAsync(chunkName);\n+            returnFuture.thenApplyAsync(retValue -> {\n+\n+                LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName);\n+\n+                return retValue;\n+            });\n+            return returnFuture;\n+        });\n+    }\n+\n+    /**\n+     * Creates a new chunk.\n+     *\n+     * @param chunkName Name of the chunk to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public CompletableFuture<ChunkHandle> create(String chunkName) {\n+        return executeAsync(() -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTAxODMxNQ=="}, "originalCommit": null, "originalPosition": 140}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3NDgyMTY4OnYy", "diffSide": "RIGHT", "path": "config/config.properties", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOVQxMTowOTo0NVrOHUn29w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNDo1OTo0OFrOHrS_0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTM4NjYxNQ==", "bodyText": "Probably this is still not the right time to change this default value", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r491386615", "createdAt": "2020-09-19T11:09:45Z", "author": {"login": "eolivelli"}, "path": "config/config.properties", "diffHunk": "@@ -102,8 +102,8 @@ pravegaservice.dataLog.impl.name=BOOKKEEPER\n # Valid values:\n #   CHUNKED_STORAGE - Using ChunkedSegmentStorage.\n #   ROLLING_STORAGE - Using RollingStorage.\n-# Default value: ROLLING_STORAGE\n-# pravegaservice.storage.layout=ROLLING_STORAGE\n+# Default value: CHUNKED_STORAGE\n+# pravegaservice.storage.layout=CHUNKED_STORAGE", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg4NjYxNg==", "bodyText": "+1. @sachin-j-joshi Let's make sure this isn't accidentally turned on by default.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r499886616", "createdAt": "2020-10-05T21:37:31Z", "author": {"login": "andreipaduroiu"}, "path": "config/config.properties", "diffHunk": "@@ -102,8 +102,8 @@ pravegaservice.dataLog.impl.name=BOOKKEEPER\n # Valid values:\n #   CHUNKED_STORAGE - Using ChunkedSegmentStorage.\n #   ROLLING_STORAGE - Using RollingStorage.\n-# Default value: ROLLING_STORAGE\n-# pravegaservice.storage.layout=ROLLING_STORAGE\n+# Default value: CHUNKED_STORAGE\n+# pravegaservice.storage.layout=CHUNKED_STORAGE", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTM4NjYxNQ=="}, "originalCommit": null, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE2MjA2Nw==", "bodyText": "If we want to turn it on, we should do it once we are fully confident that SLTS is stable enough. At this point there are 2 outstanding PR to fix problems, so we shouldn't be enabling it by default until both are in. Let's revert this for now. We can enable it explicitly using another issue/PR.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r515162067", "createdAt": "2020-10-30T14:59:48Z", "author": {"login": "andreipaduroiu"}, "path": "config/config.properties", "diffHunk": "@@ -102,8 +102,8 @@ pravegaservice.dataLog.impl.name=BOOKKEEPER\n # Valid values:\n #   CHUNKED_STORAGE - Using ChunkedSegmentStorage.\n #   ROLLING_STORAGE - Using RollingStorage.\n-# Default value: ROLLING_STORAGE\n-# pravegaservice.storage.layout=ROLLING_STORAGE\n+# Default value: CHUNKED_STORAGE\n+# pravegaservice.storage.layout=CHUNKED_STORAGE", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTM4NjYxNQ=="}, "originalCommit": null, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3NDgyNTU3OnYy", "diffSide": "RIGHT", "path": "config/config.properties", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOVQxMToxMTowOVrOHUn5aQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQyMjo0ODoxNVrOHjRtqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTM4NzI0MQ==", "bodyText": "Why should the user be able to decide this policy?\nIf it is expected that an user can configure this behaviour then it is better to state clearly what are the pros and cons", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r491387241", "createdAt": "2020-09-19T11:11:09Z", "author": {"login": "eolivelli"}, "path": "config/config.properties", "diffHunk": "@@ -147,6 +147,24 @@ pravegaservice.dataLog.impl.name=BOOKKEEPER\n # Default value: true\n # storage.appends.enable=true\n \n+# Whether the lazy commit functionality is enabled.\n+# Valid values: true, false\n+# Default value: true\n+# storage.commit.lazy.enable=true", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg4NzIzNw==", "bodyText": "@sachin-j-joshi  Please add more documentation to these settings or at least point to some code Javadoc that already explains it (to avoid duplication). Like @eolivelli said, users may have no idea what these settings do so they wouldn't know how to set them properly.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r499887237", "createdAt": "2020-10-05T21:38:56Z", "author": {"login": "andreipaduroiu"}, "path": "config/config.properties", "diffHunk": "@@ -147,6 +147,24 @@ pravegaservice.dataLog.impl.name=BOOKKEEPER\n # Default value: true\n # storage.appends.enable=true\n \n+# Whether the lazy commit functionality is enabled.\n+# Valid values: true, false\n+# Default value: true\n+# storage.commit.lazy.enable=true", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTM4NzI0MQ=="}, "originalCommit": null, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc1MjQyNg==", "bodyText": "removed from config.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r506752426", "createdAt": "2020-10-16T22:48:15Z", "author": {"login": "sachin-j-joshi"}, "path": "config/config.properties", "diffHunk": "@@ -147,6 +147,24 @@ pravegaservice.dataLog.impl.name=BOOKKEEPER\n # Default value: true\n # storage.appends.enable=true\n \n+# Whether the lazy commit functionality is enabled.\n+# Valid values: true, false\n+# Default value: true\n+# storage.commit.lazy.enable=true", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTM4NzI0MQ=="}, "originalCommit": null, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3NDgzMDU3OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorageConfig.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOVQxMToxMzoyNlrOHUn85g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxOTo1MjoxMVrOHs-MUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTM4ODEzNA==", "bodyText": "What about an enum?", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r491388134", "createdAt": "2020-09-19T11:13:26Z", "author": {"login": "eolivelli"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorageConfig.java", "diffHunk": "@@ -100,13 +105,34 @@\n     @Getter\n     final private boolean appendEnabled;\n \n+    /**\n+     * Whether the lazy commit functionality is enabled or disabled.\n+     */\n+    @Getter\n+    final private boolean lazyCommitEnabled;\n+\n+    /**\n+     * Whether the inline defrag functionality is enabled or disabled.\n+     */\n+    @Getter\n+    final private boolean inlineDefragEnabled;\n+\n+    /**\n+     * Level of self check functionality enabled.\n+     */\n+    @Getter\n+    final private int selfCheckLevel;", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg5OTYzMQ==", "bodyText": "We should at least document (here too) what these numbers mean.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r499899631", "createdAt": "2020-10-05T22:09:01Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorageConfig.java", "diffHunk": "@@ -100,13 +105,34 @@\n     @Getter\n     final private boolean appendEnabled;\n \n+    /**\n+     * Whether the lazy commit functionality is enabled or disabled.\n+     */\n+    @Getter\n+    final private boolean lazyCommitEnabled;\n+\n+    /**\n+     * Whether the inline defrag functionality is enabled or disabled.\n+     */\n+    @Getter\n+    final private boolean inlineDefragEnabled;\n+\n+    /**\n+     * Level of self check functionality enabled.\n+     */\n+    @Getter\n+    final private int selfCheckLevel;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTM4ODEzNA=="}, "originalCommit": null, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjkxODM1NA==", "bodyText": "removed this config.\nIt is better to enable individual checks separately.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r516918354", "createdAt": "2020-11-03T19:52:11Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorageConfig.java", "diffHunk": "@@ -100,13 +105,34 @@\n     @Getter\n     final private boolean appendEnabled;\n \n+    /**\n+     * Whether the lazy commit functionality is enabled or disabled.\n+     */\n+    @Getter\n+    final private boolean lazyCommitEnabled;\n+\n+    /**\n+     * Whether the inline defrag functionality is enabled or disabled.\n+     */\n+    @Getter\n+    final private boolean inlineDefragEnabled;\n+\n+    /**\n+     * Level of self check functionality enabled.\n+     */\n+    @Getter\n+    final private int selfCheckLevel;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTM4ODEzNA=="}, "originalCommit": null, "originalPosition": 40}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA3NDgzMzE3OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xOVQxMToxNDozNFrOHUn-iw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNTo1NjozNVrOHrVgAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTM4ODU1NQ==", "bodyText": "Is it possible to add more context to the error? Like tx id...", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r491388555", "createdAt": "2020-09-19T11:14:34Z", "author": {"login": "eolivelli"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,177 +133,352 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     *                  throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     *            throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     *                  throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n+        final Map<String, TransactionData> txnData = txn.getData();\n \n-        Map<String, TransactionData> txnData = txn.getData();\n+        final ArrayList<String> modifiedKeys = new ArrayList<>();\n+        final ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+                    if (fenced.get()) {\n+                        throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIwMzA3NA==", "bodyText": "This is a global error and not transaction specific. It means this instance is fenced off and no more metadata modifications are allowed.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r515203074", "createdAt": "2020-10-30T15:56:35Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,177 +133,352 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     *                  throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     *            throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     *                  throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n+        final Map<String, TransactionData> txnData = txn.getData();\n \n-        Map<String, TransactionData> txnData = txn.getData();\n+        final ArrayList<String> modifiedKeys = new ArrayList<>();\n+        final ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+                    if (fenced.get()) {\n+                        throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MTM4ODU1NQ=="}, "originalCommit": null, "originalPosition": 213}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEwMDk5OTE5OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/AsyncBaseChunkStorage.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNlQxMDozMToyNlrOHYfcNg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNjoyNTo1MVrOHsLyMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTQ0Mjk5OA==", "bodyText": "In case of exception you aren't going to execute this callback.\nYou should use whenComplete (not whenCompleteAsync)\nand return the resulting CompletableFuture", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r495442998", "createdAt": "2020-09-26T10:31:26Z", "author": {"login": "eolivelli"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/AsyncBaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,637 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ *\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ *\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkedSegmentStorage will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ *\n+ * The implementations in this repository are tested using following test suites.\n+ * <ul>\n+ * <li>SimpleStorageTests</li>\n+ * <li>ChunkedRollingStorageTests</li>\n+ * <li>ChunkStorageProviderTests</li>\n+ * <li>SystemJournalTests</li>\n+ * </ul>\n+ */\n+@Slf4j\n+@Beta\n+public abstract class AsyncBaseChunkStorage implements ChunkStorage {\n+\n+    private final AtomicBoolean closed;\n+\n+    private final Executor executor;\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param executor  An Executor for async operations.\n+     */\n+    public AsyncBaseChunkStorage(Executor executor) {\n+        this.closed = new AtomicBoolean(false);\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public CompletableFuture<Boolean> exists(String chunkName) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        checkChunkName(chunkName);\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+        // Call concrete implementation.\n+        val returnFuture =  checkExistsAsync(chunkName);\n+        returnFuture.thenApplyAsync(retValue -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjE0NTYzNQ==", "bodyText": "This is simply following the existing pattern of not logging trace leave or metrics in case of failures.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r496145635", "createdAt": "2020-09-28T18:19:22Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/AsyncBaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,637 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ *\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ *\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkedSegmentStorage will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ *\n+ * The implementations in this repository are tested using following test suites.\n+ * <ul>\n+ * <li>SimpleStorageTests</li>\n+ * <li>ChunkedRollingStorageTests</li>\n+ * <li>ChunkStorageProviderTests</li>\n+ * <li>SystemJournalTests</li>\n+ * </ul>\n+ */\n+@Slf4j\n+@Beta\n+public abstract class AsyncBaseChunkStorage implements ChunkStorage {\n+\n+    private final AtomicBoolean closed;\n+\n+    private final Executor executor;\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param executor  An Executor for async operations.\n+     */\n+    public AsyncBaseChunkStorage(Executor executor) {\n+        this.closed = new AtomicBoolean(false);\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public CompletableFuture<Boolean> exists(String chunkName) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        checkChunkName(chunkName);\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+        // Call concrete implementation.\n+        val returnFuture =  checkExistsAsync(chunkName);\n+        returnFuture.thenApplyAsync(retValue -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTQ0Mjk5OA=="}, "originalCommit": null, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjIwMTE4NA==", "bodyText": "Makes sense.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r496201184", "createdAt": "2020-09-28T20:04:44Z", "author": {"login": "eolivelli"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/AsyncBaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,637 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ *\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ *\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkedSegmentStorage will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ *\n+ * The implementations in this repository are tested using following test suites.\n+ * <ul>\n+ * <li>SimpleStorageTests</li>\n+ * <li>ChunkedRollingStorageTests</li>\n+ * <li>ChunkStorageProviderTests</li>\n+ * <li>SystemJournalTests</li>\n+ * </ul>\n+ */\n+@Slf4j\n+@Beta\n+public abstract class AsyncBaseChunkStorage implements ChunkStorage {\n+\n+    private final AtomicBoolean closed;\n+\n+    private final Executor executor;\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param executor  An Executor for async operations.\n+     */\n+    public AsyncBaseChunkStorage(Executor executor) {\n+        this.closed = new AtomicBoolean(false);\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public CompletableFuture<Boolean> exists(String chunkName) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        checkChunkName(chunkName);\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+        // Call concrete implementation.\n+        val returnFuture =  checkExistsAsync(chunkName);\n+        returnFuture.thenApplyAsync(retValue -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTQ0Mjk5OA=="}, "originalCommit": null, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg4OTIxNw==", "bodyText": "Change it to thenAcceptAsync and only attach the callback if log.isTraceEnabled() is true.\nPlease make this change everywhere else you used this pattern.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r499889217", "createdAt": "2020-10-05T21:43:39Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/AsyncBaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,637 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ *\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ *\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkedSegmentStorage will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ *\n+ * The implementations in this repository are tested using following test suites.\n+ * <ul>\n+ * <li>SimpleStorageTests</li>\n+ * <li>ChunkedRollingStorageTests</li>\n+ * <li>ChunkStorageProviderTests</li>\n+ * <li>SystemJournalTests</li>\n+ * </ul>\n+ */\n+@Slf4j\n+@Beta\n+public abstract class AsyncBaseChunkStorage implements ChunkStorage {\n+\n+    private final AtomicBoolean closed;\n+\n+    private final Executor executor;\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param executor  An Executor for async operations.\n+     */\n+    public AsyncBaseChunkStorage(Executor executor) {\n+        this.closed = new AtomicBoolean(false);\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public CompletableFuture<Boolean> exists(String chunkName) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        checkChunkName(chunkName);\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+        // Call concrete implementation.\n+        val returnFuture =  checkExistsAsync(chunkName);\n+        returnFuture.thenApplyAsync(retValue -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTQ0Mjk5OA=="}, "originalCommit": null, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjA5MjQ2Nw==", "bodyText": "done.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r516092467", "createdAt": "2020-11-02T16:25:51Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/AsyncBaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,637 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ *\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ *\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkedSegmentStorage will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ *\n+ * The implementations in this repository are tested using following test suites.\n+ * <ul>\n+ * <li>SimpleStorageTests</li>\n+ * <li>ChunkedRollingStorageTests</li>\n+ * <li>ChunkStorageProviderTests</li>\n+ * <li>SystemJournalTests</li>\n+ * </ul>\n+ */\n+@Slf4j\n+@Beta\n+public abstract class AsyncBaseChunkStorage implements ChunkStorage {\n+\n+    private final AtomicBoolean closed;\n+\n+    private final Executor executor;\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param executor  An Executor for async operations.\n+     */\n+    public AsyncBaseChunkStorage(Executor executor) {\n+        this.closed = new AtomicBoolean(false);\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public CompletableFuture<Boolean> exists(String chunkName) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        checkChunkName(chunkName);\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+        // Call concrete implementation.\n+        val returnFuture =  checkExistsAsync(chunkName);\n+        returnFuture.thenApplyAsync(retValue -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NTQ0Mjk5OA=="}, "originalCommit": null, "originalPosition": 120}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTkwNTM0OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/StreamSegmentContainer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMTozOTozN1rOHcutkg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQyMjo0NzozNFrOHjRtDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg4NzUwNg==", "bodyText": "supply executor to this call.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r499887506", "createdAt": "2020-10-05T21:39:37Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/StreamSegmentContainer.java", "diffHunk": "@@ -279,11 +280,11 @@ protected void doStart() {\n \n     private CompletableFuture<Void> initializeSecondaryServices() {\n         try {\n-            initializeStorage();\n+            return initializeStorage()\n+                    .thenComposeAsync(v -> this.metadataStore.initialize(this.config.getMetadataStoreInitTimeout()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc1MjI2OQ==", "bodyText": "done.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r506752269", "createdAt": "2020-10-16T22:47:34Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/StreamSegmentContainer.java", "diffHunk": "@@ -279,11 +280,11 @@ protected void doStart() {\n \n     private CompletableFuture<Void> initializeSecondaryServices() {\n         try {\n-            initializeStorage();\n+            return initializeStorage()\n+                    .thenComposeAsync(v -> this.metadataStore.initialize(this.config.getMetadataStoreInitTimeout()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg4NzUwNg=="}, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTkxMDU2OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/store/ServiceConfig.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMTo0MTozNlrOHcuw2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQyMjo1MTowNFrOHxhJ-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg4ODM0Ng==", "bodyText": "Same here.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r499888346", "createdAt": "2020-10-05T21:41:36Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/store/ServiceConfig.java", "diffHunk": "@@ -54,7 +54,7 @@\n     public static final Property<String> CLUSTER_NAME = Property.named(\"clusterName\", \"pravega-cluster\");\n     public static final Property<DataLogType> DATALOG_IMPLEMENTATION = Property.named(\"dataLog.impl.name\", DataLogType.INMEMORY, \"dataLogImplementation\");\n     public static final Property<StorageType> STORAGE_IMPLEMENTATION = Property.named(\"storage.impl.name\", StorageType.HDFS, \"storageImplementation\");\n-    public static final Property<StorageLayoutType> STORAGE_LAYOUT = Property.named(\"storage.layout\", StorageLayoutType.ROLLING_STORAGE);\n+    public static final Property<StorageLayoutType> STORAGE_LAYOUT = Property.named(\"storage.layout\", StorageLayoutType.CHUNKED_STORAGE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTY4NTQ5Ng==", "bodyText": "fixed.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r521685496", "createdAt": "2020-11-11T22:51:04Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/store/ServiceConfig.java", "diffHunk": "@@ -54,7 +54,7 @@\n     public static final Property<String> CLUSTER_NAME = Property.named(\"clusterName\", \"pravega-cluster\");\n     public static final Property<DataLogType> DATALOG_IMPLEMENTATION = Property.named(\"dataLog.impl.name\", DataLogType.INMEMORY, \"dataLogImplementation\");\n     public static final Property<StorageType> STORAGE_IMPLEMENTATION = Property.named(\"storage.impl.name\", StorageType.HDFS, \"storageImplementation\");\n-    public static final Property<StorageLayoutType> STORAGE_LAYOUT = Property.named(\"storage.layout\", StorageLayoutType.ROLLING_STORAGE);\n+    public static final Property<StorageLayoutType> STORAGE_LAYOUT = Property.named(\"storage.layout\", StorageLayoutType.CHUNKED_STORAGE);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg4ODM0Ng=="}, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTkyMDg2OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/AsyncBaseChunkStorage.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMTo0NToxNFrOHcu29A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQyMjo0ODo0NFrOHjRuEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg4OTkwOA==", "bodyText": "Several calls in this class have extra spaces.  I see this in exists too .", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r499889908", "createdAt": "2020-10-05T21:45:14Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/AsyncBaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,637 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ *\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ *\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkedSegmentStorage will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ *\n+ * The implementations in this repository are tested using following test suites.\n+ * <ul>\n+ * <li>SimpleStorageTests</li>\n+ * <li>ChunkedRollingStorageTests</li>\n+ * <li>ChunkStorageProviderTests</li>\n+ * <li>SystemJournalTests</li>\n+ * </ul>\n+ */\n+@Slf4j\n+@Beta\n+public abstract class AsyncBaseChunkStorage implements ChunkStorage {\n+\n+    private final AtomicBoolean closed;\n+\n+    private final Executor executor;\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param executor  An Executor for async operations.\n+     */\n+    public AsyncBaseChunkStorage(Executor executor) {\n+        this.closed = new AtomicBoolean(false);\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public CompletableFuture<Boolean> exists(String chunkName) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        checkChunkName(chunkName);\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+        // Call concrete implementation.\n+        val returnFuture =  checkExistsAsync(chunkName);\n+        returnFuture.thenApplyAsync(retValue -> {\n+            LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName);\n+            return retValue;\n+        }, executor);\n+\n+        return returnFuture;\n+    }\n+\n+    /**\n+     * Creates a new chunk.\n+     *\n+     * @param chunkName Name of the chunk to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public CompletableFuture<ChunkHandle> create(String chunkName) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        checkChunkName(chunkName);\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"create\", chunkName);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        val returnFuture =   doCreateAsync(chunkName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 145}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc1MjUyOA==", "bodyText": "fixed", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r506752528", "createdAt": "2020-10-16T22:48:44Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/AsyncBaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,637 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ *\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ *\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkedSegmentStorage will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ *\n+ * The implementations in this repository are tested using following test suites.\n+ * <ul>\n+ * <li>SimpleStorageTests</li>\n+ * <li>ChunkedRollingStorageTests</li>\n+ * <li>ChunkStorageProviderTests</li>\n+ * <li>SystemJournalTests</li>\n+ * </ul>\n+ */\n+@Slf4j\n+@Beta\n+public abstract class AsyncBaseChunkStorage implements ChunkStorage {\n+\n+    private final AtomicBoolean closed;\n+\n+    private final Executor executor;\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param executor  An Executor for async operations.\n+     */\n+    public AsyncBaseChunkStorage(Executor executor) {\n+        this.closed = new AtomicBoolean(false);\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public CompletableFuture<Boolean> exists(String chunkName) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        checkChunkName(chunkName);\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+        // Call concrete implementation.\n+        val returnFuture =  checkExistsAsync(chunkName);\n+        returnFuture.thenApplyAsync(retValue -> {\n+            LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName);\n+            return retValue;\n+        }, executor);\n+\n+        return returnFuture;\n+    }\n+\n+    /**\n+     * Creates a new chunk.\n+     *\n+     * @param chunkName Name of the chunk to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public CompletableFuture<ChunkHandle> create(String chunkName) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        checkChunkName(chunkName);\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"create\", chunkName);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        val returnFuture =   doCreateAsync(chunkName);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg4OTkwOA=="}, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 145}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTkyMjEwOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/AsyncBaseChunkStorage.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMTo0NTo0NFrOHcu3yQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQyMjo0ODo1NFrOHjRuLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg5MDEyMQ==", "bodyText": "thenAccept so you don't have to return something.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r499890121", "createdAt": "2020-10-05T21:45:44Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/AsyncBaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,637 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ *\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ *\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkedSegmentStorage will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ *\n+ * The implementations in this repository are tested using following test suites.\n+ * <ul>\n+ * <li>SimpleStorageTests</li>\n+ * <li>ChunkedRollingStorageTests</li>\n+ * <li>ChunkStorageProviderTests</li>\n+ * <li>SystemJournalTests</li>\n+ * </ul>\n+ */\n+@Slf4j\n+@Beta\n+public abstract class AsyncBaseChunkStorage implements ChunkStorage {\n+\n+    private final AtomicBoolean closed;\n+\n+    private final Executor executor;\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param executor  An Executor for async operations.\n+     */\n+    public AsyncBaseChunkStorage(Executor executor) {\n+        this.closed = new AtomicBoolean(false);\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public CompletableFuture<Boolean> exists(String chunkName) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        checkChunkName(chunkName);\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+        // Call concrete implementation.\n+        val returnFuture =  checkExistsAsync(chunkName);\n+        returnFuture.thenApplyAsync(retValue -> {\n+            LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName);\n+            return retValue;\n+        }, executor);\n+\n+        return returnFuture;\n+    }\n+\n+    /**\n+     * Creates a new chunk.\n+     *\n+     * @param chunkName Name of the chunk to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public CompletableFuture<ChunkHandle> create(String chunkName) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        checkChunkName(chunkName);\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"create\", chunkName);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        val returnFuture =   doCreateAsync(chunkName);\n+        returnFuture.thenApplyAsync(handle -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc1MjU1Ng==", "bodyText": "done.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r506752556", "createdAt": "2020-10-16T22:48:54Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/AsyncBaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,637 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ *\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ *\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkedSegmentStorage will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ *\n+ * The implementations in this repository are tested using following test suites.\n+ * <ul>\n+ * <li>SimpleStorageTests</li>\n+ * <li>ChunkedRollingStorageTests</li>\n+ * <li>ChunkStorageProviderTests</li>\n+ * <li>SystemJournalTests</li>\n+ * </ul>\n+ */\n+@Slf4j\n+@Beta\n+public abstract class AsyncBaseChunkStorage implements ChunkStorage {\n+\n+    private final AtomicBoolean closed;\n+\n+    private final Executor executor;\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param executor  An Executor for async operations.\n+     */\n+    public AsyncBaseChunkStorage(Executor executor) {\n+        this.closed = new AtomicBoolean(false);\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public CompletableFuture<Boolean> exists(String chunkName) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        checkChunkName(chunkName);\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+        // Call concrete implementation.\n+        val returnFuture =  checkExistsAsync(chunkName);\n+        returnFuture.thenApplyAsync(retValue -> {\n+            LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName);\n+            return retValue;\n+        }, executor);\n+\n+        return returnFuture;\n+    }\n+\n+    /**\n+     * Creates a new chunk.\n+     *\n+     * @param chunkName Name of the chunk to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public CompletableFuture<ChunkHandle> create(String chunkName) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        checkChunkName(chunkName);\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"create\", chunkName);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        val returnFuture =   doCreateAsync(chunkName);\n+        returnFuture.thenApplyAsync(handle -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg5MDEyMQ=="}, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 146}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTkyNjYzOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/AsyncBaseChunkStorage.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMTo0NzoyM1rOHcu6mQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNjoyNjoxNFrOHsLzSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg5MDg0MQ==", "bodyText": "only attach this callback if log.isTraceEnabled is true. Otherwise it will be an expensive operation for no gain.\nPS: for something this small, no need to specify an executor. It may be more efficient to simply execute this on the same thread pool as whatever it's attached to, which will likely invoke it synchronously as well.\nPlease change this elsewhere too.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r499890841", "createdAt": "2020-10-05T21:47:23Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/AsyncBaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,637 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ *\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ *\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkedSegmentStorage will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ *\n+ * The implementations in this repository are tested using following test suites.\n+ * <ul>\n+ * <li>SimpleStorageTests</li>\n+ * <li>ChunkedRollingStorageTests</li>\n+ * <li>ChunkStorageProviderTests</li>\n+ * <li>SystemJournalTests</li>\n+ * </ul>\n+ */\n+@Slf4j\n+@Beta\n+public abstract class AsyncBaseChunkStorage implements ChunkStorage {\n+\n+    private final AtomicBoolean closed;\n+\n+    private final Executor executor;\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param executor  An Executor for async operations.\n+     */\n+    public AsyncBaseChunkStorage(Executor executor) {\n+        this.closed = new AtomicBoolean(false);\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public CompletableFuture<Boolean> exists(String chunkName) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        checkChunkName(chunkName);\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+        // Call concrete implementation.\n+        val returnFuture =  checkExistsAsync(chunkName);\n+        returnFuture.thenApplyAsync(retValue -> {\n+            LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName);\n+            return retValue;\n+        }, executor);\n+\n+        return returnFuture;\n+    }\n+\n+    /**\n+     * Creates a new chunk.\n+     *\n+     * @param chunkName Name of the chunk to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public CompletableFuture<ChunkHandle> create(String chunkName) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        checkChunkName(chunkName);\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"create\", chunkName);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        val returnFuture =   doCreateAsync(chunkName);\n+        returnFuture.thenApplyAsync(handle -> {\n+            // Record metrics.\n+            Duration elapsed = timer.getElapsed();\n+            ChunkStorageMetrics.CREATE_LATENCY.reportSuccessEvent(elapsed);\n+            ChunkStorageMetrics.CREATE_COUNT.inc();\n+\n+            log.debug(\"Create - chunk={}, latency={}.\", chunkName, elapsed.toMillis());\n+            LoggerHelpers.traceLeave(log, \"create\", traceId, chunkName);\n+\n+            return handle;\n+        }, executor);\n+\n+        return returnFuture;\n+    }\n+\n+    /**\n+     * Deletes a chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to delete.\n+     * throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public CompletableFuture<Void> delete(ChunkHandle handle) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        checkChunkName(handle.getChunkName());\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle.getChunkName());\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        val returnFuture = doDeleteAsync(handle);\n+        returnFuture.thenApplyAsync(v -> {\n+            // Record metrics.\n+            Duration elapsed = timer.getElapsed();\n+            ChunkStorageMetrics.DELETE_LATENCY.reportSuccessEvent(elapsed);\n+            ChunkStorageMetrics.DELETE_COUNT.inc();\n+\n+            log.debug(\"Delete - chunk={}, latency={}.\", handle.getChunkName(), elapsed.toMillis());\n+            LoggerHelpers.traceLeave(log, \"delete\", traceId, handle.getChunkName());\n+            return null;\n+        }, executor);\n+\n+        return returnFuture;\n+    }\n+\n+    /**\n+     * Opens chunk for Read.\n+     *\n+     * @param chunkName String name of the chunk to read from.\n+     * @return ChunkHandle A readable handle for the given chunk.\n+     * throws ChunkStorageException    Throws ChunkStorageException in case of I/O related exceptions.\n+     * throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public CompletableFuture<ChunkHandle> openRead(String chunkName) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        checkChunkName(chunkName);\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openRead\", chunkName);\n+\n+        // Call concrete implementation.\n+        val returnFuture = doOpenReadAsync(chunkName);\n+        returnFuture.thenApplyAsync(handle -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 212}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjA5Mjc0Nw==", "bodyText": "done", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r516092747", "createdAt": "2020-11-02T16:26:14Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/AsyncBaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,637 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.time.Duration;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ *\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ *\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkedSegmentStorage will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ *\n+ * The implementations in this repository are tested using following test suites.\n+ * <ul>\n+ * <li>SimpleStorageTests</li>\n+ * <li>ChunkedRollingStorageTests</li>\n+ * <li>ChunkStorageProviderTests</li>\n+ * <li>SystemJournalTests</li>\n+ * </ul>\n+ */\n+@Slf4j\n+@Beta\n+public abstract class AsyncBaseChunkStorage implements ChunkStorage {\n+\n+    private final AtomicBoolean closed;\n+\n+    private final Executor executor;\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param executor  An Executor for async operations.\n+     */\n+    public AsyncBaseChunkStorage(Executor executor) {\n+        this.closed = new AtomicBoolean(false);\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return True if the object exists, false otherwise.\n+     */\n+    @Override\n+    final public CompletableFuture<Boolean> exists(String chunkName) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        checkChunkName(chunkName);\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+        // Call concrete implementation.\n+        val returnFuture =  checkExistsAsync(chunkName);\n+        returnFuture.thenApplyAsync(retValue -> {\n+            LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName);\n+            return retValue;\n+        }, executor);\n+\n+        return returnFuture;\n+    }\n+\n+    /**\n+     * Creates a new chunk.\n+     *\n+     * @param chunkName Name of the chunk to create.\n+     * @return ChunkHandle A writable handle for the recently created chunk.\n+     * throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public CompletableFuture<ChunkHandle> create(String chunkName) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        checkChunkName(chunkName);\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"create\", chunkName);\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        val returnFuture =   doCreateAsync(chunkName);\n+        returnFuture.thenApplyAsync(handle -> {\n+            // Record metrics.\n+            Duration elapsed = timer.getElapsed();\n+            ChunkStorageMetrics.CREATE_LATENCY.reportSuccessEvent(elapsed);\n+            ChunkStorageMetrics.CREATE_COUNT.inc();\n+\n+            log.debug(\"Create - chunk={}, latency={}.\", chunkName, elapsed.toMillis());\n+            LoggerHelpers.traceLeave(log, \"create\", traceId, chunkName);\n+\n+            return handle;\n+        }, executor);\n+\n+        return returnFuture;\n+    }\n+\n+    /**\n+     * Deletes a chunk.\n+     *\n+     * @param handle ChunkHandle of the chunk to delete.\n+     * throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     */\n+    @Override\n+    final public CompletableFuture<Void> delete(ChunkHandle handle) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        Preconditions.checkArgument(null != handle, \"handle must not be null\");\n+        checkChunkName(handle.getChunkName());\n+        Preconditions.checkArgument(!handle.isReadOnly(), \"handle must not be readonly\");\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle.getChunkName());\n+        Timer timer = new Timer();\n+\n+        // Call concrete implementation.\n+        val returnFuture = doDeleteAsync(handle);\n+        returnFuture.thenApplyAsync(v -> {\n+            // Record metrics.\n+            Duration elapsed = timer.getElapsed();\n+            ChunkStorageMetrics.DELETE_LATENCY.reportSuccessEvent(elapsed);\n+            ChunkStorageMetrics.DELETE_COUNT.inc();\n+\n+            log.debug(\"Delete - chunk={}, latency={}.\", handle.getChunkName(), elapsed.toMillis());\n+            LoggerHelpers.traceLeave(log, \"delete\", traceId, handle.getChunkName());\n+            return null;\n+        }, executor);\n+\n+        return returnFuture;\n+    }\n+\n+    /**\n+     * Opens chunk for Read.\n+     *\n+     * @param chunkName String name of the chunk to read from.\n+     * @return ChunkHandle A readable handle for the given chunk.\n+     * throws ChunkStorageException    Throws ChunkStorageException in case of I/O related exceptions.\n+     * throws IllegalArgumentException If argument is invalid.\n+     */\n+    @Override\n+    final public CompletableFuture<ChunkHandle> openRead(String chunkName) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        checkChunkName(chunkName);\n+\n+        long traceId = LoggerHelpers.traceEnter(log, \"openRead\", chunkName);\n+\n+        // Call concrete implementation.\n+        val returnFuture = doOpenReadAsync(chunkName);\n+        returnFuture.thenApplyAsync(handle -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg5MDg0MQ=="}, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 212}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTkzNjY2OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorage.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMTo1MDo1NlrOHcvAlg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQyMjo0OToxNVrOHjRunQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg5MjM3NA==", "bodyText": "It is true that Javadoc doesn't have a tag for CompletableFuture exceptions, but throughout the rest of our codebase we have (tried to) adopt the following convention:\n@return a CompletableFuture that will be completed with the result. If the operation failed, it will be completed with the appropriate exception. Notable Exceptions:\n* {@link Exception1} ....\n* {@link Exception2) ...\n\nLet's try to follow this pattern here too.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r499892374", "createdAt": "2020-10-05T21:50:56Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorage.java", "diffHunk": "@@ -72,56 +73,56 @@\n      *\n      * @param chunkName Name of the storage object to check.\n      * @return True if the object exists, false otherwise.\n-     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc1MjY2OQ==", "bodyText": "done.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r506752669", "createdAt": "2020-10-16T22:49:15Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorage.java", "diffHunk": "@@ -72,56 +73,56 @@\n      *\n      * @param chunkName Name of the storage object to check.\n      * @return True if the object exists, false otherwise.\n-     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg5MjM3NA=="}, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTk0MTg5OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMTo1MzowNlrOHcvD-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwMDozNzo0NFrOHtFTTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg5MzI0Mg==", "bodyText": "Isn't there something in Futures that can help with this syntax? Futures.exceptionallyExpecting or similar?", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r499893242", "createdAt": "2020-10-05T21:53:06Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -195,28 +199,39 @@ public void initialize(long containerEpoch) {\n     @Override\n     public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n         checkInitialized();\n-        return execute(() -> {\n+        return executeAsync(() -> {\n             long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n             Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n-            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n-                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n-                checkSegmentExists(streamSegmentName, segmentMetadata);\n-                segmentMetadata.checkInvariants();\n-                // This segment was created by an older segment store. Need to start a fresh new chunk.\n-                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n-                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n-                    claimOwnership(txn, segmentMetadata);\n-                }\n-                // If created by newer instance then abort.\n-                checkOwnership(streamSegmentName, segmentMetadata);\n-\n-                // This instance is the owner, return a handle.\n-                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n-                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n-                return retValue;\n-            } catch (StorageMetadataWritesFencedOutException ex) {\n-                throw new StorageNotPrimaryException(streamSegmentName, ex);\n-            }\n+            return tryWith(metadataStore.beginTransaction(streamSegmentName),\n+                    txn -> txn.get(streamSegmentName)\n+                            .thenComposeAsync(storageMetadata -> {\n+                                SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                                segmentMetadata.checkInvariants();\n+                                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                                CompletableFuture<Void> f = CompletableFuture.completedFuture(null);\n+                                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                                    f = claimOwnership(txn, segmentMetadata)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 113}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAzNDgzMQ==", "bodyText": "Expected behavior slightly different, I'm not sure those two are exactly equivalent.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r517034831", "createdAt": "2020-11-04T00:37:44Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -195,28 +199,39 @@ public void initialize(long containerEpoch) {\n     @Override\n     public CompletableFuture<SegmentHandle> openWrite(String streamSegmentName) {\n         checkInitialized();\n-        return execute(() -> {\n+        return executeAsync(() -> {\n             long traceId = LoggerHelpers.traceEnter(log, \"openWrite\", streamSegmentName);\n             Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n-            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n-                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n-                checkSegmentExists(streamSegmentName, segmentMetadata);\n-                segmentMetadata.checkInvariants();\n-                // This segment was created by an older segment store. Need to start a fresh new chunk.\n-                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n-                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n-                    claimOwnership(txn, segmentMetadata);\n-                }\n-                // If created by newer instance then abort.\n-                checkOwnership(streamSegmentName, segmentMetadata);\n-\n-                // This instance is the owner, return a handle.\n-                val retValue = SegmentStorageHandle.writeHandle(streamSegmentName);\n-                LoggerHelpers.traceLeave(log, \"openWrite\", traceId, retValue);\n-                return retValue;\n-            } catch (StorageMetadataWritesFencedOutException ex) {\n-                throw new StorageNotPrimaryException(streamSegmentName, ex);\n-            }\n+            return tryWith(metadataStore.beginTransaction(streamSegmentName),\n+                    txn -> txn.get(streamSegmentName)\n+                            .thenComposeAsync(storageMetadata -> {\n+                                SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                                segmentMetadata.checkInvariants();\n+                                // This segment was created by an older segment store. Need to start a fresh new chunk.\n+                                CompletableFuture<Void> f = CompletableFuture.completedFuture(null);\n+                                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                                    log.debug(\"{} openWrite - Segment needs ownership change - segment={}.\", logPrefix, segmentMetadata.getName());\n+                                    f = claimOwnership(txn, segmentMetadata)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg5MzI0Mg=="}, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 113}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTk0NTYzOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMTo1NDoyMlrOHcvGHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwMDozNzo1OVrOHtFTiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg5Mzc5MQ==", "bodyText": "I think exceptionallyExpecting may work well here.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r499893791", "createdAt": "2020-10-05T21:54:22Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -225,324 +240,132 @@ public void initialize(long containerEpoch) {\n      *\n      * @param txn             Active {@link MetadataTransaction}.\n      * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n-     * @throws ChunkStorageException In case of any chunk storage related errors.\n-     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     *                        throws ChunkStorageException    In case of any chunk storage related errors.\n+     *                        throws StorageMetadataException In case of any chunk metadata store related errors.\n      */\n-    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws ChunkStorageException, StorageMetadataException {\n-\n+    private CompletableFuture<Void> claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) {\n         // Get the last chunk\n         String lastChunkName = segmentMetadata.getLastChunk();\n+        CompletableFuture<Boolean> f = CompletableFuture.completedFuture(true);\n         if (null != lastChunkName) {\n-            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n-            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n-                    logPrefix,\n-                    segmentMetadata.getName(),\n-                    lastChunk.getName(),\n-                    lastChunk.getLength());\n-            try {\n-                ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n-                Preconditions.checkState(chunkInfo != null);\n-                Preconditions.checkState(lastChunk != null);\n-                // Adjust its length;\n-                if (chunkInfo.getLength() != lastChunk.getLength()) {\n-                    Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n-                    // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n-                    lastChunk.setLength(chunkInfo.getLength());\n-                    segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n-                    txn.update(lastChunk);\n-                    log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n-                            logPrefix,\n-                            segmentMetadata.getName(),\n-                            lastChunk.getName(),\n-                            chunkInfo.getLength());\n-                }\n-            } catch (ChunkNotFoundException e) {\n-                // This probably means that this instance is fenced out and newer instance truncated this segment.\n-                // Try a commit of unmodified data to fail fast.\n-                log.debug(\"{} claimOwnership - Last chunk was missing, failing fast - segment={}, last chunk={}.\",\n-                        logPrefix,\n-                        segmentMetadata.getName(),\n-                        lastChunk.getName());\n-                txn.update(segmentMetadata);\n-                txn.commit();\n-                throw e;\n-            }\n+            f = txn.get(lastChunkName)\n+                    .thenComposeAsync(storageMetadata -> {\n+                        ChunkMetadata lastChunk = (ChunkMetadata) storageMetadata;\n+                        log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                                logPrefix,\n+                                segmentMetadata.getName(),\n+                                lastChunk.getName(),\n+                                lastChunk.getLength());\n+                        return chunkStorage.getInfo(lastChunkName)\n+                                .thenComposeAsync(chunkInfo -> {\n+                                    Preconditions.checkState(chunkInfo != null);\n+                                    Preconditions.checkState(lastChunk != null);\n+                                    // Adjust its length;\n+                                    if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                                        Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                                        // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                                        lastChunk.setLength(chunkInfo.getLength());\n+                                        segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                                        txn.update(lastChunk);\n+                                        log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                                                logPrefix,\n+                                                segmentMetadata.getName(),\n+                                                lastChunk.getName(),\n+                                                chunkInfo.getLength());\n+                                    }\n+                                    return CompletableFuture.completedFuture(true);\n+                                }, executor)\n+                                .exceptionally(e -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAzNDg4OQ==", "bodyText": "same as above", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r517034889", "createdAt": "2020-11-04T00:37:59Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -225,324 +240,132 @@ public void initialize(long containerEpoch) {\n      *\n      * @param txn             Active {@link MetadataTransaction}.\n      * @param segmentMetadata {@link SegmentMetadata} for the segment to change ownership for.\n-     * @throws ChunkStorageException In case of any chunk storage related errors.\n-     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     *                        throws ChunkStorageException    In case of any chunk storage related errors.\n+     *                        throws StorageMetadataException In case of any chunk metadata store related errors.\n      */\n-    private void claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) throws ChunkStorageException, StorageMetadataException {\n-\n+    private CompletableFuture<Void> claimOwnership(MetadataTransaction txn, SegmentMetadata segmentMetadata) {\n         // Get the last chunk\n         String lastChunkName = segmentMetadata.getLastChunk();\n+        CompletableFuture<Boolean> f = CompletableFuture.completedFuture(true);\n         if (null != lastChunkName) {\n-            ChunkMetadata lastChunk = (ChunkMetadata) txn.get(lastChunkName);\n-            log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n-                    logPrefix,\n-                    segmentMetadata.getName(),\n-                    lastChunk.getName(),\n-                    lastChunk.getLength());\n-            try {\n-                ChunkInfo chunkInfo = chunkStorage.getInfo(lastChunkName);\n-                Preconditions.checkState(chunkInfo != null);\n-                Preconditions.checkState(lastChunk != null);\n-                // Adjust its length;\n-                if (chunkInfo.getLength() != lastChunk.getLength()) {\n-                    Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n-                    // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n-                    lastChunk.setLength(chunkInfo.getLength());\n-                    segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n-                    txn.update(lastChunk);\n-                    log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n-                            logPrefix,\n-                            segmentMetadata.getName(),\n-                            lastChunk.getName(),\n-                            chunkInfo.getLength());\n-                }\n-            } catch (ChunkNotFoundException e) {\n-                // This probably means that this instance is fenced out and newer instance truncated this segment.\n-                // Try a commit of unmodified data to fail fast.\n-                log.debug(\"{} claimOwnership - Last chunk was missing, failing fast - segment={}, last chunk={}.\",\n-                        logPrefix,\n-                        segmentMetadata.getName(),\n-                        lastChunk.getName());\n-                txn.update(segmentMetadata);\n-                txn.commit();\n-                throw e;\n-            }\n+            f = txn.get(lastChunkName)\n+                    .thenComposeAsync(storageMetadata -> {\n+                        ChunkMetadata lastChunk = (ChunkMetadata) storageMetadata;\n+                        log.debug(\"{} claimOwnership - current last chunk - segment={}, last chunk={}, Length={}.\",\n+                                logPrefix,\n+                                segmentMetadata.getName(),\n+                                lastChunk.getName(),\n+                                lastChunk.getLength());\n+                        return chunkStorage.getInfo(lastChunkName)\n+                                .thenComposeAsync(chunkInfo -> {\n+                                    Preconditions.checkState(chunkInfo != null);\n+                                    Preconditions.checkState(lastChunk != null);\n+                                    // Adjust its length;\n+                                    if (chunkInfo.getLength() != lastChunk.getLength()) {\n+                                        Preconditions.checkState(chunkInfo.getLength() > lastChunk.getLength());\n+                                        // Whatever length you see right now is the final \"sealed\" length of the last chunk.\n+                                        lastChunk.setLength(chunkInfo.getLength());\n+                                        segmentMetadata.setLength(segmentMetadata.getLastChunkStartOffset() + lastChunk.getLength());\n+                                        txn.update(lastChunk);\n+                                        log.debug(\"{} claimOwnership - Length of last chunk adjusted - segment={}, last chunk={}, Length={}.\",\n+                                                logPrefix,\n+                                                segmentMetadata.getName(),\n+                                                lastChunk.getName(),\n+                                                chunkInfo.getLength());\n+                                    }\n+                                    return CompletableFuture.completedFuture(true);\n+                                }, executor)\n+                                .exceptionally(e -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg5Mzc5MQ=="}, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 213}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTk1NTk0OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMTo1ODowOFrOHcvMZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwMDozODo0MVrOHtFULg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg5NTM5OA==", "bodyText": "Can e merge these 2 catch blocks into one?", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r499895398", "createdAt": "2020-10-05T21:58:08Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -555,575 +378,700 @@ private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n         return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n     }\n \n-    /**\n-     * Adds a system log.\n-     *\n-     * @param systemLogRecords\n-     * @param streamSegmentName Name of the segment.\n-     * @param offset            Offset at which new chunk was added.\n-     * @param oldChunkName      Name of the previous last chunk.\n-     * @param newChunkName      Name of the new last chunk.\n-     */\n-    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n-        systemLogRecords.add(\n-                SystemJournal.ChunkAddedRecord.builder()\n-                        .segmentName(streamSegmentName)\n-                        .offset(offset)\n-                        .oldChunkName(oldChunkName == null ? null : oldChunkName)\n-                        .newChunkName(newChunkName)\n-                        .build());\n-    }\n-\n     /**\n      * Delete the garbage chunks.\n      *\n      * @param chunksTodelete List of chunks to delete.\n      */\n-    private void collectGarbage(Collection<String> chunksTodelete) {\n+    private CompletableFuture<Void> collectGarbage(Collection<String> chunksTodelete) {\n+        CompletableFuture[] futures = new CompletableFuture[chunksTodelete.size()];\n+        int i = 0;\n         for (val chunkTodelete : chunksTodelete) {\n-            try {\n-                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n-                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n-            } catch (ChunkNotFoundException e) {\n-                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n-            } catch (Exception e) {\n-                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n-                // Add it to garbage chunks.\n-                synchronized (garbageChunks) {\n-                    garbageChunks.add(chunkTodelete);\n-                }\n-            }\n+            futures[i++] = chunkStorage.openWrite(chunkTodelete)\n+                    .thenComposeAsync(chunkStorage::delete, executor)\n+                    .thenRunAsync(() -> log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete), executor)\n+                    .exceptionally(e -> {\n+                        val ex = Exceptions.unwrap(e);\n+                        if (ex instanceof ChunkNotFoundException) {\n+                            log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                        } else {\n+                            log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                            // Add it to garbage chunks.\n+                            synchronized (garbageChunks) {\n+                                garbageChunks.add(chunkTodelete);\n+                            }\n+                        }\n+                        return null;\n+                    });\n         }\n+        return CompletableFuture.allOf(futures);\n     }\n \n     @Override\n     public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n         checkInitialized();\n-        return execute(() -> {\n+        return executeAsync(() -> {\n             long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n             Preconditions.checkNotNull(handle, \"handle\");\n             String streamSegmentName = handle.getSegmentName();\n             Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n             Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n \n-            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n-                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n-                // Validate preconditions.\n-                checkSegmentExists(streamSegmentName, segmentMetadata);\n-                checkOwnership(streamSegmentName, segmentMetadata);\n-\n-                // seal if it is not already sealed.\n-                if (!segmentMetadata.isSealed()) {\n-                    segmentMetadata.setSealed(true);\n-                    txn.update(segmentMetadata);\n-                    txn.commit();\n-                }\n-\n-                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n-                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n-                return null;\n-            } catch (StorageMetadataWritesFencedOutException ex) {\n-                throw new StorageNotPrimaryException(streamSegmentName, ex);\n-            }\n+            return tryWith(metadataStore.beginTransaction(handle.getSegmentName()), txn ->\n+                    txn.get(streamSegmentName)\n+                            .thenComposeAsync(storageMetadata -> {\n+                                SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                // Validate preconditions.\n+                                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                                // seal if it is not already sealed.\n+                                if (!segmentMetadata.isSealed()) {\n+                                    segmentMetadata.setSealed(true);\n+                                    txn.update(segmentMetadata);\n+                                    return txn.commit();\n+                                }\n+                                return CompletableFuture.completedFuture(null);\n+                            }, executor)\n+                            .thenRunAsync(() -> {\n+                                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                            }, executor)\n+                            .exceptionally(e -> {\n+                                val ex = Exceptions.unwrap(e);\n+                                if (ex instanceof StorageMetadataWritesFencedOutException) {\n+                                    throw new CompletionException(new StorageNotPrimaryException(streamSegmentName, ex));\n+                                }\n+                                throw new CompletionException(ex);\n+                            }), executor);\n         });\n     }\n \n     @Override\n     public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n         checkInitialized();\n-        return execute(() -> {\n-            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n-            Timer timer = new Timer();\n-\n-            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n-            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n-            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n-            Preconditions.checkArgument(offset >= 0, \"offset\");\n-            String targetSegmentName = targetHandle.getSegmentName();\n-\n-            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n-\n-                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n-\n-                // Validate preconditions.\n-                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n-                targetSegmentMetadata.checkInvariants();\n-                checkNotSealed(targetSegmentName, targetSegmentMetadata);\n-\n-                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n-                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n-                sourceSegmentMetadata.checkInvariants();\n-\n-                // This is a critical assumption at this point which should not be broken,\n-                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n-                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n-\n-                checkSealed(sourceSegmentMetadata);\n-                checkOwnership(targetSegmentMetadata.getName(), targetSegmentMetadata);\n-\n-                if (sourceSegmentMetadata.getStartOffset() != 0) {\n-                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n-                }\n-\n-                if (offset != targetSegmentMetadata.getLength()) {\n-                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n-                }\n-\n-                // Update list of chunks by appending sources list of chunks.\n-                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n-                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n-\n-                if (targetLastChunk != null) {\n-                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n-                    txn.update(targetLastChunk);\n-                } else {\n-                    if (sourceFirstChunk != null) {\n-                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n-                        txn.update(sourceFirstChunk);\n-                    }\n-                }\n-\n-                // Update segments's last chunk to point to the sources last segment.\n-                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n-\n-                // Update the length of segment.\n-                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n-                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n-\n-                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n-\n-                txn.update(targetSegmentMetadata);\n-                txn.delete(sourceSegment);\n-\n-                // Finally defrag immediately.\n-                ArrayList<String> chunksToDelete = new ArrayList<>();\n-                if (shouldDefrag() && null != targetLastChunk) {\n-                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null, chunksToDelete);\n-                }\n-\n-                targetSegmentMetadata.checkInvariants();\n-\n-                // Finally commit transaction.\n-                txn.commit();\n-\n-                // Collect garbage.\n-                collectGarbage(chunksToDelete);\n-\n-                // Update the read index.\n-                readIndexCache.remove(sourceSegment);\n-\n-                Duration elapsed = timer.getElapsed();\n-                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n-                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n-\n-            } catch (StorageMetadataWritesFencedOutException ex) {\n-                throw new StorageNotPrimaryException(targetSegmentName, ex);\n-            }\n-\n-            return null;\n-        });\n+        return executeAsync(new ConcatOperation(this, targetHandle, offset, sourceSegment));\n     }\n \n     private boolean shouldAppend() {\n         return chunkStorage.supportsAppend() && config.isAppendEnabled();\n     }\n \n-    private boolean shouldDefrag() {\n-        return shouldAppend() || chunkStorage.supportsConcat();\n-    }\n-\n     /**\n      * Defragments the list of chunks for a given segment.\n      * It finds eligible consecutive chunks that can be merged together.\n      * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n      * Conceptually this is like deleting nodes from middle of the list of chunks.\n      *\n-     * <Ul>\n-     * <li> In the absence of defragmentation, the number of chunks for individual segments keeps on increasing.\n-     * When we have too many small chunks (say because many transactions with little data on some segments), the segment\n-     * is fragmented - this may impact both the read throughput and the performance of the metadata store.\n-     * This problem is further intensified when we have stores that do not support append semantics (e.g., stock S3) and\n-     * each write becomes a separate chunk.\n-     * </li>\n-     * <li>\n-     * If the underlying storage provides some facility to stitch together smaller chunk into larger chunks, then we do\n-     * actually want to exploit that, specially when the underlying implementation is only a metadata operation. We want\n-     * to leverage multi-part uploads in object stores that support it (e.g., AWS S3, Dell EMC ECS) as they are typically\n-     * only metadata operations, reducing the overall cost of the merging them together. HDFS also supports merges,\n-     * whereas NFS has no concept of merging natively.\n-     *\n-     * As chunks become larger, append writes (read source completely and append it back at the end of target)\n-     * become inefficient. Consequently, a native option for merging is desirable. We use such native merge capability\n-     * when available, and if not available, then we use appends.\n-     * </li>\n-     * <li>\n-     * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n-     * We can then fine tune that background task to run optimally with low overhead.\n-     * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n-     * </li>\n-     * <li>\n-     * <li>\n-     * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n-     * </li>\n-     * </ul>\n-     *\n-     * What controls whether we invoke concat or simulate through appends?\n-     * There are a few different capabilities that ChunkStorage needs to provide.\n-     * <ul>\n-     * <li>Does ChunkStorage support appending to existing chunks? For vanilla S3 compatible this would return false.\n-     * This is indicated by supportsAppend.</li>\n-     * <li>Does ChunkStorage support for concatenating chunks ? This is indicated by supportsConcat.\n-     * If this is true then concat operation will be invoked otherwise chunks will be appended.</li>\n-     * <li>There are some obvious constraints - For ChunkStorage support any concat functionality it must support either\n-     * append or concat.</li>\n-     * <li>Also when ChunkStorage supports both concat and append, ChunkedSegmentStorage will invoke appropriate method\n-     * depending on size of target and source chunks. (Eg. ECS)</li>\n-     * </ul>\n-     *\n-     * <li>\n-     * What controls defrag?\n-     * There are two additional parameters that control when concat\n-     * <li>minSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered a small object.\n-     * For small source objects, append is used instead of using concat. (For really small txn it is rather efficient to use append than MPU).</li>\n-     * <li>maxSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered for concat. (Eg S3 might have max limit on chunk size).</li>\n-     * In short there is a size beyond which using append is not advisable. Conversely there is a size below which concat is not efficient.(minSizeLimitForConcat )\n-     * Then there is limit which concating does not make sense maxSizeLimitForConcat\n-     * </li>\n-     * <li>\n-     * What is the defrag algorithm\n-     * <pre>\n-     * While(segment.hasConcatableChunks()){\n-     *     Set<List<Chunk>> s = FindConsecutiveConcatableChunks();\n-     *     For (List<chunk> list : s){\n-     *        ConcatChunks (list);\n-     *     }\n-     * }\n-     * </pre>\n-     * </li>\n-     * </ul>\n-     *\n      * @param txn             Active {@link MetadataTransaction}.\n      * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n      * @param startChunkName  Name of the first chunk to start defragmentation.\n      * @param lastChunkName   Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n      * @param chunksToDelete  List of chunks to which names of chunks to be deleted are added. It is the responsibility\n      *                        of caller to garbage collect these chunks.\n-     * @throws ChunkStorageException In case of any chunk storage related errors.\n-     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     *                        throws ChunkStorageException    In case of any chunk storage related errors.\n+     *                        throws StorageMetadataException In case of any chunk metadata store related errors.\n      */\n-    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata,\n-                        String startChunkName,\n-                        String lastChunkName,\n-                        ArrayList<String> chunksToDelete)\n-            throws StorageMetadataException, ChunkStorageException {\n-        // The algorithm is actually very simple.\n-        // It tries to concat all small chunks using appends first.\n-        // Then it tries to concat remaining chunks using concat if available.\n-        // To implement it using single loop we toggle between concat with append and concat modes. (Instead of two passes.)\n-        boolean useAppend = true;\n-        String targetChunkName = startChunkName;\n-\n-        // Iterate through chunk list\n-        while (null != targetChunkName && !targetChunkName.equals(lastChunkName)) {\n-            ChunkMetadata target = (ChunkMetadata) txn.get(targetChunkName);\n-\n-            ArrayList<ChunkInfo> chunksToConcat = new ArrayList<>();\n-            long targetSizeAfterConcat = target.getLength();\n-\n-            // Add target to the list of chunks\n-            chunksToConcat.add(new ChunkInfo(targetSizeAfterConcat, targetChunkName));\n-\n-            String nextChunkName = target.getNextChunk();\n-            ChunkMetadata next = null;\n-\n-            // Gather list of chunks that can be appended together.\n-            while (null != nextChunkName) {\n-                next = (ChunkMetadata) txn.get(nextChunkName);\n-\n-                if (useAppend && config.getMinSizeLimitForConcat() < next.getLength()) {\n-                    break;\n-                }\n+    private CompletableFuture<Void> defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata,\n+                                           String startChunkName,\n+                                           String lastChunkName,\n+                                           ArrayList<String> chunksToDelete) {\n+        return new DefragmentOperation(this, txn, segmentMetadata, startChunkName, lastChunkName, chunksToDelete).call();\n+    }\n \n-                if (targetSizeAfterConcat + next.getLength() > segmentMetadata.getMaxRollinglength() || next.getLength() > config.getMaxSizeLimitForConcat()) {\n-                    break;\n-                }\n+    @Override\n+    public CompletableFuture<Void> delete(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return executeAsync(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle);\n+            Timer timer = new Timer();\n \n-                chunksToConcat.add(new ChunkInfo(next.getLength(), nextChunkName));\n-                targetSizeAfterConcat += next.getLength();\n+            String streamSegmentName = handle.getSegmentName();\n+            return tryWith(metadataStore.beginTransaction(streamSegmentName), txn -> txn.get(streamSegmentName)\n+                    .thenComposeAsync(storageMetadata -> {\n+                        SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                        // Check preconditions\n+                        checkSegmentExists(streamSegmentName, segmentMetadata);\n+                        checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                        segmentMetadata.setActive(false);\n+\n+                        // Delete chunks\n+                        ArrayList<String> chunksToDelete = new ArrayList<>();\n+                        return new ChunkIterator(txn, segmentMetadata)\n+                                .forEach((metadata, name) -> {\n+                                    txn.delete(name);\n+                                    chunksToDelete.add(name);\n+                                })\n+                                .thenRunAsync(() -> txn.delete(streamSegmentName), executor)\n+                                .thenComposeAsync(v ->\n+                                        txn.commit()\n+                                                .thenComposeAsync(vv -> {\n+                                                    // Collect garbage.\n+                                                    return collectGarbage(chunksToDelete);\n+                                                }, executor)\n+                                                .thenRunAsync(() -> {\n+                                                    // Update the read index.\n+                                                    readIndexCache.remove(streamSegmentName);\n+\n+                                                    Duration elapsed = timer.getElapsed();\n+                                                    log.debug(\"{} delete - segment={}, latency={}.\", logPrefix, handle.getSegmentName(), elapsed.toMillis());\n+                                                    LoggerHelpers.traceLeave(log, \"delete\", traceId, handle);\n+                                                }, executor)\n+                                                .exceptionally(e -> {\n+                                                    val ex = Exceptions.unwrap(e);\n+                                                    if (ex instanceof StorageMetadataWritesFencedOutException) {\n+                                                        throw new CompletionException(new StorageNotPrimaryException(streamSegmentName, ex));\n+                                                    }\n+                                                    throw new CompletionException(ex);\n+                                                }), executor);\n+                    }, executor), executor);\n+        });\n+    }\n \n-                nextChunkName = next.getNextChunk();\n-            }\n-            // Note - After above while loop is exited nextChunkName points to chunk next to last one to be concat.\n-            // Which means target should now point to it as next after concat is complete.\n+    @Override\n+    public CompletableFuture<Void> truncate(SegmentHandle handle, long offset, Duration timeout) {\n+        checkInitialized();\n+        return executeAsync(new TruncateOperation(this, handle, offset));\n+    }\n \n-            // If there are chunks that can be appended together then concat them.\n-            if (chunksToConcat.size() > 1) {\n-                // Concat\n+    @Override\n+    public boolean supportsTruncation() {\n+        return true;\n+    }\n \n-                ConcatArgument[] concatArgs = new ConcatArgument[chunksToConcat.size()];\n-                for (int i = 0; i < chunksToConcat.size(); i++) {\n-                    concatArgs[i] = ConcatArgument.fromChunkInfo(chunksToConcat.get(i));\n-                }\n+    @Override\n+    public Iterator<SegmentProperties> listSegments() {\n+        throw new UnsupportedOperationException(\"listSegments is not yet supported\");\n+    }\n \n-                if (!useAppend && chunkStorage.supportsConcat()) {\n-                    int length = chunkStorage.concat(concatArgs);\n-                } else {\n-                    concatUsingAppend(concatArgs);\n-                }\n+    @Override\n+    public CompletableFuture<SegmentHandle> openRead(String streamSegmentName) {\n+        checkInitialized();\n+        return executeAsync(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openRead\", streamSegmentName);\n+            // Validate preconditions and return handle.\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            return tryWith(metadataStore.beginTransaction(streamSegmentName), txn ->\n+                            txn.get(streamSegmentName).thenComposeAsync(storageMetadata -> {\n+                                SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                                segmentMetadata.checkInvariants();\n+                                // This segment was created by an older segment store. Then claim ownership and adjust length.\n+                                CompletableFuture<Void> f = CompletableFuture.completedFuture(null);\n+                                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                                    log.debug(\"{} openRead - Segment needs ownership change. segment={}.\", logPrefix, segmentMetadata.getName());\n+                                    // In case of a failover, length recorded in metadata will be lagging behind its actual length in the storage.\n+                                    // This can happen with lazy commits that were still not committed at the time of failover.\n+                                    f = claimOwnership(txn, segmentMetadata);\n+                                }\n+                                return f.thenApplyAsync(v -> {\n+                                    val retValue = SegmentStorageHandle.readHandle(streamSegmentName);\n+                                    LoggerHelpers.traceLeave(log, \"openRead\", traceId, retValue);\n+                                    return retValue;\n+                                }, executor);\n+                            }, executor),\n+                    executor);\n+        });\n+    }\n \n-                // Delete chunks.\n-                for (int i = 1; i < chunksToConcat.size(); i++) {\n-                    chunksToDelete.add(chunksToConcat.get(i).getName());\n-                }\n+    @Override\n+    public CompletableFuture<Integer> read(SegmentHandle handle, long offset, byte[] buffer, int bufferOffset, int length, Duration timeout) {\n+        checkInitialized();\n+        return executeAsync(new ReadOperation(this, handle, offset, buffer, bufferOffset, length));\n+    }\n \n-                // Set the pointers\n-                target.setLength(targetSizeAfterConcat);\n-                target.setNextChunk(nextChunkName);\n+    @Override\n+    public CompletableFuture<SegmentProperties> getStreamSegmentInfo(String streamSegmentName, Duration timeout) {\n+        checkInitialized();\n+        return executeAsync(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"getStreamSegmentInfo\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            return tryWith(metadataStore.beginTransaction(streamSegmentName), txn ->\n+                    txn.get(streamSegmentName)\n+                            .thenApplyAsync(storageMetadata -> {\n+                                SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                if (null == segmentMetadata) {\n+                                    throw new CompletionException(new StreamSegmentNotExistsException(streamSegmentName));\n+                                }\n+                                segmentMetadata.checkInvariants();\n+\n+                                val retValue = StreamSegmentInformation.builder()\n+                                        .name(streamSegmentName)\n+                                        .sealed(segmentMetadata.isSealed())\n+                                        .length(segmentMetadata.getLength())\n+                                        .startOffset(segmentMetadata.getStartOffset())\n+                                        .lastModified(new ImmutableDate(segmentMetadata.getLastModified()))\n+                                        .build();\n+                                LoggerHelpers.traceLeave(log, \"getStreamSegmentInfo\", traceId, retValue);\n+                                return retValue;\n+                            }, executor), executor);\n+        });\n+    }\n \n-                // If target is the last chunk after this then update metadata accordingly\n-                if (null == nextChunkName) {\n-                    segmentMetadata.setLastChunk(target.getName());\n-                    segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength() - target.getLength());\n-                }\n+    @Override\n+    public CompletableFuture<Boolean> exists(String streamSegmentName, Duration timeout) {\n+        checkInitialized();\n+        return executeAsync(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"exists\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            return tryWith(metadataStore.beginTransaction(streamSegmentName),\n+                    txn -> txn.get(streamSegmentName)\n+                            .thenApplyAsync(storageMetadata -> {\n+                                SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                val retValue = segmentMetadata != null && segmentMetadata.isActive();\n+                                LoggerHelpers.traceLeave(log, \"exists\", traceId, retValue);\n+                                return retValue;\n+                            }, executor),\n+                    executor);\n+        });\n+    }\n \n-                // Update metadata for affected chunks.\n-                for (int i = 1; i < concatArgs.length; i++) {\n-                    txn.delete(concatArgs[i].getName());\n-                    segmentMetadata.decrementChunkCount();\n-                }\n-                txn.update(target);\n-                txn.update(segmentMetadata);\n+    @Override\n+    public void close() {\n+        try {\n+            if (null != this.metadataStore) {\n+                this.metadataStore.close();\n             }\n+        } catch (Exception e) {\n+            log.warn(\"Error during close\", e);\n+        }\n+        this.closed.set(true);\n+    }\n \n-            // Move on to next place in list where we can concat if we are done with append based concats.\n-            if (!useAppend) {\n-                targetChunkName = nextChunkName;\n+    /**\n+     * Executes the given Callable and returns its result, while translating any Exceptions bubbling out of it into\n+     * StreamSegmentExceptions.\n+     *\n+     * @param operation The function to execute.\n+     * @param <R>       Return type of the operation.\n+     * @return CompletableFuture<R> of the return type of the operation.\n+     */\n+    private <R> CompletableFuture<R> executeAsync(Callable<CompletableFuture<R>> operation) {\n+        return CompletableFuture.completedFuture(null).thenComposeAsync(v -> {\n+            Exceptions.checkNotClosed(this.closed.get(), this);\n+            try {\n+                return operation.call();\n+            } catch (CompletionException e) {\n+                throw new CompletionException(Exceptions.unwrap(e));\n+            } catch (Exception e) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 1133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAzNTA1NA==", "bodyText": "why unwrap when you know it is not really required?", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r517035054", "createdAt": "2020-11-04T00:38:41Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -555,575 +378,700 @@ private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n         return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n     }\n \n-    /**\n-     * Adds a system log.\n-     *\n-     * @param systemLogRecords\n-     * @param streamSegmentName Name of the segment.\n-     * @param offset            Offset at which new chunk was added.\n-     * @param oldChunkName      Name of the previous last chunk.\n-     * @param newChunkName      Name of the new last chunk.\n-     */\n-    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n-        systemLogRecords.add(\n-                SystemJournal.ChunkAddedRecord.builder()\n-                        .segmentName(streamSegmentName)\n-                        .offset(offset)\n-                        .oldChunkName(oldChunkName == null ? null : oldChunkName)\n-                        .newChunkName(newChunkName)\n-                        .build());\n-    }\n-\n     /**\n      * Delete the garbage chunks.\n      *\n      * @param chunksTodelete List of chunks to delete.\n      */\n-    private void collectGarbage(Collection<String> chunksTodelete) {\n+    private CompletableFuture<Void> collectGarbage(Collection<String> chunksTodelete) {\n+        CompletableFuture[] futures = new CompletableFuture[chunksTodelete.size()];\n+        int i = 0;\n         for (val chunkTodelete : chunksTodelete) {\n-            try {\n-                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n-                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n-            } catch (ChunkNotFoundException e) {\n-                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n-            } catch (Exception e) {\n-                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n-                // Add it to garbage chunks.\n-                synchronized (garbageChunks) {\n-                    garbageChunks.add(chunkTodelete);\n-                }\n-            }\n+            futures[i++] = chunkStorage.openWrite(chunkTodelete)\n+                    .thenComposeAsync(chunkStorage::delete, executor)\n+                    .thenRunAsync(() -> log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete), executor)\n+                    .exceptionally(e -> {\n+                        val ex = Exceptions.unwrap(e);\n+                        if (ex instanceof ChunkNotFoundException) {\n+                            log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                        } else {\n+                            log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                            // Add it to garbage chunks.\n+                            synchronized (garbageChunks) {\n+                                garbageChunks.add(chunkTodelete);\n+                            }\n+                        }\n+                        return null;\n+                    });\n         }\n+        return CompletableFuture.allOf(futures);\n     }\n \n     @Override\n     public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n         checkInitialized();\n-        return execute(() -> {\n+        return executeAsync(() -> {\n             long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n             Preconditions.checkNotNull(handle, \"handle\");\n             String streamSegmentName = handle.getSegmentName();\n             Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n             Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n \n-            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n-                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n-                // Validate preconditions.\n-                checkSegmentExists(streamSegmentName, segmentMetadata);\n-                checkOwnership(streamSegmentName, segmentMetadata);\n-\n-                // seal if it is not already sealed.\n-                if (!segmentMetadata.isSealed()) {\n-                    segmentMetadata.setSealed(true);\n-                    txn.update(segmentMetadata);\n-                    txn.commit();\n-                }\n-\n-                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n-                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n-                return null;\n-            } catch (StorageMetadataWritesFencedOutException ex) {\n-                throw new StorageNotPrimaryException(streamSegmentName, ex);\n-            }\n+            return tryWith(metadataStore.beginTransaction(handle.getSegmentName()), txn ->\n+                    txn.get(streamSegmentName)\n+                            .thenComposeAsync(storageMetadata -> {\n+                                SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                // Validate preconditions.\n+                                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                                // seal if it is not already sealed.\n+                                if (!segmentMetadata.isSealed()) {\n+                                    segmentMetadata.setSealed(true);\n+                                    txn.update(segmentMetadata);\n+                                    return txn.commit();\n+                                }\n+                                return CompletableFuture.completedFuture(null);\n+                            }, executor)\n+                            .thenRunAsync(() -> {\n+                                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                            }, executor)\n+                            .exceptionally(e -> {\n+                                val ex = Exceptions.unwrap(e);\n+                                if (ex instanceof StorageMetadataWritesFencedOutException) {\n+                                    throw new CompletionException(new StorageNotPrimaryException(streamSegmentName, ex));\n+                                }\n+                                throw new CompletionException(ex);\n+                            }), executor);\n         });\n     }\n \n     @Override\n     public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n         checkInitialized();\n-        return execute(() -> {\n-            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n-            Timer timer = new Timer();\n-\n-            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n-            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n-            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n-            Preconditions.checkArgument(offset >= 0, \"offset\");\n-            String targetSegmentName = targetHandle.getSegmentName();\n-\n-            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n-\n-                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n-\n-                // Validate preconditions.\n-                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n-                targetSegmentMetadata.checkInvariants();\n-                checkNotSealed(targetSegmentName, targetSegmentMetadata);\n-\n-                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n-                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n-                sourceSegmentMetadata.checkInvariants();\n-\n-                // This is a critical assumption at this point which should not be broken,\n-                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n-                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n-\n-                checkSealed(sourceSegmentMetadata);\n-                checkOwnership(targetSegmentMetadata.getName(), targetSegmentMetadata);\n-\n-                if (sourceSegmentMetadata.getStartOffset() != 0) {\n-                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n-                }\n-\n-                if (offset != targetSegmentMetadata.getLength()) {\n-                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n-                }\n-\n-                // Update list of chunks by appending sources list of chunks.\n-                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n-                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n-\n-                if (targetLastChunk != null) {\n-                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n-                    txn.update(targetLastChunk);\n-                } else {\n-                    if (sourceFirstChunk != null) {\n-                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n-                        txn.update(sourceFirstChunk);\n-                    }\n-                }\n-\n-                // Update segments's last chunk to point to the sources last segment.\n-                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n-\n-                // Update the length of segment.\n-                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n-                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n-\n-                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n-\n-                txn.update(targetSegmentMetadata);\n-                txn.delete(sourceSegment);\n-\n-                // Finally defrag immediately.\n-                ArrayList<String> chunksToDelete = new ArrayList<>();\n-                if (shouldDefrag() && null != targetLastChunk) {\n-                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null, chunksToDelete);\n-                }\n-\n-                targetSegmentMetadata.checkInvariants();\n-\n-                // Finally commit transaction.\n-                txn.commit();\n-\n-                // Collect garbage.\n-                collectGarbage(chunksToDelete);\n-\n-                // Update the read index.\n-                readIndexCache.remove(sourceSegment);\n-\n-                Duration elapsed = timer.getElapsed();\n-                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n-                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n-\n-            } catch (StorageMetadataWritesFencedOutException ex) {\n-                throw new StorageNotPrimaryException(targetSegmentName, ex);\n-            }\n-\n-            return null;\n-        });\n+        return executeAsync(new ConcatOperation(this, targetHandle, offset, sourceSegment));\n     }\n \n     private boolean shouldAppend() {\n         return chunkStorage.supportsAppend() && config.isAppendEnabled();\n     }\n \n-    private boolean shouldDefrag() {\n-        return shouldAppend() || chunkStorage.supportsConcat();\n-    }\n-\n     /**\n      * Defragments the list of chunks for a given segment.\n      * It finds eligible consecutive chunks that can be merged together.\n      * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n      * Conceptually this is like deleting nodes from middle of the list of chunks.\n      *\n-     * <Ul>\n-     * <li> In the absence of defragmentation, the number of chunks for individual segments keeps on increasing.\n-     * When we have too many small chunks (say because many transactions with little data on some segments), the segment\n-     * is fragmented - this may impact both the read throughput and the performance of the metadata store.\n-     * This problem is further intensified when we have stores that do not support append semantics (e.g., stock S3) and\n-     * each write becomes a separate chunk.\n-     * </li>\n-     * <li>\n-     * If the underlying storage provides some facility to stitch together smaller chunk into larger chunks, then we do\n-     * actually want to exploit that, specially when the underlying implementation is only a metadata operation. We want\n-     * to leverage multi-part uploads in object stores that support it (e.g., AWS S3, Dell EMC ECS) as they are typically\n-     * only metadata operations, reducing the overall cost of the merging them together. HDFS also supports merges,\n-     * whereas NFS has no concept of merging natively.\n-     *\n-     * As chunks become larger, append writes (read source completely and append it back at the end of target)\n-     * become inefficient. Consequently, a native option for merging is desirable. We use such native merge capability\n-     * when available, and if not available, then we use appends.\n-     * </li>\n-     * <li>\n-     * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n-     * We can then fine tune that background task to run optimally with low overhead.\n-     * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n-     * </li>\n-     * <li>\n-     * <li>\n-     * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n-     * </li>\n-     * </ul>\n-     *\n-     * What controls whether we invoke concat or simulate through appends?\n-     * There are a few different capabilities that ChunkStorage needs to provide.\n-     * <ul>\n-     * <li>Does ChunkStorage support appending to existing chunks? For vanilla S3 compatible this would return false.\n-     * This is indicated by supportsAppend.</li>\n-     * <li>Does ChunkStorage support for concatenating chunks ? This is indicated by supportsConcat.\n-     * If this is true then concat operation will be invoked otherwise chunks will be appended.</li>\n-     * <li>There are some obvious constraints - For ChunkStorage support any concat functionality it must support either\n-     * append or concat.</li>\n-     * <li>Also when ChunkStorage supports both concat and append, ChunkedSegmentStorage will invoke appropriate method\n-     * depending on size of target and source chunks. (Eg. ECS)</li>\n-     * </ul>\n-     *\n-     * <li>\n-     * What controls defrag?\n-     * There are two additional parameters that control when concat\n-     * <li>minSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered a small object.\n-     * For small source objects, append is used instead of using concat. (For really small txn it is rather efficient to use append than MPU).</li>\n-     * <li>maxSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered for concat. (Eg S3 might have max limit on chunk size).</li>\n-     * In short there is a size beyond which using append is not advisable. Conversely there is a size below which concat is not efficient.(minSizeLimitForConcat )\n-     * Then there is limit which concating does not make sense maxSizeLimitForConcat\n-     * </li>\n-     * <li>\n-     * What is the defrag algorithm\n-     * <pre>\n-     * While(segment.hasConcatableChunks()){\n-     *     Set<List<Chunk>> s = FindConsecutiveConcatableChunks();\n-     *     For (List<chunk> list : s){\n-     *        ConcatChunks (list);\n-     *     }\n-     * }\n-     * </pre>\n-     * </li>\n-     * </ul>\n-     *\n      * @param txn             Active {@link MetadataTransaction}.\n      * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n      * @param startChunkName  Name of the first chunk to start defragmentation.\n      * @param lastChunkName   Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n      * @param chunksToDelete  List of chunks to which names of chunks to be deleted are added. It is the responsibility\n      *                        of caller to garbage collect these chunks.\n-     * @throws ChunkStorageException In case of any chunk storage related errors.\n-     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     *                        throws ChunkStorageException    In case of any chunk storage related errors.\n+     *                        throws StorageMetadataException In case of any chunk metadata store related errors.\n      */\n-    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata,\n-                        String startChunkName,\n-                        String lastChunkName,\n-                        ArrayList<String> chunksToDelete)\n-            throws StorageMetadataException, ChunkStorageException {\n-        // The algorithm is actually very simple.\n-        // It tries to concat all small chunks using appends first.\n-        // Then it tries to concat remaining chunks using concat if available.\n-        // To implement it using single loop we toggle between concat with append and concat modes. (Instead of two passes.)\n-        boolean useAppend = true;\n-        String targetChunkName = startChunkName;\n-\n-        // Iterate through chunk list\n-        while (null != targetChunkName && !targetChunkName.equals(lastChunkName)) {\n-            ChunkMetadata target = (ChunkMetadata) txn.get(targetChunkName);\n-\n-            ArrayList<ChunkInfo> chunksToConcat = new ArrayList<>();\n-            long targetSizeAfterConcat = target.getLength();\n-\n-            // Add target to the list of chunks\n-            chunksToConcat.add(new ChunkInfo(targetSizeAfterConcat, targetChunkName));\n-\n-            String nextChunkName = target.getNextChunk();\n-            ChunkMetadata next = null;\n-\n-            // Gather list of chunks that can be appended together.\n-            while (null != nextChunkName) {\n-                next = (ChunkMetadata) txn.get(nextChunkName);\n-\n-                if (useAppend && config.getMinSizeLimitForConcat() < next.getLength()) {\n-                    break;\n-                }\n+    private CompletableFuture<Void> defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata,\n+                                           String startChunkName,\n+                                           String lastChunkName,\n+                                           ArrayList<String> chunksToDelete) {\n+        return new DefragmentOperation(this, txn, segmentMetadata, startChunkName, lastChunkName, chunksToDelete).call();\n+    }\n \n-                if (targetSizeAfterConcat + next.getLength() > segmentMetadata.getMaxRollinglength() || next.getLength() > config.getMaxSizeLimitForConcat()) {\n-                    break;\n-                }\n+    @Override\n+    public CompletableFuture<Void> delete(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return executeAsync(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle);\n+            Timer timer = new Timer();\n \n-                chunksToConcat.add(new ChunkInfo(next.getLength(), nextChunkName));\n-                targetSizeAfterConcat += next.getLength();\n+            String streamSegmentName = handle.getSegmentName();\n+            return tryWith(metadataStore.beginTransaction(streamSegmentName), txn -> txn.get(streamSegmentName)\n+                    .thenComposeAsync(storageMetadata -> {\n+                        SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                        // Check preconditions\n+                        checkSegmentExists(streamSegmentName, segmentMetadata);\n+                        checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                        segmentMetadata.setActive(false);\n+\n+                        // Delete chunks\n+                        ArrayList<String> chunksToDelete = new ArrayList<>();\n+                        return new ChunkIterator(txn, segmentMetadata)\n+                                .forEach((metadata, name) -> {\n+                                    txn.delete(name);\n+                                    chunksToDelete.add(name);\n+                                })\n+                                .thenRunAsync(() -> txn.delete(streamSegmentName), executor)\n+                                .thenComposeAsync(v ->\n+                                        txn.commit()\n+                                                .thenComposeAsync(vv -> {\n+                                                    // Collect garbage.\n+                                                    return collectGarbage(chunksToDelete);\n+                                                }, executor)\n+                                                .thenRunAsync(() -> {\n+                                                    // Update the read index.\n+                                                    readIndexCache.remove(streamSegmentName);\n+\n+                                                    Duration elapsed = timer.getElapsed();\n+                                                    log.debug(\"{} delete - segment={}, latency={}.\", logPrefix, handle.getSegmentName(), elapsed.toMillis());\n+                                                    LoggerHelpers.traceLeave(log, \"delete\", traceId, handle);\n+                                                }, executor)\n+                                                .exceptionally(e -> {\n+                                                    val ex = Exceptions.unwrap(e);\n+                                                    if (ex instanceof StorageMetadataWritesFencedOutException) {\n+                                                        throw new CompletionException(new StorageNotPrimaryException(streamSegmentName, ex));\n+                                                    }\n+                                                    throw new CompletionException(ex);\n+                                                }), executor);\n+                    }, executor), executor);\n+        });\n+    }\n \n-                nextChunkName = next.getNextChunk();\n-            }\n-            // Note - After above while loop is exited nextChunkName points to chunk next to last one to be concat.\n-            // Which means target should now point to it as next after concat is complete.\n+    @Override\n+    public CompletableFuture<Void> truncate(SegmentHandle handle, long offset, Duration timeout) {\n+        checkInitialized();\n+        return executeAsync(new TruncateOperation(this, handle, offset));\n+    }\n \n-            // If there are chunks that can be appended together then concat them.\n-            if (chunksToConcat.size() > 1) {\n-                // Concat\n+    @Override\n+    public boolean supportsTruncation() {\n+        return true;\n+    }\n \n-                ConcatArgument[] concatArgs = new ConcatArgument[chunksToConcat.size()];\n-                for (int i = 0; i < chunksToConcat.size(); i++) {\n-                    concatArgs[i] = ConcatArgument.fromChunkInfo(chunksToConcat.get(i));\n-                }\n+    @Override\n+    public Iterator<SegmentProperties> listSegments() {\n+        throw new UnsupportedOperationException(\"listSegments is not yet supported\");\n+    }\n \n-                if (!useAppend && chunkStorage.supportsConcat()) {\n-                    int length = chunkStorage.concat(concatArgs);\n-                } else {\n-                    concatUsingAppend(concatArgs);\n-                }\n+    @Override\n+    public CompletableFuture<SegmentHandle> openRead(String streamSegmentName) {\n+        checkInitialized();\n+        return executeAsync(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openRead\", streamSegmentName);\n+            // Validate preconditions and return handle.\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            return tryWith(metadataStore.beginTransaction(streamSegmentName), txn ->\n+                            txn.get(streamSegmentName).thenComposeAsync(storageMetadata -> {\n+                                SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                                segmentMetadata.checkInvariants();\n+                                // This segment was created by an older segment store. Then claim ownership and adjust length.\n+                                CompletableFuture<Void> f = CompletableFuture.completedFuture(null);\n+                                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                                    log.debug(\"{} openRead - Segment needs ownership change. segment={}.\", logPrefix, segmentMetadata.getName());\n+                                    // In case of a failover, length recorded in metadata will be lagging behind its actual length in the storage.\n+                                    // This can happen with lazy commits that were still not committed at the time of failover.\n+                                    f = claimOwnership(txn, segmentMetadata);\n+                                }\n+                                return f.thenApplyAsync(v -> {\n+                                    val retValue = SegmentStorageHandle.readHandle(streamSegmentName);\n+                                    LoggerHelpers.traceLeave(log, \"openRead\", traceId, retValue);\n+                                    return retValue;\n+                                }, executor);\n+                            }, executor),\n+                    executor);\n+        });\n+    }\n \n-                // Delete chunks.\n-                for (int i = 1; i < chunksToConcat.size(); i++) {\n-                    chunksToDelete.add(chunksToConcat.get(i).getName());\n-                }\n+    @Override\n+    public CompletableFuture<Integer> read(SegmentHandle handle, long offset, byte[] buffer, int bufferOffset, int length, Duration timeout) {\n+        checkInitialized();\n+        return executeAsync(new ReadOperation(this, handle, offset, buffer, bufferOffset, length));\n+    }\n \n-                // Set the pointers\n-                target.setLength(targetSizeAfterConcat);\n-                target.setNextChunk(nextChunkName);\n+    @Override\n+    public CompletableFuture<SegmentProperties> getStreamSegmentInfo(String streamSegmentName, Duration timeout) {\n+        checkInitialized();\n+        return executeAsync(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"getStreamSegmentInfo\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            return tryWith(metadataStore.beginTransaction(streamSegmentName), txn ->\n+                    txn.get(streamSegmentName)\n+                            .thenApplyAsync(storageMetadata -> {\n+                                SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                if (null == segmentMetadata) {\n+                                    throw new CompletionException(new StreamSegmentNotExistsException(streamSegmentName));\n+                                }\n+                                segmentMetadata.checkInvariants();\n+\n+                                val retValue = StreamSegmentInformation.builder()\n+                                        .name(streamSegmentName)\n+                                        .sealed(segmentMetadata.isSealed())\n+                                        .length(segmentMetadata.getLength())\n+                                        .startOffset(segmentMetadata.getStartOffset())\n+                                        .lastModified(new ImmutableDate(segmentMetadata.getLastModified()))\n+                                        .build();\n+                                LoggerHelpers.traceLeave(log, \"getStreamSegmentInfo\", traceId, retValue);\n+                                return retValue;\n+                            }, executor), executor);\n+        });\n+    }\n \n-                // If target is the last chunk after this then update metadata accordingly\n-                if (null == nextChunkName) {\n-                    segmentMetadata.setLastChunk(target.getName());\n-                    segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength() - target.getLength());\n-                }\n+    @Override\n+    public CompletableFuture<Boolean> exists(String streamSegmentName, Duration timeout) {\n+        checkInitialized();\n+        return executeAsync(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"exists\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            return tryWith(metadataStore.beginTransaction(streamSegmentName),\n+                    txn -> txn.get(streamSegmentName)\n+                            .thenApplyAsync(storageMetadata -> {\n+                                SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                val retValue = segmentMetadata != null && segmentMetadata.isActive();\n+                                LoggerHelpers.traceLeave(log, \"exists\", traceId, retValue);\n+                                return retValue;\n+                            }, executor),\n+                    executor);\n+        });\n+    }\n \n-                // Update metadata for affected chunks.\n-                for (int i = 1; i < concatArgs.length; i++) {\n-                    txn.delete(concatArgs[i].getName());\n-                    segmentMetadata.decrementChunkCount();\n-                }\n-                txn.update(target);\n-                txn.update(segmentMetadata);\n+    @Override\n+    public void close() {\n+        try {\n+            if (null != this.metadataStore) {\n+                this.metadataStore.close();\n             }\n+        } catch (Exception e) {\n+            log.warn(\"Error during close\", e);\n+        }\n+        this.closed.set(true);\n+    }\n \n-            // Move on to next place in list where we can concat if we are done with append based concats.\n-            if (!useAppend) {\n-                targetChunkName = nextChunkName;\n+    /**\n+     * Executes the given Callable and returns its result, while translating any Exceptions bubbling out of it into\n+     * StreamSegmentExceptions.\n+     *\n+     * @param operation The function to execute.\n+     * @param <R>       Return type of the operation.\n+     * @return CompletableFuture<R> of the return type of the operation.\n+     */\n+    private <R> CompletableFuture<R> executeAsync(Callable<CompletableFuture<R>> operation) {\n+        return CompletableFuture.completedFuture(null).thenComposeAsync(v -> {\n+            Exceptions.checkNotClosed(this.closed.get(), this);\n+            try {\n+                return operation.call();\n+            } catch (CompletionException e) {\n+                throw new CompletionException(Exceptions.unwrap(e));\n+            } catch (Exception e) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg5NTM5OA=="}, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 1133}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTk2NTk2OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMjowMTo1MlrOHcvSWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQyMjo1MzoxOFrOHjRysA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg5NjkyMw==", "bodyText": "This is not thread safe. Here (and in other classes within this big class) you use a number of non-final fields which you frequently modify using different threads. While even though you do it in sequence, due to thread local caching of memory addresses, such modifications may not be immediately visible to other threads, and hence even for subsequent iterations of this loop.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r499896923", "createdAt": "2020-10-05T22:01:52Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -555,575 +378,700 @@ private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n         return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n     }\n \n-    /**\n-     * Adds a system log.\n-     *\n-     * @param systemLogRecords\n-     * @param streamSegmentName Name of the segment.\n-     * @param offset            Offset at which new chunk was added.\n-     * @param oldChunkName      Name of the previous last chunk.\n-     * @param newChunkName      Name of the new last chunk.\n-     */\n-    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n-        systemLogRecords.add(\n-                SystemJournal.ChunkAddedRecord.builder()\n-                        .segmentName(streamSegmentName)\n-                        .offset(offset)\n-                        .oldChunkName(oldChunkName == null ? null : oldChunkName)\n-                        .newChunkName(newChunkName)\n-                        .build());\n-    }\n-\n     /**\n      * Delete the garbage chunks.\n      *\n      * @param chunksTodelete List of chunks to delete.\n      */\n-    private void collectGarbage(Collection<String> chunksTodelete) {\n+    private CompletableFuture<Void> collectGarbage(Collection<String> chunksTodelete) {\n+        CompletableFuture[] futures = new CompletableFuture[chunksTodelete.size()];\n+        int i = 0;\n         for (val chunkTodelete : chunksTodelete) {\n-            try {\n-                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n-                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n-            } catch (ChunkNotFoundException e) {\n-                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n-            } catch (Exception e) {\n-                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n-                // Add it to garbage chunks.\n-                synchronized (garbageChunks) {\n-                    garbageChunks.add(chunkTodelete);\n-                }\n-            }\n+            futures[i++] = chunkStorage.openWrite(chunkTodelete)\n+                    .thenComposeAsync(chunkStorage::delete, executor)\n+                    .thenRunAsync(() -> log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete), executor)\n+                    .exceptionally(e -> {\n+                        val ex = Exceptions.unwrap(e);\n+                        if (ex instanceof ChunkNotFoundException) {\n+                            log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                        } else {\n+                            log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                            // Add it to garbage chunks.\n+                            synchronized (garbageChunks) {\n+                                garbageChunks.add(chunkTodelete);\n+                            }\n+                        }\n+                        return null;\n+                    });\n         }\n+        return CompletableFuture.allOf(futures);\n     }\n \n     @Override\n     public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n         checkInitialized();\n-        return execute(() -> {\n+        return executeAsync(() -> {\n             long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n             Preconditions.checkNotNull(handle, \"handle\");\n             String streamSegmentName = handle.getSegmentName();\n             Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n             Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n \n-            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n-                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n-                // Validate preconditions.\n-                checkSegmentExists(streamSegmentName, segmentMetadata);\n-                checkOwnership(streamSegmentName, segmentMetadata);\n-\n-                // seal if it is not already sealed.\n-                if (!segmentMetadata.isSealed()) {\n-                    segmentMetadata.setSealed(true);\n-                    txn.update(segmentMetadata);\n-                    txn.commit();\n-                }\n-\n-                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n-                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n-                return null;\n-            } catch (StorageMetadataWritesFencedOutException ex) {\n-                throw new StorageNotPrimaryException(streamSegmentName, ex);\n-            }\n+            return tryWith(metadataStore.beginTransaction(handle.getSegmentName()), txn ->\n+                    txn.get(streamSegmentName)\n+                            .thenComposeAsync(storageMetadata -> {\n+                                SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                // Validate preconditions.\n+                                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                                // seal if it is not already sealed.\n+                                if (!segmentMetadata.isSealed()) {\n+                                    segmentMetadata.setSealed(true);\n+                                    txn.update(segmentMetadata);\n+                                    return txn.commit();\n+                                }\n+                                return CompletableFuture.completedFuture(null);\n+                            }, executor)\n+                            .thenRunAsync(() -> {\n+                                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                            }, executor)\n+                            .exceptionally(e -> {\n+                                val ex = Exceptions.unwrap(e);\n+                                if (ex instanceof StorageMetadataWritesFencedOutException) {\n+                                    throw new CompletionException(new StorageNotPrimaryException(streamSegmentName, ex));\n+                                }\n+                                throw new CompletionException(ex);\n+                            }), executor);\n         });\n     }\n \n     @Override\n     public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n         checkInitialized();\n-        return execute(() -> {\n-            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n-            Timer timer = new Timer();\n-\n-            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n-            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n-            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n-            Preconditions.checkArgument(offset >= 0, \"offset\");\n-            String targetSegmentName = targetHandle.getSegmentName();\n-\n-            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n-\n-                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n-\n-                // Validate preconditions.\n-                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n-                targetSegmentMetadata.checkInvariants();\n-                checkNotSealed(targetSegmentName, targetSegmentMetadata);\n-\n-                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n-                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n-                sourceSegmentMetadata.checkInvariants();\n-\n-                // This is a critical assumption at this point which should not be broken,\n-                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n-                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n-\n-                checkSealed(sourceSegmentMetadata);\n-                checkOwnership(targetSegmentMetadata.getName(), targetSegmentMetadata);\n-\n-                if (sourceSegmentMetadata.getStartOffset() != 0) {\n-                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n-                }\n-\n-                if (offset != targetSegmentMetadata.getLength()) {\n-                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n-                }\n-\n-                // Update list of chunks by appending sources list of chunks.\n-                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n-                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n-\n-                if (targetLastChunk != null) {\n-                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n-                    txn.update(targetLastChunk);\n-                } else {\n-                    if (sourceFirstChunk != null) {\n-                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n-                        txn.update(sourceFirstChunk);\n-                    }\n-                }\n-\n-                // Update segments's last chunk to point to the sources last segment.\n-                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n-\n-                // Update the length of segment.\n-                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n-                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n-\n-                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n-\n-                txn.update(targetSegmentMetadata);\n-                txn.delete(sourceSegment);\n-\n-                // Finally defrag immediately.\n-                ArrayList<String> chunksToDelete = new ArrayList<>();\n-                if (shouldDefrag() && null != targetLastChunk) {\n-                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null, chunksToDelete);\n-                }\n-\n-                targetSegmentMetadata.checkInvariants();\n-\n-                // Finally commit transaction.\n-                txn.commit();\n-\n-                // Collect garbage.\n-                collectGarbage(chunksToDelete);\n-\n-                // Update the read index.\n-                readIndexCache.remove(sourceSegment);\n-\n-                Duration elapsed = timer.getElapsed();\n-                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n-                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n-\n-            } catch (StorageMetadataWritesFencedOutException ex) {\n-                throw new StorageNotPrimaryException(targetSegmentName, ex);\n-            }\n-\n-            return null;\n-        });\n+        return executeAsync(new ConcatOperation(this, targetHandle, offset, sourceSegment));\n     }\n \n     private boolean shouldAppend() {\n         return chunkStorage.supportsAppend() && config.isAppendEnabled();\n     }\n \n-    private boolean shouldDefrag() {\n-        return shouldAppend() || chunkStorage.supportsConcat();\n-    }\n-\n     /**\n      * Defragments the list of chunks for a given segment.\n      * It finds eligible consecutive chunks that can be merged together.\n      * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n      * Conceptually this is like deleting nodes from middle of the list of chunks.\n      *\n-     * <Ul>\n-     * <li> In the absence of defragmentation, the number of chunks for individual segments keeps on increasing.\n-     * When we have too many small chunks (say because many transactions with little data on some segments), the segment\n-     * is fragmented - this may impact both the read throughput and the performance of the metadata store.\n-     * This problem is further intensified when we have stores that do not support append semantics (e.g., stock S3) and\n-     * each write becomes a separate chunk.\n-     * </li>\n-     * <li>\n-     * If the underlying storage provides some facility to stitch together smaller chunk into larger chunks, then we do\n-     * actually want to exploit that, specially when the underlying implementation is only a metadata operation. We want\n-     * to leverage multi-part uploads in object stores that support it (e.g., AWS S3, Dell EMC ECS) as they are typically\n-     * only metadata operations, reducing the overall cost of the merging them together. HDFS also supports merges,\n-     * whereas NFS has no concept of merging natively.\n-     *\n-     * As chunks become larger, append writes (read source completely and append it back at the end of target)\n-     * become inefficient. Consequently, a native option for merging is desirable. We use such native merge capability\n-     * when available, and if not available, then we use appends.\n-     * </li>\n-     * <li>\n-     * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n-     * We can then fine tune that background task to run optimally with low overhead.\n-     * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n-     * </li>\n-     * <li>\n-     * <li>\n-     * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n-     * </li>\n-     * </ul>\n-     *\n-     * What controls whether we invoke concat or simulate through appends?\n-     * There are a few different capabilities that ChunkStorage needs to provide.\n-     * <ul>\n-     * <li>Does ChunkStorage support appending to existing chunks? For vanilla S3 compatible this would return false.\n-     * This is indicated by supportsAppend.</li>\n-     * <li>Does ChunkStorage support for concatenating chunks ? This is indicated by supportsConcat.\n-     * If this is true then concat operation will be invoked otherwise chunks will be appended.</li>\n-     * <li>There are some obvious constraints - For ChunkStorage support any concat functionality it must support either\n-     * append or concat.</li>\n-     * <li>Also when ChunkStorage supports both concat and append, ChunkedSegmentStorage will invoke appropriate method\n-     * depending on size of target and source chunks. (Eg. ECS)</li>\n-     * </ul>\n-     *\n-     * <li>\n-     * What controls defrag?\n-     * There are two additional parameters that control when concat\n-     * <li>minSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered a small object.\n-     * For small source objects, append is used instead of using concat. (For really small txn it is rather efficient to use append than MPU).</li>\n-     * <li>maxSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered for concat. (Eg S3 might have max limit on chunk size).</li>\n-     * In short there is a size beyond which using append is not advisable. Conversely there is a size below which concat is not efficient.(minSizeLimitForConcat )\n-     * Then there is limit which concating does not make sense maxSizeLimitForConcat\n-     * </li>\n-     * <li>\n-     * What is the defrag algorithm\n-     * <pre>\n-     * While(segment.hasConcatableChunks()){\n-     *     Set<List<Chunk>> s = FindConsecutiveConcatableChunks();\n-     *     For (List<chunk> list : s){\n-     *        ConcatChunks (list);\n-     *     }\n-     * }\n-     * </pre>\n-     * </li>\n-     * </ul>\n-     *\n      * @param txn             Active {@link MetadataTransaction}.\n      * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n      * @param startChunkName  Name of the first chunk to start defragmentation.\n      * @param lastChunkName   Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n      * @param chunksToDelete  List of chunks to which names of chunks to be deleted are added. It is the responsibility\n      *                        of caller to garbage collect these chunks.\n-     * @throws ChunkStorageException In case of any chunk storage related errors.\n-     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     *                        throws ChunkStorageException    In case of any chunk storage related errors.\n+     *                        throws StorageMetadataException In case of any chunk metadata store related errors.\n      */\n-    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata,\n-                        String startChunkName,\n-                        String lastChunkName,\n-                        ArrayList<String> chunksToDelete)\n-            throws StorageMetadataException, ChunkStorageException {\n-        // The algorithm is actually very simple.\n-        // It tries to concat all small chunks using appends first.\n-        // Then it tries to concat remaining chunks using concat if available.\n-        // To implement it using single loop we toggle between concat with append and concat modes. (Instead of two passes.)\n-        boolean useAppend = true;\n-        String targetChunkName = startChunkName;\n-\n-        // Iterate through chunk list\n-        while (null != targetChunkName && !targetChunkName.equals(lastChunkName)) {\n-            ChunkMetadata target = (ChunkMetadata) txn.get(targetChunkName);\n-\n-            ArrayList<ChunkInfo> chunksToConcat = new ArrayList<>();\n-            long targetSizeAfterConcat = target.getLength();\n-\n-            // Add target to the list of chunks\n-            chunksToConcat.add(new ChunkInfo(targetSizeAfterConcat, targetChunkName));\n-\n-            String nextChunkName = target.getNextChunk();\n-            ChunkMetadata next = null;\n-\n-            // Gather list of chunks that can be appended together.\n-            while (null != nextChunkName) {\n-                next = (ChunkMetadata) txn.get(nextChunkName);\n-\n-                if (useAppend && config.getMinSizeLimitForConcat() < next.getLength()) {\n-                    break;\n-                }\n+    private CompletableFuture<Void> defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata,\n+                                           String startChunkName,\n+                                           String lastChunkName,\n+                                           ArrayList<String> chunksToDelete) {\n+        return new DefragmentOperation(this, txn, segmentMetadata, startChunkName, lastChunkName, chunksToDelete).call();\n+    }\n \n-                if (targetSizeAfterConcat + next.getLength() > segmentMetadata.getMaxRollinglength() || next.getLength() > config.getMaxSizeLimitForConcat()) {\n-                    break;\n-                }\n+    @Override\n+    public CompletableFuture<Void> delete(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return executeAsync(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle);\n+            Timer timer = new Timer();\n \n-                chunksToConcat.add(new ChunkInfo(next.getLength(), nextChunkName));\n-                targetSizeAfterConcat += next.getLength();\n+            String streamSegmentName = handle.getSegmentName();\n+            return tryWith(metadataStore.beginTransaction(streamSegmentName), txn -> txn.get(streamSegmentName)\n+                    .thenComposeAsync(storageMetadata -> {\n+                        SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                        // Check preconditions\n+                        checkSegmentExists(streamSegmentName, segmentMetadata);\n+                        checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                        segmentMetadata.setActive(false);\n+\n+                        // Delete chunks\n+                        ArrayList<String> chunksToDelete = new ArrayList<>();\n+                        return new ChunkIterator(txn, segmentMetadata)\n+                                .forEach((metadata, name) -> {\n+                                    txn.delete(name);\n+                                    chunksToDelete.add(name);\n+                                })\n+                                .thenRunAsync(() -> txn.delete(streamSegmentName), executor)\n+                                .thenComposeAsync(v ->\n+                                        txn.commit()\n+                                                .thenComposeAsync(vv -> {\n+                                                    // Collect garbage.\n+                                                    return collectGarbage(chunksToDelete);\n+                                                }, executor)\n+                                                .thenRunAsync(() -> {\n+                                                    // Update the read index.\n+                                                    readIndexCache.remove(streamSegmentName);\n+\n+                                                    Duration elapsed = timer.getElapsed();\n+                                                    log.debug(\"{} delete - segment={}, latency={}.\", logPrefix, handle.getSegmentName(), elapsed.toMillis());\n+                                                    LoggerHelpers.traceLeave(log, \"delete\", traceId, handle);\n+                                                }, executor)\n+                                                .exceptionally(e -> {\n+                                                    val ex = Exceptions.unwrap(e);\n+                                                    if (ex instanceof StorageMetadataWritesFencedOutException) {\n+                                                        throw new CompletionException(new StorageNotPrimaryException(streamSegmentName, ex));\n+                                                    }\n+                                                    throw new CompletionException(ex);\n+                                                }), executor);\n+                    }, executor), executor);\n+        });\n+    }\n \n-                nextChunkName = next.getNextChunk();\n-            }\n-            // Note - After above while loop is exited nextChunkName points to chunk next to last one to be concat.\n-            // Which means target should now point to it as next after concat is complete.\n+    @Override\n+    public CompletableFuture<Void> truncate(SegmentHandle handle, long offset, Duration timeout) {\n+        checkInitialized();\n+        return executeAsync(new TruncateOperation(this, handle, offset));\n+    }\n \n-            // If there are chunks that can be appended together then concat them.\n-            if (chunksToConcat.size() > 1) {\n-                // Concat\n+    @Override\n+    public boolean supportsTruncation() {\n+        return true;\n+    }\n \n-                ConcatArgument[] concatArgs = new ConcatArgument[chunksToConcat.size()];\n-                for (int i = 0; i < chunksToConcat.size(); i++) {\n-                    concatArgs[i] = ConcatArgument.fromChunkInfo(chunksToConcat.get(i));\n-                }\n+    @Override\n+    public Iterator<SegmentProperties> listSegments() {\n+        throw new UnsupportedOperationException(\"listSegments is not yet supported\");\n+    }\n \n-                if (!useAppend && chunkStorage.supportsConcat()) {\n-                    int length = chunkStorage.concat(concatArgs);\n-                } else {\n-                    concatUsingAppend(concatArgs);\n-                }\n+    @Override\n+    public CompletableFuture<SegmentHandle> openRead(String streamSegmentName) {\n+        checkInitialized();\n+        return executeAsync(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openRead\", streamSegmentName);\n+            // Validate preconditions and return handle.\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            return tryWith(metadataStore.beginTransaction(streamSegmentName), txn ->\n+                            txn.get(streamSegmentName).thenComposeAsync(storageMetadata -> {\n+                                SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                                segmentMetadata.checkInvariants();\n+                                // This segment was created by an older segment store. Then claim ownership and adjust length.\n+                                CompletableFuture<Void> f = CompletableFuture.completedFuture(null);\n+                                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                                    log.debug(\"{} openRead - Segment needs ownership change. segment={}.\", logPrefix, segmentMetadata.getName());\n+                                    // In case of a failover, length recorded in metadata will be lagging behind its actual length in the storage.\n+                                    // This can happen with lazy commits that were still not committed at the time of failover.\n+                                    f = claimOwnership(txn, segmentMetadata);\n+                                }\n+                                return f.thenApplyAsync(v -> {\n+                                    val retValue = SegmentStorageHandle.readHandle(streamSegmentName);\n+                                    LoggerHelpers.traceLeave(log, \"openRead\", traceId, retValue);\n+                                    return retValue;\n+                                }, executor);\n+                            }, executor),\n+                    executor);\n+        });\n+    }\n \n-                // Delete chunks.\n-                for (int i = 1; i < chunksToConcat.size(); i++) {\n-                    chunksToDelete.add(chunksToConcat.get(i).getName());\n-                }\n+    @Override\n+    public CompletableFuture<Integer> read(SegmentHandle handle, long offset, byte[] buffer, int bufferOffset, int length, Duration timeout) {\n+        checkInitialized();\n+        return executeAsync(new ReadOperation(this, handle, offset, buffer, bufferOffset, length));\n+    }\n \n-                // Set the pointers\n-                target.setLength(targetSizeAfterConcat);\n-                target.setNextChunk(nextChunkName);\n+    @Override\n+    public CompletableFuture<SegmentProperties> getStreamSegmentInfo(String streamSegmentName, Duration timeout) {\n+        checkInitialized();\n+        return executeAsync(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"getStreamSegmentInfo\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            return tryWith(metadataStore.beginTransaction(streamSegmentName), txn ->\n+                    txn.get(streamSegmentName)\n+                            .thenApplyAsync(storageMetadata -> {\n+                                SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                if (null == segmentMetadata) {\n+                                    throw new CompletionException(new StreamSegmentNotExistsException(streamSegmentName));\n+                                }\n+                                segmentMetadata.checkInvariants();\n+\n+                                val retValue = StreamSegmentInformation.builder()\n+                                        .name(streamSegmentName)\n+                                        .sealed(segmentMetadata.isSealed())\n+                                        .length(segmentMetadata.getLength())\n+                                        .startOffset(segmentMetadata.getStartOffset())\n+                                        .lastModified(new ImmutableDate(segmentMetadata.getLastModified()))\n+                                        .build();\n+                                LoggerHelpers.traceLeave(log, \"getStreamSegmentInfo\", traceId, retValue);\n+                                return retValue;\n+                            }, executor), executor);\n+        });\n+    }\n \n-                // If target is the last chunk after this then update metadata accordingly\n-                if (null == nextChunkName) {\n-                    segmentMetadata.setLastChunk(target.getName());\n-                    segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength() - target.getLength());\n-                }\n+    @Override\n+    public CompletableFuture<Boolean> exists(String streamSegmentName, Duration timeout) {\n+        checkInitialized();\n+        return executeAsync(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"exists\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            return tryWith(metadataStore.beginTransaction(streamSegmentName),\n+                    txn -> txn.get(streamSegmentName)\n+                            .thenApplyAsync(storageMetadata -> {\n+                                SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                val retValue = segmentMetadata != null && segmentMetadata.isActive();\n+                                LoggerHelpers.traceLeave(log, \"exists\", traceId, retValue);\n+                                return retValue;\n+                            }, executor),\n+                    executor);\n+        });\n+    }\n \n-                // Update metadata for affected chunks.\n-                for (int i = 1; i < concatArgs.length; i++) {\n-                    txn.delete(concatArgs[i].getName());\n-                    segmentMetadata.decrementChunkCount();\n-                }\n-                txn.update(target);\n-                txn.update(segmentMetadata);\n+    @Override\n+    public void close() {\n+        try {\n+            if (null != this.metadataStore) {\n+                this.metadataStore.close();\n             }\n+        } catch (Exception e) {\n+            log.warn(\"Error during close\", e);\n+        }\n+        this.closed.set(true);\n+    }\n \n-            // Move on to next place in list where we can concat if we are done with append based concats.\n-            if (!useAppend) {\n-                targetChunkName = nextChunkName;\n+    /**\n+     * Executes the given Callable and returns its result, while translating any Exceptions bubbling out of it into\n+     * StreamSegmentExceptions.\n+     *\n+     * @param operation The function to execute.\n+     * @param <R>       Return type of the operation.\n+     * @return CompletableFuture<R> of the return type of the operation.\n+     */\n+    private <R> CompletableFuture<R> executeAsync(Callable<CompletableFuture<R>> operation) {\n+        return CompletableFuture.completedFuture(null).thenComposeAsync(v -> {\n+            Exceptions.checkNotClosed(this.closed.get(), this);\n+            try {\n+                return operation.call();\n+            } catch (CompletionException e) {\n+                throw new CompletionException(Exceptions.unwrap(e));\n+            } catch (Exception e) {\n+                throw new CompletionException(e);\n             }\n+        }, this.executor);\n+    }\n \n-            // Toggle\n-            useAppend = !useAppend;\n-        }\n-\n-        // Make sure no invariants are broken.\n-        segmentMetadata.checkInvariants();\n+    private static <T extends AutoCloseable, R> CompletableFuture<R> tryWith(T closeable, Function<T, CompletableFuture<R>> function, Executor executor) {\n+        return function.apply(closeable)\n+                    .whenCompleteAsync((v, ex) -> {\n+                        try {\n+                            closeable.close();\n+                        } catch (Exception e) {\n+                            throw new CompletionException(e);\n+                        }\n+                    }, executor);\n     }\n \n-    private void concatUsingAppend(ConcatArgument[] concatArgs) throws ChunkStorageException {\n-        long writeAtOffset = concatArgs[0].getLength();\n-        val writeHandle = ChunkHandle.writeHandle(concatArgs[0].getName());\n-        for (int i = 1; i < concatArgs.length; i++) {\n-            int readAtOffset = 0;\n-            val arg = concatArgs[i];\n-            int bytesToRead = Math.toIntExact(arg.getLength());\n-\n-            while (bytesToRead > 0) {\n-                byte[] buffer = new byte[Math.min(config.getMaxBufferSizeForChunkDataTransfer(), bytesToRead)];\n-                int size = chunkStorage.read(ChunkHandle.readHandle(arg.getName()), readAtOffset, buffer.length, buffer, 0);\n-                bytesToRead -= size;\n-                readAtOffset += size;\n-                writeAtOffset += chunkStorage.write(writeHandle, writeAtOffset, size, new ByteArrayInputStream(buffer, 0, size));\n-            }\n+    private void checkSegmentExists(String streamSegmentName, SegmentMetadata segmentMetadata) {\n+        if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+            throw new CompletionException(new StreamSegmentNotExistsException(streamSegmentName));\n         }\n     }\n \n-    @Override\n-    public CompletableFuture<Void> delete(SegmentHandle handle, Duration timeout) {\n-        checkInitialized();\n-        return execute(() -> {\n-            long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle);\n-            Timer timer = new Timer();\n+    private void checkOwnership(String streamSegmentName, SegmentMetadata segmentMetadata) {\n+        if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+            throw new CompletionException(new StorageNotPrimaryException(streamSegmentName));\n+        }\n+    }\n \n-            String streamSegmentName = handle.getSegmentName();\n-            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n-                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n-\n-                // Check preconditions\n-                checkSegmentExists(streamSegmentName, segmentMetadata);\n-                checkOwnership(streamSegmentName, segmentMetadata);\n-\n-                segmentMetadata.setActive(false);\n-\n-                // Delete chunks\n-                String currentChunkName = segmentMetadata.getFirstChunk();\n-                ChunkMetadata currentMetadata;\n-                ArrayList<String> chunksToDelete = new ArrayList<>();\n-                while (currentChunkName != null) {\n-                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n-                    // Delete underlying file.\n-                    chunksToDelete.add(currentChunkName);\n-                    currentChunkName = currentMetadata.getNextChunk();\n-                    txn.delete(currentMetadata.getName());\n-                }\n+    private void checkNotSealed(String streamSegmentName, SegmentMetadata segmentMetadata) {\n+        if (segmentMetadata.isSealed()) {\n+            throw new CompletionException(new StreamSegmentSealedException(streamSegmentName));\n+        }\n+    }\n \n-                // Commit.\n-                txn.delete(streamSegmentName);\n-                txn.commit();\n+    private void checkInitialized() {\n+        Preconditions.checkState(null != this.metadataStore);\n+        Preconditions.checkState(0 != this.epoch);\n+        Preconditions.checkState(!closed.get());\n+    }\n \n-                // Collect garbage.\n-                collectGarbage(chunksToDelete);\n+    private class ChunkIterator {\n+        private final MetadataTransaction txn;\n+        private String currentChunkName;\n+        private ChunkMetadata currentMetadata;\n \n-                // Update the read index.\n-                readIndexCache.remove(streamSegmentName);\n+        ChunkIterator(MetadataTransaction txn, SegmentMetadata segmentMetadata) {\n+            this.txn = txn;\n+            currentChunkName = segmentMetadata.getFirstChunk();\n+        }\n \n-                Duration elapsed = timer.getElapsed();\n-                log.debug(\"{} delete - segment={}, latency={}.\", logPrefix, handle.getSegmentName(), elapsed.toMillis());\n-                LoggerHelpers.traceLeave(log, \"delete\", traceId, handle);\n-                return null;\n-            } catch (StorageMetadataWritesFencedOutException ex) {\n-                throw new StorageNotPrimaryException(streamSegmentName, ex);\n-            }\n-        });\n+        public CompletableFuture<Void> forEach(BiConsumer<ChunkMetadata, String> consumer) {\n+            return Futures.loop(\n+                    () -> currentChunkName != null,\n+                    () -> txn.get(currentChunkName)\n+                            .thenApplyAsync(storageMetadata -> {\n+                                currentMetadata = (ChunkMetadata) storageMetadata;\n+                                consumer.accept(currentMetadata, currentChunkName);\n+                                // Move next\n+                                currentChunkName = currentMetadata.getNextChunk();\n+                                return null;\n+                            }, executor),\n+                    executor);\n+        }\n     }\n \n-    @Override\n-    public CompletableFuture<Void> truncate(SegmentHandle handle, long offset, Duration timeout) {\n-        checkInitialized();\n-        return execute(() -> {\n-            long traceId = LoggerHelpers.traceEnter(log, \"truncate\", handle, offset);\n-            Timer timer = new Timer();\n+    private static class TruncateOperation implements Callable<CompletableFuture<Void>> {\n+        private final SegmentHandle handle;\n+        private final long offset;\n+        private final ChunkedSegmentStorage chunkedSegmentStorage;\n+\n+        private String currentChunkName;\n+        private ChunkMetadata currentMetadata;\n+        private long oldLength;\n+        private long startOffset;\n+        private ArrayList<String> chunksToDelete = new ArrayList<>();\n+        private SegmentMetadata segmentMetadata;\n+        private String streamSegmentName;\n+\n+        private boolean isLoopExited;\n+        private long traceId;\n+        private Timer timer;\n+\n+        TruncateOperation(ChunkedSegmentStorage chunkedSegmentStorage, SegmentHandle handle, long offset) {\n+            this.handle = handle;\n+            this.offset = offset;\n+            this.chunkedSegmentStorage = chunkedSegmentStorage;\n+        }\n \n-            Preconditions.checkArgument(null != handle, \"handle\");\n-            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n-            Preconditions.checkArgument(offset >= 0, \"offset\");\n+        public CompletableFuture<Void> call() {\n+            traceId = LoggerHelpers.traceEnter(log, \"truncate\", handle, offset);\n+            timer = new Timer();\n+\n+            checkPreconditions();\n+\n+            streamSegmentName = handle.getSegmentName();\n+            return tryWith(chunkedSegmentStorage.metadataStore.beginTransaction(streamSegmentName), txn ->\n+                    txn.get(streamSegmentName)\n+                            .thenComposeAsync(storageMetadata -> {\n+                                segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                // Check preconditions\n+                                checkPreconditions(streamSegmentName, segmentMetadata);\n+\n+                                if (segmentMetadata.getStartOffset() == offset) {\n+                                    // Nothing to do\n+                                    return CompletableFuture.completedFuture(null);\n+                                }\n+\n+                                return updateFirstChunk(txn)\n+                                        .thenComposeAsync(v -> {\n+                                            deleteChunks(txn);\n+\n+                                            txn.update(segmentMetadata);\n+\n+                                            // Check invariants.\n+                                            Preconditions.checkState(segmentMetadata.getLength() == oldLength, \"truncate should not change segment length\");\n+                                            segmentMetadata.checkInvariants();\n+\n+                                            // Finally commit.\n+                                            return commit(txn)\n+                                                    .handleAsync(this::handleException, chunkedSegmentStorage.executor)\n+                                                    .thenComposeAsync(vv ->\n+                                                                    chunkedSegmentStorage.collectGarbage(chunksToDelete).thenApplyAsync(vvv -> {\n+                                                                        postCommit();\n+                                                                        return null;\n+                                                                    }, chunkedSegmentStorage.executor),\n+                                                            chunkedSegmentStorage.executor);\n+                                        }, chunkedSegmentStorage.executor);\n+                            }, chunkedSegmentStorage.executor), chunkedSegmentStorage.executor);\n+        }\n \n-            String streamSegmentName = handle.getSegmentName();\n-            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n-                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+        private void postCommit() {\n+            // Update the read index by removing all entries below truncate offset.\n+            chunkedSegmentStorage.readIndexCache.truncateReadIndex(streamSegmentName, segmentMetadata.getStartOffset());\n \n-                // Check preconditions\n-                checkSegmentExists(streamSegmentName, segmentMetadata);\n-                checkNotSealed(streamSegmentName, segmentMetadata);\n-                checkOwnership(streamSegmentName, segmentMetadata);\n+            logEnd();\n+        }\n+\n+        private void logEnd() {\n+            Duration elapsed = timer.getElapsed();\n+            log.debug(\"{} truncate - segment={}, offset={}, latency={}.\", chunkedSegmentStorage.logPrefix, handle.getSegmentName(), offset, elapsed.toMillis());\n+            LoggerHelpers.traceLeave(log, \"truncate\", traceId, handle, offset);\n+        }\n \n-                if (segmentMetadata.getLength() < offset || segmentMetadata.getStartOffset() > offset) {\n-                    throw new IllegalArgumentException(String.format(\"offset %d is outside of valid range [%d, %d) for segment %s\",\n-                            offset, segmentMetadata.getStartOffset(), segmentMetadata.getLength(), streamSegmentName));\n+        private Void handleException(Void value, Throwable e) {\n+            if (null != e) {\n+                val ex = Exceptions.unwrap(e);\n+                if (ex instanceof StorageMetadataWritesFencedOutException) {\n+                    throw new CompletionException(new StorageNotPrimaryException(streamSegmentName, ex));\n                 }\n+                throw new CompletionException(ex);\n+            }\n+            return value;\n+        }\n \n-                if (segmentMetadata.getStartOffset() == offset) {\n-                    // Nothing to do\n+        private CompletableFuture<Void> commit(MetadataTransaction txn) {\n+            // Commit system logs.\n+            if (chunkedSegmentStorage.isStorageSystemSegment(segmentMetadata)) {\n+                val finalStartOffset = startOffset;\n+                txn.setExternalCommitStep(() -> {\n+                    chunkedSegmentStorage.systemJournal.commitRecord(\n+                            SystemJournal.TruncationRecord.builder()\n+                                    .segmentName(streamSegmentName)\n+                                    .offset(offset)\n+                                    .firstChunkName(segmentMetadata.getFirstChunk())\n+                                    .startOffset(finalStartOffset)\n+                                    .build());\n                     return null;\n-                }\n+                });\n+            }\n \n-                String currentChunkName = segmentMetadata.getFirstChunk();\n-                ChunkMetadata currentMetadata;\n-                long oldLength = segmentMetadata.getLength();\n-                long startOffset = segmentMetadata.getFirstChunkStartOffset();\n-                ArrayList<String> chunksToDelete = new ArrayList<>();\n-                while (currentChunkName != null) {\n-                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n-                    Preconditions.checkState(null != currentMetadata, \"currentMetadata is null.\");\n-\n-                    // If for given chunk start <= offset < end  then we have found the chunk that will be the first chunk.\n-                    if ((startOffset <= offset) && (startOffset + currentMetadata.getLength() > offset)) {\n-                        break;\n-                    }\n-\n-                    startOffset += currentMetadata.getLength();\n-                    chunksToDelete.add(currentMetadata.getName());\n-                    segmentMetadata.decrementChunkCount();\n+            // Finally commit.\n+            return txn.commit();\n+        }\n \n-                    // move to next chunk\n-                    currentChunkName = currentMetadata.getNextChunk();\n-                }\n+        private CompletableFuture<Void> updateFirstChunk(MetadataTransaction txn) {\n+            currentChunkName = segmentMetadata.getFirstChunk();\n+            oldLength = segmentMetadata.getLength();\n+            startOffset = segmentMetadata.getFirstChunkStartOffset();\n+            return Futures.loop(\n+                    () -> currentChunkName != null && !isLoopExited,\n+                    () -> txn.get(currentChunkName)\n+                            .thenApplyAsync(storageMetadata -> {\n+                                currentMetadata = (ChunkMetadata) storageMetadata;\n+                                Preconditions.checkState(null != currentMetadata, \"currentMetadata is null.\");\n+\n+                                // If for given chunk start <= offset < end  then we have found the chunk that will be the first chunk.\n+                                if ((startOffset <= offset) && (startOffset + currentMetadata.getLength() > offset)) {\n+                                    isLoopExited = true;\n+                                    return null;\n+                                }\n+\n+                                startOffset += currentMetadata.getLength();\n+                                chunksToDelete.add(currentMetadata.getName());\n+                                segmentMetadata.decrementChunkCount();\n+\n+                                // move to next chunk\n+                                currentChunkName = currentMetadata.getNextChunk();\n+                                return null;\n+                            }, chunkedSegmentStorage.executor),\n+                    chunkedSegmentStorage.executor\n+            ).thenApplyAsync(v -> {\n                 segmentMetadata.setFirstChunk(currentChunkName);\n                 segmentMetadata.setStartOffset(offset);\n                 segmentMetadata.setFirstChunkStartOffset(startOffset);\n-                for (String toDelete : chunksToDelete) {\n-                    txn.delete(toDelete);\n-                    // Adjust last chunk if required.\n-                    if (toDelete.equals(segmentMetadata.getLastChunk())) {\n-                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n-                        segmentMetadata.setLastChunk(null);\n-                    }\n-                }\n-                txn.update(segmentMetadata);\n+                return null;\n+            }, chunkedSegmentStorage.executor);\n+        }\n \n-                // Check invariants.\n-                Preconditions.checkState(segmentMetadata.getLength() == oldLength, \"truncate should not change segment length\");\n-                segmentMetadata.checkInvariants();\n-\n-                // Commit system logs.\n-                if (isStorageSystemSegment(segmentMetadata)) {\n-                    val finalStartOffset = startOffset;\n-                    txn.setExternalCommitStep(() -> {\n-                        systemJournal.commitRecord(\n-                                SystemJournal.TruncationRecord.builder()\n-                                        .segmentName(streamSegmentName)\n-                                        .offset(offset)\n-                                        .firstChunkName(segmentMetadata.getFirstChunk())\n-                                        .startOffset(finalStartOffset)\n-                                        .build());\n-                        return null;\n-                    });\n+        private void deleteChunks(MetadataTransaction txn) {\n+            for (String toDelete : chunksToDelete) {\n+                txn.delete(toDelete);\n+                // Adjust last chunk if required.\n+                if (toDelete.equals(segmentMetadata.getLastChunk())) {\n+                    segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+                    segmentMetadata.setLastChunk(null);\n                 }\n+            }\n+        }\n \n-                // Finally commit.\n-                txn.commit();\n-\n-                collectGarbage(chunksToDelete);\n-\n-                // Update the read index by removing all entries below truncate offset.\n-                readIndexCache.truncateReadIndex(streamSegmentName, segmentMetadata.getStartOffset());\n+        private void checkPreconditions(String streamSegmentName, SegmentMetadata segmentMetadata) {\n+            chunkedSegmentStorage.checkSegmentExists(streamSegmentName, segmentMetadata);\n+            chunkedSegmentStorage.checkNotSealed(streamSegmentName, segmentMetadata);\n+            chunkedSegmentStorage.checkOwnership(streamSegmentName, segmentMetadata);\n \n-                Duration elapsed = timer.getElapsed();\n-                log.debug(\"{} truncate - segment={}, offset={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, elapsed.toMillis());\n-                LoggerHelpers.traceLeave(log, \"truncate\", traceId, handle, offset);\n-                return null;\n-            } catch (StorageMetadataWritesFencedOutException ex) {\n-                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            if (segmentMetadata.getLength() < offset || segmentMetadata.getStartOffset() > offset) {\n+                throw new IllegalArgumentException(String.format(\"offset %d is outside of valid range [%d, %d) for segment %s\",\n+                        offset, segmentMetadata.getStartOffset(), segmentMetadata.getLength(), streamSegmentName));\n             }\n-        });\n-    }\n+        }\n \n-    @Override\n-    public boolean supportsTruncation() {\n-        return true;\n+        private void checkPreconditions() {\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+        }\n     }\n \n-    @Override\n-    public Iterator<SegmentProperties> listSegments() throws IOException {\n-        throw new UnsupportedOperationException(\"listSegments is not yet supported\");\n-    }\n+    private static class ReadOperation implements Callable<CompletableFuture<Integer>> {\n+        private final SegmentHandle handle;\n+        private final long offset;\n+        private final byte[] buffer;\n+        private final int bufferOffset;\n+        private final int length;\n+        private final ChunkedSegmentStorage chunkedSegmentStorage;\n+        private long traceId;\n+        private Timer timer;\n+        private String streamSegmentName;\n+        private SegmentMetadata segmentMetadata;\n+        private int bytesRemaining;\n+        private int currentBufferOffset;\n+        private long currentOffset;\n+        private int totalBytesRead = 0;\n+        private long startOffsetForCurrentChunk;\n+        private String currentChunkName;\n+        private ChunkMetadata chunkToReadFrom = null;\n+        private boolean isLoopExited;\n+        private int cntScanned = 0;\n+        private int bytesToRead;\n+\n+        ReadOperation(ChunkedSegmentStorage chunkedSegmentStorage, SegmentHandle handle, long offset, byte[] buffer, int bufferOffset, int length) {\n+            this.handle = handle;\n+            this.offset = offset;\n+            this.buffer = buffer;\n+            this.bufferOffset = bufferOffset;\n+            this.length = length;\n+            this.chunkedSegmentStorage = chunkedSegmentStorage;\n+        }\n \n-    @Override\n-    public CompletableFuture<SegmentHandle> openRead(String streamSegmentName) {\n-        checkInitialized();\n-        return execute(() -> {\n-            long traceId = LoggerHelpers.traceEnter(log, \"openRead\", streamSegmentName);\n-            // Validate preconditions and return handle.\n-            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n-            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n-                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n-                checkSegmentExists(streamSegmentName, segmentMetadata);\n-                segmentMetadata.checkInvariants();\n-                // This segment was created by an older segment store. Then claim ownership and adjust length.\n-                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n-                    log.debug(\"{} openRead - Segment needs ownership change. segment={}.\", logPrefix, segmentMetadata.getName());\n-                    // In case of a failover, length recorded in metadata will be lagging behind its actual length in the storage.\n-                    // This can happen with lazy commits that were still not committed at the time of failover.\n-                    claimOwnership(txn, segmentMetadata);\n-                }\n-                val retValue = SegmentStorageHandle.readHandle(streamSegmentName);\n-                LoggerHelpers.traceLeave(log, \"openRead\", traceId, retValue);\n-                return retValue;\n+        public CompletableFuture<Integer> call() {\n+            traceId = LoggerHelpers.traceEnter(log, \"read\", handle, offset, length);\n+            timer = new Timer();\n+\n+            // Validate preconditions.\n+            checkPreconditions();\n+            streamSegmentName = handle.getSegmentName();\n+            return tryWith(chunkedSegmentStorage.metadataStore.beginTransaction(streamSegmentName),\n+                    txn -> txn.get(streamSegmentName)\n+                            .thenComposeAsync(storageMetadata -> {\n+                                segmentMetadata = (SegmentMetadata) storageMetadata;\n+\n+                                // Validate preconditions.\n+                                checkState();\n+\n+                                if (length == 0) {\n+                                    return CompletableFuture.completedFuture(0);\n+                                }\n+\n+                                return findChunkForOffset(txn)\n+                                        .thenComposeAsync(v -> {\n+                                            // Now read.\n+                                            return readData(txn);\n+                                        }, chunkedSegmentStorage.executor)\n+                                        .thenApplyAsync(v -> {\n+                                            logEnd();\n+                                            return totalBytesRead;\n+                                        }, chunkedSegmentStorage.executor);\n+                            }, chunkedSegmentStorage.executor),\n+                    chunkedSegmentStorage.executor);\n+        }\n+\n+        private void logEnd() {\n+            Duration elapsed = timer.getElapsed();\n+            log.debug(\"{} read - segment={}, offset={}, bytesRead={}, latency={}.\", chunkedSegmentStorage.logPrefix, handle.getSegmentName(), offset, totalBytesRead, elapsed.toMillis());\n+            LoggerHelpers.traceLeave(log, \"read\", traceId, handle, offset, totalBytesRead);\n+        }\n+\n+        private CompletableFuture<Void> readData(MetadataTransaction txn) {\n+            return Futures.loop(\n+                    () -> bytesRemaining > 0 && null != currentChunkName,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 1616}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc1MzQ2OA==", "bodyText": "These methods are called on a single object one after another - so there is no possibility of concurrent modifications.  But there may be visibility problems. Making the mutable fields volatile and other fields final solves the visibility problem.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r506753468", "createdAt": "2020-10-16T22:52:29Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -555,575 +378,700 @@ private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n         return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n     }\n \n-    /**\n-     * Adds a system log.\n-     *\n-     * @param systemLogRecords\n-     * @param streamSegmentName Name of the segment.\n-     * @param offset            Offset at which new chunk was added.\n-     * @param oldChunkName      Name of the previous last chunk.\n-     * @param newChunkName      Name of the new last chunk.\n-     */\n-    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n-        systemLogRecords.add(\n-                SystemJournal.ChunkAddedRecord.builder()\n-                        .segmentName(streamSegmentName)\n-                        .offset(offset)\n-                        .oldChunkName(oldChunkName == null ? null : oldChunkName)\n-                        .newChunkName(newChunkName)\n-                        .build());\n-    }\n-\n     /**\n      * Delete the garbage chunks.\n      *\n      * @param chunksTodelete List of chunks to delete.\n      */\n-    private void collectGarbage(Collection<String> chunksTodelete) {\n+    private CompletableFuture<Void> collectGarbage(Collection<String> chunksTodelete) {\n+        CompletableFuture[] futures = new CompletableFuture[chunksTodelete.size()];\n+        int i = 0;\n         for (val chunkTodelete : chunksTodelete) {\n-            try {\n-                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n-                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n-            } catch (ChunkNotFoundException e) {\n-                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n-            } catch (Exception e) {\n-                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n-                // Add it to garbage chunks.\n-                synchronized (garbageChunks) {\n-                    garbageChunks.add(chunkTodelete);\n-                }\n-            }\n+            futures[i++] = chunkStorage.openWrite(chunkTodelete)\n+                    .thenComposeAsync(chunkStorage::delete, executor)\n+                    .thenRunAsync(() -> log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete), executor)\n+                    .exceptionally(e -> {\n+                        val ex = Exceptions.unwrap(e);\n+                        if (ex instanceof ChunkNotFoundException) {\n+                            log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                        } else {\n+                            log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                            // Add it to garbage chunks.\n+                            synchronized (garbageChunks) {\n+                                garbageChunks.add(chunkTodelete);\n+                            }\n+                        }\n+                        return null;\n+                    });\n         }\n+        return CompletableFuture.allOf(futures);\n     }\n \n     @Override\n     public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n         checkInitialized();\n-        return execute(() -> {\n+        return executeAsync(() -> {\n             long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n             Preconditions.checkNotNull(handle, \"handle\");\n             String streamSegmentName = handle.getSegmentName();\n             Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n             Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n \n-            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n-                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n-                // Validate preconditions.\n-                checkSegmentExists(streamSegmentName, segmentMetadata);\n-                checkOwnership(streamSegmentName, segmentMetadata);\n-\n-                // seal if it is not already sealed.\n-                if (!segmentMetadata.isSealed()) {\n-                    segmentMetadata.setSealed(true);\n-                    txn.update(segmentMetadata);\n-                    txn.commit();\n-                }\n-\n-                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n-                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n-                return null;\n-            } catch (StorageMetadataWritesFencedOutException ex) {\n-                throw new StorageNotPrimaryException(streamSegmentName, ex);\n-            }\n+            return tryWith(metadataStore.beginTransaction(handle.getSegmentName()), txn ->\n+                    txn.get(streamSegmentName)\n+                            .thenComposeAsync(storageMetadata -> {\n+                                SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                // Validate preconditions.\n+                                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                                // seal if it is not already sealed.\n+                                if (!segmentMetadata.isSealed()) {\n+                                    segmentMetadata.setSealed(true);\n+                                    txn.update(segmentMetadata);\n+                                    return txn.commit();\n+                                }\n+                                return CompletableFuture.completedFuture(null);\n+                            }, executor)\n+                            .thenRunAsync(() -> {\n+                                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                            }, executor)\n+                            .exceptionally(e -> {\n+                                val ex = Exceptions.unwrap(e);\n+                                if (ex instanceof StorageMetadataWritesFencedOutException) {\n+                                    throw new CompletionException(new StorageNotPrimaryException(streamSegmentName, ex));\n+                                }\n+                                throw new CompletionException(ex);\n+                            }), executor);\n         });\n     }\n \n     @Override\n     public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n         checkInitialized();\n-        return execute(() -> {\n-            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n-            Timer timer = new Timer();\n-\n-            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n-            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n-            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n-            Preconditions.checkArgument(offset >= 0, \"offset\");\n-            String targetSegmentName = targetHandle.getSegmentName();\n-\n-            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n-\n-                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n-\n-                // Validate preconditions.\n-                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n-                targetSegmentMetadata.checkInvariants();\n-                checkNotSealed(targetSegmentName, targetSegmentMetadata);\n-\n-                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n-                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n-                sourceSegmentMetadata.checkInvariants();\n-\n-                // This is a critical assumption at this point which should not be broken,\n-                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n-                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n-\n-                checkSealed(sourceSegmentMetadata);\n-                checkOwnership(targetSegmentMetadata.getName(), targetSegmentMetadata);\n-\n-                if (sourceSegmentMetadata.getStartOffset() != 0) {\n-                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n-                }\n-\n-                if (offset != targetSegmentMetadata.getLength()) {\n-                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n-                }\n-\n-                // Update list of chunks by appending sources list of chunks.\n-                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n-                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n-\n-                if (targetLastChunk != null) {\n-                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n-                    txn.update(targetLastChunk);\n-                } else {\n-                    if (sourceFirstChunk != null) {\n-                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n-                        txn.update(sourceFirstChunk);\n-                    }\n-                }\n-\n-                // Update segments's last chunk to point to the sources last segment.\n-                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n-\n-                // Update the length of segment.\n-                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n-                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n-\n-                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n-\n-                txn.update(targetSegmentMetadata);\n-                txn.delete(sourceSegment);\n-\n-                // Finally defrag immediately.\n-                ArrayList<String> chunksToDelete = new ArrayList<>();\n-                if (shouldDefrag() && null != targetLastChunk) {\n-                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null, chunksToDelete);\n-                }\n-\n-                targetSegmentMetadata.checkInvariants();\n-\n-                // Finally commit transaction.\n-                txn.commit();\n-\n-                // Collect garbage.\n-                collectGarbage(chunksToDelete);\n-\n-                // Update the read index.\n-                readIndexCache.remove(sourceSegment);\n-\n-                Duration elapsed = timer.getElapsed();\n-                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n-                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n-\n-            } catch (StorageMetadataWritesFencedOutException ex) {\n-                throw new StorageNotPrimaryException(targetSegmentName, ex);\n-            }\n-\n-            return null;\n-        });\n+        return executeAsync(new ConcatOperation(this, targetHandle, offset, sourceSegment));\n     }\n \n     private boolean shouldAppend() {\n         return chunkStorage.supportsAppend() && config.isAppendEnabled();\n     }\n \n-    private boolean shouldDefrag() {\n-        return shouldAppend() || chunkStorage.supportsConcat();\n-    }\n-\n     /**\n      * Defragments the list of chunks for a given segment.\n      * It finds eligible consecutive chunks that can be merged together.\n      * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n      * Conceptually this is like deleting nodes from middle of the list of chunks.\n      *\n-     * <Ul>\n-     * <li> In the absence of defragmentation, the number of chunks for individual segments keeps on increasing.\n-     * When we have too many small chunks (say because many transactions with little data on some segments), the segment\n-     * is fragmented - this may impact both the read throughput and the performance of the metadata store.\n-     * This problem is further intensified when we have stores that do not support append semantics (e.g., stock S3) and\n-     * each write becomes a separate chunk.\n-     * </li>\n-     * <li>\n-     * If the underlying storage provides some facility to stitch together smaller chunk into larger chunks, then we do\n-     * actually want to exploit that, specially when the underlying implementation is only a metadata operation. We want\n-     * to leverage multi-part uploads in object stores that support it (e.g., AWS S3, Dell EMC ECS) as they are typically\n-     * only metadata operations, reducing the overall cost of the merging them together. HDFS also supports merges,\n-     * whereas NFS has no concept of merging natively.\n-     *\n-     * As chunks become larger, append writes (read source completely and append it back at the end of target)\n-     * become inefficient. Consequently, a native option for merging is desirable. We use such native merge capability\n-     * when available, and if not available, then we use appends.\n-     * </li>\n-     * <li>\n-     * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n-     * We can then fine tune that background task to run optimally with low overhead.\n-     * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n-     * </li>\n-     * <li>\n-     * <li>\n-     * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n-     * </li>\n-     * </ul>\n-     *\n-     * What controls whether we invoke concat or simulate through appends?\n-     * There are a few different capabilities that ChunkStorage needs to provide.\n-     * <ul>\n-     * <li>Does ChunkStorage support appending to existing chunks? For vanilla S3 compatible this would return false.\n-     * This is indicated by supportsAppend.</li>\n-     * <li>Does ChunkStorage support for concatenating chunks ? This is indicated by supportsConcat.\n-     * If this is true then concat operation will be invoked otherwise chunks will be appended.</li>\n-     * <li>There are some obvious constraints - For ChunkStorage support any concat functionality it must support either\n-     * append or concat.</li>\n-     * <li>Also when ChunkStorage supports both concat and append, ChunkedSegmentStorage will invoke appropriate method\n-     * depending on size of target and source chunks. (Eg. ECS)</li>\n-     * </ul>\n-     *\n-     * <li>\n-     * What controls defrag?\n-     * There are two additional parameters that control when concat\n-     * <li>minSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered a small object.\n-     * For small source objects, append is used instead of using concat. (For really small txn it is rather efficient to use append than MPU).</li>\n-     * <li>maxSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered for concat. (Eg S3 might have max limit on chunk size).</li>\n-     * In short there is a size beyond which using append is not advisable. Conversely there is a size below which concat is not efficient.(minSizeLimitForConcat )\n-     * Then there is limit which concating does not make sense maxSizeLimitForConcat\n-     * </li>\n-     * <li>\n-     * What is the defrag algorithm\n-     * <pre>\n-     * While(segment.hasConcatableChunks()){\n-     *     Set<List<Chunk>> s = FindConsecutiveConcatableChunks();\n-     *     For (List<chunk> list : s){\n-     *        ConcatChunks (list);\n-     *     }\n-     * }\n-     * </pre>\n-     * </li>\n-     * </ul>\n-     *\n      * @param txn             Active {@link MetadataTransaction}.\n      * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n      * @param startChunkName  Name of the first chunk to start defragmentation.\n      * @param lastChunkName   Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n      * @param chunksToDelete  List of chunks to which names of chunks to be deleted are added. It is the responsibility\n      *                        of caller to garbage collect these chunks.\n-     * @throws ChunkStorageException In case of any chunk storage related errors.\n-     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     *                        throws ChunkStorageException    In case of any chunk storage related errors.\n+     *                        throws StorageMetadataException In case of any chunk metadata store related errors.\n      */\n-    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata,\n-                        String startChunkName,\n-                        String lastChunkName,\n-                        ArrayList<String> chunksToDelete)\n-            throws StorageMetadataException, ChunkStorageException {\n-        // The algorithm is actually very simple.\n-        // It tries to concat all small chunks using appends first.\n-        // Then it tries to concat remaining chunks using concat if available.\n-        // To implement it using single loop we toggle between concat with append and concat modes. (Instead of two passes.)\n-        boolean useAppend = true;\n-        String targetChunkName = startChunkName;\n-\n-        // Iterate through chunk list\n-        while (null != targetChunkName && !targetChunkName.equals(lastChunkName)) {\n-            ChunkMetadata target = (ChunkMetadata) txn.get(targetChunkName);\n-\n-            ArrayList<ChunkInfo> chunksToConcat = new ArrayList<>();\n-            long targetSizeAfterConcat = target.getLength();\n-\n-            // Add target to the list of chunks\n-            chunksToConcat.add(new ChunkInfo(targetSizeAfterConcat, targetChunkName));\n-\n-            String nextChunkName = target.getNextChunk();\n-            ChunkMetadata next = null;\n-\n-            // Gather list of chunks that can be appended together.\n-            while (null != nextChunkName) {\n-                next = (ChunkMetadata) txn.get(nextChunkName);\n-\n-                if (useAppend && config.getMinSizeLimitForConcat() < next.getLength()) {\n-                    break;\n-                }\n+    private CompletableFuture<Void> defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata,\n+                                           String startChunkName,\n+                                           String lastChunkName,\n+                                           ArrayList<String> chunksToDelete) {\n+        return new DefragmentOperation(this, txn, segmentMetadata, startChunkName, lastChunkName, chunksToDelete).call();\n+    }\n \n-                if (targetSizeAfterConcat + next.getLength() > segmentMetadata.getMaxRollinglength() || next.getLength() > config.getMaxSizeLimitForConcat()) {\n-                    break;\n-                }\n+    @Override\n+    public CompletableFuture<Void> delete(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return executeAsync(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle);\n+            Timer timer = new Timer();\n \n-                chunksToConcat.add(new ChunkInfo(next.getLength(), nextChunkName));\n-                targetSizeAfterConcat += next.getLength();\n+            String streamSegmentName = handle.getSegmentName();\n+            return tryWith(metadataStore.beginTransaction(streamSegmentName), txn -> txn.get(streamSegmentName)\n+                    .thenComposeAsync(storageMetadata -> {\n+                        SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                        // Check preconditions\n+                        checkSegmentExists(streamSegmentName, segmentMetadata);\n+                        checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                        segmentMetadata.setActive(false);\n+\n+                        // Delete chunks\n+                        ArrayList<String> chunksToDelete = new ArrayList<>();\n+                        return new ChunkIterator(txn, segmentMetadata)\n+                                .forEach((metadata, name) -> {\n+                                    txn.delete(name);\n+                                    chunksToDelete.add(name);\n+                                })\n+                                .thenRunAsync(() -> txn.delete(streamSegmentName), executor)\n+                                .thenComposeAsync(v ->\n+                                        txn.commit()\n+                                                .thenComposeAsync(vv -> {\n+                                                    // Collect garbage.\n+                                                    return collectGarbage(chunksToDelete);\n+                                                }, executor)\n+                                                .thenRunAsync(() -> {\n+                                                    // Update the read index.\n+                                                    readIndexCache.remove(streamSegmentName);\n+\n+                                                    Duration elapsed = timer.getElapsed();\n+                                                    log.debug(\"{} delete - segment={}, latency={}.\", logPrefix, handle.getSegmentName(), elapsed.toMillis());\n+                                                    LoggerHelpers.traceLeave(log, \"delete\", traceId, handle);\n+                                                }, executor)\n+                                                .exceptionally(e -> {\n+                                                    val ex = Exceptions.unwrap(e);\n+                                                    if (ex instanceof StorageMetadataWritesFencedOutException) {\n+                                                        throw new CompletionException(new StorageNotPrimaryException(streamSegmentName, ex));\n+                                                    }\n+                                                    throw new CompletionException(ex);\n+                                                }), executor);\n+                    }, executor), executor);\n+        });\n+    }\n \n-                nextChunkName = next.getNextChunk();\n-            }\n-            // Note - After above while loop is exited nextChunkName points to chunk next to last one to be concat.\n-            // Which means target should now point to it as next after concat is complete.\n+    @Override\n+    public CompletableFuture<Void> truncate(SegmentHandle handle, long offset, Duration timeout) {\n+        checkInitialized();\n+        return executeAsync(new TruncateOperation(this, handle, offset));\n+    }\n \n-            // If there are chunks that can be appended together then concat them.\n-            if (chunksToConcat.size() > 1) {\n-                // Concat\n+    @Override\n+    public boolean supportsTruncation() {\n+        return true;\n+    }\n \n-                ConcatArgument[] concatArgs = new ConcatArgument[chunksToConcat.size()];\n-                for (int i = 0; i < chunksToConcat.size(); i++) {\n-                    concatArgs[i] = ConcatArgument.fromChunkInfo(chunksToConcat.get(i));\n-                }\n+    @Override\n+    public Iterator<SegmentProperties> listSegments() {\n+        throw new UnsupportedOperationException(\"listSegments is not yet supported\");\n+    }\n \n-                if (!useAppend && chunkStorage.supportsConcat()) {\n-                    int length = chunkStorage.concat(concatArgs);\n-                } else {\n-                    concatUsingAppend(concatArgs);\n-                }\n+    @Override\n+    public CompletableFuture<SegmentHandle> openRead(String streamSegmentName) {\n+        checkInitialized();\n+        return executeAsync(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openRead\", streamSegmentName);\n+            // Validate preconditions and return handle.\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            return tryWith(metadataStore.beginTransaction(streamSegmentName), txn ->\n+                            txn.get(streamSegmentName).thenComposeAsync(storageMetadata -> {\n+                                SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                                segmentMetadata.checkInvariants();\n+                                // This segment was created by an older segment store. Then claim ownership and adjust length.\n+                                CompletableFuture<Void> f = CompletableFuture.completedFuture(null);\n+                                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                                    log.debug(\"{} openRead - Segment needs ownership change. segment={}.\", logPrefix, segmentMetadata.getName());\n+                                    // In case of a failover, length recorded in metadata will be lagging behind its actual length in the storage.\n+                                    // This can happen with lazy commits that were still not committed at the time of failover.\n+                                    f = claimOwnership(txn, segmentMetadata);\n+                                }\n+                                return f.thenApplyAsync(v -> {\n+                                    val retValue = SegmentStorageHandle.readHandle(streamSegmentName);\n+                                    LoggerHelpers.traceLeave(log, \"openRead\", traceId, retValue);\n+                                    return retValue;\n+                                }, executor);\n+                            }, executor),\n+                    executor);\n+        });\n+    }\n \n-                // Delete chunks.\n-                for (int i = 1; i < chunksToConcat.size(); i++) {\n-                    chunksToDelete.add(chunksToConcat.get(i).getName());\n-                }\n+    @Override\n+    public CompletableFuture<Integer> read(SegmentHandle handle, long offset, byte[] buffer, int bufferOffset, int length, Duration timeout) {\n+        checkInitialized();\n+        return executeAsync(new ReadOperation(this, handle, offset, buffer, bufferOffset, length));\n+    }\n \n-                // Set the pointers\n-                target.setLength(targetSizeAfterConcat);\n-                target.setNextChunk(nextChunkName);\n+    @Override\n+    public CompletableFuture<SegmentProperties> getStreamSegmentInfo(String streamSegmentName, Duration timeout) {\n+        checkInitialized();\n+        return executeAsync(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"getStreamSegmentInfo\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            return tryWith(metadataStore.beginTransaction(streamSegmentName), txn ->\n+                    txn.get(streamSegmentName)\n+                            .thenApplyAsync(storageMetadata -> {\n+                                SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                if (null == segmentMetadata) {\n+                                    throw new CompletionException(new StreamSegmentNotExistsException(streamSegmentName));\n+                                }\n+                                segmentMetadata.checkInvariants();\n+\n+                                val retValue = StreamSegmentInformation.builder()\n+                                        .name(streamSegmentName)\n+                                        .sealed(segmentMetadata.isSealed())\n+                                        .length(segmentMetadata.getLength())\n+                                        .startOffset(segmentMetadata.getStartOffset())\n+                                        .lastModified(new ImmutableDate(segmentMetadata.getLastModified()))\n+                                        .build();\n+                                LoggerHelpers.traceLeave(log, \"getStreamSegmentInfo\", traceId, retValue);\n+                                return retValue;\n+                            }, executor), executor);\n+        });\n+    }\n \n-                // If target is the last chunk after this then update metadata accordingly\n-                if (null == nextChunkName) {\n-                    segmentMetadata.setLastChunk(target.getName());\n-                    segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength() - target.getLength());\n-                }\n+    @Override\n+    public CompletableFuture<Boolean> exists(String streamSegmentName, Duration timeout) {\n+        checkInitialized();\n+        return executeAsync(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"exists\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            return tryWith(metadataStore.beginTransaction(streamSegmentName),\n+                    txn -> txn.get(streamSegmentName)\n+                            .thenApplyAsync(storageMetadata -> {\n+                                SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                val retValue = segmentMetadata != null && segmentMetadata.isActive();\n+                                LoggerHelpers.traceLeave(log, \"exists\", traceId, retValue);\n+                                return retValue;\n+                            }, executor),\n+                    executor);\n+        });\n+    }\n \n-                // Update metadata for affected chunks.\n-                for (int i = 1; i < concatArgs.length; i++) {\n-                    txn.delete(concatArgs[i].getName());\n-                    segmentMetadata.decrementChunkCount();\n-                }\n-                txn.update(target);\n-                txn.update(segmentMetadata);\n+    @Override\n+    public void close() {\n+        try {\n+            if (null != this.metadataStore) {\n+                this.metadataStore.close();\n             }\n+        } catch (Exception e) {\n+            log.warn(\"Error during close\", e);\n+        }\n+        this.closed.set(true);\n+    }\n \n-            // Move on to next place in list where we can concat if we are done with append based concats.\n-            if (!useAppend) {\n-                targetChunkName = nextChunkName;\n+    /**\n+     * Executes the given Callable and returns its result, while translating any Exceptions bubbling out of it into\n+     * StreamSegmentExceptions.\n+     *\n+     * @param operation The function to execute.\n+     * @param <R>       Return type of the operation.\n+     * @return CompletableFuture<R> of the return type of the operation.\n+     */\n+    private <R> CompletableFuture<R> executeAsync(Callable<CompletableFuture<R>> operation) {\n+        return CompletableFuture.completedFuture(null).thenComposeAsync(v -> {\n+            Exceptions.checkNotClosed(this.closed.get(), this);\n+            try {\n+                return operation.call();\n+            } catch (CompletionException e) {\n+                throw new CompletionException(Exceptions.unwrap(e));\n+            } catch (Exception e) {\n+                throw new CompletionException(e);\n             }\n+        }, this.executor);\n+    }\n \n-            // Toggle\n-            useAppend = !useAppend;\n-        }\n-\n-        // Make sure no invariants are broken.\n-        segmentMetadata.checkInvariants();\n+    private static <T extends AutoCloseable, R> CompletableFuture<R> tryWith(T closeable, Function<T, CompletableFuture<R>> function, Executor executor) {\n+        return function.apply(closeable)\n+                    .whenCompleteAsync((v, ex) -> {\n+                        try {\n+                            closeable.close();\n+                        } catch (Exception e) {\n+                            throw new CompletionException(e);\n+                        }\n+                    }, executor);\n     }\n \n-    private void concatUsingAppend(ConcatArgument[] concatArgs) throws ChunkStorageException {\n-        long writeAtOffset = concatArgs[0].getLength();\n-        val writeHandle = ChunkHandle.writeHandle(concatArgs[0].getName());\n-        for (int i = 1; i < concatArgs.length; i++) {\n-            int readAtOffset = 0;\n-            val arg = concatArgs[i];\n-            int bytesToRead = Math.toIntExact(arg.getLength());\n-\n-            while (bytesToRead > 0) {\n-                byte[] buffer = new byte[Math.min(config.getMaxBufferSizeForChunkDataTransfer(), bytesToRead)];\n-                int size = chunkStorage.read(ChunkHandle.readHandle(arg.getName()), readAtOffset, buffer.length, buffer, 0);\n-                bytesToRead -= size;\n-                readAtOffset += size;\n-                writeAtOffset += chunkStorage.write(writeHandle, writeAtOffset, size, new ByteArrayInputStream(buffer, 0, size));\n-            }\n+    private void checkSegmentExists(String streamSegmentName, SegmentMetadata segmentMetadata) {\n+        if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+            throw new CompletionException(new StreamSegmentNotExistsException(streamSegmentName));\n         }\n     }\n \n-    @Override\n-    public CompletableFuture<Void> delete(SegmentHandle handle, Duration timeout) {\n-        checkInitialized();\n-        return execute(() -> {\n-            long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle);\n-            Timer timer = new Timer();\n+    private void checkOwnership(String streamSegmentName, SegmentMetadata segmentMetadata) {\n+        if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+            throw new CompletionException(new StorageNotPrimaryException(streamSegmentName));\n+        }\n+    }\n \n-            String streamSegmentName = handle.getSegmentName();\n-            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n-                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n-\n-                // Check preconditions\n-                checkSegmentExists(streamSegmentName, segmentMetadata);\n-                checkOwnership(streamSegmentName, segmentMetadata);\n-\n-                segmentMetadata.setActive(false);\n-\n-                // Delete chunks\n-                String currentChunkName = segmentMetadata.getFirstChunk();\n-                ChunkMetadata currentMetadata;\n-                ArrayList<String> chunksToDelete = new ArrayList<>();\n-                while (currentChunkName != null) {\n-                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n-                    // Delete underlying file.\n-                    chunksToDelete.add(currentChunkName);\n-                    currentChunkName = currentMetadata.getNextChunk();\n-                    txn.delete(currentMetadata.getName());\n-                }\n+    private void checkNotSealed(String streamSegmentName, SegmentMetadata segmentMetadata) {\n+        if (segmentMetadata.isSealed()) {\n+            throw new CompletionException(new StreamSegmentSealedException(streamSegmentName));\n+        }\n+    }\n \n-                // Commit.\n-                txn.delete(streamSegmentName);\n-                txn.commit();\n+    private void checkInitialized() {\n+        Preconditions.checkState(null != this.metadataStore);\n+        Preconditions.checkState(0 != this.epoch);\n+        Preconditions.checkState(!closed.get());\n+    }\n \n-                // Collect garbage.\n-                collectGarbage(chunksToDelete);\n+    private class ChunkIterator {\n+        private final MetadataTransaction txn;\n+        private String currentChunkName;\n+        private ChunkMetadata currentMetadata;\n \n-                // Update the read index.\n-                readIndexCache.remove(streamSegmentName);\n+        ChunkIterator(MetadataTransaction txn, SegmentMetadata segmentMetadata) {\n+            this.txn = txn;\n+            currentChunkName = segmentMetadata.getFirstChunk();\n+        }\n \n-                Duration elapsed = timer.getElapsed();\n-                log.debug(\"{} delete - segment={}, latency={}.\", logPrefix, handle.getSegmentName(), elapsed.toMillis());\n-                LoggerHelpers.traceLeave(log, \"delete\", traceId, handle);\n-                return null;\n-            } catch (StorageMetadataWritesFencedOutException ex) {\n-                throw new StorageNotPrimaryException(streamSegmentName, ex);\n-            }\n-        });\n+        public CompletableFuture<Void> forEach(BiConsumer<ChunkMetadata, String> consumer) {\n+            return Futures.loop(\n+                    () -> currentChunkName != null,\n+                    () -> txn.get(currentChunkName)\n+                            .thenApplyAsync(storageMetadata -> {\n+                                currentMetadata = (ChunkMetadata) storageMetadata;\n+                                consumer.accept(currentMetadata, currentChunkName);\n+                                // Move next\n+                                currentChunkName = currentMetadata.getNextChunk();\n+                                return null;\n+                            }, executor),\n+                    executor);\n+        }\n     }\n \n-    @Override\n-    public CompletableFuture<Void> truncate(SegmentHandle handle, long offset, Duration timeout) {\n-        checkInitialized();\n-        return execute(() -> {\n-            long traceId = LoggerHelpers.traceEnter(log, \"truncate\", handle, offset);\n-            Timer timer = new Timer();\n+    private static class TruncateOperation implements Callable<CompletableFuture<Void>> {\n+        private final SegmentHandle handle;\n+        private final long offset;\n+        private final ChunkedSegmentStorage chunkedSegmentStorage;\n+\n+        private String currentChunkName;\n+        private ChunkMetadata currentMetadata;\n+        private long oldLength;\n+        private long startOffset;\n+        private ArrayList<String> chunksToDelete = new ArrayList<>();\n+        private SegmentMetadata segmentMetadata;\n+        private String streamSegmentName;\n+\n+        private boolean isLoopExited;\n+        private long traceId;\n+        private Timer timer;\n+\n+        TruncateOperation(ChunkedSegmentStorage chunkedSegmentStorage, SegmentHandle handle, long offset) {\n+            this.handle = handle;\n+            this.offset = offset;\n+            this.chunkedSegmentStorage = chunkedSegmentStorage;\n+        }\n \n-            Preconditions.checkArgument(null != handle, \"handle\");\n-            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n-            Preconditions.checkArgument(offset >= 0, \"offset\");\n+        public CompletableFuture<Void> call() {\n+            traceId = LoggerHelpers.traceEnter(log, \"truncate\", handle, offset);\n+            timer = new Timer();\n+\n+            checkPreconditions();\n+\n+            streamSegmentName = handle.getSegmentName();\n+            return tryWith(chunkedSegmentStorage.metadataStore.beginTransaction(streamSegmentName), txn ->\n+                    txn.get(streamSegmentName)\n+                            .thenComposeAsync(storageMetadata -> {\n+                                segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                // Check preconditions\n+                                checkPreconditions(streamSegmentName, segmentMetadata);\n+\n+                                if (segmentMetadata.getStartOffset() == offset) {\n+                                    // Nothing to do\n+                                    return CompletableFuture.completedFuture(null);\n+                                }\n+\n+                                return updateFirstChunk(txn)\n+                                        .thenComposeAsync(v -> {\n+                                            deleteChunks(txn);\n+\n+                                            txn.update(segmentMetadata);\n+\n+                                            // Check invariants.\n+                                            Preconditions.checkState(segmentMetadata.getLength() == oldLength, \"truncate should not change segment length\");\n+                                            segmentMetadata.checkInvariants();\n+\n+                                            // Finally commit.\n+                                            return commit(txn)\n+                                                    .handleAsync(this::handleException, chunkedSegmentStorage.executor)\n+                                                    .thenComposeAsync(vv ->\n+                                                                    chunkedSegmentStorage.collectGarbage(chunksToDelete).thenApplyAsync(vvv -> {\n+                                                                        postCommit();\n+                                                                        return null;\n+                                                                    }, chunkedSegmentStorage.executor),\n+                                                            chunkedSegmentStorage.executor);\n+                                        }, chunkedSegmentStorage.executor);\n+                            }, chunkedSegmentStorage.executor), chunkedSegmentStorage.executor);\n+        }\n \n-            String streamSegmentName = handle.getSegmentName();\n-            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n-                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+        private void postCommit() {\n+            // Update the read index by removing all entries below truncate offset.\n+            chunkedSegmentStorage.readIndexCache.truncateReadIndex(streamSegmentName, segmentMetadata.getStartOffset());\n \n-                // Check preconditions\n-                checkSegmentExists(streamSegmentName, segmentMetadata);\n-                checkNotSealed(streamSegmentName, segmentMetadata);\n-                checkOwnership(streamSegmentName, segmentMetadata);\n+            logEnd();\n+        }\n+\n+        private void logEnd() {\n+            Duration elapsed = timer.getElapsed();\n+            log.debug(\"{} truncate - segment={}, offset={}, latency={}.\", chunkedSegmentStorage.logPrefix, handle.getSegmentName(), offset, elapsed.toMillis());\n+            LoggerHelpers.traceLeave(log, \"truncate\", traceId, handle, offset);\n+        }\n \n-                if (segmentMetadata.getLength() < offset || segmentMetadata.getStartOffset() > offset) {\n-                    throw new IllegalArgumentException(String.format(\"offset %d is outside of valid range [%d, %d) for segment %s\",\n-                            offset, segmentMetadata.getStartOffset(), segmentMetadata.getLength(), streamSegmentName));\n+        private Void handleException(Void value, Throwable e) {\n+            if (null != e) {\n+                val ex = Exceptions.unwrap(e);\n+                if (ex instanceof StorageMetadataWritesFencedOutException) {\n+                    throw new CompletionException(new StorageNotPrimaryException(streamSegmentName, ex));\n                 }\n+                throw new CompletionException(ex);\n+            }\n+            return value;\n+        }\n \n-                if (segmentMetadata.getStartOffset() == offset) {\n-                    // Nothing to do\n+        private CompletableFuture<Void> commit(MetadataTransaction txn) {\n+            // Commit system logs.\n+            if (chunkedSegmentStorage.isStorageSystemSegment(segmentMetadata)) {\n+                val finalStartOffset = startOffset;\n+                txn.setExternalCommitStep(() -> {\n+                    chunkedSegmentStorage.systemJournal.commitRecord(\n+                            SystemJournal.TruncationRecord.builder()\n+                                    .segmentName(streamSegmentName)\n+                                    .offset(offset)\n+                                    .firstChunkName(segmentMetadata.getFirstChunk())\n+                                    .startOffset(finalStartOffset)\n+                                    .build());\n                     return null;\n-                }\n+                });\n+            }\n \n-                String currentChunkName = segmentMetadata.getFirstChunk();\n-                ChunkMetadata currentMetadata;\n-                long oldLength = segmentMetadata.getLength();\n-                long startOffset = segmentMetadata.getFirstChunkStartOffset();\n-                ArrayList<String> chunksToDelete = new ArrayList<>();\n-                while (currentChunkName != null) {\n-                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n-                    Preconditions.checkState(null != currentMetadata, \"currentMetadata is null.\");\n-\n-                    // If for given chunk start <= offset < end  then we have found the chunk that will be the first chunk.\n-                    if ((startOffset <= offset) && (startOffset + currentMetadata.getLength() > offset)) {\n-                        break;\n-                    }\n-\n-                    startOffset += currentMetadata.getLength();\n-                    chunksToDelete.add(currentMetadata.getName());\n-                    segmentMetadata.decrementChunkCount();\n+            // Finally commit.\n+            return txn.commit();\n+        }\n \n-                    // move to next chunk\n-                    currentChunkName = currentMetadata.getNextChunk();\n-                }\n+        private CompletableFuture<Void> updateFirstChunk(MetadataTransaction txn) {\n+            currentChunkName = segmentMetadata.getFirstChunk();\n+            oldLength = segmentMetadata.getLength();\n+            startOffset = segmentMetadata.getFirstChunkStartOffset();\n+            return Futures.loop(\n+                    () -> currentChunkName != null && !isLoopExited,\n+                    () -> txn.get(currentChunkName)\n+                            .thenApplyAsync(storageMetadata -> {\n+                                currentMetadata = (ChunkMetadata) storageMetadata;\n+                                Preconditions.checkState(null != currentMetadata, \"currentMetadata is null.\");\n+\n+                                // If for given chunk start <= offset < end  then we have found the chunk that will be the first chunk.\n+                                if ((startOffset <= offset) && (startOffset + currentMetadata.getLength() > offset)) {\n+                                    isLoopExited = true;\n+                                    return null;\n+                                }\n+\n+                                startOffset += currentMetadata.getLength();\n+                                chunksToDelete.add(currentMetadata.getName());\n+                                segmentMetadata.decrementChunkCount();\n+\n+                                // move to next chunk\n+                                currentChunkName = currentMetadata.getNextChunk();\n+                                return null;\n+                            }, chunkedSegmentStorage.executor),\n+                    chunkedSegmentStorage.executor\n+            ).thenApplyAsync(v -> {\n                 segmentMetadata.setFirstChunk(currentChunkName);\n                 segmentMetadata.setStartOffset(offset);\n                 segmentMetadata.setFirstChunkStartOffset(startOffset);\n-                for (String toDelete : chunksToDelete) {\n-                    txn.delete(toDelete);\n-                    // Adjust last chunk if required.\n-                    if (toDelete.equals(segmentMetadata.getLastChunk())) {\n-                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n-                        segmentMetadata.setLastChunk(null);\n-                    }\n-                }\n-                txn.update(segmentMetadata);\n+                return null;\n+            }, chunkedSegmentStorage.executor);\n+        }\n \n-                // Check invariants.\n-                Preconditions.checkState(segmentMetadata.getLength() == oldLength, \"truncate should not change segment length\");\n-                segmentMetadata.checkInvariants();\n-\n-                // Commit system logs.\n-                if (isStorageSystemSegment(segmentMetadata)) {\n-                    val finalStartOffset = startOffset;\n-                    txn.setExternalCommitStep(() -> {\n-                        systemJournal.commitRecord(\n-                                SystemJournal.TruncationRecord.builder()\n-                                        .segmentName(streamSegmentName)\n-                                        .offset(offset)\n-                                        .firstChunkName(segmentMetadata.getFirstChunk())\n-                                        .startOffset(finalStartOffset)\n-                                        .build());\n-                        return null;\n-                    });\n+        private void deleteChunks(MetadataTransaction txn) {\n+            for (String toDelete : chunksToDelete) {\n+                txn.delete(toDelete);\n+                // Adjust last chunk if required.\n+                if (toDelete.equals(segmentMetadata.getLastChunk())) {\n+                    segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+                    segmentMetadata.setLastChunk(null);\n                 }\n+            }\n+        }\n \n-                // Finally commit.\n-                txn.commit();\n-\n-                collectGarbage(chunksToDelete);\n-\n-                // Update the read index by removing all entries below truncate offset.\n-                readIndexCache.truncateReadIndex(streamSegmentName, segmentMetadata.getStartOffset());\n+        private void checkPreconditions(String streamSegmentName, SegmentMetadata segmentMetadata) {\n+            chunkedSegmentStorage.checkSegmentExists(streamSegmentName, segmentMetadata);\n+            chunkedSegmentStorage.checkNotSealed(streamSegmentName, segmentMetadata);\n+            chunkedSegmentStorage.checkOwnership(streamSegmentName, segmentMetadata);\n \n-                Duration elapsed = timer.getElapsed();\n-                log.debug(\"{} truncate - segment={}, offset={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, elapsed.toMillis());\n-                LoggerHelpers.traceLeave(log, \"truncate\", traceId, handle, offset);\n-                return null;\n-            } catch (StorageMetadataWritesFencedOutException ex) {\n-                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            if (segmentMetadata.getLength() < offset || segmentMetadata.getStartOffset() > offset) {\n+                throw new IllegalArgumentException(String.format(\"offset %d is outside of valid range [%d, %d) for segment %s\",\n+                        offset, segmentMetadata.getStartOffset(), segmentMetadata.getLength(), streamSegmentName));\n             }\n-        });\n-    }\n+        }\n \n-    @Override\n-    public boolean supportsTruncation() {\n-        return true;\n+        private void checkPreconditions() {\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+        }\n     }\n \n-    @Override\n-    public Iterator<SegmentProperties> listSegments() throws IOException {\n-        throw new UnsupportedOperationException(\"listSegments is not yet supported\");\n-    }\n+    private static class ReadOperation implements Callable<CompletableFuture<Integer>> {\n+        private final SegmentHandle handle;\n+        private final long offset;\n+        private final byte[] buffer;\n+        private final int bufferOffset;\n+        private final int length;\n+        private final ChunkedSegmentStorage chunkedSegmentStorage;\n+        private long traceId;\n+        private Timer timer;\n+        private String streamSegmentName;\n+        private SegmentMetadata segmentMetadata;\n+        private int bytesRemaining;\n+        private int currentBufferOffset;\n+        private long currentOffset;\n+        private int totalBytesRead = 0;\n+        private long startOffsetForCurrentChunk;\n+        private String currentChunkName;\n+        private ChunkMetadata chunkToReadFrom = null;\n+        private boolean isLoopExited;\n+        private int cntScanned = 0;\n+        private int bytesToRead;\n+\n+        ReadOperation(ChunkedSegmentStorage chunkedSegmentStorage, SegmentHandle handle, long offset, byte[] buffer, int bufferOffset, int length) {\n+            this.handle = handle;\n+            this.offset = offset;\n+            this.buffer = buffer;\n+            this.bufferOffset = bufferOffset;\n+            this.length = length;\n+            this.chunkedSegmentStorage = chunkedSegmentStorage;\n+        }\n \n-    @Override\n-    public CompletableFuture<SegmentHandle> openRead(String streamSegmentName) {\n-        checkInitialized();\n-        return execute(() -> {\n-            long traceId = LoggerHelpers.traceEnter(log, \"openRead\", streamSegmentName);\n-            // Validate preconditions and return handle.\n-            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n-            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n-                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n-                checkSegmentExists(streamSegmentName, segmentMetadata);\n-                segmentMetadata.checkInvariants();\n-                // This segment was created by an older segment store. Then claim ownership and adjust length.\n-                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n-                    log.debug(\"{} openRead - Segment needs ownership change. segment={}.\", logPrefix, segmentMetadata.getName());\n-                    // In case of a failover, length recorded in metadata will be lagging behind its actual length in the storage.\n-                    // This can happen with lazy commits that were still not committed at the time of failover.\n-                    claimOwnership(txn, segmentMetadata);\n-                }\n-                val retValue = SegmentStorageHandle.readHandle(streamSegmentName);\n-                LoggerHelpers.traceLeave(log, \"openRead\", traceId, retValue);\n-                return retValue;\n+        public CompletableFuture<Integer> call() {\n+            traceId = LoggerHelpers.traceEnter(log, \"read\", handle, offset, length);\n+            timer = new Timer();\n+\n+            // Validate preconditions.\n+            checkPreconditions();\n+            streamSegmentName = handle.getSegmentName();\n+            return tryWith(chunkedSegmentStorage.metadataStore.beginTransaction(streamSegmentName),\n+                    txn -> txn.get(streamSegmentName)\n+                            .thenComposeAsync(storageMetadata -> {\n+                                segmentMetadata = (SegmentMetadata) storageMetadata;\n+\n+                                // Validate preconditions.\n+                                checkState();\n+\n+                                if (length == 0) {\n+                                    return CompletableFuture.completedFuture(0);\n+                                }\n+\n+                                return findChunkForOffset(txn)\n+                                        .thenComposeAsync(v -> {\n+                                            // Now read.\n+                                            return readData(txn);\n+                                        }, chunkedSegmentStorage.executor)\n+                                        .thenApplyAsync(v -> {\n+                                            logEnd();\n+                                            return totalBytesRead;\n+                                        }, chunkedSegmentStorage.executor);\n+                            }, chunkedSegmentStorage.executor),\n+                    chunkedSegmentStorage.executor);\n+        }\n+\n+        private void logEnd() {\n+            Duration elapsed = timer.getElapsed();\n+            log.debug(\"{} read - segment={}, offset={}, bytesRead={}, latency={}.\", chunkedSegmentStorage.logPrefix, handle.getSegmentName(), offset, totalBytesRead, elapsed.toMillis());\n+            LoggerHelpers.traceLeave(log, \"read\", traceId, handle, offset, totalBytesRead);\n+        }\n+\n+        private CompletableFuture<Void> readData(MetadataTransaction txn) {\n+            return Futures.loop(\n+                    () -> bytesRemaining > 0 && null != currentChunkName,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg5NjkyMw=="}, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 1616}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc1MzcxMg==", "bodyText": "Also now I have split big bad class into smaller classes.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r506753712", "createdAt": "2020-10-16T22:53:18Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -555,575 +378,700 @@ private boolean isStorageSystemSegment(SegmentMetadata segmentMetadata) {\n         return null != systemJournal && segmentMetadata.isStorageSystemSegment();\n     }\n \n-    /**\n-     * Adds a system log.\n-     *\n-     * @param systemLogRecords\n-     * @param streamSegmentName Name of the segment.\n-     * @param offset            Offset at which new chunk was added.\n-     * @param oldChunkName      Name of the previous last chunk.\n-     * @param newChunkName      Name of the new last chunk.\n-     */\n-    private void addSystemLogRecord(ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords, String streamSegmentName, long offset, String oldChunkName, String newChunkName) {\n-        systemLogRecords.add(\n-                SystemJournal.ChunkAddedRecord.builder()\n-                        .segmentName(streamSegmentName)\n-                        .offset(offset)\n-                        .oldChunkName(oldChunkName == null ? null : oldChunkName)\n-                        .newChunkName(newChunkName)\n-                        .build());\n-    }\n-\n     /**\n      * Delete the garbage chunks.\n      *\n      * @param chunksTodelete List of chunks to delete.\n      */\n-    private void collectGarbage(Collection<String> chunksTodelete) {\n+    private CompletableFuture<Void> collectGarbage(Collection<String> chunksTodelete) {\n+        CompletableFuture[] futures = new CompletableFuture[chunksTodelete.size()];\n+        int i = 0;\n         for (val chunkTodelete : chunksTodelete) {\n-            try {\n-                chunkStorage.delete(chunkStorage.openWrite(chunkTodelete));\n-                log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete);\n-            } catch (ChunkNotFoundException e) {\n-                log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n-            } catch (Exception e) {\n-                log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n-                // Add it to garbage chunks.\n-                synchronized (garbageChunks) {\n-                    garbageChunks.add(chunkTodelete);\n-                }\n-            }\n+            futures[i++] = chunkStorage.openWrite(chunkTodelete)\n+                    .thenComposeAsync(chunkStorage::delete, executor)\n+                    .thenRunAsync(() -> log.debug(\"{} collectGarbage - deleted chunk={}.\", logPrefix, chunkTodelete), executor)\n+                    .exceptionally(e -> {\n+                        val ex = Exceptions.unwrap(e);\n+                        if (ex instanceof ChunkNotFoundException) {\n+                            log.debug(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                        } else {\n+                            log.warn(\"{} collectGarbage - Could not delete garbage chunk {}.\", logPrefix, chunkTodelete);\n+                            // Add it to garbage chunks.\n+                            synchronized (garbageChunks) {\n+                                garbageChunks.add(chunkTodelete);\n+                            }\n+                        }\n+                        return null;\n+                    });\n         }\n+        return CompletableFuture.allOf(futures);\n     }\n \n     @Override\n     public CompletableFuture<Void> seal(SegmentHandle handle, Duration timeout) {\n         checkInitialized();\n-        return execute(() -> {\n+        return executeAsync(() -> {\n             long traceId = LoggerHelpers.traceEnter(log, \"seal\", handle);\n             Preconditions.checkNotNull(handle, \"handle\");\n             String streamSegmentName = handle.getSegmentName();\n             Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n             Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n \n-            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n-                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n-                // Validate preconditions.\n-                checkSegmentExists(streamSegmentName, segmentMetadata);\n-                checkOwnership(streamSegmentName, segmentMetadata);\n-\n-                // seal if it is not already sealed.\n-                if (!segmentMetadata.isSealed()) {\n-                    segmentMetadata.setSealed(true);\n-                    txn.update(segmentMetadata);\n-                    txn.commit();\n-                }\n-\n-                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n-                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n-                return null;\n-            } catch (StorageMetadataWritesFencedOutException ex) {\n-                throw new StorageNotPrimaryException(streamSegmentName, ex);\n-            }\n+            return tryWith(metadataStore.beginTransaction(handle.getSegmentName()), txn ->\n+                    txn.get(streamSegmentName)\n+                            .thenComposeAsync(storageMetadata -> {\n+                                SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                // Validate preconditions.\n+                                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                                checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                                // seal if it is not already sealed.\n+                                if (!segmentMetadata.isSealed()) {\n+                                    segmentMetadata.setSealed(true);\n+                                    txn.update(segmentMetadata);\n+                                    return txn.commit();\n+                                }\n+                                return CompletableFuture.completedFuture(null);\n+                            }, executor)\n+                            .thenRunAsync(() -> {\n+                                log.debug(\"{} seal - segment={}.\", logPrefix, handle.getSegmentName());\n+                                LoggerHelpers.traceLeave(log, \"seal\", traceId, handle);\n+                            }, executor)\n+                            .exceptionally(e -> {\n+                                val ex = Exceptions.unwrap(e);\n+                                if (ex instanceof StorageMetadataWritesFencedOutException) {\n+                                    throw new CompletionException(new StorageNotPrimaryException(streamSegmentName, ex));\n+                                }\n+                                throw new CompletionException(ex);\n+                            }), executor);\n         });\n     }\n \n     @Override\n     public CompletableFuture<Void> concat(SegmentHandle targetHandle, long offset, String sourceSegment, Duration timeout) {\n         checkInitialized();\n-        return execute(() -> {\n-            long traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n-            Timer timer = new Timer();\n-\n-            Preconditions.checkArgument(null != targetHandle, \"targetHandle\");\n-            Preconditions.checkArgument(!targetHandle.isReadOnly(), \"targetHandle\");\n-            Preconditions.checkArgument(null != sourceSegment, \"targetHandle\");\n-            Preconditions.checkArgument(offset >= 0, \"offset\");\n-            String targetSegmentName = targetHandle.getSegmentName();\n-\n-            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n-\n-                SegmentMetadata targetSegmentMetadata = (SegmentMetadata) txn.get(targetSegmentName);\n-\n-                // Validate preconditions.\n-                checkSegmentExists(targetSegmentName, targetSegmentMetadata);\n-                targetSegmentMetadata.checkInvariants();\n-                checkNotSealed(targetSegmentName, targetSegmentMetadata);\n-\n-                SegmentMetadata sourceSegmentMetadata = (SegmentMetadata) txn.get(sourceSegment);\n-                checkSegmentExists(sourceSegment, sourceSegmentMetadata);\n-                sourceSegmentMetadata.checkInvariants();\n-\n-                // This is a critical assumption at this point which should not be broken,\n-                Preconditions.checkState(!targetSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n-                Preconditions.checkState(!sourceSegmentMetadata.isStorageSystemSegment(), \"Storage system segments cannot be concatenated.\");\n-\n-                checkSealed(sourceSegmentMetadata);\n-                checkOwnership(targetSegmentMetadata.getName(), targetSegmentMetadata);\n-\n-                if (sourceSegmentMetadata.getStartOffset() != 0) {\n-                    throw new StreamSegmentTruncatedException(sourceSegment, sourceSegmentMetadata.getLength(), 0);\n-                }\n-\n-                if (offset != targetSegmentMetadata.getLength()) {\n-                    throw new BadOffsetException(targetHandle.getSegmentName(), targetSegmentMetadata.getLength(), offset);\n-                }\n-\n-                // Update list of chunks by appending sources list of chunks.\n-                ChunkMetadata targetLastChunk = (ChunkMetadata) txn.get(targetSegmentMetadata.getLastChunk());\n-                ChunkMetadata sourceFirstChunk = (ChunkMetadata) txn.get(sourceSegmentMetadata.getFirstChunk());\n-\n-                if (targetLastChunk != null) {\n-                    targetLastChunk.setNextChunk(sourceFirstChunk.getName());\n-                    txn.update(targetLastChunk);\n-                } else {\n-                    if (sourceFirstChunk != null) {\n-                        targetSegmentMetadata.setFirstChunk(sourceFirstChunk.getName());\n-                        txn.update(sourceFirstChunk);\n-                    }\n-                }\n-\n-                // Update segments's last chunk to point to the sources last segment.\n-                targetSegmentMetadata.setLastChunk(sourceSegmentMetadata.getLastChunk());\n-\n-                // Update the length of segment.\n-                targetSegmentMetadata.setLastChunkStartOffset(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLastChunkStartOffset());\n-                targetSegmentMetadata.setLength(targetSegmentMetadata.getLength() + sourceSegmentMetadata.getLength() - sourceSegmentMetadata.getStartOffset());\n-\n-                targetSegmentMetadata.setChunkCount(targetSegmentMetadata.getChunkCount() + sourceSegmentMetadata.getChunkCount());\n-\n-                txn.update(targetSegmentMetadata);\n-                txn.delete(sourceSegment);\n-\n-                // Finally defrag immediately.\n-                ArrayList<String> chunksToDelete = new ArrayList<>();\n-                if (shouldDefrag() && null != targetLastChunk) {\n-                    defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null, chunksToDelete);\n-                }\n-\n-                targetSegmentMetadata.checkInvariants();\n-\n-                // Finally commit transaction.\n-                txn.commit();\n-\n-                // Collect garbage.\n-                collectGarbage(chunksToDelete);\n-\n-                // Update the read index.\n-                readIndexCache.remove(sourceSegment);\n-\n-                Duration elapsed = timer.getElapsed();\n-                log.debug(\"{} concat - target={}, source={}, offset={}, latency={}.\", logPrefix, targetHandle.getSegmentName(), sourceSegment, offset, elapsed.toMillis());\n-                LoggerHelpers.traceLeave(log, \"concat\", traceId, targetHandle, offset, sourceSegment);\n-\n-            } catch (StorageMetadataWritesFencedOutException ex) {\n-                throw new StorageNotPrimaryException(targetSegmentName, ex);\n-            }\n-\n-            return null;\n-        });\n+        return executeAsync(new ConcatOperation(this, targetHandle, offset, sourceSegment));\n     }\n \n     private boolean shouldAppend() {\n         return chunkStorage.supportsAppend() && config.isAppendEnabled();\n     }\n \n-    private boolean shouldDefrag() {\n-        return shouldAppend() || chunkStorage.supportsConcat();\n-    }\n-\n     /**\n      * Defragments the list of chunks for a given segment.\n      * It finds eligible consecutive chunks that can be merged together.\n      * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n      * Conceptually this is like deleting nodes from middle of the list of chunks.\n      *\n-     * <Ul>\n-     * <li> In the absence of defragmentation, the number of chunks for individual segments keeps on increasing.\n-     * When we have too many small chunks (say because many transactions with little data on some segments), the segment\n-     * is fragmented - this may impact both the read throughput and the performance of the metadata store.\n-     * This problem is further intensified when we have stores that do not support append semantics (e.g., stock S3) and\n-     * each write becomes a separate chunk.\n-     * </li>\n-     * <li>\n-     * If the underlying storage provides some facility to stitch together smaller chunk into larger chunks, then we do\n-     * actually want to exploit that, specially when the underlying implementation is only a metadata operation. We want\n-     * to leverage multi-part uploads in object stores that support it (e.g., AWS S3, Dell EMC ECS) as they are typically\n-     * only metadata operations, reducing the overall cost of the merging them together. HDFS also supports merges,\n-     * whereas NFS has no concept of merging natively.\n-     *\n-     * As chunks become larger, append writes (read source completely and append it back at the end of target)\n-     * become inefficient. Consequently, a native option for merging is desirable. We use such native merge capability\n-     * when available, and if not available, then we use appends.\n-     * </li>\n-     * <li>\n-     * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n-     * We can then fine tune that background task to run optimally with low overhead.\n-     * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n-     * </li>\n-     * <li>\n-     * <li>\n-     * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n-     * </li>\n-     * </ul>\n-     *\n-     * What controls whether we invoke concat or simulate through appends?\n-     * There are a few different capabilities that ChunkStorage needs to provide.\n-     * <ul>\n-     * <li>Does ChunkStorage support appending to existing chunks? For vanilla S3 compatible this would return false.\n-     * This is indicated by supportsAppend.</li>\n-     * <li>Does ChunkStorage support for concatenating chunks ? This is indicated by supportsConcat.\n-     * If this is true then concat operation will be invoked otherwise chunks will be appended.</li>\n-     * <li>There are some obvious constraints - For ChunkStorage support any concat functionality it must support either\n-     * append or concat.</li>\n-     * <li>Also when ChunkStorage supports both concat and append, ChunkedSegmentStorage will invoke appropriate method\n-     * depending on size of target and source chunks. (Eg. ECS)</li>\n-     * </ul>\n-     *\n-     * <li>\n-     * What controls defrag?\n-     * There are two additional parameters that control when concat\n-     * <li>minSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered a small object.\n-     * For small source objects, append is used instead of using concat. (For really small txn it is rather efficient to use append than MPU).</li>\n-     * <li>maxSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered for concat. (Eg S3 might have max limit on chunk size).</li>\n-     * In short there is a size beyond which using append is not advisable. Conversely there is a size below which concat is not efficient.(minSizeLimitForConcat )\n-     * Then there is limit which concating does not make sense maxSizeLimitForConcat\n-     * </li>\n-     * <li>\n-     * What is the defrag algorithm\n-     * <pre>\n-     * While(segment.hasConcatableChunks()){\n-     *     Set<List<Chunk>> s = FindConsecutiveConcatableChunks();\n-     *     For (List<chunk> list : s){\n-     *        ConcatChunks (list);\n-     *     }\n-     * }\n-     * </pre>\n-     * </li>\n-     * </ul>\n-     *\n      * @param txn             Active {@link MetadataTransaction}.\n      * @param segmentMetadata {@link SegmentMetadata} for the segment to defrag.\n      * @param startChunkName  Name of the first chunk to start defragmentation.\n      * @param lastChunkName   Name of the last chunk before which to stop defragmentation. (last chunk is not concatenated).\n      * @param chunksToDelete  List of chunks to which names of chunks to be deleted are added. It is the responsibility\n      *                        of caller to garbage collect these chunks.\n-     * @throws ChunkStorageException In case of any chunk storage related errors.\n-     * @throws StorageMetadataException In case of any chunk metadata store related errors.\n+     *                        throws ChunkStorageException    In case of any chunk storage related errors.\n+     *                        throws StorageMetadataException In case of any chunk metadata store related errors.\n      */\n-    private void defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata,\n-                        String startChunkName,\n-                        String lastChunkName,\n-                        ArrayList<String> chunksToDelete)\n-            throws StorageMetadataException, ChunkStorageException {\n-        // The algorithm is actually very simple.\n-        // It tries to concat all small chunks using appends first.\n-        // Then it tries to concat remaining chunks using concat if available.\n-        // To implement it using single loop we toggle between concat with append and concat modes. (Instead of two passes.)\n-        boolean useAppend = true;\n-        String targetChunkName = startChunkName;\n-\n-        // Iterate through chunk list\n-        while (null != targetChunkName && !targetChunkName.equals(lastChunkName)) {\n-            ChunkMetadata target = (ChunkMetadata) txn.get(targetChunkName);\n-\n-            ArrayList<ChunkInfo> chunksToConcat = new ArrayList<>();\n-            long targetSizeAfterConcat = target.getLength();\n-\n-            // Add target to the list of chunks\n-            chunksToConcat.add(new ChunkInfo(targetSizeAfterConcat, targetChunkName));\n-\n-            String nextChunkName = target.getNextChunk();\n-            ChunkMetadata next = null;\n-\n-            // Gather list of chunks that can be appended together.\n-            while (null != nextChunkName) {\n-                next = (ChunkMetadata) txn.get(nextChunkName);\n-\n-                if (useAppend && config.getMinSizeLimitForConcat() < next.getLength()) {\n-                    break;\n-                }\n+    private CompletableFuture<Void> defrag(MetadataTransaction txn, SegmentMetadata segmentMetadata,\n+                                           String startChunkName,\n+                                           String lastChunkName,\n+                                           ArrayList<String> chunksToDelete) {\n+        return new DefragmentOperation(this, txn, segmentMetadata, startChunkName, lastChunkName, chunksToDelete).call();\n+    }\n \n-                if (targetSizeAfterConcat + next.getLength() > segmentMetadata.getMaxRollinglength() || next.getLength() > config.getMaxSizeLimitForConcat()) {\n-                    break;\n-                }\n+    @Override\n+    public CompletableFuture<Void> delete(SegmentHandle handle, Duration timeout) {\n+        checkInitialized();\n+        return executeAsync(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle);\n+            Timer timer = new Timer();\n \n-                chunksToConcat.add(new ChunkInfo(next.getLength(), nextChunkName));\n-                targetSizeAfterConcat += next.getLength();\n+            String streamSegmentName = handle.getSegmentName();\n+            return tryWith(metadataStore.beginTransaction(streamSegmentName), txn -> txn.get(streamSegmentName)\n+                    .thenComposeAsync(storageMetadata -> {\n+                        SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                        // Check preconditions\n+                        checkSegmentExists(streamSegmentName, segmentMetadata);\n+                        checkOwnership(streamSegmentName, segmentMetadata);\n+\n+                        segmentMetadata.setActive(false);\n+\n+                        // Delete chunks\n+                        ArrayList<String> chunksToDelete = new ArrayList<>();\n+                        return new ChunkIterator(txn, segmentMetadata)\n+                                .forEach((metadata, name) -> {\n+                                    txn.delete(name);\n+                                    chunksToDelete.add(name);\n+                                })\n+                                .thenRunAsync(() -> txn.delete(streamSegmentName), executor)\n+                                .thenComposeAsync(v ->\n+                                        txn.commit()\n+                                                .thenComposeAsync(vv -> {\n+                                                    // Collect garbage.\n+                                                    return collectGarbage(chunksToDelete);\n+                                                }, executor)\n+                                                .thenRunAsync(() -> {\n+                                                    // Update the read index.\n+                                                    readIndexCache.remove(streamSegmentName);\n+\n+                                                    Duration elapsed = timer.getElapsed();\n+                                                    log.debug(\"{} delete - segment={}, latency={}.\", logPrefix, handle.getSegmentName(), elapsed.toMillis());\n+                                                    LoggerHelpers.traceLeave(log, \"delete\", traceId, handle);\n+                                                }, executor)\n+                                                .exceptionally(e -> {\n+                                                    val ex = Exceptions.unwrap(e);\n+                                                    if (ex instanceof StorageMetadataWritesFencedOutException) {\n+                                                        throw new CompletionException(new StorageNotPrimaryException(streamSegmentName, ex));\n+                                                    }\n+                                                    throw new CompletionException(ex);\n+                                                }), executor);\n+                    }, executor), executor);\n+        });\n+    }\n \n-                nextChunkName = next.getNextChunk();\n-            }\n-            // Note - After above while loop is exited nextChunkName points to chunk next to last one to be concat.\n-            // Which means target should now point to it as next after concat is complete.\n+    @Override\n+    public CompletableFuture<Void> truncate(SegmentHandle handle, long offset, Duration timeout) {\n+        checkInitialized();\n+        return executeAsync(new TruncateOperation(this, handle, offset));\n+    }\n \n-            // If there are chunks that can be appended together then concat them.\n-            if (chunksToConcat.size() > 1) {\n-                // Concat\n+    @Override\n+    public boolean supportsTruncation() {\n+        return true;\n+    }\n \n-                ConcatArgument[] concatArgs = new ConcatArgument[chunksToConcat.size()];\n-                for (int i = 0; i < chunksToConcat.size(); i++) {\n-                    concatArgs[i] = ConcatArgument.fromChunkInfo(chunksToConcat.get(i));\n-                }\n+    @Override\n+    public Iterator<SegmentProperties> listSegments() {\n+        throw new UnsupportedOperationException(\"listSegments is not yet supported\");\n+    }\n \n-                if (!useAppend && chunkStorage.supportsConcat()) {\n-                    int length = chunkStorage.concat(concatArgs);\n-                } else {\n-                    concatUsingAppend(concatArgs);\n-                }\n+    @Override\n+    public CompletableFuture<SegmentHandle> openRead(String streamSegmentName) {\n+        checkInitialized();\n+        return executeAsync(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"openRead\", streamSegmentName);\n+            // Validate preconditions and return handle.\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            return tryWith(metadataStore.beginTransaction(streamSegmentName), txn ->\n+                            txn.get(streamSegmentName).thenComposeAsync(storageMetadata -> {\n+                                SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                checkSegmentExists(streamSegmentName, segmentMetadata);\n+                                segmentMetadata.checkInvariants();\n+                                // This segment was created by an older segment store. Then claim ownership and adjust length.\n+                                CompletableFuture<Void> f = CompletableFuture.completedFuture(null);\n+                                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n+                                    log.debug(\"{} openRead - Segment needs ownership change. segment={}.\", logPrefix, segmentMetadata.getName());\n+                                    // In case of a failover, length recorded in metadata will be lagging behind its actual length in the storage.\n+                                    // This can happen with lazy commits that were still not committed at the time of failover.\n+                                    f = claimOwnership(txn, segmentMetadata);\n+                                }\n+                                return f.thenApplyAsync(v -> {\n+                                    val retValue = SegmentStorageHandle.readHandle(streamSegmentName);\n+                                    LoggerHelpers.traceLeave(log, \"openRead\", traceId, retValue);\n+                                    return retValue;\n+                                }, executor);\n+                            }, executor),\n+                    executor);\n+        });\n+    }\n \n-                // Delete chunks.\n-                for (int i = 1; i < chunksToConcat.size(); i++) {\n-                    chunksToDelete.add(chunksToConcat.get(i).getName());\n-                }\n+    @Override\n+    public CompletableFuture<Integer> read(SegmentHandle handle, long offset, byte[] buffer, int bufferOffset, int length, Duration timeout) {\n+        checkInitialized();\n+        return executeAsync(new ReadOperation(this, handle, offset, buffer, bufferOffset, length));\n+    }\n \n-                // Set the pointers\n-                target.setLength(targetSizeAfterConcat);\n-                target.setNextChunk(nextChunkName);\n+    @Override\n+    public CompletableFuture<SegmentProperties> getStreamSegmentInfo(String streamSegmentName, Duration timeout) {\n+        checkInitialized();\n+        return executeAsync(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"getStreamSegmentInfo\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            return tryWith(metadataStore.beginTransaction(streamSegmentName), txn ->\n+                    txn.get(streamSegmentName)\n+                            .thenApplyAsync(storageMetadata -> {\n+                                SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                if (null == segmentMetadata) {\n+                                    throw new CompletionException(new StreamSegmentNotExistsException(streamSegmentName));\n+                                }\n+                                segmentMetadata.checkInvariants();\n+\n+                                val retValue = StreamSegmentInformation.builder()\n+                                        .name(streamSegmentName)\n+                                        .sealed(segmentMetadata.isSealed())\n+                                        .length(segmentMetadata.getLength())\n+                                        .startOffset(segmentMetadata.getStartOffset())\n+                                        .lastModified(new ImmutableDate(segmentMetadata.getLastModified()))\n+                                        .build();\n+                                LoggerHelpers.traceLeave(log, \"getStreamSegmentInfo\", traceId, retValue);\n+                                return retValue;\n+                            }, executor), executor);\n+        });\n+    }\n \n-                // If target is the last chunk after this then update metadata accordingly\n-                if (null == nextChunkName) {\n-                    segmentMetadata.setLastChunk(target.getName());\n-                    segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength() - target.getLength());\n-                }\n+    @Override\n+    public CompletableFuture<Boolean> exists(String streamSegmentName, Duration timeout) {\n+        checkInitialized();\n+        return executeAsync(() -> {\n+            long traceId = LoggerHelpers.traceEnter(log, \"exists\", streamSegmentName);\n+            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n+            return tryWith(metadataStore.beginTransaction(streamSegmentName),\n+                    txn -> txn.get(streamSegmentName)\n+                            .thenApplyAsync(storageMetadata -> {\n+                                SegmentMetadata segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                val retValue = segmentMetadata != null && segmentMetadata.isActive();\n+                                LoggerHelpers.traceLeave(log, \"exists\", traceId, retValue);\n+                                return retValue;\n+                            }, executor),\n+                    executor);\n+        });\n+    }\n \n-                // Update metadata for affected chunks.\n-                for (int i = 1; i < concatArgs.length; i++) {\n-                    txn.delete(concatArgs[i].getName());\n-                    segmentMetadata.decrementChunkCount();\n-                }\n-                txn.update(target);\n-                txn.update(segmentMetadata);\n+    @Override\n+    public void close() {\n+        try {\n+            if (null != this.metadataStore) {\n+                this.metadataStore.close();\n             }\n+        } catch (Exception e) {\n+            log.warn(\"Error during close\", e);\n+        }\n+        this.closed.set(true);\n+    }\n \n-            // Move on to next place in list where we can concat if we are done with append based concats.\n-            if (!useAppend) {\n-                targetChunkName = nextChunkName;\n+    /**\n+     * Executes the given Callable and returns its result, while translating any Exceptions bubbling out of it into\n+     * StreamSegmentExceptions.\n+     *\n+     * @param operation The function to execute.\n+     * @param <R>       Return type of the operation.\n+     * @return CompletableFuture<R> of the return type of the operation.\n+     */\n+    private <R> CompletableFuture<R> executeAsync(Callable<CompletableFuture<R>> operation) {\n+        return CompletableFuture.completedFuture(null).thenComposeAsync(v -> {\n+            Exceptions.checkNotClosed(this.closed.get(), this);\n+            try {\n+                return operation.call();\n+            } catch (CompletionException e) {\n+                throw new CompletionException(Exceptions.unwrap(e));\n+            } catch (Exception e) {\n+                throw new CompletionException(e);\n             }\n+        }, this.executor);\n+    }\n \n-            // Toggle\n-            useAppend = !useAppend;\n-        }\n-\n-        // Make sure no invariants are broken.\n-        segmentMetadata.checkInvariants();\n+    private static <T extends AutoCloseable, R> CompletableFuture<R> tryWith(T closeable, Function<T, CompletableFuture<R>> function, Executor executor) {\n+        return function.apply(closeable)\n+                    .whenCompleteAsync((v, ex) -> {\n+                        try {\n+                            closeable.close();\n+                        } catch (Exception e) {\n+                            throw new CompletionException(e);\n+                        }\n+                    }, executor);\n     }\n \n-    private void concatUsingAppend(ConcatArgument[] concatArgs) throws ChunkStorageException {\n-        long writeAtOffset = concatArgs[0].getLength();\n-        val writeHandle = ChunkHandle.writeHandle(concatArgs[0].getName());\n-        for (int i = 1; i < concatArgs.length; i++) {\n-            int readAtOffset = 0;\n-            val arg = concatArgs[i];\n-            int bytesToRead = Math.toIntExact(arg.getLength());\n-\n-            while (bytesToRead > 0) {\n-                byte[] buffer = new byte[Math.min(config.getMaxBufferSizeForChunkDataTransfer(), bytesToRead)];\n-                int size = chunkStorage.read(ChunkHandle.readHandle(arg.getName()), readAtOffset, buffer.length, buffer, 0);\n-                bytesToRead -= size;\n-                readAtOffset += size;\n-                writeAtOffset += chunkStorage.write(writeHandle, writeAtOffset, size, new ByteArrayInputStream(buffer, 0, size));\n-            }\n+    private void checkSegmentExists(String streamSegmentName, SegmentMetadata segmentMetadata) {\n+        if (null == segmentMetadata || !segmentMetadata.isActive()) {\n+            throw new CompletionException(new StreamSegmentNotExistsException(streamSegmentName));\n         }\n     }\n \n-    @Override\n-    public CompletableFuture<Void> delete(SegmentHandle handle, Duration timeout) {\n-        checkInitialized();\n-        return execute(() -> {\n-            long traceId = LoggerHelpers.traceEnter(log, \"delete\", handle);\n-            Timer timer = new Timer();\n+    private void checkOwnership(String streamSegmentName, SegmentMetadata segmentMetadata) {\n+        if (segmentMetadata.getOwnerEpoch() > this.epoch) {\n+            throw new CompletionException(new StorageNotPrimaryException(streamSegmentName));\n+        }\n+    }\n \n-            String streamSegmentName = handle.getSegmentName();\n-            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n-                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n-\n-                // Check preconditions\n-                checkSegmentExists(streamSegmentName, segmentMetadata);\n-                checkOwnership(streamSegmentName, segmentMetadata);\n-\n-                segmentMetadata.setActive(false);\n-\n-                // Delete chunks\n-                String currentChunkName = segmentMetadata.getFirstChunk();\n-                ChunkMetadata currentMetadata;\n-                ArrayList<String> chunksToDelete = new ArrayList<>();\n-                while (currentChunkName != null) {\n-                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n-                    // Delete underlying file.\n-                    chunksToDelete.add(currentChunkName);\n-                    currentChunkName = currentMetadata.getNextChunk();\n-                    txn.delete(currentMetadata.getName());\n-                }\n+    private void checkNotSealed(String streamSegmentName, SegmentMetadata segmentMetadata) {\n+        if (segmentMetadata.isSealed()) {\n+            throw new CompletionException(new StreamSegmentSealedException(streamSegmentName));\n+        }\n+    }\n \n-                // Commit.\n-                txn.delete(streamSegmentName);\n-                txn.commit();\n+    private void checkInitialized() {\n+        Preconditions.checkState(null != this.metadataStore);\n+        Preconditions.checkState(0 != this.epoch);\n+        Preconditions.checkState(!closed.get());\n+    }\n \n-                // Collect garbage.\n-                collectGarbage(chunksToDelete);\n+    private class ChunkIterator {\n+        private final MetadataTransaction txn;\n+        private String currentChunkName;\n+        private ChunkMetadata currentMetadata;\n \n-                // Update the read index.\n-                readIndexCache.remove(streamSegmentName);\n+        ChunkIterator(MetadataTransaction txn, SegmentMetadata segmentMetadata) {\n+            this.txn = txn;\n+            currentChunkName = segmentMetadata.getFirstChunk();\n+        }\n \n-                Duration elapsed = timer.getElapsed();\n-                log.debug(\"{} delete - segment={}, latency={}.\", logPrefix, handle.getSegmentName(), elapsed.toMillis());\n-                LoggerHelpers.traceLeave(log, \"delete\", traceId, handle);\n-                return null;\n-            } catch (StorageMetadataWritesFencedOutException ex) {\n-                throw new StorageNotPrimaryException(streamSegmentName, ex);\n-            }\n-        });\n+        public CompletableFuture<Void> forEach(BiConsumer<ChunkMetadata, String> consumer) {\n+            return Futures.loop(\n+                    () -> currentChunkName != null,\n+                    () -> txn.get(currentChunkName)\n+                            .thenApplyAsync(storageMetadata -> {\n+                                currentMetadata = (ChunkMetadata) storageMetadata;\n+                                consumer.accept(currentMetadata, currentChunkName);\n+                                // Move next\n+                                currentChunkName = currentMetadata.getNextChunk();\n+                                return null;\n+                            }, executor),\n+                    executor);\n+        }\n     }\n \n-    @Override\n-    public CompletableFuture<Void> truncate(SegmentHandle handle, long offset, Duration timeout) {\n-        checkInitialized();\n-        return execute(() -> {\n-            long traceId = LoggerHelpers.traceEnter(log, \"truncate\", handle, offset);\n-            Timer timer = new Timer();\n+    private static class TruncateOperation implements Callable<CompletableFuture<Void>> {\n+        private final SegmentHandle handle;\n+        private final long offset;\n+        private final ChunkedSegmentStorage chunkedSegmentStorage;\n+\n+        private String currentChunkName;\n+        private ChunkMetadata currentMetadata;\n+        private long oldLength;\n+        private long startOffset;\n+        private ArrayList<String> chunksToDelete = new ArrayList<>();\n+        private SegmentMetadata segmentMetadata;\n+        private String streamSegmentName;\n+\n+        private boolean isLoopExited;\n+        private long traceId;\n+        private Timer timer;\n+\n+        TruncateOperation(ChunkedSegmentStorage chunkedSegmentStorage, SegmentHandle handle, long offset) {\n+            this.handle = handle;\n+            this.offset = offset;\n+            this.chunkedSegmentStorage = chunkedSegmentStorage;\n+        }\n \n-            Preconditions.checkArgument(null != handle, \"handle\");\n-            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n-            Preconditions.checkArgument(offset >= 0, \"offset\");\n+        public CompletableFuture<Void> call() {\n+            traceId = LoggerHelpers.traceEnter(log, \"truncate\", handle, offset);\n+            timer = new Timer();\n+\n+            checkPreconditions();\n+\n+            streamSegmentName = handle.getSegmentName();\n+            return tryWith(chunkedSegmentStorage.metadataStore.beginTransaction(streamSegmentName), txn ->\n+                    txn.get(streamSegmentName)\n+                            .thenComposeAsync(storageMetadata -> {\n+                                segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                // Check preconditions\n+                                checkPreconditions(streamSegmentName, segmentMetadata);\n+\n+                                if (segmentMetadata.getStartOffset() == offset) {\n+                                    // Nothing to do\n+                                    return CompletableFuture.completedFuture(null);\n+                                }\n+\n+                                return updateFirstChunk(txn)\n+                                        .thenComposeAsync(v -> {\n+                                            deleteChunks(txn);\n+\n+                                            txn.update(segmentMetadata);\n+\n+                                            // Check invariants.\n+                                            Preconditions.checkState(segmentMetadata.getLength() == oldLength, \"truncate should not change segment length\");\n+                                            segmentMetadata.checkInvariants();\n+\n+                                            // Finally commit.\n+                                            return commit(txn)\n+                                                    .handleAsync(this::handleException, chunkedSegmentStorage.executor)\n+                                                    .thenComposeAsync(vv ->\n+                                                                    chunkedSegmentStorage.collectGarbage(chunksToDelete).thenApplyAsync(vvv -> {\n+                                                                        postCommit();\n+                                                                        return null;\n+                                                                    }, chunkedSegmentStorage.executor),\n+                                                            chunkedSegmentStorage.executor);\n+                                        }, chunkedSegmentStorage.executor);\n+                            }, chunkedSegmentStorage.executor), chunkedSegmentStorage.executor);\n+        }\n \n-            String streamSegmentName = handle.getSegmentName();\n-            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n-                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+        private void postCommit() {\n+            // Update the read index by removing all entries below truncate offset.\n+            chunkedSegmentStorage.readIndexCache.truncateReadIndex(streamSegmentName, segmentMetadata.getStartOffset());\n \n-                // Check preconditions\n-                checkSegmentExists(streamSegmentName, segmentMetadata);\n-                checkNotSealed(streamSegmentName, segmentMetadata);\n-                checkOwnership(streamSegmentName, segmentMetadata);\n+            logEnd();\n+        }\n+\n+        private void logEnd() {\n+            Duration elapsed = timer.getElapsed();\n+            log.debug(\"{} truncate - segment={}, offset={}, latency={}.\", chunkedSegmentStorage.logPrefix, handle.getSegmentName(), offset, elapsed.toMillis());\n+            LoggerHelpers.traceLeave(log, \"truncate\", traceId, handle, offset);\n+        }\n \n-                if (segmentMetadata.getLength() < offset || segmentMetadata.getStartOffset() > offset) {\n-                    throw new IllegalArgumentException(String.format(\"offset %d is outside of valid range [%d, %d) for segment %s\",\n-                            offset, segmentMetadata.getStartOffset(), segmentMetadata.getLength(), streamSegmentName));\n+        private Void handleException(Void value, Throwable e) {\n+            if (null != e) {\n+                val ex = Exceptions.unwrap(e);\n+                if (ex instanceof StorageMetadataWritesFencedOutException) {\n+                    throw new CompletionException(new StorageNotPrimaryException(streamSegmentName, ex));\n                 }\n+                throw new CompletionException(ex);\n+            }\n+            return value;\n+        }\n \n-                if (segmentMetadata.getStartOffset() == offset) {\n-                    // Nothing to do\n+        private CompletableFuture<Void> commit(MetadataTransaction txn) {\n+            // Commit system logs.\n+            if (chunkedSegmentStorage.isStorageSystemSegment(segmentMetadata)) {\n+                val finalStartOffset = startOffset;\n+                txn.setExternalCommitStep(() -> {\n+                    chunkedSegmentStorage.systemJournal.commitRecord(\n+                            SystemJournal.TruncationRecord.builder()\n+                                    .segmentName(streamSegmentName)\n+                                    .offset(offset)\n+                                    .firstChunkName(segmentMetadata.getFirstChunk())\n+                                    .startOffset(finalStartOffset)\n+                                    .build());\n                     return null;\n-                }\n+                });\n+            }\n \n-                String currentChunkName = segmentMetadata.getFirstChunk();\n-                ChunkMetadata currentMetadata;\n-                long oldLength = segmentMetadata.getLength();\n-                long startOffset = segmentMetadata.getFirstChunkStartOffset();\n-                ArrayList<String> chunksToDelete = new ArrayList<>();\n-                while (currentChunkName != null) {\n-                    currentMetadata = (ChunkMetadata) txn.get(currentChunkName);\n-                    Preconditions.checkState(null != currentMetadata, \"currentMetadata is null.\");\n-\n-                    // If for given chunk start <= offset < end  then we have found the chunk that will be the first chunk.\n-                    if ((startOffset <= offset) && (startOffset + currentMetadata.getLength() > offset)) {\n-                        break;\n-                    }\n-\n-                    startOffset += currentMetadata.getLength();\n-                    chunksToDelete.add(currentMetadata.getName());\n-                    segmentMetadata.decrementChunkCount();\n+            // Finally commit.\n+            return txn.commit();\n+        }\n \n-                    // move to next chunk\n-                    currentChunkName = currentMetadata.getNextChunk();\n-                }\n+        private CompletableFuture<Void> updateFirstChunk(MetadataTransaction txn) {\n+            currentChunkName = segmentMetadata.getFirstChunk();\n+            oldLength = segmentMetadata.getLength();\n+            startOffset = segmentMetadata.getFirstChunkStartOffset();\n+            return Futures.loop(\n+                    () -> currentChunkName != null && !isLoopExited,\n+                    () -> txn.get(currentChunkName)\n+                            .thenApplyAsync(storageMetadata -> {\n+                                currentMetadata = (ChunkMetadata) storageMetadata;\n+                                Preconditions.checkState(null != currentMetadata, \"currentMetadata is null.\");\n+\n+                                // If for given chunk start <= offset < end  then we have found the chunk that will be the first chunk.\n+                                if ((startOffset <= offset) && (startOffset + currentMetadata.getLength() > offset)) {\n+                                    isLoopExited = true;\n+                                    return null;\n+                                }\n+\n+                                startOffset += currentMetadata.getLength();\n+                                chunksToDelete.add(currentMetadata.getName());\n+                                segmentMetadata.decrementChunkCount();\n+\n+                                // move to next chunk\n+                                currentChunkName = currentMetadata.getNextChunk();\n+                                return null;\n+                            }, chunkedSegmentStorage.executor),\n+                    chunkedSegmentStorage.executor\n+            ).thenApplyAsync(v -> {\n                 segmentMetadata.setFirstChunk(currentChunkName);\n                 segmentMetadata.setStartOffset(offset);\n                 segmentMetadata.setFirstChunkStartOffset(startOffset);\n-                for (String toDelete : chunksToDelete) {\n-                    txn.delete(toDelete);\n-                    // Adjust last chunk if required.\n-                    if (toDelete.equals(segmentMetadata.getLastChunk())) {\n-                        segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n-                        segmentMetadata.setLastChunk(null);\n-                    }\n-                }\n-                txn.update(segmentMetadata);\n+                return null;\n+            }, chunkedSegmentStorage.executor);\n+        }\n \n-                // Check invariants.\n-                Preconditions.checkState(segmentMetadata.getLength() == oldLength, \"truncate should not change segment length\");\n-                segmentMetadata.checkInvariants();\n-\n-                // Commit system logs.\n-                if (isStorageSystemSegment(segmentMetadata)) {\n-                    val finalStartOffset = startOffset;\n-                    txn.setExternalCommitStep(() -> {\n-                        systemJournal.commitRecord(\n-                                SystemJournal.TruncationRecord.builder()\n-                                        .segmentName(streamSegmentName)\n-                                        .offset(offset)\n-                                        .firstChunkName(segmentMetadata.getFirstChunk())\n-                                        .startOffset(finalStartOffset)\n-                                        .build());\n-                        return null;\n-                    });\n+        private void deleteChunks(MetadataTransaction txn) {\n+            for (String toDelete : chunksToDelete) {\n+                txn.delete(toDelete);\n+                // Adjust last chunk if required.\n+                if (toDelete.equals(segmentMetadata.getLastChunk())) {\n+                    segmentMetadata.setLastChunkStartOffset(segmentMetadata.getLength());\n+                    segmentMetadata.setLastChunk(null);\n                 }\n+            }\n+        }\n \n-                // Finally commit.\n-                txn.commit();\n-\n-                collectGarbage(chunksToDelete);\n-\n-                // Update the read index by removing all entries below truncate offset.\n-                readIndexCache.truncateReadIndex(streamSegmentName, segmentMetadata.getStartOffset());\n+        private void checkPreconditions(String streamSegmentName, SegmentMetadata segmentMetadata) {\n+            chunkedSegmentStorage.checkSegmentExists(streamSegmentName, segmentMetadata);\n+            chunkedSegmentStorage.checkNotSealed(streamSegmentName, segmentMetadata);\n+            chunkedSegmentStorage.checkOwnership(streamSegmentName, segmentMetadata);\n \n-                Duration elapsed = timer.getElapsed();\n-                log.debug(\"{} truncate - segment={}, offset={}, latency={}.\", logPrefix, handle.getSegmentName(), offset, elapsed.toMillis());\n-                LoggerHelpers.traceLeave(log, \"truncate\", traceId, handle, offset);\n-                return null;\n-            } catch (StorageMetadataWritesFencedOutException ex) {\n-                throw new StorageNotPrimaryException(streamSegmentName, ex);\n+            if (segmentMetadata.getLength() < offset || segmentMetadata.getStartOffset() > offset) {\n+                throw new IllegalArgumentException(String.format(\"offset %d is outside of valid range [%d, %d) for segment %s\",\n+                        offset, segmentMetadata.getStartOffset(), segmentMetadata.getLength(), streamSegmentName));\n             }\n-        });\n-    }\n+        }\n \n-    @Override\n-    public boolean supportsTruncation() {\n-        return true;\n+        private void checkPreconditions() {\n+            Preconditions.checkArgument(null != handle, \"handle\");\n+            Preconditions.checkArgument(!handle.isReadOnly(), \"handle\");\n+            Preconditions.checkArgument(offset >= 0, \"offset\");\n+        }\n     }\n \n-    @Override\n-    public Iterator<SegmentProperties> listSegments() throws IOException {\n-        throw new UnsupportedOperationException(\"listSegments is not yet supported\");\n-    }\n+    private static class ReadOperation implements Callable<CompletableFuture<Integer>> {\n+        private final SegmentHandle handle;\n+        private final long offset;\n+        private final byte[] buffer;\n+        private final int bufferOffset;\n+        private final int length;\n+        private final ChunkedSegmentStorage chunkedSegmentStorage;\n+        private long traceId;\n+        private Timer timer;\n+        private String streamSegmentName;\n+        private SegmentMetadata segmentMetadata;\n+        private int bytesRemaining;\n+        private int currentBufferOffset;\n+        private long currentOffset;\n+        private int totalBytesRead = 0;\n+        private long startOffsetForCurrentChunk;\n+        private String currentChunkName;\n+        private ChunkMetadata chunkToReadFrom = null;\n+        private boolean isLoopExited;\n+        private int cntScanned = 0;\n+        private int bytesToRead;\n+\n+        ReadOperation(ChunkedSegmentStorage chunkedSegmentStorage, SegmentHandle handle, long offset, byte[] buffer, int bufferOffset, int length) {\n+            this.handle = handle;\n+            this.offset = offset;\n+            this.buffer = buffer;\n+            this.bufferOffset = bufferOffset;\n+            this.length = length;\n+            this.chunkedSegmentStorage = chunkedSegmentStorage;\n+        }\n \n-    @Override\n-    public CompletableFuture<SegmentHandle> openRead(String streamSegmentName) {\n-        checkInitialized();\n-        return execute(() -> {\n-            long traceId = LoggerHelpers.traceEnter(log, \"openRead\", streamSegmentName);\n-            // Validate preconditions and return handle.\n-            Preconditions.checkNotNull(streamSegmentName, \"streamSegmentName\");\n-            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n-                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n-                checkSegmentExists(streamSegmentName, segmentMetadata);\n-                segmentMetadata.checkInvariants();\n-                // This segment was created by an older segment store. Then claim ownership and adjust length.\n-                if (segmentMetadata.getOwnerEpoch() < this.epoch) {\n-                    log.debug(\"{} openRead - Segment needs ownership change. segment={}.\", logPrefix, segmentMetadata.getName());\n-                    // In case of a failover, length recorded in metadata will be lagging behind its actual length in the storage.\n-                    // This can happen with lazy commits that were still not committed at the time of failover.\n-                    claimOwnership(txn, segmentMetadata);\n-                }\n-                val retValue = SegmentStorageHandle.readHandle(streamSegmentName);\n-                LoggerHelpers.traceLeave(log, \"openRead\", traceId, retValue);\n-                return retValue;\n+        public CompletableFuture<Integer> call() {\n+            traceId = LoggerHelpers.traceEnter(log, \"read\", handle, offset, length);\n+            timer = new Timer();\n+\n+            // Validate preconditions.\n+            checkPreconditions();\n+            streamSegmentName = handle.getSegmentName();\n+            return tryWith(chunkedSegmentStorage.metadataStore.beginTransaction(streamSegmentName),\n+                    txn -> txn.get(streamSegmentName)\n+                            .thenComposeAsync(storageMetadata -> {\n+                                segmentMetadata = (SegmentMetadata) storageMetadata;\n+\n+                                // Validate preconditions.\n+                                checkState();\n+\n+                                if (length == 0) {\n+                                    return CompletableFuture.completedFuture(0);\n+                                }\n+\n+                                return findChunkForOffset(txn)\n+                                        .thenComposeAsync(v -> {\n+                                            // Now read.\n+                                            return readData(txn);\n+                                        }, chunkedSegmentStorage.executor)\n+                                        .thenApplyAsync(v -> {\n+                                            logEnd();\n+                                            return totalBytesRead;\n+                                        }, chunkedSegmentStorage.executor);\n+                            }, chunkedSegmentStorage.executor),\n+                    chunkedSegmentStorage.executor);\n+        }\n+\n+        private void logEnd() {\n+            Duration elapsed = timer.getElapsed();\n+            log.debug(\"{} read - segment={}, offset={}, bytesRead={}, latency={}.\", chunkedSegmentStorage.logPrefix, handle.getSegmentName(), offset, totalBytesRead, elapsed.toMillis());\n+            LoggerHelpers.traceLeave(log, \"read\", traceId, handle, offset, totalBytesRead);\n+        }\n+\n+        private CompletableFuture<Void> readData(MetadataTransaction txn) {\n+            return Futures.loop(\n+                    () -> bytesRemaining > 0 && null != currentChunkName,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg5NjkyMw=="}, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 1616}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTk2ODI0OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMjowMjo1OFrOHcvT7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQyMjo1MDowNFrOHjRvWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg5NzMyNQ==", "bodyText": "Is there another way to format this method? My head hurts :)", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r499897325", "createdAt": "2020-10-05T22:02:58Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -1132,207 +1080,792 @@ public boolean supportsTruncation() {\n                         \"Offset (%s) must be non-negative, and bufferOffset (%s) and length (%s) must be valid indices into buffer of size %s.\",\n                         offset, bufferOffset, length, buffer.length));\n             }\n+        }\n+    }\n \n-            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n-                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+    /**\n+     * Implements the write operation.\n+     */\n+    private static class WriteOperation implements Callable<CompletableFuture<Void>> {\n+        private final SegmentHandle handle;\n+        private final long offset;\n+        private final InputStream data;\n+        private final int length;\n+        private final ChunkedSegmentStorage chunkedSegmentStorage;\n+\n+        private final ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+        private final List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<>();\n+        private int chunksAddedCount = 0;\n+        private boolean isCommited = false;\n+\n+        private long traceId;\n+        private Timer timer;\n+\n+        private String streamSegmentName;\n+        private SegmentMetadata segmentMetadata;\n+\n+        private boolean isSystemSegment;\n+\n+        // Check if this is a first write after ownership changed.\n+        private boolean isFirstWriteAfterFailover;\n+\n+        private ChunkMetadata lastChunkMetadata = null;\n+        private ChunkHandle chunkHandle = null;\n+        private int bytesRemaining;\n+        private long currentOffset;\n+\n+        private boolean didSegmentLayoutChange = false;\n+\n+        WriteOperation(ChunkedSegmentStorage chunkedSegmentStorage, SegmentHandle handle, long offset, InputStream data, int length) {\n+            this.handle = handle;\n+            this.offset = offset;\n+            this.data = data;\n+            this.length = length;\n+            this.chunkedSegmentStorage = chunkedSegmentStorage;\n+        }\n \n-                // Validate preconditions.\n-                checkSegmentExists(streamSegmentName, segmentMetadata);\n+        public CompletableFuture<Void> call() {\n+            traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            timer = new Timer();\n \n-                segmentMetadata.checkInvariants();\n+            // Validate preconditions.\n+            checkPreconditions();\n+\n+            streamSegmentName = handle.getSegmentName();\n+            return tryWith(chunkedSegmentStorage.metadataStore.beginTransaction(handle.getSegmentName()),\n+                    txn -> {\n+                        didSegmentLayoutChange = false;\n+\n+                        // Retrieve metadata.\n+                        return txn.get(streamSegmentName)\n+                                .thenComposeAsync(storageMetadata -> {\n+                                    segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                    // Validate preconditions.\n+                                    checkState();\n+\n+                                    isSystemSegment = chunkedSegmentStorage.isStorageSystemSegment(segmentMetadata);\n+\n+                                    // Check if this is a first write after ownership changed.\n+                                    isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                                    lastChunkMetadata = null;\n+                                    chunkHandle = null;\n+                                    bytesRemaining = length;\n+                                    currentOffset = offset;\n+\n+                                    // Get the last chunk segmentMetadata for the segment.\n+\n+                                    return getLastChunk(txn)\n+                                            .thenComposeAsync(v ->", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 1824}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc1Mjg1OA==", "bodyText": "mine too \ud83d\udc4d", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r506752858", "createdAt": "2020-10-16T22:50:04Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -1132,207 +1080,792 @@ public boolean supportsTruncation() {\n                         \"Offset (%s) must be non-negative, and bufferOffset (%s) and length (%s) must be valid indices into buffer of size %s.\",\n                         offset, bufferOffset, length, buffer.length));\n             }\n+        }\n+    }\n \n-            try (MetadataTransaction txn = metadataStore.beginTransaction()) {\n-                SegmentMetadata segmentMetadata = (SegmentMetadata) txn.get(streamSegmentName);\n+    /**\n+     * Implements the write operation.\n+     */\n+    private static class WriteOperation implements Callable<CompletableFuture<Void>> {\n+        private final SegmentHandle handle;\n+        private final long offset;\n+        private final InputStream data;\n+        private final int length;\n+        private final ChunkedSegmentStorage chunkedSegmentStorage;\n+\n+        private final ArrayList<SystemJournal.SystemJournalRecord> systemLogRecords = new ArrayList<>();\n+        private final List<ChunkNameOffsetPair> newReadIndexEntries = new ArrayList<>();\n+        private int chunksAddedCount = 0;\n+        private boolean isCommited = false;\n+\n+        private long traceId;\n+        private Timer timer;\n+\n+        private String streamSegmentName;\n+        private SegmentMetadata segmentMetadata;\n+\n+        private boolean isSystemSegment;\n+\n+        // Check if this is a first write after ownership changed.\n+        private boolean isFirstWriteAfterFailover;\n+\n+        private ChunkMetadata lastChunkMetadata = null;\n+        private ChunkHandle chunkHandle = null;\n+        private int bytesRemaining;\n+        private long currentOffset;\n+\n+        private boolean didSegmentLayoutChange = false;\n+\n+        WriteOperation(ChunkedSegmentStorage chunkedSegmentStorage, SegmentHandle handle, long offset, InputStream data, int length) {\n+            this.handle = handle;\n+            this.offset = offset;\n+            this.data = data;\n+            this.length = length;\n+            this.chunkedSegmentStorage = chunkedSegmentStorage;\n+        }\n \n-                // Validate preconditions.\n-                checkSegmentExists(streamSegmentName, segmentMetadata);\n+        public CompletableFuture<Void> call() {\n+            traceId = LoggerHelpers.traceEnter(log, \"write\", handle, offset, length);\n+            timer = new Timer();\n \n-                segmentMetadata.checkInvariants();\n+            // Validate preconditions.\n+            checkPreconditions();\n+\n+            streamSegmentName = handle.getSegmentName();\n+            return tryWith(chunkedSegmentStorage.metadataStore.beginTransaction(handle.getSegmentName()),\n+                    txn -> {\n+                        didSegmentLayoutChange = false;\n+\n+                        // Retrieve metadata.\n+                        return txn.get(streamSegmentName)\n+                                .thenComposeAsync(storageMetadata -> {\n+                                    segmentMetadata = (SegmentMetadata) storageMetadata;\n+                                    // Validate preconditions.\n+                                    checkState();\n+\n+                                    isSystemSegment = chunkedSegmentStorage.isStorageSystemSegment(segmentMetadata);\n+\n+                                    // Check if this is a first write after ownership changed.\n+                                    isFirstWriteAfterFailover = segmentMetadata.isOwnershipChanged();\n+\n+                                    lastChunkMetadata = null;\n+                                    chunkHandle = null;\n+                                    bytesRemaining = length;\n+                                    currentOffset = offset;\n+\n+                                    // Get the last chunk segmentMetadata for the segment.\n+\n+                                    return getLastChunk(txn)\n+                                            .thenComposeAsync(v ->", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg5NzMyNQ=="}, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 1824}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTk4MjEzOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMjowODoyOVrOHcvcOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQyMjo1Mzo1NFrOHjRzXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg5OTQ0OA==", "bodyText": "This has become the biggest class in the whole project. Can we do something to make it smaller? Can we move each of those operations in their own, separate files?", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r499899448", "createdAt": "2020-10-05T22:08:29Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -171,7 +173,7 @@ public ChunkedSegmentStorage(ChunkStorage chunkStorage, ChunkMetadataStore metad\n      * @param containerId   container id.\n      * @throws Exception In case of any errors.\n      */\n-    public void bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {\n+    public CompletableFuture<Void> bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc1Mzg4NA==", "bodyText": "now I have split big bad class into smaller classes.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r506753884", "createdAt": "2020-10-16T22:53:54Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -171,7 +173,7 @@ public ChunkedSegmentStorage(ChunkStorage chunkStorage, ChunkMetadataStore metad\n      * @param containerId   container id.\n      * @throws Exception In case of any errors.\n      */\n-    public void bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {\n+    public CompletableFuture<Void> bootstrap(int containerId, ChunkMetadataStore metadataStore) throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg5OTQ0OA=="}, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTk4NDcxOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMjowOTo0MVrOHcvd3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQyMjo1NToyM1rOHjR1SA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg5OTg3MQ==", "bodyText": "Is there a plan to fix these?", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r499899871", "createdAt": "2020-10-05T22:09:41Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -140,7 +150,7 @@ public SystemJournal(int containerId, long epoch, ChunkStorage chunkStorage, Chu\n         this.systemSegments = getChunkStorageSystemSegments(containerId);\n         this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n \n-        Preconditions.checkState(!chunkStorage.exists(getSystemJournalChunkName()));\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalChunkName()).get());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc1NDM3Ng==", "bodyText": "not yet. This happens at the very beginning when these are the only SLTS operations at the bootstrap time. There is no table store access here.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r506754376", "createdAt": "2020-10-16T22:55:23Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/SystemJournal.java", "diffHunk": "@@ -140,7 +150,7 @@ public SystemJournal(int containerId, long epoch, ChunkStorage chunkStorage, Chu\n         this.systemSegments = getChunkStorageSystemSegments(containerId);\n         this.systemSegmentsPrefix = NameUtils.INTERNAL_SCOPE_NAME;\n \n-        Preconditions.checkState(!chunkStorage.exists(getSystemJournalChunkName()));\n+        Preconditions.checkState(!chunkStorage.exists(getSystemJournalChunkName()).get());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTg5OTg3MQ=="}, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTk5NDczOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMjoxMzozN1rOHcvjsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQyMjo1Njo0OVrOHjR23w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTkwMTM2MQ==", "bodyText": "You are not holding a lock here while accessing activeKeys while below you do. Please be consistent (and correct).", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r499901361", "createdAt": "2020-10-05T22:13:37Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -299,110 +487,169 @@ public void abort(MetadataTransaction txn) throws StorageMetadataException {\n      * @param txn Transaction.\n      * @param key key to use to retrieve metadata.\n      * @return Metadata for given key. Null if key was not found.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public StorageMetadata get(MetadataTransaction txn, String key) throws StorageMetadataException {\n+    public CompletableFuture<StorageMetadata> get(MetadataTransaction txn, String key) {\n         Preconditions.checkArgument(null != txn);\n-        TransactionData dataFromBuffer = null;\n         if (null == key) {\n-            return null;\n+            return CompletableFuture.completedFuture(null);\n         }\n-        StorageMetadata retValue = null;\n \n         Map<String, TransactionData> txnData = txn.getData();\n+\n+        // Record is found in transaction data itself.\n         TransactionData data = txnData.get(key);\n+        if (null != data) {\n+            return CompletableFuture.completedFuture(data.getValue());\n+        }\n \n-        // Search in the buffer.\n-        if (null == data) {\n-            synchronized (lock) {\n-                dataFromBuffer = bufferedTxnData.get(key);\n-            }\n-            // If we did not find in buffer then load it from store\n-            if (null == dataFromBuffer) {\n-                // NOTE: This call to read MUST be outside the lock, it is most likely cause re-entry.\n-                loadFromStore(key);\n-                dataFromBuffer = bufferedTxnData.get(key);\n-                Preconditions.checkState(null != dataFromBuffer);\n-            }\n+        // Prevent the key from getting evicted.\n+        addToActiveKeySet(key);\n+\n+        // Try to find it in buffer. Access buffer using reader lock.\n+        val readLock = scheduler.getReadLock(new String[]{key});\n+        return readLock.lock()\n+                .thenApplyAsync(v -> bufferedTxnData.get(key), executor)\n+                .thenApplyAsync(dataFromBuffer -> {\n+                    if (dataFromBuffer != null) {\n+                        // Make sure it is a deep copy.\n+                        val retValue = dataFromBuffer.getValue();\n+                        if (null != retValue) {\n+                            return retValue.deepCopy();\n+                        }\n+                        return null;\n+                    }\n+                    return null;\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> readLock.unlock(), executor)\n+                .thenComposeAsync(retValue -> {\n+                    if (retValue != null) {\n+                        return CompletableFuture.completedFuture(retValue);\n+                    }\n+                    // We did not find it in the buffer either.\n+                    // Try to find it in store.\n+                    return loadFromStore(key, false)\n+                            .thenApplyAsync(TransactionData::getValue, executor);\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> removeFromActiveKeySet(key), executor);\n+    }\n \n-            if (null != dataFromBuffer && null != dataFromBuffer.getValue()) {\n-                // Make copy.\n-                data = dataFromBuffer.toBuilder()\n-                        .key(key)\n-                        .value(dataFromBuffer.getValue().deepCopy())\n-                        .build();\n-                txnData.put(key, data);\n+    private void removeFromActiveKeySet(String key) {\n+        activeKeys.remove(key);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 584}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc1NDc4Mw==", "bodyText": "activeKeys is concurrent multi hash - so addition/deleting is safe in this context.\nadded comment explaining why not having lock here is safe.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r506754783", "createdAt": "2020-10-16T22:56:49Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -299,110 +487,169 @@ public void abort(MetadataTransaction txn) throws StorageMetadataException {\n      * @param txn Transaction.\n      * @param key key to use to retrieve metadata.\n      * @return Metadata for given key. Null if key was not found.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public StorageMetadata get(MetadataTransaction txn, String key) throws StorageMetadataException {\n+    public CompletableFuture<StorageMetadata> get(MetadataTransaction txn, String key) {\n         Preconditions.checkArgument(null != txn);\n-        TransactionData dataFromBuffer = null;\n         if (null == key) {\n-            return null;\n+            return CompletableFuture.completedFuture(null);\n         }\n-        StorageMetadata retValue = null;\n \n         Map<String, TransactionData> txnData = txn.getData();\n+\n+        // Record is found in transaction data itself.\n         TransactionData data = txnData.get(key);\n+        if (null != data) {\n+            return CompletableFuture.completedFuture(data.getValue());\n+        }\n \n-        // Search in the buffer.\n-        if (null == data) {\n-            synchronized (lock) {\n-                dataFromBuffer = bufferedTxnData.get(key);\n-            }\n-            // If we did not find in buffer then load it from store\n-            if (null == dataFromBuffer) {\n-                // NOTE: This call to read MUST be outside the lock, it is most likely cause re-entry.\n-                loadFromStore(key);\n-                dataFromBuffer = bufferedTxnData.get(key);\n-                Preconditions.checkState(null != dataFromBuffer);\n-            }\n+        // Prevent the key from getting evicted.\n+        addToActiveKeySet(key);\n+\n+        // Try to find it in buffer. Access buffer using reader lock.\n+        val readLock = scheduler.getReadLock(new String[]{key});\n+        return readLock.lock()\n+                .thenApplyAsync(v -> bufferedTxnData.get(key), executor)\n+                .thenApplyAsync(dataFromBuffer -> {\n+                    if (dataFromBuffer != null) {\n+                        // Make sure it is a deep copy.\n+                        val retValue = dataFromBuffer.getValue();\n+                        if (null != retValue) {\n+                            return retValue.deepCopy();\n+                        }\n+                        return null;\n+                    }\n+                    return null;\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> readLock.unlock(), executor)\n+                .thenComposeAsync(retValue -> {\n+                    if (retValue != null) {\n+                        return CompletableFuture.completedFuture(retValue);\n+                    }\n+                    // We did not find it in the buffer either.\n+                    // Try to find it in store.\n+                    return loadFromStore(key, false)\n+                            .thenApplyAsync(TransactionData::getValue, executor);\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> removeFromActiveKeySet(key), executor);\n+    }\n \n-            if (null != dataFromBuffer && null != dataFromBuffer.getValue()) {\n-                // Make copy.\n-                data = dataFromBuffer.toBuilder()\n-                        .key(key)\n-                        .value(dataFromBuffer.getValue().deepCopy())\n-                        .build();\n-                txnData.put(key, data);\n+    private void removeFromActiveKeySet(String key) {\n+        activeKeys.remove(key);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTkwMTM2MQ=="}, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 584}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEyOTk5NTM3OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMjoxMzo1NFrOHcvkDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQyMjo1NzowNlrOHjR3Hw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTkwMTQ1NA==", "bodyText": "here too", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r499901454", "createdAt": "2020-10-05T22:13:54Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -299,110 +487,169 @@ public void abort(MetadataTransaction txn) throws StorageMetadataException {\n      * @param txn Transaction.\n      * @param key key to use to retrieve metadata.\n      * @return Metadata for given key. Null if key was not found.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public StorageMetadata get(MetadataTransaction txn, String key) throws StorageMetadataException {\n+    public CompletableFuture<StorageMetadata> get(MetadataTransaction txn, String key) {\n         Preconditions.checkArgument(null != txn);\n-        TransactionData dataFromBuffer = null;\n         if (null == key) {\n-            return null;\n+            return CompletableFuture.completedFuture(null);\n         }\n-        StorageMetadata retValue = null;\n \n         Map<String, TransactionData> txnData = txn.getData();\n+\n+        // Record is found in transaction data itself.\n         TransactionData data = txnData.get(key);\n+        if (null != data) {\n+            return CompletableFuture.completedFuture(data.getValue());\n+        }\n \n-        // Search in the buffer.\n-        if (null == data) {\n-            synchronized (lock) {\n-                dataFromBuffer = bufferedTxnData.get(key);\n-            }\n-            // If we did not find in buffer then load it from store\n-            if (null == dataFromBuffer) {\n-                // NOTE: This call to read MUST be outside the lock, it is most likely cause re-entry.\n-                loadFromStore(key);\n-                dataFromBuffer = bufferedTxnData.get(key);\n-                Preconditions.checkState(null != dataFromBuffer);\n-            }\n+        // Prevent the key from getting evicted.\n+        addToActiveKeySet(key);\n+\n+        // Try to find it in buffer. Access buffer using reader lock.\n+        val readLock = scheduler.getReadLock(new String[]{key});\n+        return readLock.lock()\n+                .thenApplyAsync(v -> bufferedTxnData.get(key), executor)\n+                .thenApplyAsync(dataFromBuffer -> {\n+                    if (dataFromBuffer != null) {\n+                        // Make sure it is a deep copy.\n+                        val retValue = dataFromBuffer.getValue();\n+                        if (null != retValue) {\n+                            return retValue.deepCopy();\n+                        }\n+                        return null;\n+                    }\n+                    return null;\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> readLock.unlock(), executor)\n+                .thenComposeAsync(retValue -> {\n+                    if (retValue != null) {\n+                        return CompletableFuture.completedFuture(retValue);\n+                    }\n+                    // We did not find it in the buffer either.\n+                    // Try to find it in store.\n+                    return loadFromStore(key, false)\n+                            .thenApplyAsync(TransactionData::getValue, executor);\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> removeFromActiveKeySet(key), executor);\n+    }\n \n-            if (null != dataFromBuffer && null != dataFromBuffer.getValue()) {\n-                // Make copy.\n-                data = dataFromBuffer.toBuilder()\n-                        .key(key)\n-                        .value(dataFromBuffer.getValue().deepCopy())\n-                        .build();\n-                txnData.put(key, data);\n+    private void removeFromActiveKeySet(String key) {\n+        activeKeys.remove(key);\n+    }\n+\n+    private void addToActiveKeySet(String key) {\n+        // No need to synchronize if the eviction is not running.\n+        if (isEvictionRunning.get()) {\n+            synchronized (evictionLock) {\n+                activeKeys.add(key);\n             }\n+        } else {\n+            activeKeys.add(key);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 594}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc1NDg0Nw==", "bodyText": "activeKeys is concurrent multi hash - so addition/deleting is safe in this context.\nadded comment explaining why not having lock here is safe.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r506754847", "createdAt": "2020-10-16T22:57:06Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -299,110 +487,169 @@ public void abort(MetadataTransaction txn) throws StorageMetadataException {\n      * @param txn Transaction.\n      * @param key key to use to retrieve metadata.\n      * @return Metadata for given key. Null if key was not found.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public StorageMetadata get(MetadataTransaction txn, String key) throws StorageMetadataException {\n+    public CompletableFuture<StorageMetadata> get(MetadataTransaction txn, String key) {\n         Preconditions.checkArgument(null != txn);\n-        TransactionData dataFromBuffer = null;\n         if (null == key) {\n-            return null;\n+            return CompletableFuture.completedFuture(null);\n         }\n-        StorageMetadata retValue = null;\n \n         Map<String, TransactionData> txnData = txn.getData();\n+\n+        // Record is found in transaction data itself.\n         TransactionData data = txnData.get(key);\n+        if (null != data) {\n+            return CompletableFuture.completedFuture(data.getValue());\n+        }\n \n-        // Search in the buffer.\n-        if (null == data) {\n-            synchronized (lock) {\n-                dataFromBuffer = bufferedTxnData.get(key);\n-            }\n-            // If we did not find in buffer then load it from store\n-            if (null == dataFromBuffer) {\n-                // NOTE: This call to read MUST be outside the lock, it is most likely cause re-entry.\n-                loadFromStore(key);\n-                dataFromBuffer = bufferedTxnData.get(key);\n-                Preconditions.checkState(null != dataFromBuffer);\n-            }\n+        // Prevent the key from getting evicted.\n+        addToActiveKeySet(key);\n+\n+        // Try to find it in buffer. Access buffer using reader lock.\n+        val readLock = scheduler.getReadLock(new String[]{key});\n+        return readLock.lock()\n+                .thenApplyAsync(v -> bufferedTxnData.get(key), executor)\n+                .thenApplyAsync(dataFromBuffer -> {\n+                    if (dataFromBuffer != null) {\n+                        // Make sure it is a deep copy.\n+                        val retValue = dataFromBuffer.getValue();\n+                        if (null != retValue) {\n+                            return retValue.deepCopy();\n+                        }\n+                        return null;\n+                    }\n+                    return null;\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> readLock.unlock(), executor)\n+                .thenComposeAsync(retValue -> {\n+                    if (retValue != null) {\n+                        return CompletableFuture.completedFuture(retValue);\n+                    }\n+                    // We did not find it in the buffer either.\n+                    // Try to find it in store.\n+                    return loadFromStore(key, false)\n+                            .thenApplyAsync(TransactionData::getValue, executor);\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> removeFromActiveKeySet(key), executor);\n+    }\n \n-            if (null != dataFromBuffer && null != dataFromBuffer.getValue()) {\n-                // Make copy.\n-                data = dataFromBuffer.toBuilder()\n-                        .key(key)\n-                        .value(dataFromBuffer.getValue().deepCopy())\n-                        .build();\n-                txnData.put(key, data);\n+    private void removeFromActiveKeySet(String key) {\n+        activeKeys.remove(key);\n+    }\n+\n+    private void addToActiveKeySet(String key) {\n+        // No need to synchronize if the eviction is not running.\n+        if (isEvictionRunning.get()) {\n+            synchronized (evictionLock) {\n+                activeKeys.add(key);\n             }\n+        } else {\n+            activeKeys.add(key);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTkwMTQ1NA=="}, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 594}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMDAwMzQ0OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MultiKeyReaderWriterScheduler.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMjoxNzoxM1rOHcvosg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQyMjo0NzoxMlrOHjRsug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTkwMjY0Mg==", "bodyText": "Please use @GuardedBy on the fields that need synchronization on, then in your IDE settings, make sure you enable Concurrency warnings. The IDE will tell you every single invocation where you accessing this outside of declared guards.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r499902642", "createdAt": "2020-10-05T22:17:13Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MultiKeyReaderWriterScheduler.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import lombok.Getter;\n+import lombok.RequiredArgsConstructor;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.concurrent.CompletableFuture;\n+\n+/**\n+ * A scheduler utility that implements pattern similar to Multiple Readers - Single Writer pattern.\n+ */\n+public class MultiKeyReaderWriterScheduler {\n+    /**\n+     * Scheduler data for each key.\n+     */\n+    private final HashMap<String, SchedulerData> keyToDataMap = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc1MjE4Ng==", "bodyText": "done", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r506752186", "createdAt": "2020-10-16T22:47:12Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MultiKeyReaderWriterScheduler.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import lombok.Getter;\n+import lombok.RequiredArgsConstructor;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.concurrent.CompletableFuture;\n+\n+/**\n+ * A scheduler utility that implements pattern similar to Multiple Readers - Single Writer pattern.\n+ */\n+public class MultiKeyReaderWriterScheduler {\n+    /**\n+     * Scheduler data for each key.\n+     */\n+    private final HashMap<String, SchedulerData> keyToDataMap = new HashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTkwMjY0Mg=="}, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMDAwNDA4OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MultiKeyReaderWriterScheduler.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMjoxNzozM1rOHcvpEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQyMjo0NjozN1rOHjRsCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTkwMjczOQ==", "bodyText": "Why aren't any of these fields final?", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r499902739", "createdAt": "2020-10-05T22:17:33Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MultiKeyReaderWriterScheduler.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import lombok.Getter;\n+import lombok.RequiredArgsConstructor;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.concurrent.CompletableFuture;\n+\n+/**\n+ * A scheduler utility that implements pattern similar to Multiple Readers - Single Writer pattern.\n+ */\n+public class MultiKeyReaderWriterScheduler {\n+    /**\n+     * Scheduler data for each key.\n+     */\n+    private final HashMap<String, SchedulerData> keyToDataMap = new HashMap<>();\n+\n+    /**\n+     * Scheduler data for each key.\n+     */\n+    private static class SchedulerData {\n+        int count;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc1MjAwOQ==", "bodyText": "It is modified later.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r506752009", "createdAt": "2020-10-16T22:46:37Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MultiKeyReaderWriterScheduler.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import lombok.Getter;\n+import lombok.RequiredArgsConstructor;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.concurrent.CompletableFuture;\n+\n+/**\n+ * A scheduler utility that implements pattern similar to Multiple Readers - Single Writer pattern.\n+ */\n+public class MultiKeyReaderWriterScheduler {\n+    /**\n+     * Scheduler data for each key.\n+     */\n+    private final HashMap<String, SchedulerData> keyToDataMap = new HashMap<>();\n+\n+    /**\n+     * Scheduler data for each key.\n+     */\n+    private static class SchedulerData {\n+        int count;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTkwMjczOQ=="}, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMDAwODI5OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MultiKeyReaderWriterScheduler.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMjoxOToxN1rOHcvrfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNjoyNzo1MlrOHsL3zQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTkwMzM1OQ==", "bodyText": "Futures.allOf?", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r499903359", "createdAt": "2020-10-05T22:19:17Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MultiKeyReaderWriterScheduler.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import lombok.Getter;\n+import lombok.RequiredArgsConstructor;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.concurrent.CompletableFuture;\n+\n+/**\n+ * A scheduler utility that implements pattern similar to Multiple Readers - Single Writer pattern.\n+ */\n+public class MultiKeyReaderWriterScheduler {\n+    /**\n+     * Scheduler data for each key.\n+     */\n+    private final HashMap<String, SchedulerData> keyToDataMap = new HashMap<>();\n+\n+    /**\n+     * Scheduler data for each key.\n+     */\n+    private static class SchedulerData {\n+        int count;\n+        CompletableFuture blockingFuture = CompletableFuture.completedFuture(null);\n+        ArrayList<CompletableFuture> readerFutures = new ArrayList<>();\n+    }\n+\n+    /**\n+     * Represents a lock.\n+     */\n+    @RequiredArgsConstructor\n+    static class MultiKeyReaderWriterAsyncLock {\n+        /**\n+         * Keys to synchronize on.\n+         */\n+        @Getter\n+        private final String[] keys;\n+\n+        /**\n+         * Indicates whether the lock is a reader lock or a writer lock.\n+         */\n+        @Getter\n+        private final boolean isReadonly;\n+\n+        /**\n+         * Reference to the scheduler.\n+         */\n+        private final MultiKeyReaderWriterScheduler scheduler;\n+\n+        /**\n+         * The future is completed when all keys for this lock become available.\n+         */\n+        private CompletableFuture<Void> readyFuture;\n+\n+        /**\n+         * The future which is completed when lock is released.\n+         */\n+        @Getter\n+        private CompletableFuture<Void> doneFuture = new CompletableFuture<>();\n+\n+        CompletableFuture<Void> lock() {\n+            if (isReadonly) {\n+                return scheduler.scheduleForRead(this);\n+            } else {\n+                return scheduler.scheduleForWrite(this);\n+            }\n+        }\n+\n+        void unlock() {\n+            scheduler.release(this);\n+        }\n+    }\n+\n+    /**\n+     * Adds a reference for the key.\n+     */\n+    private SchedulerData addReference(String key) {\n+        SchedulerData schedulerData;\n+        synchronized (keyToDataMap) {\n+            schedulerData = keyToDataMap.get(key);\n+            // Add if this is a new key.\n+            if (null == schedulerData) {\n+                schedulerData = new SchedulerData();\n+                keyToDataMap.put(key, schedulerData);\n+            }\n+            // Increment ref count.\n+            schedulerData.count++;\n+        }\n+        return schedulerData;\n+    }\n+\n+    /**\n+     * Releases a reference for the key.\n+     */\n+    private void releaseReference(String key) {\n+        synchronized (keyToDataMap) {\n+            SchedulerData schedulerData = keyToDataMap.get(key);\n+            // Decrement ref count.\n+            schedulerData.count--;\n+            // clean up if required.\n+            if (0 == schedulerData.count) {\n+                keyToDataMap.remove(key);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Gets a read lock over given set of keys.\n+     */\n+    MultiKeyReaderWriterAsyncLock getReadLock(String[] keys) {\n+        return new MultiKeyReaderWriterAsyncLock(keys, true, this);\n+    }\n+\n+    /**\n+     * Gets a write lock over given set of keys.\n+     */\n+    MultiKeyReaderWriterAsyncLock getWriteLock(String[] keys) {\n+        return new MultiKeyReaderWriterAsyncLock(keys, false, this);\n+    }\n+\n+    /**\n+     * Schedules the lock for read.\n+     */\n+    private synchronized CompletableFuture<Void> scheduleForRead(MultiKeyReaderWriterAsyncLock lock) {\n+        CompletableFuture[] futuresToBlockOn = new CompletableFuture[lock.getKeys().length];\n+        for (int i = 0; i < lock.getKeys().length; i++) {\n+            val key = lock.getKeys()[i];\n+            SchedulerData schedulerData = addReference(key);\n+\n+            futuresToBlockOn[i] = schedulerData.blockingFuture;\n+            // Add this as reader\n+            schedulerData.readerFutures.add(lock.doneFuture);\n+        }\n+        lock.readyFuture = CompletableFuture.allOf(futuresToBlockOn);\n+        return lock.readyFuture;\n+    }\n+\n+    /**\n+     * Schedules the lock for write.\n+     */\n+    private synchronized CompletableFuture<Void> scheduleForWrite(MultiKeyReaderWriterAsyncLock lock) {\n+        CompletableFuture[] futuresToBlockOn = new CompletableFuture[lock.getKeys().length];\n+        for (int i = 0; i < lock.getKeys().length; i++) {\n+            val key = lock.getKeys()[i];\n+            // Get existing data.\n+            SchedulerData schedulerData = addReference(key);\n+\n+            // If there are outstanding readers then first \"drain\" all readers by making this write wait on them.\n+            if (schedulerData.readerFutures.size() > 0) {\n+                CompletableFuture[] readFutures = schedulerData.readerFutures.toArray(new CompletableFuture[schedulerData.readerFutures.size()]);\n+                schedulerData.blockingFuture = CompletableFuture.allOf(readFutures);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjA5MzkwMQ==", "bodyText": "done.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r516093901", "createdAt": "2020-11-02T16:27:52Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MultiKeyReaderWriterScheduler.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import lombok.Getter;\n+import lombok.RequiredArgsConstructor;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.concurrent.CompletableFuture;\n+\n+/**\n+ * A scheduler utility that implements pattern similar to Multiple Readers - Single Writer pattern.\n+ */\n+public class MultiKeyReaderWriterScheduler {\n+    /**\n+     * Scheduler data for each key.\n+     */\n+    private final HashMap<String, SchedulerData> keyToDataMap = new HashMap<>();\n+\n+    /**\n+     * Scheduler data for each key.\n+     */\n+    private static class SchedulerData {\n+        int count;\n+        CompletableFuture blockingFuture = CompletableFuture.completedFuture(null);\n+        ArrayList<CompletableFuture> readerFutures = new ArrayList<>();\n+    }\n+\n+    /**\n+     * Represents a lock.\n+     */\n+    @RequiredArgsConstructor\n+    static class MultiKeyReaderWriterAsyncLock {\n+        /**\n+         * Keys to synchronize on.\n+         */\n+        @Getter\n+        private final String[] keys;\n+\n+        /**\n+         * Indicates whether the lock is a reader lock or a writer lock.\n+         */\n+        @Getter\n+        private final boolean isReadonly;\n+\n+        /**\n+         * Reference to the scheduler.\n+         */\n+        private final MultiKeyReaderWriterScheduler scheduler;\n+\n+        /**\n+         * The future is completed when all keys for this lock become available.\n+         */\n+        private CompletableFuture<Void> readyFuture;\n+\n+        /**\n+         * The future which is completed when lock is released.\n+         */\n+        @Getter\n+        private CompletableFuture<Void> doneFuture = new CompletableFuture<>();\n+\n+        CompletableFuture<Void> lock() {\n+            if (isReadonly) {\n+                return scheduler.scheduleForRead(this);\n+            } else {\n+                return scheduler.scheduleForWrite(this);\n+            }\n+        }\n+\n+        void unlock() {\n+            scheduler.release(this);\n+        }\n+    }\n+\n+    /**\n+     * Adds a reference for the key.\n+     */\n+    private SchedulerData addReference(String key) {\n+        SchedulerData schedulerData;\n+        synchronized (keyToDataMap) {\n+            schedulerData = keyToDataMap.get(key);\n+            // Add if this is a new key.\n+            if (null == schedulerData) {\n+                schedulerData = new SchedulerData();\n+                keyToDataMap.put(key, schedulerData);\n+            }\n+            // Increment ref count.\n+            schedulerData.count++;\n+        }\n+        return schedulerData;\n+    }\n+\n+    /**\n+     * Releases a reference for the key.\n+     */\n+    private void releaseReference(String key) {\n+        synchronized (keyToDataMap) {\n+            SchedulerData schedulerData = keyToDataMap.get(key);\n+            // Decrement ref count.\n+            schedulerData.count--;\n+            // clean up if required.\n+            if (0 == schedulerData.count) {\n+                keyToDataMap.remove(key);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Gets a read lock over given set of keys.\n+     */\n+    MultiKeyReaderWriterAsyncLock getReadLock(String[] keys) {\n+        return new MultiKeyReaderWriterAsyncLock(keys, true, this);\n+    }\n+\n+    /**\n+     * Gets a write lock over given set of keys.\n+     */\n+    MultiKeyReaderWriterAsyncLock getWriteLock(String[] keys) {\n+        return new MultiKeyReaderWriterAsyncLock(keys, false, this);\n+    }\n+\n+    /**\n+     * Schedules the lock for read.\n+     */\n+    private synchronized CompletableFuture<Void> scheduleForRead(MultiKeyReaderWriterAsyncLock lock) {\n+        CompletableFuture[] futuresToBlockOn = new CompletableFuture[lock.getKeys().length];\n+        for (int i = 0; i < lock.getKeys().length; i++) {\n+            val key = lock.getKeys()[i];\n+            SchedulerData schedulerData = addReference(key);\n+\n+            futuresToBlockOn[i] = schedulerData.blockingFuture;\n+            // Add this as reader\n+            schedulerData.readerFutures.add(lock.doneFuture);\n+        }\n+        lock.readyFuture = CompletableFuture.allOf(futuresToBlockOn);\n+        return lock.readyFuture;\n+    }\n+\n+    /**\n+     * Schedules the lock for write.\n+     */\n+    private synchronized CompletableFuture<Void> scheduleForWrite(MultiKeyReaderWriterAsyncLock lock) {\n+        CompletableFuture[] futuresToBlockOn = new CompletableFuture[lock.getKeys().length];\n+        for (int i = 0; i < lock.getKeys().length; i++) {\n+            val key = lock.getKeys()[i];\n+            // Get existing data.\n+            SchedulerData schedulerData = addReference(key);\n+\n+            // If there are outstanding readers then first \"drain\" all readers by making this write wait on them.\n+            if (schedulerData.readerFutures.size() > 0) {\n+                CompletableFuture[] readFutures = schedulerData.readerFutures.toArray(new CompletableFuture[schedulerData.readerFutures.size()]);\n+                schedulerData.blockingFuture = CompletableFuture.allOf(readFutures);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTkwMzM1OQ=="}, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 161}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMDAwOTA4OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MultiKeyReaderWriterScheduler.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMjoxOTo0MFrOHcvsDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQyMzowMjowOFrOHjR7cA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTkwMzUwMQ==", "bodyText": "final", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r499903501", "createdAt": "2020-10-05T22:19:40Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MultiKeyReaderWriterScheduler.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import lombok.Getter;\n+import lombok.RequiredArgsConstructor;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.concurrent.CompletableFuture;\n+\n+/**\n+ * A scheduler utility that implements pattern similar to Multiple Readers - Single Writer pattern.\n+ */\n+public class MultiKeyReaderWriterScheduler {\n+    /**\n+     * Scheduler data for each key.\n+     */\n+    private final HashMap<String, SchedulerData> keyToDataMap = new HashMap<>();\n+\n+    /**\n+     * Scheduler data for each key.\n+     */\n+    private static class SchedulerData {\n+        int count;\n+        CompletableFuture blockingFuture = CompletableFuture.completedFuture(null);\n+        ArrayList<CompletableFuture> readerFutures = new ArrayList<>();\n+    }\n+\n+    /**\n+     * Represents a lock.\n+     */\n+    @RequiredArgsConstructor\n+    static class MultiKeyReaderWriterAsyncLock {\n+        /**\n+         * Keys to synchronize on.\n+         */\n+        @Getter\n+        private final String[] keys;\n+\n+        /**\n+         * Indicates whether the lock is a reader lock or a writer lock.\n+         */\n+        @Getter\n+        private final boolean isReadonly;\n+\n+        /**\n+         * Reference to the scheduler.\n+         */\n+        private final MultiKeyReaderWriterScheduler scheduler;\n+\n+        /**\n+         * The future is completed when all keys for this lock become available.\n+         */\n+        private CompletableFuture<Void> readyFuture;\n+\n+        /**\n+         * The future which is completed when lock is released.\n+         */\n+        @Getter\n+        private CompletableFuture<Void> doneFuture = new CompletableFuture<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 69}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc1NTk1Mg==", "bodyText": "done.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r506755952", "createdAt": "2020-10-16T23:02:08Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MultiKeyReaderWriterScheduler.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import lombok.Getter;\n+import lombok.RequiredArgsConstructor;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.concurrent.CompletableFuture;\n+\n+/**\n+ * A scheduler utility that implements pattern similar to Multiple Readers - Single Writer pattern.\n+ */\n+public class MultiKeyReaderWriterScheduler {\n+    /**\n+     * Scheduler data for each key.\n+     */\n+    private final HashMap<String, SchedulerData> keyToDataMap = new HashMap<>();\n+\n+    /**\n+     * Scheduler data for each key.\n+     */\n+    private static class SchedulerData {\n+        int count;\n+        CompletableFuture blockingFuture = CompletableFuture.completedFuture(null);\n+        ArrayList<CompletableFuture> readerFutures = new ArrayList<>();\n+    }\n+\n+    /**\n+     * Represents a lock.\n+     */\n+    @RequiredArgsConstructor\n+    static class MultiKeyReaderWriterAsyncLock {\n+        /**\n+         * Keys to synchronize on.\n+         */\n+        @Getter\n+        private final String[] keys;\n+\n+        /**\n+         * Indicates whether the lock is a reader lock or a writer lock.\n+         */\n+        @Getter\n+        private final boolean isReadonly;\n+\n+        /**\n+         * Reference to the scheduler.\n+         */\n+        private final MultiKeyReaderWriterScheduler scheduler;\n+\n+        /**\n+         * The future is completed when all keys for this lock become available.\n+         */\n+        private CompletableFuture<Void> readyFuture;\n+\n+        /**\n+         * The future which is completed when lock is released.\n+         */\n+        @Getter\n+        private CompletableFuture<Void> doneFuture = new CompletableFuture<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTkwMzUwMQ=="}, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 69}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzEzMDAxMjY5OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MultiKeyReaderWriterScheduler.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wNVQyMjoyMTowOVrOHcvuLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQyMzowMDo0M1rOHjR6HA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTkwNDA0NQ==", "bodyText": "This is a very complex class with a lot of things that can go wrong, so it will be a likely candidate for tasks to \"hang indefinitely\". Is there a reason why:\n\nThis is needed?\nA reader-writer lock is needed? Can MultiKeyAsyncSequentialProcessor be used instead (it also serializes based on arbitrary string keys).", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r499904045", "createdAt": "2020-10-05T22:21:09Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MultiKeyReaderWriterScheduler.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import lombok.Getter;\n+import lombok.RequiredArgsConstructor;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.concurrent.CompletableFuture;\n+\n+/**\n+ * A scheduler utility that implements pattern similar to Multiple Readers - Single Writer pattern.\n+ */\n+public class MultiKeyReaderWriterScheduler {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc1NTYxMg==", "bodyText": "This data structure is used to control concurrent access to the SLTS metadata cache. We need to be able to have multiple reader/single writer access to the metadata in order to avoid deadlocks that we saw during 0.8 testing.\nThe use case is bit different", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r506755612", "createdAt": "2020-10-16T23:00:43Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MultiKeyReaderWriterScheduler.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import lombok.Getter;\n+import lombok.RequiredArgsConstructor;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.concurrent.CompletableFuture;\n+\n+/**\n+ * A scheduler utility that implements pattern similar to Multiple Readers - Single Writer pattern.\n+ */\n+public class MultiKeyReaderWriterScheduler {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5OTkwNDA0NQ=="}, "originalCommit": {"oid": "611a9cd6bf2580818c99c864184735b682439da8"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE0ODMwNTQwOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/TableBasedMetadataStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMFQwNDoxMDo1MVrOHfdAfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMVQxNToyMTo1MVrOHfoYRA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc0MzE2Nw==", "bodyText": "I wonder why the initial if statement is false. The first thread that gets to create the table will set it to true after that all other calls of this method shouldn't get inside the if block but we saw otherwise and Table segment... message was kept logging even after 2 hours of creation.\nMy initial thought was that isTableInitialized doesn't have volatile keyword but it's of AtomicBoolean type so that shouldn't be required.\nStill curious to know why the logic doesn't work without such explicit set to true in exceptionally block", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r502743167", "createdAt": "2020-10-10T04:10:51Z", "author": {"login": "medvedevigorek"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/TableBasedMetadataStore.java", "diffHunk": "@@ -167,25 +181,32 @@ private StorageMetadataException handleException(Throwable e) throws StorageMeta\n         return new StorageMetadataException(\"Transaction failed\", e);\n     }\n \n-    private void ensureInitialized() {\n+    private CompletableFuture<Void> ensureInitialized() {\n         if (!isTableInitialized.get()) {\n-            try {\n-                this.tableStore.createSegment(tableName, timeout).join();\n-                log.info(\"Created table segment {}\", tableName);\n-            } catch (CompletionException e) {\n-                if (e.getCause() instanceof StreamSegmentExistsException) {\n-                    log.info(\"Table segment {} already exists.\", tableName);\n-                }\n-            }\n-            isTableInitialized.set(true);\n+            return this.tableStore.createSegment(tableName, timeout)\n+                    .thenRunAsync(() -> {\n+                        log.info(\"Created table segment {}\", tableName);\n+                        isTableInitialized.set(true);\n+                    }, getExecutor())\n+                    .exceptionally(e -> {\n+                        val ex = Exceptions.unwrap(e);\n+                        if (e.getCause() instanceof StreamSegmentExistsException) {\n+                            log.info(\"Table segment {} already exists.\", tableName);\n+                            isTableInitialized.set(true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": null, "originalPosition": 229}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjkyOTQ3Ng==", "bodyText": "On a freshly installed system there won't exist any storage metadata segment. Once this table segment is created, then from that point onwards we never recreate it - only use existing one.\nWhen a container is restarted (eg. during failover), then Storage and its TableBasedMetadataStore instances are recreated every time but the persistent  storage metadata segment already exists.\nThere is no way for code to know whether it is running for the very first time or after restart - other than to actually check the existence of table segment.  Now, either way this check needs to happen only once for current incarnation of the container instance.  isTableInitialized tracks that.\nStreamSegmentExistsException also means that table is confirmed to be already created.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r502929476", "createdAt": "2020-10-11T15:21:51Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/TableBasedMetadataStore.java", "diffHunk": "@@ -167,25 +181,32 @@ private StorageMetadataException handleException(Throwable e) throws StorageMeta\n         return new StorageMetadataException(\"Transaction failed\", e);\n     }\n \n-    private void ensureInitialized() {\n+    private CompletableFuture<Void> ensureInitialized() {\n         if (!isTableInitialized.get()) {\n-            try {\n-                this.tableStore.createSegment(tableName, timeout).join();\n-                log.info(\"Created table segment {}\", tableName);\n-            } catch (CompletionException e) {\n-                if (e.getCause() instanceof StreamSegmentExistsException) {\n-                    log.info(\"Table segment {} already exists.\", tableName);\n-                }\n-            }\n-            isTableInitialized.set(true);\n+            return this.tableStore.createSegment(tableName, timeout)\n+                    .thenRunAsync(() -> {\n+                        log.info(\"Created table segment {}\", tableName);\n+                        isTableInitialized.set(true);\n+                    }, getExecutor())\n+                    .exceptionally(e -> {\n+                        val ex = Exceptions.unwrap(e);\n+                        if (e.getCause() instanceof StreamSegmentExistsException) {\n+                            log.info(\"Table segment {} already exists.\", tableName);\n+                            isTableInitialized.set(true);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjc0MzE2Nw=="}, "originalCommit": null, "originalPosition": 229}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyNzgyNTczOnYy", "diffSide": "RIGHT", "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNDo1NToxNlrOHrSzlg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMVQwMToyODozNVrOHrj7_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE1ODkzNA==", "bodyText": "This is not an atomic operation. Can we change the putObject below to incorporate this? You may need to build a PutRequest object.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r515158934", "createdAt": "2020-10-30T14:55:16Z", "author": {"login": "andreipaduroiu"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -133,7 +136,10 @@ protected int doWrite(ChunkHandle handle, long offset, int length, InputStream d\n         try {\n             val objectPath = getObjectPath(handle.getChunkName());\n             // Check object exists.\n-            client.getObjectMetadata(config.getBucket(), objectPath);\n+            val metadata = client.getObjectMetadata(config.getBucket(), objectPath);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTQzOTYxNA==", "bodyText": "There is no need to fence as only this instance should be accessing this chunk\nThere is pending work #4967 which will be done later.\nIn general converting this code to \"optimistic\" version will be done later.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r515439614", "createdAt": "2020-10-31T01:28:35Z", "author": {"login": "sachin-j-joshi"}, "path": "bindings/src/main/java/io/pravega/storage/extendeds3/ExtendedS3ChunkStorage.java", "diffHunk": "@@ -133,7 +136,10 @@ protected int doWrite(ChunkHandle handle, long offset, int length, InputStream d\n         try {\n             val objectPath = getObjectPath(handle.getChunkName());\n             // Check object exists.\n-            client.getObjectMetadata(config.getBucket(), objectPath);\n+            val metadata = client.getObjectMetadata(config.getBucket(), objectPath);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE1ODkzNA=="}, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyNzg1MDE2OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/StreamSegmentContainer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNTowMDo1OFrOHrTC6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNjoxMDoyMlrOHsLHzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE2Mjg1Nw==", "bodyText": "Let's refactor this so we only pass the TableMetadataStore via this class' constructor.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r515162857", "createdAt": "2020-10-30T15:00:58Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/StreamSegmentContainer.java", "diffHunk": "@@ -195,11 +196,16 @@ private void initializeStorage() throws Exception {\n             ContainerTableExtension tableExtension = getExtension(ContainerTableExtension.class);\n             String s = NameUtils.getStorageMetadataSegmentName(this.metadata.getContainerId());\n \n-            val metadata = new TableBasedMetadataStore(s, tableExtension);\n+            val metadataStore = new TableBasedMetadataStore(s, tableExtension, chunkedStorage.getExecutor());\n \n             // Bootstrap\n-            chunkedStorage.bootstrap(this.metadata.getContainerId(), metadata);\n+            return chunkedStorage.bootstrap(this.metadata.getContainerId(), metadataStore)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 19}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjA4MTYxMw==", "bodyText": "done", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r516081613", "createdAt": "2020-11-02T16:10:22Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/StreamSegmentContainer.java", "diffHunk": "@@ -195,11 +196,16 @@ private void initializeStorage() throws Exception {\n             ContainerTableExtension tableExtension = getExtension(ContainerTableExtension.class);\n             String s = NameUtils.getStorageMetadataSegmentName(this.metadata.getContainerId());\n \n-            val metadata = new TableBasedMetadataStore(s, tableExtension);\n+            val metadataStore = new TableBasedMetadataStore(s, tableExtension, chunkedStorage.getExecutor());\n \n             // Bootstrap\n-            chunkedStorage.bootstrap(this.metadata.getContainerId(), metadata);\n+            return chunkedStorage.bootstrap(this.metadata.getContainerId(), metadataStore)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE2Mjg1Nw=="}, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 19}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyNzg1NDI1OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkIterator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNTowMTo1NVrOHrTFcg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNzoyNjo1MlrOHsOjqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE2MzUwNg==", "bodyText": "thenAcceptAsync. That way you can get rid of that return null at the end.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r515163506", "createdAt": "2020-10-30T15:01:55Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkIterator.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.function.BiConsumer;\n+\n+/**\n+ * Helper class for iterating over list of chunks.\n+ */\n+class ChunkIterator {\n+    private final ChunkedSegmentStorage chunkedSegmentStorage;\n+    private final MetadataTransaction txn;\n+    private volatile String currentChunkName;\n+    private volatile ChunkMetadata currentMetadata;\n+\n+    ChunkIterator(ChunkedSegmentStorage chunkedSegmentStorage, MetadataTransaction txn, SegmentMetadata segmentMetadata) {\n+        this.chunkedSegmentStorage = chunkedSegmentStorage;\n+        this.txn = txn;\n+        currentChunkName = segmentMetadata.getFirstChunk();\n+    }\n+\n+    public CompletableFuture<Void> forEach(BiConsumer<ChunkMetadata, String> consumer) {\n+        return Futures.loop(\n+                () -> currentChunkName != null,\n+                () -> txn.get(currentChunkName)\n+                        .thenApplyAsync(storageMetadata -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjEzNzg5OA==", "bodyText": "done.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r516137898", "createdAt": "2020-11-02T17:26:52Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkIterator.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.function.BiConsumer;\n+\n+/**\n+ * Helper class for iterating over list of chunks.\n+ */\n+class ChunkIterator {\n+    private final ChunkedSegmentStorage chunkedSegmentStorage;\n+    private final MetadataTransaction txn;\n+    private volatile String currentChunkName;\n+    private volatile ChunkMetadata currentMetadata;\n+\n+    ChunkIterator(ChunkedSegmentStorage chunkedSegmentStorage, MetadataTransaction txn, SegmentMetadata segmentMetadata) {\n+        this.chunkedSegmentStorage = chunkedSegmentStorage;\n+        this.txn = txn;\n+        currentChunkName = segmentMetadata.getFirstChunk();\n+    }\n+\n+    public CompletableFuture<Void> forEach(BiConsumer<ChunkMetadata, String> consumer) {\n+        return Futures.loop(\n+                () -> currentChunkName != null,\n+                () -> txn.get(currentChunkName)\n+                        .thenApplyAsync(storageMetadata -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE2MzUwNg=="}, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyNzg2MjMwOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkIterator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNTowNDowM1rOHrTKxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNjozMTowNFrOHsMA7A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE2NDg2OA==", "bodyText": "BTW, you're already executing on this executor and your callback is very insignificant in terms of CPU requirements. In this case, it is advisable to not use Async (i.e., thenAccept is enough) which will force this callback on the same thread the previous call was working, thus saving a thread context change.\nFYI: For other scenarios, please be very careful when deciding whether or not to do this. A very valid scenario where you do want to keep the Async variant is when you want to switch executors. However in this particular case, we do not need that.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r515164868", "createdAt": "2020-10-30T15:04:03Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkIterator.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.function.BiConsumer;\n+\n+/**\n+ * Helper class for iterating over list of chunks.\n+ */\n+class ChunkIterator {\n+    private final ChunkedSegmentStorage chunkedSegmentStorage;\n+    private final MetadataTransaction txn;\n+    private volatile String currentChunkName;\n+    private volatile ChunkMetadata currentMetadata;\n+\n+    ChunkIterator(ChunkedSegmentStorage chunkedSegmentStorage, MetadataTransaction txn, SegmentMetadata segmentMetadata) {\n+        this.chunkedSegmentStorage = chunkedSegmentStorage;\n+        this.txn = txn;\n+        currentChunkName = segmentMetadata.getFirstChunk();\n+    }\n+\n+    public CompletableFuture<Void> forEach(BiConsumer<ChunkMetadata, String> consumer) {\n+        return Futures.loop(\n+                () -> currentChunkName != null,\n+                () -> txn.get(currentChunkName)\n+                        .thenApplyAsync(storageMetadata -> {\n+                            currentMetadata = (ChunkMetadata) storageMetadata;\n+                            consumer.accept(currentMetadata, currentChunkName);\n+                            // Move next\n+                            currentChunkName = currentMetadata.getNextChunk();\n+                            return null;\n+                        }, chunkedSegmentStorage.getExecutor()),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 45}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjA5NjIzNg==", "bodyText": "I think The reason for not using non-async overload is that they can end up running on some Fork Join pool or caller pool.\nBy providing the same executor we are sure it will only run on given thread pool, which is what we want.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r516096236", "createdAt": "2020-11-02T16:31:04Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkIterator.java", "diffHunk": "@@ -0,0 +1,48 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+\n+import java.util.concurrent.CompletableFuture;\n+import java.util.function.BiConsumer;\n+\n+/**\n+ * Helper class for iterating over list of chunks.\n+ */\n+class ChunkIterator {\n+    private final ChunkedSegmentStorage chunkedSegmentStorage;\n+    private final MetadataTransaction txn;\n+    private volatile String currentChunkName;\n+    private volatile ChunkMetadata currentMetadata;\n+\n+    ChunkIterator(ChunkedSegmentStorage chunkedSegmentStorage, MetadataTransaction txn, SegmentMetadata segmentMetadata) {\n+        this.chunkedSegmentStorage = chunkedSegmentStorage;\n+        this.txn = txn;\n+        currentChunkName = segmentMetadata.getFirstChunk();\n+    }\n+\n+    public CompletableFuture<Void> forEach(BiConsumer<ChunkMetadata, String> consumer) {\n+        return Futures.loop(\n+                () -> currentChunkName != null,\n+                () -> txn.get(currentChunkName)\n+                        .thenApplyAsync(storageMetadata -> {\n+                            currentMetadata = (ChunkMetadata) storageMetadata;\n+                            consumer.accept(currentMetadata, currentChunkName);\n+                            // Move next\n+                            currentChunkName = currentMetadata.getNextChunk();\n+                            return null;\n+                        }, chunkedSegmentStorage.getExecutor()),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE2NDg2OA=="}, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 45}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyNzg3MDAxOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorage.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNTowNTo1OFrOHrTPqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwMDozNTozNFrOHtFRLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE2NjEyMg==", "bodyText": "This doesn't throw a CompletionException. In either case, the CompletionException is just a wrapper for some other exception, so I wouldn't bother documenting it here.\nPlease fix the Javadoc throughout this class to reflect that you are returning CompletableFutures and not plain old booleans. In the @return tag, you can mention any notable exceptions that may be thrown within the future itself (search for \"Notable Exceptions\" within the codebase for examples).", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r515166122", "createdAt": "2020-10-30T15:05:58Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorage.java", "diffHunk": "@@ -72,56 +74,62 @@\n      *\n      * @param chunkName Name of the storage object to check.\n      * @return True if the object exists, false otherwise.\n-     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws CompletionException If the operation failed, it will be completed with the appropriate exception. Notable Exceptions:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAzNDI4NQ==", "bodyText": "Some of these methods throw other exceptions and while it declare in CompletionException signature, putting only that one in return seems  odd. Putting it in a throws is not incorrect either.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r517034285", "createdAt": "2020-11-04T00:35:34Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorage.java", "diffHunk": "@@ -72,56 +74,62 @@\n      *\n      * @param chunkName Name of the storage object to check.\n      * @return True if the object exists, false otherwise.\n-     * @throws ChunkStorageException Throws ChunkStorageException in case of I/O related exceptions.\n+     * @throws CompletionException If the operation failed, it will be completed with the appropriate exception. Notable Exceptions:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE2NjEyMg=="}, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyNzg3NjM3OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageMetrics.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNTowNzo0MlrOHrTTzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNjoyNToxNFrOHrWlhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE2NzE4Mg==", "bodyText": "Why do we duplicate metrics? Can't we just reuse the same ones as before? All except one are the same.\nIf you duplicate them, you'll introduce debt that has to be fixed later. Plus anyone who has a metrics dashboard UI already set up will not be able to see these without some work there too.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r515167182", "createdAt": "2020-10-30T15:07:42Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageMetrics.java", "diffHunk": "@@ -27,6 +27,14 @@\n     static final OpStatsLogger DELETE_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_DELETE_LATENCY);\n     static final OpStatsLogger CONCAT_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_CONCAT_LATENCY);\n \n+    static final OpStatsLogger SLTS_READ_LATENCY = STATS_LOGGER.createStats(MetricsNames.SLTS_READ_LATENCY);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIyMDg3MQ==", "bodyText": "These are two different metrics\n\nSLTS_READ_LATENCY - The end to end overall time Storage call takes (inclusive of all possible multiple calls to table segment + chunkstorage)\nREAD_LATENCY - The overall time , the individual chunk storage call takes.\n\nWe did not have this second set of metrics before. I find this second set of metrics are independent and very useful.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r515220871", "createdAt": "2020-10-30T16:25:14Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkStorageMetrics.java", "diffHunk": "@@ -27,6 +27,14 @@\n     static final OpStatsLogger DELETE_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_DELETE_LATENCY);\n     static final OpStatsLogger CONCAT_LATENCY = STATS_LOGGER.createStats(MetricsNames.STORAGE_CONCAT_LATENCY);\n \n+    static final OpStatsLogger SLTS_READ_LATENCY = STATS_LOGGER.createStats(MetricsNames.SLTS_READ_LATENCY);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE2NzE4Mg=="}, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyNzg3OTU5OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorageConfig.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNTowODozNFrOHrTV6g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjozMDoyOFrOHwEMog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE2NzcyMg==", "bodyText": "It would be nice to document what lazy and defrag mean. If already documented somewhere else, a Javadoc link would suffice too.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r515167722", "createdAt": "2020-10-30T15:08:34Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorageConfig.java", "diffHunk": "@@ -100,13 +108,36 @@\n     @Getter\n     final private boolean appendEnabled;\n \n+    /**\n+     * Whether the lazy commit functionality is enabled or disabled.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ1OTg1OQ==", "bodyText": "Ping!", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r518459859", "createdAt": "2020-11-06T00:53:51Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorageConfig.java", "diffHunk": "@@ -100,13 +108,36 @@\n     @Getter\n     final private boolean appendEnabled;\n \n+    /**\n+     * Whether the lazy commit functionality is enabled or disabled.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE2NzcyMg=="}, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MjQ2Ng==", "bodyText": "Fixed", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520162466", "createdAt": "2020-11-09T22:30:28Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorageConfig.java", "diffHunk": "@@ -100,13 +108,36 @@\n     @Getter\n     final private boolean appendEnabled;\n \n+    /**\n+     * Whether the lazy commit functionality is enabled or disabled.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE2NzcyMg=="}, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyNzg4NzM5OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ConcatOperation.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNToxMDoxOFrOHrTaog==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNjozMTozMVrOHsMCWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE2ODkzMA==", "bodyText": "You do not need this. You can always get it from the target handle.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r515168930", "createdAt": "2020-10-30T15:10:18Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ConcatOperation.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.CompletionStage;\n+\n+import static io.pravega.segmentstore.storage.chunklayer.ChunkStorageMetrics.SLTS_CONCAT_COUNT;\n+import static io.pravega.segmentstore.storage.chunklayer.ChunkStorageMetrics.SLTS_CONCAT_LATENCY;\n+\n+/**\n+ * Implements the concat operation.\n+ */\n+@Slf4j\n+class ConcatOperation implements Callable<CompletableFuture<Void>> {\n+    private final long traceId;\n+    private final SegmentHandle targetHandle;\n+    private final long offset;\n+    private final String sourceSegment;\n+    private final ChunkedSegmentStorage chunkedSegmentStorage;\n+    private final ArrayList<String> chunksToDelete = new ArrayList<>();\n+\n+    private volatile Timer timer;\n+    private volatile String targetSegmentName;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjA5NjYwMg==", "bodyText": "fixed.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r516096602", "createdAt": "2020-11-02T16:31:31Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ConcatOperation.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.CompletionStage;\n+\n+import static io.pravega.segmentstore.storage.chunklayer.ChunkStorageMetrics.SLTS_CONCAT_COUNT;\n+import static io.pravega.segmentstore.storage.chunklayer.ChunkStorageMetrics.SLTS_CONCAT_LATENCY;\n+\n+/**\n+ * Implements the concat operation.\n+ */\n+@Slf4j\n+class ConcatOperation implements Callable<CompletableFuture<Void>> {\n+    private final long traceId;\n+    private final SegmentHandle targetHandle;\n+    private final long offset;\n+    private final String sourceSegment;\n+    private final ChunkedSegmentStorage chunkedSegmentStorage;\n+    private final ArrayList<String> chunksToDelete = new ArrayList<>();\n+\n+    private volatile Timer timer;\n+    private volatile String targetSegmentName;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE2ODkzMA=="}, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyNzg5MTMzOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ConcatOperation.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNToxMToxM1rOHrTdDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwMDozMjo0M1rOHtFOSg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE2OTU0OQ==", "bodyText": "These look like they can be transformed to local variables which you can pass along as needed.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r515169549", "createdAt": "2020-10-30T15:11:13Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ConcatOperation.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.CompletionStage;\n+\n+import static io.pravega.segmentstore.storage.chunklayer.ChunkStorageMetrics.SLTS_CONCAT_COUNT;\n+import static io.pravega.segmentstore.storage.chunklayer.ChunkStorageMetrics.SLTS_CONCAT_LATENCY;\n+\n+/**\n+ * Implements the concat operation.\n+ */\n+@Slf4j\n+class ConcatOperation implements Callable<CompletableFuture<Void>> {\n+    private final long traceId;\n+    private final SegmentHandle targetHandle;\n+    private final long offset;\n+    private final String sourceSegment;\n+    private final ChunkedSegmentStorage chunkedSegmentStorage;\n+    private final ArrayList<String> chunksToDelete = new ArrayList<>();\n+\n+    private volatile Timer timer;\n+    private volatile String targetSegmentName;\n+    private volatile SegmentMetadata targetSegmentMetadata;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAzMzU0Ng==", "bodyText": "This should not matter as much, I used operation class so that I don't have to pass lots of parameters around. passing parameters becomes troublesome specially when multiple values need to be returned. These field are modified and then used in next step.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r517033546", "createdAt": "2020-11-04T00:32:43Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ConcatOperation.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.CompletionStage;\n+\n+import static io.pravega.segmentstore.storage.chunklayer.ChunkStorageMetrics.SLTS_CONCAT_COUNT;\n+import static io.pravega.segmentstore.storage.chunklayer.ChunkStorageMetrics.SLTS_CONCAT_LATENCY;\n+\n+/**\n+ * Implements the concat operation.\n+ */\n+@Slf4j\n+class ConcatOperation implements Callable<CompletableFuture<Void>> {\n+    private final long traceId;\n+    private final SegmentHandle targetHandle;\n+    private final long offset;\n+    private final String sourceSegment;\n+    private final ChunkedSegmentStorage chunkedSegmentStorage;\n+    private final ArrayList<String> chunksToDelete = new ArrayList<>();\n+\n+    private volatile Timer timer;\n+    private volatile String targetSegmentName;\n+    private volatile SegmentMetadata targetSegmentMetadata;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE2OTU0OQ=="}, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyNzg5NzE0OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ConcatOperation.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNToxMjo0MFrOHrTgxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNjozMzoxN1rOHsMHqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE3MDUwMA==", "bodyText": "accept", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r515170500", "createdAt": "2020-10-30T15:12:40Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ConcatOperation.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.CompletionStage;\n+\n+import static io.pravega.segmentstore.storage.chunklayer.ChunkStorageMetrics.SLTS_CONCAT_COUNT;\n+import static io.pravega.segmentstore.storage.chunklayer.ChunkStorageMetrics.SLTS_CONCAT_LATENCY;\n+\n+/**\n+ * Implements the concat operation.\n+ */\n+@Slf4j\n+class ConcatOperation implements Callable<CompletableFuture<Void>> {\n+    private final long traceId;\n+    private final SegmentHandle targetHandle;\n+    private final long offset;\n+    private final String sourceSegment;\n+    private final ChunkedSegmentStorage chunkedSegmentStorage;\n+    private final ArrayList<String> chunksToDelete = new ArrayList<>();\n+\n+    private volatile Timer timer;\n+    private volatile String targetSegmentName;\n+    private volatile SegmentMetadata targetSegmentMetadata;\n+    private volatile SegmentMetadata sourceSegmentMetadata;\n+    private volatile ChunkMetadata targetLastChunk;\n+    private volatile ChunkMetadata sourceFirstChunk;\n+\n+    ConcatOperation(ChunkedSegmentStorage chunkedSegmentStorage, SegmentHandle targetHandle, long offset, String sourceSegment) {\n+        this.targetHandle = targetHandle;\n+        this.offset = offset;\n+        this.sourceSegment = sourceSegment;\n+        this.chunkedSegmentStorage = chunkedSegmentStorage;\n+        traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+    }\n+\n+    public CompletableFuture<Void> call() {\n+        timer = new Timer();\n+        checkPreconditions();\n+        log.debug(\"{} concat - started op={}, target={}, source={}, offset={}.\",\n+                chunkedSegmentStorage.getLogPrefix(), System.identityHashCode(this), targetHandle.getSegmentName(), sourceSegment, offset);\n+\n+        targetSegmentName = targetHandle.getSegmentName();\n+\n+        return ChunkedSegmentStorage.tryWith(chunkedSegmentStorage.getMetadataStore().beginTransaction(targetHandle.getSegmentName(), sourceSegment),\n+                txn -> txn.get(targetSegmentName)\n+                        .thenComposeAsync(storageMetadata1 -> {\n+                            targetSegmentMetadata = (SegmentMetadata) storageMetadata1;\n+                            return txn.get(sourceSegment)\n+                                    .thenComposeAsync(storageMetadata2 -> {\n+                                        sourceSegmentMetadata = (SegmentMetadata) storageMetadata2;\n+                                        return performConcat(txn);\n+                                    }, chunkedSegmentStorage.getExecutor());\n+                        }, chunkedSegmentStorage.getExecutor()), chunkedSegmentStorage.getExecutor());\n+    }\n+\n+    private CompletionStage<Void> performConcat(MetadataTransaction txn) {\n+        // Validate preconditions.\n+        checkState();\n+\n+        // Update list of chunks by appending sources list of chunks.\n+        return updateMetadata(txn).thenComposeAsync(v -> {\n+            // Finally defrag immediately.\n+            final CompletableFuture<Void> f;\n+            if (shouldDefrag() && null != targetLastChunk) {\n+                f = chunkedSegmentStorage.defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null, chunksToDelete);\n+            } else {\n+                f = CompletableFuture.completedFuture(null);\n+            }\n+            return f.thenComposeAsync(v2 -> {\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                return txn.commit()\n+                        .exceptionally(this::handleException)\n+                        .thenComposeAsync(v3 -> postCommit(), chunkedSegmentStorage.getExecutor());\n+            }, chunkedSegmentStorage.getExecutor());\n+        }, chunkedSegmentStorage.getExecutor());\n+    }\n+\n+    private Void handleException(Throwable e) {\n+        log.debug(\"{} concat - exception op={}, target={}, source={}, offset={}.\",\n+                chunkedSegmentStorage.getLogPrefix(), System.identityHashCode(this), targetHandle.getSegmentName(), sourceSegment, offset);\n+        val ex = Exceptions.unwrap(e);\n+        if (ex instanceof StorageMetadataWritesFencedOutException) {\n+            throw new CompletionException(new StorageNotPrimaryException(targetSegmentName, ex));\n+        }\n+        throw new CompletionException(ex);\n+    }\n+\n+    private CompletionStage<Void> postCommit() {\n+        // Collect garbage.\n+        return chunkedSegmentStorage.collectGarbage(chunksToDelete)\n+                .thenApplyAsync(v4 -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 120}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjA5Nzk2Mw==", "bodyText": "done", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r516097963", "createdAt": "2020-11-02T16:33:17Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ConcatOperation.java", "diffHunk": "@@ -0,0 +1,217 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.segmentstore.contracts.BadOffsetException;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.StorageNotPrimaryException;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import io.pravega.segmentstore.storage.metadata.StorageMetadataWritesFencedOutException;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.util.ArrayList;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.CompletionStage;\n+\n+import static io.pravega.segmentstore.storage.chunklayer.ChunkStorageMetrics.SLTS_CONCAT_COUNT;\n+import static io.pravega.segmentstore.storage.chunklayer.ChunkStorageMetrics.SLTS_CONCAT_LATENCY;\n+\n+/**\n+ * Implements the concat operation.\n+ */\n+@Slf4j\n+class ConcatOperation implements Callable<CompletableFuture<Void>> {\n+    private final long traceId;\n+    private final SegmentHandle targetHandle;\n+    private final long offset;\n+    private final String sourceSegment;\n+    private final ChunkedSegmentStorage chunkedSegmentStorage;\n+    private final ArrayList<String> chunksToDelete = new ArrayList<>();\n+\n+    private volatile Timer timer;\n+    private volatile String targetSegmentName;\n+    private volatile SegmentMetadata targetSegmentMetadata;\n+    private volatile SegmentMetadata sourceSegmentMetadata;\n+    private volatile ChunkMetadata targetLastChunk;\n+    private volatile ChunkMetadata sourceFirstChunk;\n+\n+    ConcatOperation(ChunkedSegmentStorage chunkedSegmentStorage, SegmentHandle targetHandle, long offset, String sourceSegment) {\n+        this.targetHandle = targetHandle;\n+        this.offset = offset;\n+        this.sourceSegment = sourceSegment;\n+        this.chunkedSegmentStorage = chunkedSegmentStorage;\n+        traceId = LoggerHelpers.traceEnter(log, \"concat\", targetHandle, offset, sourceSegment);\n+    }\n+\n+    public CompletableFuture<Void> call() {\n+        timer = new Timer();\n+        checkPreconditions();\n+        log.debug(\"{} concat - started op={}, target={}, source={}, offset={}.\",\n+                chunkedSegmentStorage.getLogPrefix(), System.identityHashCode(this), targetHandle.getSegmentName(), sourceSegment, offset);\n+\n+        targetSegmentName = targetHandle.getSegmentName();\n+\n+        return ChunkedSegmentStorage.tryWith(chunkedSegmentStorage.getMetadataStore().beginTransaction(targetHandle.getSegmentName(), sourceSegment),\n+                txn -> txn.get(targetSegmentName)\n+                        .thenComposeAsync(storageMetadata1 -> {\n+                            targetSegmentMetadata = (SegmentMetadata) storageMetadata1;\n+                            return txn.get(sourceSegment)\n+                                    .thenComposeAsync(storageMetadata2 -> {\n+                                        sourceSegmentMetadata = (SegmentMetadata) storageMetadata2;\n+                                        return performConcat(txn);\n+                                    }, chunkedSegmentStorage.getExecutor());\n+                        }, chunkedSegmentStorage.getExecutor()), chunkedSegmentStorage.getExecutor());\n+    }\n+\n+    private CompletionStage<Void> performConcat(MetadataTransaction txn) {\n+        // Validate preconditions.\n+        checkState();\n+\n+        // Update list of chunks by appending sources list of chunks.\n+        return updateMetadata(txn).thenComposeAsync(v -> {\n+            // Finally defrag immediately.\n+            final CompletableFuture<Void> f;\n+            if (shouldDefrag() && null != targetLastChunk) {\n+                f = chunkedSegmentStorage.defrag(txn, targetSegmentMetadata, targetLastChunk.getName(), null, chunksToDelete);\n+            } else {\n+                f = CompletableFuture.completedFuture(null);\n+            }\n+            return f.thenComposeAsync(v2 -> {\n+                targetSegmentMetadata.checkInvariants();\n+\n+                // Finally commit transaction.\n+                return txn.commit()\n+                        .exceptionally(this::handleException)\n+                        .thenComposeAsync(v3 -> postCommit(), chunkedSegmentStorage.getExecutor());\n+            }, chunkedSegmentStorage.getExecutor());\n+        }, chunkedSegmentStorage.getExecutor());\n+    }\n+\n+    private Void handleException(Throwable e) {\n+        log.debug(\"{} concat - exception op={}, target={}, source={}, offset={}.\",\n+                chunkedSegmentStorage.getLogPrefix(), System.identityHashCode(this), targetHandle.getSegmentName(), sourceSegment, offset);\n+        val ex = Exceptions.unwrap(e);\n+        if (ex instanceof StorageMetadataWritesFencedOutException) {\n+            throw new CompletionException(new StorageNotPrimaryException(targetSegmentName, ex));\n+        }\n+        throw new CompletionException(ex);\n+    }\n+\n+    private CompletionStage<Void> postCommit() {\n+        // Collect garbage.\n+        return chunkedSegmentStorage.collectGarbage(chunksToDelete)\n+                .thenApplyAsync(v4 -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE3MDUwMA=="}, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 120}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyNzkwNjA2OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNToxNDo1MVrOHrTmfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNzoyNzo0M1rOHsOmSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE3MTk2Nw==", "bodyText": "Please check your phrasing in this Javadoc. I found at least one sentence (such as this one) that aren't well formed. This includes grammar and singular/plurals.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r515171967", "createdAt": "2020-10-30T15:14:51Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -0,0 +1,282 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.util.ArrayList;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * Defragments the list of chunks for a given segment.\n+ * It finds eligible consecutive chunks that can be merged together.\n+ * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjEzODU3MQ==", "bodyText": "done", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r516138571", "createdAt": "2020-11-02T17:27:43Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -0,0 +1,282 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.util.ArrayList;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * Defragments the list of chunks for a given segment.\n+ * It finds eligible consecutive chunks that can be merged together.\n+ * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE3MTk2Nw=="}, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyNzkxNjQ1OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNToxNzozN1rOHrTtPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNzoyNzo1MVrOHsOmvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE3MzY5NA==", "bodyText": "you use vanilla s3 here but block s3 above. Please be consistent.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r515173694", "createdAt": "2020-10-30T15:17:37Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -0,0 +1,282 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.util.ArrayList;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * Defragments the list of chunks for a given segment.\n+ * It finds eligible consecutive chunks that can be merged together.\n+ * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+ * Conceptually this is like deleting nodes from middle of the list of chunks.\n+ *\n+ * <Ul>\n+ * <li> In the absence of defragmentation, the number of chunks for individual segments keeps on increasing.\n+ * When we have too many small chunks (say because many transactions with little data on some segments), the segment\n+ * is fragmented - this may impact both the read throughput and the performance of the metadata store.\n+ * This problem is further intensified when we have stores that do not support append semantics (e.g., stock S3) and\n+ * each write becomes a separate chunk.\n+ * </li>\n+ * <li>\n+ * If the underlying storage provides some facility to stitch together smaller chunk into larger chunks, then we do\n+ * actually want to exploit that, specially when the underlying implementation is only a metadata operation. We want\n+ * to leverage multi-part uploads in object stores that support it (e.g., AWS S3, Dell EMC ECS) as they are typically\n+ * only metadata operations, reducing the overall cost of the merging them together. HDFS also supports merges,\n+ * whereas NFS has no concept of merging natively.\n+ *\n+ * As chunks become larger, append writes (read source completely and append it back at the end of target)\n+ * become inefficient. Consequently, a native option for merging is desirable. We use such native merge capability\n+ * when available, and if not available, then we use appends.\n+ * </li>\n+ * <li>\n+ * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n+ * We can then fine tune that background task to run optimally with low overhead.\n+ * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n+ * </li>\n+ * <li>\n+ * <li>\n+ * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+ * </li>\n+ * </ul>\n+ *\n+ * What controls whether we invoke concat or simulate through appends?\n+ * There are a few different capabilities that ChunkStorage needs to provide.\n+ * <ul>\n+ * <li>Does ChunkStorage support appending to existing chunks? For vanilla S3 compatible this would return false.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjEzODY4NA==", "bodyText": "done", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r516138684", "createdAt": "2020-11-02T17:27:51Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -0,0 +1,282 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.util.ArrayList;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * Defragments the list of chunks for a given segment.\n+ * It finds eligible consecutive chunks that can be merged together.\n+ * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+ * Conceptually this is like deleting nodes from middle of the list of chunks.\n+ *\n+ * <Ul>\n+ * <li> In the absence of defragmentation, the number of chunks for individual segments keeps on increasing.\n+ * When we have too many small chunks (say because many transactions with little data on some segments), the segment\n+ * is fragmented - this may impact both the read throughput and the performance of the metadata store.\n+ * This problem is further intensified when we have stores that do not support append semantics (e.g., stock S3) and\n+ * each write becomes a separate chunk.\n+ * </li>\n+ * <li>\n+ * If the underlying storage provides some facility to stitch together smaller chunk into larger chunks, then we do\n+ * actually want to exploit that, specially when the underlying implementation is only a metadata operation. We want\n+ * to leverage multi-part uploads in object stores that support it (e.g., AWS S3, Dell EMC ECS) as they are typically\n+ * only metadata operations, reducing the overall cost of the merging them together. HDFS also supports merges,\n+ * whereas NFS has no concept of merging natively.\n+ *\n+ * As chunks become larger, append writes (read source completely and append it back at the end of target)\n+ * become inefficient. Consequently, a native option for merging is desirable. We use such native merge capability\n+ * when available, and if not available, then we use appends.\n+ * </li>\n+ * <li>\n+ * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n+ * We can then fine tune that background task to run optimally with low overhead.\n+ * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n+ * </li>\n+ * <li>\n+ * <li>\n+ * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+ * </li>\n+ * </ul>\n+ *\n+ * What controls whether we invoke concat or simulate through appends?\n+ * There are a few different capabilities that ChunkStorage needs to provide.\n+ * <ul>\n+ * <li>Does ChunkStorage support appending to existing chunks? For vanilla S3 compatible this would return false.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE3MzY5NA=="}, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyNzkyMjA4OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNToxODo1MlrOHrTw7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNzoyNzo1OVrOHsOnIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE3NDYzOQ==", "bodyText": "incomplete sentence", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r515174639", "createdAt": "2020-10-30T15:18:52Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -0,0 +1,282 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.util.ArrayList;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * Defragments the list of chunks for a given segment.\n+ * It finds eligible consecutive chunks that can be merged together.\n+ * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+ * Conceptually this is like deleting nodes from middle of the list of chunks.\n+ *\n+ * <Ul>\n+ * <li> In the absence of defragmentation, the number of chunks for individual segments keeps on increasing.\n+ * When we have too many small chunks (say because many transactions with little data on some segments), the segment\n+ * is fragmented - this may impact both the read throughput and the performance of the metadata store.\n+ * This problem is further intensified when we have stores that do not support append semantics (e.g., stock S3) and\n+ * each write becomes a separate chunk.\n+ * </li>\n+ * <li>\n+ * If the underlying storage provides some facility to stitch together smaller chunk into larger chunks, then we do\n+ * actually want to exploit that, specially when the underlying implementation is only a metadata operation. We want\n+ * to leverage multi-part uploads in object stores that support it (e.g., AWS S3, Dell EMC ECS) as they are typically\n+ * only metadata operations, reducing the overall cost of the merging them together. HDFS also supports merges,\n+ * whereas NFS has no concept of merging natively.\n+ *\n+ * As chunks become larger, append writes (read source completely and append it back at the end of target)\n+ * become inefficient. Consequently, a native option for merging is desirable. We use such native merge capability\n+ * when available, and if not available, then we use appends.\n+ * </li>\n+ * <li>\n+ * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n+ * We can then fine tune that background task to run optimally with low overhead.\n+ * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n+ * </li>\n+ * <li>\n+ * <li>\n+ * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+ * </li>\n+ * </ul>\n+ *\n+ * What controls whether we invoke concat or simulate through appends?\n+ * There are a few different capabilities that ChunkStorage needs to provide.\n+ * <ul>\n+ * <li>Does ChunkStorage support appending to existing chunks? For vanilla S3 compatible this would return false.\n+ * This is indicated by supportsAppend.</li>\n+ * <li>Does ChunkStorage support for concatenating chunks ? This is indicated by supportsConcat.\n+ * If this is true then concat operation will be invoked otherwise chunks will be appended.</li>\n+ * <li>There are some obvious constraints - For ChunkStorage support any concat functionality it must support either\n+ * append or concat.</li>\n+ * <li>Also when ChunkStorage supports both concat and append, ChunkedSegmentStorage will invoke appropriate method\n+ * depending on size of target and source chunks. (Eg. ECS)</li>\n+ * </ul>\n+ *\n+ * <li>\n+ * What controls defrag?\n+ * There are two additional parameters that control when concat", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjEzODc4Ng==", "bodyText": "done", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r516138786", "createdAt": "2020-11-02T17:27:59Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -0,0 +1,282 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.util.ArrayList;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * Defragments the list of chunks for a given segment.\n+ * It finds eligible consecutive chunks that can be merged together.\n+ * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+ * Conceptually this is like deleting nodes from middle of the list of chunks.\n+ *\n+ * <Ul>\n+ * <li> In the absence of defragmentation, the number of chunks for individual segments keeps on increasing.\n+ * When we have too many small chunks (say because many transactions with little data on some segments), the segment\n+ * is fragmented - this may impact both the read throughput and the performance of the metadata store.\n+ * This problem is further intensified when we have stores that do not support append semantics (e.g., stock S3) and\n+ * each write becomes a separate chunk.\n+ * </li>\n+ * <li>\n+ * If the underlying storage provides some facility to stitch together smaller chunk into larger chunks, then we do\n+ * actually want to exploit that, specially when the underlying implementation is only a metadata operation. We want\n+ * to leverage multi-part uploads in object stores that support it (e.g., AWS S3, Dell EMC ECS) as they are typically\n+ * only metadata operations, reducing the overall cost of the merging them together. HDFS also supports merges,\n+ * whereas NFS has no concept of merging natively.\n+ *\n+ * As chunks become larger, append writes (read source completely and append it back at the end of target)\n+ * become inefficient. Consequently, a native option for merging is desirable. We use such native merge capability\n+ * when available, and if not available, then we use appends.\n+ * </li>\n+ * <li>\n+ * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n+ * We can then fine tune that background task to run optimally with low overhead.\n+ * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n+ * </li>\n+ * <li>\n+ * <li>\n+ * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+ * </li>\n+ * </ul>\n+ *\n+ * What controls whether we invoke concat or simulate through appends?\n+ * There are a few different capabilities that ChunkStorage needs to provide.\n+ * <ul>\n+ * <li>Does ChunkStorage support appending to existing chunks? For vanilla S3 compatible this would return false.\n+ * This is indicated by supportsAppend.</li>\n+ * <li>Does ChunkStorage support for concatenating chunks ? This is indicated by supportsConcat.\n+ * If this is true then concat operation will be invoked otherwise chunks will be appended.</li>\n+ * <li>There are some obvious constraints - For ChunkStorage support any concat functionality it must support either\n+ * append or concat.</li>\n+ * <li>Also when ChunkStorage supports both concat and append, ChunkedSegmentStorage will invoke appropriate method\n+ * depending on size of target and source chunks. (Eg. ECS)</li>\n+ * </ul>\n+ *\n+ * <li>\n+ * What controls defrag?\n+ * There are two additional parameters that control when concat", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTE3NDYzOQ=="}, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyODExNjIyOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNjowMToxNVrOHrVsQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNjozNDo0M1rOHsMMAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIwNjIxMQ==", "bodyText": "You can get these names from the handles. No need to store them separately.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r515206211", "createdAt": "2020-10-30T16:01:15Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -0,0 +1,282 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.util.ArrayList;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * Defragments the list of chunks for a given segment.\n+ * It finds eligible consecutive chunks that can be merged together.\n+ * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+ * Conceptually this is like deleting nodes from middle of the list of chunks.\n+ *\n+ * <Ul>\n+ * <li> In the absence of defragmentation, the number of chunks for individual segments keeps on increasing.\n+ * When we have too many small chunks (say because many transactions with little data on some segments), the segment\n+ * is fragmented - this may impact both the read throughput and the performance of the metadata store.\n+ * This problem is further intensified when we have stores that do not support append semantics (e.g., stock S3) and\n+ * each write becomes a separate chunk.\n+ * </li>\n+ * <li>\n+ * If the underlying storage provides some facility to stitch together smaller chunk into larger chunks, then we do\n+ * actually want to exploit that, specially when the underlying implementation is only a metadata operation. We want\n+ * to leverage multi-part uploads in object stores that support it (e.g., AWS S3, Dell EMC ECS) as they are typically\n+ * only metadata operations, reducing the overall cost of the merging them together. HDFS also supports merges,\n+ * whereas NFS has no concept of merging natively.\n+ *\n+ * As chunks become larger, append writes (read source completely and append it back at the end of target)\n+ * become inefficient. Consequently, a native option for merging is desirable. We use such native merge capability\n+ * when available, and if not available, then we use appends.\n+ * </li>\n+ * <li>\n+ * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n+ * We can then fine tune that background task to run optimally with low overhead.\n+ * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n+ * </li>\n+ * <li>\n+ * <li>\n+ * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+ * </li>\n+ * </ul>\n+ *\n+ * What controls whether we invoke concat or simulate through appends?\n+ * There are a few different capabilities that ChunkStorage needs to provide.\n+ * <ul>\n+ * <li>Does ChunkStorage support appending to existing chunks? For vanilla S3 compatible this would return false.\n+ * This is indicated by supportsAppend.</li>\n+ * <li>Does ChunkStorage support for concatenating chunks ? This is indicated by supportsConcat.\n+ * If this is true then concat operation will be invoked otherwise chunks will be appended.</li>\n+ * <li>There are some obvious constraints - For ChunkStorage support any concat functionality it must support either\n+ * append or concat.</li>\n+ * <li>Also when ChunkStorage supports both concat and append, ChunkedSegmentStorage will invoke appropriate method\n+ * depending on size of target and source chunks. (Eg. ECS)</li>\n+ * </ul>\n+ *\n+ * <li>\n+ * What controls defrag?\n+ * There are two additional parameters that control when concat\n+ * <li>minSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered a small object.\n+ * For small source objects, append is used instead of using concat. (For really small txn it is rather efficient to use append than MPU).</li>\n+ * <li>maxSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered for concat. (Eg S3 might have max limit on chunk size).</li>\n+ * In short there is a size beyond which using append is not advisable. Conversely there is a size below which concat is not efficient.(minSizeLimitForConcat )\n+ * Then there is limit which concating does not make sense maxSizeLimitForConcat\n+ * </li>\n+ * <li>\n+ * What is the defrag algorithm\n+ * <pre>\n+ * While(segment.hasConcatableChunks()){\n+ *     Set<List<Chunk>> s = FindConsecutiveConcatableChunks();\n+ *     For (List<chunk> list : s){\n+ *        ConcatChunks (list);\n+ *     }\n+ * }\n+ * </pre>\n+ * </li>\n+ * </ul>\n+ */\n+class DefragmentOperation implements Callable<CompletableFuture<Void>> {\n+    private final MetadataTransaction txn;\n+    private final SegmentMetadata segmentMetadata;\n+    private final String startChunkName;\n+    private final String lastChunkName;\n+    private final ArrayList<String> chunksToDelete;\n+    private final ChunkedSegmentStorage chunkedSegmentStorage;\n+\n+    private volatile ArrayList<ChunkInfo> chunksToConcat = new ArrayList<>();\n+\n+    private volatile ChunkMetadata target;\n+    private volatile String targetChunkName;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjA5OTA3Mw==", "bodyText": "done", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r516099073", "createdAt": "2020-11-02T16:34:43Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -0,0 +1,282 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.util.ArrayList;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * Defragments the list of chunks for a given segment.\n+ * It finds eligible consecutive chunks that can be merged together.\n+ * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+ * Conceptually this is like deleting nodes from middle of the list of chunks.\n+ *\n+ * <Ul>\n+ * <li> In the absence of defragmentation, the number of chunks for individual segments keeps on increasing.\n+ * When we have too many small chunks (say because many transactions with little data on some segments), the segment\n+ * is fragmented - this may impact both the read throughput and the performance of the metadata store.\n+ * This problem is further intensified when we have stores that do not support append semantics (e.g., stock S3) and\n+ * each write becomes a separate chunk.\n+ * </li>\n+ * <li>\n+ * If the underlying storage provides some facility to stitch together smaller chunk into larger chunks, then we do\n+ * actually want to exploit that, specially when the underlying implementation is only a metadata operation. We want\n+ * to leverage multi-part uploads in object stores that support it (e.g., AWS S3, Dell EMC ECS) as they are typically\n+ * only metadata operations, reducing the overall cost of the merging them together. HDFS also supports merges,\n+ * whereas NFS has no concept of merging natively.\n+ *\n+ * As chunks become larger, append writes (read source completely and append it back at the end of target)\n+ * become inefficient. Consequently, a native option for merging is desirable. We use such native merge capability\n+ * when available, and if not available, then we use appends.\n+ * </li>\n+ * <li>\n+ * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n+ * We can then fine tune that background task to run optimally with low overhead.\n+ * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n+ * </li>\n+ * <li>\n+ * <li>\n+ * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+ * </li>\n+ * </ul>\n+ *\n+ * What controls whether we invoke concat or simulate through appends?\n+ * There are a few different capabilities that ChunkStorage needs to provide.\n+ * <ul>\n+ * <li>Does ChunkStorage support appending to existing chunks? For vanilla S3 compatible this would return false.\n+ * This is indicated by supportsAppend.</li>\n+ * <li>Does ChunkStorage support for concatenating chunks ? This is indicated by supportsConcat.\n+ * If this is true then concat operation will be invoked otherwise chunks will be appended.</li>\n+ * <li>There are some obvious constraints - For ChunkStorage support any concat functionality it must support either\n+ * append or concat.</li>\n+ * <li>Also when ChunkStorage supports both concat and append, ChunkedSegmentStorage will invoke appropriate method\n+ * depending on size of target and source chunks. (Eg. ECS)</li>\n+ * </ul>\n+ *\n+ * <li>\n+ * What controls defrag?\n+ * There are two additional parameters that control when concat\n+ * <li>minSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered a small object.\n+ * For small source objects, append is used instead of using concat. (For really small txn it is rather efficient to use append than MPU).</li>\n+ * <li>maxSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered for concat. (Eg S3 might have max limit on chunk size).</li>\n+ * In short there is a size beyond which using append is not advisable. Conversely there is a size below which concat is not efficient.(minSizeLimitForConcat )\n+ * Then there is limit which concating does not make sense maxSizeLimitForConcat\n+ * </li>\n+ * <li>\n+ * What is the defrag algorithm\n+ * <pre>\n+ * While(segment.hasConcatableChunks()){\n+ *     Set<List<Chunk>> s = FindConsecutiveConcatableChunks();\n+ *     For (List<chunk> list : s){\n+ *        ConcatChunks (list);\n+ *     }\n+ * }\n+ * </pre>\n+ * </li>\n+ * </ul>\n+ */\n+class DefragmentOperation implements Callable<CompletableFuture<Void>> {\n+    private final MetadataTransaction txn;\n+    private final SegmentMetadata segmentMetadata;\n+    private final String startChunkName;\n+    private final String lastChunkName;\n+    private final ArrayList<String> chunksToDelete;\n+    private final ChunkedSegmentStorage chunkedSegmentStorage;\n+\n+    private volatile ArrayList<ChunkInfo> chunksToConcat = new ArrayList<>();\n+\n+    private volatile ChunkMetadata target;\n+    private volatile String targetChunkName;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIwNjIxMQ=="}, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyODExNzg0OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNjowMTo0M1rOHrVtWg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNFQwMDozMjowMVrOHtFNmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIwNjQ5MA==", "bodyText": "Please try to either make these final or pass them along as arguments to your method calls.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r515206490", "createdAt": "2020-10-30T16:01:43Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -0,0 +1,282 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.util.ArrayList;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * Defragments the list of chunks for a given segment.\n+ * It finds eligible consecutive chunks that can be merged together.\n+ * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+ * Conceptually this is like deleting nodes from middle of the list of chunks.\n+ *\n+ * <Ul>\n+ * <li> In the absence of defragmentation, the number of chunks for individual segments keeps on increasing.\n+ * When we have too many small chunks (say because many transactions with little data on some segments), the segment\n+ * is fragmented - this may impact both the read throughput and the performance of the metadata store.\n+ * This problem is further intensified when we have stores that do not support append semantics (e.g., stock S3) and\n+ * each write becomes a separate chunk.\n+ * </li>\n+ * <li>\n+ * If the underlying storage provides some facility to stitch together smaller chunk into larger chunks, then we do\n+ * actually want to exploit that, specially when the underlying implementation is only a metadata operation. We want\n+ * to leverage multi-part uploads in object stores that support it (e.g., AWS S3, Dell EMC ECS) as they are typically\n+ * only metadata operations, reducing the overall cost of the merging them together. HDFS also supports merges,\n+ * whereas NFS has no concept of merging natively.\n+ *\n+ * As chunks become larger, append writes (read source completely and append it back at the end of target)\n+ * become inefficient. Consequently, a native option for merging is desirable. We use such native merge capability\n+ * when available, and if not available, then we use appends.\n+ * </li>\n+ * <li>\n+ * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n+ * We can then fine tune that background task to run optimally with low overhead.\n+ * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n+ * </li>\n+ * <li>\n+ * <li>\n+ * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+ * </li>\n+ * </ul>\n+ *\n+ * What controls whether we invoke concat or simulate through appends?\n+ * There are a few different capabilities that ChunkStorage needs to provide.\n+ * <ul>\n+ * <li>Does ChunkStorage support appending to existing chunks? For vanilla S3 compatible this would return false.\n+ * This is indicated by supportsAppend.</li>\n+ * <li>Does ChunkStorage support for concatenating chunks ? This is indicated by supportsConcat.\n+ * If this is true then concat operation will be invoked otherwise chunks will be appended.</li>\n+ * <li>There are some obvious constraints - For ChunkStorage support any concat functionality it must support either\n+ * append or concat.</li>\n+ * <li>Also when ChunkStorage supports both concat and append, ChunkedSegmentStorage will invoke appropriate method\n+ * depending on size of target and source chunks. (Eg. ECS)</li>\n+ * </ul>\n+ *\n+ * <li>\n+ * What controls defrag?\n+ * There are two additional parameters that control when concat\n+ * <li>minSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered a small object.\n+ * For small source objects, append is used instead of using concat. (For really small txn it is rather efficient to use append than MPU).</li>\n+ * <li>maxSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered for concat. (Eg S3 might have max limit on chunk size).</li>\n+ * In short there is a size beyond which using append is not advisable. Conversely there is a size below which concat is not efficient.(minSizeLimitForConcat )\n+ * Then there is limit which concating does not make sense maxSizeLimitForConcat\n+ * </li>\n+ * <li>\n+ * What is the defrag algorithm\n+ * <pre>\n+ * While(segment.hasConcatableChunks()){\n+ *     Set<List<Chunk>> s = FindConsecutiveConcatableChunks();\n+ *     For (List<chunk> list : s){\n+ *        ConcatChunks (list);\n+ *     }\n+ * }\n+ * </pre>\n+ * </li>\n+ * </ul>\n+ */\n+class DefragmentOperation implements Callable<CompletableFuture<Void>> {\n+    private final MetadataTransaction txn;\n+    private final SegmentMetadata segmentMetadata;\n+    private final String startChunkName;\n+    private final String lastChunkName;\n+    private final ArrayList<String> chunksToDelete;\n+    private final ChunkedSegmentStorage chunkedSegmentStorage;\n+\n+    private volatile ArrayList<ChunkInfo> chunksToConcat = new ArrayList<>();\n+\n+    private volatile ChunkMetadata target;\n+    private volatile String targetChunkName;\n+    private volatile boolean useAppend;\n+    private volatile long targetSizeAfterConcat;\n+    private volatile String nextChunkName;\n+    private volatile ChunkMetadata next = null;\n+\n+    private volatile long writeAtOffset;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzAzMzM3MQ==", "bodyText": "This should not matter as much, I used operation class so that I don't have to pass lots of parameters around. passing parameters becomes troublesome specially when multiple values need to be returned. These field are modified and then used in next step.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r517033371", "createdAt": "2020-11-04T00:32:01Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -0,0 +1,282 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import lombok.val;\n+\n+import java.io.ByteArrayInputStream;\n+import java.util.ArrayList;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+/**\n+ * Defragments the list of chunks for a given segment.\n+ * It finds eligible consecutive chunks that can be merged together.\n+ * The sublist such elgible chunks is replaced with single new chunk record corresponding to new large chunk.\n+ * Conceptually this is like deleting nodes from middle of the list of chunks.\n+ *\n+ * <Ul>\n+ * <li> In the absence of defragmentation, the number of chunks for individual segments keeps on increasing.\n+ * When we have too many small chunks (say because many transactions with little data on some segments), the segment\n+ * is fragmented - this may impact both the read throughput and the performance of the metadata store.\n+ * This problem is further intensified when we have stores that do not support append semantics (e.g., stock S3) and\n+ * each write becomes a separate chunk.\n+ * </li>\n+ * <li>\n+ * If the underlying storage provides some facility to stitch together smaller chunk into larger chunks, then we do\n+ * actually want to exploit that, specially when the underlying implementation is only a metadata operation. We want\n+ * to leverage multi-part uploads in object stores that support it (e.g., AWS S3, Dell EMC ECS) as they are typically\n+ * only metadata operations, reducing the overall cost of the merging them together. HDFS also supports merges,\n+ * whereas NFS has no concept of merging natively.\n+ *\n+ * As chunks become larger, append writes (read source completely and append it back at the end of target)\n+ * become inefficient. Consequently, a native option for merging is desirable. We use such native merge capability\n+ * when available, and if not available, then we use appends.\n+ * </li>\n+ * <li>\n+ * Ideally we want the defrag to be run in the background periodically and not on the write/concat path.\n+ * We can then fine tune that background task to run optimally with low overhead.\n+ * We might be able to give more knobs to tune its parameters (Eg. threshold on number of chunks).\n+ * </li>\n+ * <li>\n+ * <li>\n+ * Defrag operation will respect max rolling size and will not create chunks greater than that size.\n+ * </li>\n+ * </ul>\n+ *\n+ * What controls whether we invoke concat or simulate through appends?\n+ * There are a few different capabilities that ChunkStorage needs to provide.\n+ * <ul>\n+ * <li>Does ChunkStorage support appending to existing chunks? For vanilla S3 compatible this would return false.\n+ * This is indicated by supportsAppend.</li>\n+ * <li>Does ChunkStorage support for concatenating chunks ? This is indicated by supportsConcat.\n+ * If this is true then concat operation will be invoked otherwise chunks will be appended.</li>\n+ * <li>There are some obvious constraints - For ChunkStorage support any concat functionality it must support either\n+ * append or concat.</li>\n+ * <li>Also when ChunkStorage supports both concat and append, ChunkedSegmentStorage will invoke appropriate method\n+ * depending on size of target and source chunks. (Eg. ECS)</li>\n+ * </ul>\n+ *\n+ * <li>\n+ * What controls defrag?\n+ * There are two additional parameters that control when concat\n+ * <li>minSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered a small object.\n+ * For small source objects, append is used instead of using concat. (For really small txn it is rather efficient to use append than MPU).</li>\n+ * <li>maxSizeLimitForConcat: Size of chunk in bytes above which it is no longer considered for concat. (Eg S3 might have max limit on chunk size).</li>\n+ * In short there is a size beyond which using append is not advisable. Conversely there is a size below which concat is not efficient.(minSizeLimitForConcat )\n+ * Then there is limit which concating does not make sense maxSizeLimitForConcat\n+ * </li>\n+ * <li>\n+ * What is the defrag algorithm\n+ * <pre>\n+ * While(segment.hasConcatableChunks()){\n+ *     Set<List<Chunk>> s = FindConsecutiveConcatableChunks();\n+ *     For (List<chunk> list : s){\n+ *        ConcatChunks (list);\n+ *     }\n+ * }\n+ * </pre>\n+ * </li>\n+ * </ul>\n+ */\n+class DefragmentOperation implements Callable<CompletableFuture<Void>> {\n+    private final MetadataTransaction txn;\n+    private final SegmentMetadata segmentMetadata;\n+    private final String startChunkName;\n+    private final String lastChunkName;\n+    private final ArrayList<String> chunksToDelete;\n+    private final ChunkedSegmentStorage chunkedSegmentStorage;\n+\n+    private volatile ArrayList<ChunkInfo> chunksToConcat = new ArrayList<>();\n+\n+    private volatile ChunkMetadata target;\n+    private volatile String targetChunkName;\n+    private volatile boolean useAppend;\n+    private volatile long targetSizeAfterConcat;\n+    private volatile String nextChunkName;\n+    private volatile ChunkMetadata next = null;\n+\n+    private volatile long writeAtOffset;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIwNjQ5MA=="}, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 111}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyODEyMjE3OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadOperation.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNjowMjo0NVrOHrVwCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNjozNDo1NFrOHsMMhQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIwNzE3OA==", "bodyText": "Same comments here. And in all the other operations below.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r515207178", "createdAt": "2020-10-30T16:02:45Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadOperation.java", "diffHunk": "@@ -0,0 +1,246 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import static io.pravega.segmentstore.storage.chunklayer.ChunkStorageMetrics.SLTS_READ_BYTES;\n+import static io.pravega.segmentstore.storage.chunklayer.ChunkStorageMetrics.SLTS_READ_LATENCY;\n+import static io.pravega.segmentstore.storage.chunklayer.ChunkStorageMetrics.SLTS_READ_INDEX_SCAN_LATENCY;\n+\n+@Slf4j\n+class ReadOperation implements Callable<CompletableFuture<Integer>> {\n+    private final SegmentHandle handle;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjA5OTIwNQ==", "bodyText": "done", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r516099205", "createdAt": "2020-11-02T16:34:54Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadOperation.java", "diffHunk": "@@ -0,0 +1,246 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.StreamSegmentTruncatedException;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadata;\n+import io.pravega.segmentstore.storage.metadata.MetadataTransaction;\n+import io.pravega.segmentstore.storage.metadata.SegmentMetadata;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.atomic.AtomicInteger;\n+\n+import static io.pravega.segmentstore.storage.chunklayer.ChunkStorageMetrics.SLTS_READ_BYTES;\n+import static io.pravega.segmentstore.storage.chunklayer.ChunkStorageMetrics.SLTS_READ_LATENCY;\n+import static io.pravega.segmentstore.storage.chunklayer.ChunkStorageMetrics.SLTS_READ_INDEX_SCAN_LATENCY;\n+\n+@Slf4j\n+class ReadOperation implements Callable<CompletableFuture<Integer>> {\n+    private final SegmentHandle handle;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIwNzE3OA=="}, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyODEyNDY3OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/ChunkMetadataStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNjowMzoyOFrOHrVxyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNjowOTozNFrOHsLFVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIwNzYyNA==", "bodyText": "Update Javadoc to reflect that you're returning a future.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r515207624", "createdAt": "2020-10-30T16:03:28Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/ChunkMetadataStore.java", "diffHunk": "@@ -78,107 +81,116 @@\n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of {@link MetadataTransaction}.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n-    MetadataTransaction beginTransaction() throws StorageMetadataException;\n+    MetadataTransaction beginTransaction(String... keysToLock);\n \n     /**\n      * Retrieves the metadata for given key.\n      *\n      * @param txn Transaction.\n      * @param key key to use to retrieve metadata.\n      * @return Metadata for given key. Null if key was not found.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @throws CompletionException If the operation failed, it will be completed with the appropriate exception. Notable Exceptions:\n+     * {@link StorageMetadataException} Exception related to storage metadata operations.\n      */\n-    StorageMetadata get(MetadataTransaction txn, String key) throws StorageMetadataException;\n+    CompletableFuture<StorageMetadata> get(MetadataTransaction txn, String key);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjA4MDk4MQ==", "bodyText": "fixed", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r516080981", "createdAt": "2020-11-02T16:09:34Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/ChunkMetadataStore.java", "diffHunk": "@@ -78,107 +81,116 @@\n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of {@link MetadataTransaction}.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n-    MetadataTransaction beginTransaction() throws StorageMetadataException;\n+    MetadataTransaction beginTransaction(String... keysToLock);\n \n     /**\n      * Retrieves the metadata for given key.\n      *\n      * @param txn Transaction.\n      * @param key key to use to retrieve metadata.\n      * @return Metadata for given key. Null if key was not found.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @throws CompletionException If the operation failed, it will be completed with the appropriate exception. Notable Exceptions:\n+     * {@link StorageMetadataException} Exception related to storage metadata operations.\n      */\n-    StorageMetadata get(MetadataTransaction txn, String key) throws StorageMetadataException;\n+    CompletableFuture<StorageMetadata> get(MetadataTransaction txn, String key);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIwNzYyNA=="}, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyODEyNTQ3OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/ChunkMetadataStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNjowMzozOVrOHrVyRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNTo1Nzo1MFrOHsKkjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIwNzc1MA==", "bodyText": "and here, and below too.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r515207750", "createdAt": "2020-10-30T16:03:39Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/ChunkMetadataStore.java", "diffHunk": "@@ -78,107 +81,116 @@\n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of {@link MetadataTransaction}.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n-    MetadataTransaction beginTransaction() throws StorageMetadataException;\n+    MetadataTransaction beginTransaction(String... keysToLock);\n \n     /**\n      * Retrieves the metadata for given key.\n      *\n      * @param txn Transaction.\n      * @param key key to use to retrieve metadata.\n      * @return Metadata for given key. Null if key was not found.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @throws CompletionException If the operation failed, it will be completed with the appropriate exception. Notable Exceptions:\n+     * {@link StorageMetadataException} Exception related to storage metadata operations.\n      */\n-    StorageMetadata get(MetadataTransaction txn, String key) throws StorageMetadataException;\n+    CompletableFuture<StorageMetadata> get(MetadataTransaction txn, String key);\n \n     /**\n      * Updates existing metadata.\n      *\n      * @param txn      Transaction.\n      * @param metadata metadata record.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @throws CompletionException If the operation failed, it will be completed with the appropriate exception. Notable Exceptions:\n+     * {@link StorageMetadataException} Exception related to storage metadata operations.\n      */\n-    void update(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException;\n+    void update(MetadataTransaction txn, StorageMetadata metadata);\n \n     /**\n      * Creates a new metadata record.\n      *\n      * @param txn      Transaction.\n      * @param metadata metadata record.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @throws CompletionException If the operation failed, it will be completed with the appropriate exception. Notable Exceptions:\n+     * {@link StorageMetadataException} Exception related to storage metadata operations.\n      */\n-    void create(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException;\n+    void create(MetadataTransaction txn, StorageMetadata metadata);\n \n     /**\n      * Marks given single record as pinned.\n      * Pinned records are not evicted from memory and are not written to the underlying storage.\n      *\n      * @param txn      Transaction.\n      * @param metadata metadata record.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @throws CompletionException If the operation failed, it will be completed with the appropriate exception. Notable Exceptions:\n+     * {@link StorageMetadataException} Exception related to storage metadata operations.\n      */\n-    void markPinned(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException;\n+    void markPinned(MetadataTransaction txn, StorageMetadata metadata);\n \n     /**\n      * Deletes a metadata record given the key.\n-     * The transaction data is validated and changes are commited to underlying storage.\n+     * The transaction data is validated and changes are committed to underlying storage.\n      * This call blocks until write to underlying storage is confirmed.\n      *\n      * @param txn Transaction.\n      * @param key key to use to retrieve metadata.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @throws CompletionException If the operation failed, it will be completed with the appropriate exception. Notable Exceptions:\n+     * {@link StorageMetadataException} Exception related to storage metadata operations.\n      */\n-    void delete(MetadataTransaction txn, String key) throws StorageMetadataException;\n+    void delete(MetadataTransaction txn, String key);\n \n     /**\n      * Commits given transaction.\n-     * If  skipStoreCheck is set to true then the transaction data is validated without realoding.\n+     * If  skipStoreCheck is set to true then the transaction data is validated without reloading.\n      * This call blocks until write to underlying storage is confirmed. This helps avoid circular dependency on storage\n      * system segments.\n-     * If lazyWrite is true then the transaction data is validated but the changes are not commited to underlying storage.\n+     * If lazyWrite is true then the transaction data is validated but the changes are not committed to underlying storage.\n      * Changes are put in the in memory buffer only. Note that in case of crash, the changes in the in buffer are lost.\n-     * In this case the state must be re-created using application specific recovery/failover logic.\n+     * In this case the state must be re-created using application specific recovery/fail-over logic.\n      * Do not commit lazily if such recovery is not possible.\n      * This call does not blocks until write to underlying storage is confirmed if lazyWrite is true.\n      *\n      * @param txn            transaction to commit.\n      * @param lazyWrite      true if data can be written lazily.\n      * @param skipStoreCheck true if data is not to be reloaded from store.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @throws CompletionException If the operation failed, it will be completed with the appropriate exception. Notable Exceptions:\n+     * {@link StorageMetadataException} If transaction can not be committed.\n      */\n-    void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException;\n+    CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjA3MjU5MQ==", "bodyText": "fixed", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r516072591", "createdAt": "2020-11-02T15:57:50Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/ChunkMetadataStore.java", "diffHunk": "@@ -78,107 +81,116 @@\n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of {@link MetadataTransaction}.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n-    MetadataTransaction beginTransaction() throws StorageMetadataException;\n+    MetadataTransaction beginTransaction(String... keysToLock);\n \n     /**\n      * Retrieves the metadata for given key.\n      *\n      * @param txn Transaction.\n      * @param key key to use to retrieve metadata.\n      * @return Metadata for given key. Null if key was not found.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @throws CompletionException If the operation failed, it will be completed with the appropriate exception. Notable Exceptions:\n+     * {@link StorageMetadataException} Exception related to storage metadata operations.\n      */\n-    StorageMetadata get(MetadataTransaction txn, String key) throws StorageMetadataException;\n+    CompletableFuture<StorageMetadata> get(MetadataTransaction txn, String key);\n \n     /**\n      * Updates existing metadata.\n      *\n      * @param txn      Transaction.\n      * @param metadata metadata record.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @throws CompletionException If the operation failed, it will be completed with the appropriate exception. Notable Exceptions:\n+     * {@link StorageMetadataException} Exception related to storage metadata operations.\n      */\n-    void update(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException;\n+    void update(MetadataTransaction txn, StorageMetadata metadata);\n \n     /**\n      * Creates a new metadata record.\n      *\n      * @param txn      Transaction.\n      * @param metadata metadata record.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @throws CompletionException If the operation failed, it will be completed with the appropriate exception. Notable Exceptions:\n+     * {@link StorageMetadataException} Exception related to storage metadata operations.\n      */\n-    void create(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException;\n+    void create(MetadataTransaction txn, StorageMetadata metadata);\n \n     /**\n      * Marks given single record as pinned.\n      * Pinned records are not evicted from memory and are not written to the underlying storage.\n      *\n      * @param txn      Transaction.\n      * @param metadata metadata record.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @throws CompletionException If the operation failed, it will be completed with the appropriate exception. Notable Exceptions:\n+     * {@link StorageMetadataException} Exception related to storage metadata operations.\n      */\n-    void markPinned(MetadataTransaction txn, StorageMetadata metadata) throws StorageMetadataException;\n+    void markPinned(MetadataTransaction txn, StorageMetadata metadata);\n \n     /**\n      * Deletes a metadata record given the key.\n-     * The transaction data is validated and changes are commited to underlying storage.\n+     * The transaction data is validated and changes are committed to underlying storage.\n      * This call blocks until write to underlying storage is confirmed.\n      *\n      * @param txn Transaction.\n      * @param key key to use to retrieve metadata.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @throws CompletionException If the operation failed, it will be completed with the appropriate exception. Notable Exceptions:\n+     * {@link StorageMetadataException} Exception related to storage metadata operations.\n      */\n-    void delete(MetadataTransaction txn, String key) throws StorageMetadataException;\n+    void delete(MetadataTransaction txn, String key);\n \n     /**\n      * Commits given transaction.\n-     * If  skipStoreCheck is set to true then the transaction data is validated without realoding.\n+     * If  skipStoreCheck is set to true then the transaction data is validated without reloading.\n      * This call blocks until write to underlying storage is confirmed. This helps avoid circular dependency on storage\n      * system segments.\n-     * If lazyWrite is true then the transaction data is validated but the changes are not commited to underlying storage.\n+     * If lazyWrite is true then the transaction data is validated but the changes are not committed to underlying storage.\n      * Changes are put in the in memory buffer only. Note that in case of crash, the changes in the in buffer are lost.\n-     * In this case the state must be re-created using application specific recovery/failover logic.\n+     * In this case the state must be re-created using application specific recovery/fail-over logic.\n      * Do not commit lazily if such recovery is not possible.\n      * This call does not blocks until write to underlying storage is confirmed if lazyWrite is true.\n      *\n      * @param txn            transaction to commit.\n      * @param lazyWrite      true if data can be written lazily.\n      * @param skipStoreCheck true if data is not to be reloaded from store.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @throws CompletionException If the operation failed, it will be completed with the appropriate exception. Notable Exceptions:\n+     * {@link StorageMetadataException} If transaction can not be committed.\n      */\n-    void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException;\n+    CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIwNzc1MA=="}, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 124}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIyODEzNDU2OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MultiKeyReaderWriterScheduler.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0zMFQxNjowNTo1OFrOHrV36w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxNjowOToyMVrOHsLEjQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIwOTE5NQ==", "bodyText": "What are you synchronizing this on? Why does it need to be different than keysToLock?", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r515209195", "createdAt": "2020-10-30T16:05:58Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MultiKeyReaderWriterScheduler.java", "diffHunk": "@@ -0,0 +1,196 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import lombok.Getter;\n+import lombok.RequiredArgsConstructor;\n+import lombok.val;\n+\n+import javax.annotation.concurrent.GuardedBy;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.concurrent.CompletableFuture;\n+\n+/**\n+ * A scheduler utility that implements pattern similar to Multiple Readers - Single Writer pattern.\n+ */\n+public class MultiKeyReaderWriterScheduler {\n+    /**\n+     * Scheduler data for each key.\n+     */\n+    @GuardedBy(\"keyToDataMap\")\n+    private final HashMap<String, SchedulerData> keyToDataMap = new HashMap<>();\n+\n+    /**\n+     * Scheduler data for each key.\n+     */\n+    private static class SchedulerData {\n+        int count;\n+        CompletableFuture blockingFuture = CompletableFuture.completedFuture(null);\n+        ArrayList<CompletableFuture> readerFutures = new ArrayList<>();\n+    }\n+\n+    /**\n+     * Represents a lock.\n+     */\n+    @RequiredArgsConstructor\n+    static class MultiKeyReaderWriterAsyncLock {\n+        /**\n+         * Keys to synchronize on.\n+         */\n+        @Getter\n+        private final String[] keys;\n+\n+        /**\n+         * Indicates whether the lock is a reader lock or a writer lock.\n+         */\n+        @Getter\n+        private final boolean isReadonly;\n+\n+        /**\n+         * Reference to the scheduler.\n+         */\n+        private final MultiKeyReaderWriterScheduler scheduler;\n+\n+        /**\n+         * The future is completed when all keys for this lock become available.\n+         */\n+        private CompletableFuture<Void> readyFuture;\n+\n+        /**\n+         * The future which is completed when lock is released.\n+         */\n+        @Getter\n+        private final CompletableFuture<Void> doneFuture = new CompletableFuture<>();\n+\n+        /**\n+         * Returns a CompletableFuture that will be completed when the lock is obtained.\n+         * @return CompletableFuture which will be complete when the lock is obtained.\n+         */\n+        CompletableFuture<Void> lock() {\n+            if (isReadonly) {\n+                return scheduler.scheduleForRead(this);\n+            } else {\n+                return scheduler.scheduleForWrite(this);\n+            }\n+        }\n+\n+        /**\n+         * Releases the lock.\n+         */\n+        void unlock() {\n+            scheduler.release(this);\n+        }\n+    }\n+\n+    /**\n+     * Adds a reference for the key.\n+     */\n+    private SchedulerData addReference(String key) {\n+        synchronized (keyToDataMap) {\n+            SchedulerData schedulerData = keyToDataMap.get(key);\n+            // Add if this is a new key.\n+            if (null == schedulerData) {\n+                schedulerData = new SchedulerData();\n+                keyToDataMap.put(key, schedulerData);\n+            }\n+            // Increment ref count.\n+            schedulerData.count++;\n+            return schedulerData;\n+        }\n+    }\n+\n+    /**\n+     * Releases a reference for the key.\n+     */\n+    private void releaseReference(String key) {\n+        synchronized (keyToDataMap) {\n+            SchedulerData schedulerData = keyToDataMap.get(key);\n+            // Decrement ref count.\n+            schedulerData.count--;\n+            // clean up if required.\n+            if (0 == schedulerData.count) {\n+                keyToDataMap.remove(key);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Gets a read lock over given set of keys.\n+     */\n+    MultiKeyReaderWriterAsyncLock getReadLock(String[] keys) {\n+        return new MultiKeyReaderWriterAsyncLock(keys, true, this);\n+    }\n+\n+    /**\n+     * Gets a write lock over given set of keys.\n+     */\n+    MultiKeyReaderWriterAsyncLock getWriteLock(String[] keys) {\n+        return new MultiKeyReaderWriterAsyncLock(keys, false, this);\n+    }\n+\n+    /**\n+     * Schedules the lock for read.\n+     */\n+    private synchronized CompletableFuture<Void> scheduleForRead(MultiKeyReaderWriterAsyncLock lock) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 142}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjA4MDc4MQ==", "bodyText": "fixed", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r516080781", "createdAt": "2020-11-02T16:09:21Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/MultiKeyReaderWriterScheduler.java", "diffHunk": "@@ -0,0 +1,196 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.metadata;\n+\n+import lombok.Getter;\n+import lombok.RequiredArgsConstructor;\n+import lombok.val;\n+\n+import javax.annotation.concurrent.GuardedBy;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.concurrent.CompletableFuture;\n+\n+/**\n+ * A scheduler utility that implements pattern similar to Multiple Readers - Single Writer pattern.\n+ */\n+public class MultiKeyReaderWriterScheduler {\n+    /**\n+     * Scheduler data for each key.\n+     */\n+    @GuardedBy(\"keyToDataMap\")\n+    private final HashMap<String, SchedulerData> keyToDataMap = new HashMap<>();\n+\n+    /**\n+     * Scheduler data for each key.\n+     */\n+    private static class SchedulerData {\n+        int count;\n+        CompletableFuture blockingFuture = CompletableFuture.completedFuture(null);\n+        ArrayList<CompletableFuture> readerFutures = new ArrayList<>();\n+    }\n+\n+    /**\n+     * Represents a lock.\n+     */\n+    @RequiredArgsConstructor\n+    static class MultiKeyReaderWriterAsyncLock {\n+        /**\n+         * Keys to synchronize on.\n+         */\n+        @Getter\n+        private final String[] keys;\n+\n+        /**\n+         * Indicates whether the lock is a reader lock or a writer lock.\n+         */\n+        @Getter\n+        private final boolean isReadonly;\n+\n+        /**\n+         * Reference to the scheduler.\n+         */\n+        private final MultiKeyReaderWriterScheduler scheduler;\n+\n+        /**\n+         * The future is completed when all keys for this lock become available.\n+         */\n+        private CompletableFuture<Void> readyFuture;\n+\n+        /**\n+         * The future which is completed when lock is released.\n+         */\n+        @Getter\n+        private final CompletableFuture<Void> doneFuture = new CompletableFuture<>();\n+\n+        /**\n+         * Returns a CompletableFuture that will be completed when the lock is obtained.\n+         * @return CompletableFuture which will be complete when the lock is obtained.\n+         */\n+        CompletableFuture<Void> lock() {\n+            if (isReadonly) {\n+                return scheduler.scheduleForRead(this);\n+            } else {\n+                return scheduler.scheduleForWrite(this);\n+            }\n+        }\n+\n+        /**\n+         * Releases the lock.\n+         */\n+        void unlock() {\n+            scheduler.release(this);\n+        }\n+    }\n+\n+    /**\n+     * Adds a reference for the key.\n+     */\n+    private SchedulerData addReference(String key) {\n+        synchronized (keyToDataMap) {\n+            SchedulerData schedulerData = keyToDataMap.get(key);\n+            // Add if this is a new key.\n+            if (null == schedulerData) {\n+                schedulerData = new SchedulerData();\n+                keyToDataMap.put(key, schedulerData);\n+            }\n+            // Increment ref count.\n+            schedulerData.count++;\n+            return schedulerData;\n+        }\n+    }\n+\n+    /**\n+     * Releases a reference for the key.\n+     */\n+    private void releaseReference(String key) {\n+        synchronized (keyToDataMap) {\n+            SchedulerData schedulerData = keyToDataMap.get(key);\n+            // Decrement ref count.\n+            schedulerData.count--;\n+            // clean up if required.\n+            if (0 == schedulerData.count) {\n+                keyToDataMap.remove(key);\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Gets a read lock over given set of keys.\n+     */\n+    MultiKeyReaderWriterAsyncLock getReadLock(String[] keys) {\n+        return new MultiKeyReaderWriterAsyncLock(keys, true, this);\n+    }\n+\n+    /**\n+     * Gets a write lock over given set of keys.\n+     */\n+    MultiKeyReaderWriterAsyncLock getWriteLock(String[] keys) {\n+        return new MultiKeyReaderWriterAsyncLock(keys, false, this);\n+    }\n+\n+    /**\n+     * Schedules the lock for read.\n+     */\n+    private synchronized CompletableFuture<Void> scheduleForRead(MultiKeyReaderWriterAsyncLock lock) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNTIwOTE5NQ=="}, "originalCommit": {"oid": "c6be777f67dd855512f0bdfbf4a21e69c2939104"}, "originalPosition": 142}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTU3NTY2OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/StreamSegmentContainer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwMDo1MDowOFrOHucNqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjoyODoxNlrOHwEI1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ1ODc5Mw==", "bodyText": "debug for both of these. We only need them for debugging; otherwise we will pollute the logs every time we boot up the container for your average user.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r518458793", "createdAt": "2020-11-06T00:50:08Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/StreamSegmentContainer.java", "diffHunk": "@@ -190,21 +206,21 @@ private MetadataStore createMetadataStore() {\n      *\n      * @throws Exception\n      */\n-    private void initializeStorage() throws Exception {\n+    private CompletableFuture<Void> initializeStorage() throws Exception {\n+        log.info(\"{}: Storage initialization started.\", this.traceObjectId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MTQ5NQ==", "bodyText": "done", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520161495", "createdAt": "2020-11-09T22:28:16Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/StreamSegmentContainer.java", "diffHunk": "@@ -190,21 +206,21 @@ private MetadataStore createMetadataStore() {\n      *\n      * @throws Exception\n      */\n-    private void initializeStorage() throws Exception {\n+    private CompletableFuture<Void> initializeStorage() throws Exception {\n+        log.info(\"{}: Storage initialization started.\", this.traceObjectId);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ1ODc5Mw=="}, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTU3NTk3OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/StreamSegmentContainer.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwMDo1MDoyMVrOHucN4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjoyODoyN1rOHwEJIQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ1ODg0OQ==", "bodyText": "thenRunAsync", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r518458849", "createdAt": "2020-11-06T00:50:21Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/StreamSegmentContainer.java", "diffHunk": "@@ -190,21 +206,21 @@ private MetadataStore createMetadataStore() {\n      *\n      * @throws Exception\n      */\n-    private void initializeStorage() throws Exception {\n+    private CompletableFuture<Void> initializeStorage() throws Exception {\n+        log.info(\"{}: Storage initialization started.\", this.traceObjectId);\n         this.storage.initialize(this.metadata.getContainerEpoch());\n \n         if (this.storage instanceof ChunkedSegmentStorage) {\n             ChunkedSegmentStorage chunkedStorage = (ChunkedSegmentStorage) this.storage;\n \n-            // Initialize storage metadata table segment\n-            ContainerTableExtension tableExtension = getExtension(ContainerTableExtension.class);\n-            String s = NameUtils.getStorageMetadataSegmentName(this.metadata.getContainerId());\n-\n-            val metadata = new TableBasedMetadataStore(s, tableExtension);\n-\n             // Bootstrap\n-            chunkedStorage.bootstrap(this.metadata.getContainerId(), metadata);\n+            return chunkedStorage.bootstrap()\n+                    .thenApplyAsync( v -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ1OTMzMg==", "bodyText": "Alternatively you can delete this whole callback and do a simple log.debug at line 304, saying \"Initializing Metadata Store\". That will imply both of these and it won't look like we're over-logging in some areas while neglecting others.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r518459332", "createdAt": "2020-11-06T00:51:58Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/StreamSegmentContainer.java", "diffHunk": "@@ -190,21 +206,21 @@ private MetadataStore createMetadataStore() {\n      *\n      * @throws Exception\n      */\n-    private void initializeStorage() throws Exception {\n+    private CompletableFuture<Void> initializeStorage() throws Exception {\n+        log.info(\"{}: Storage initialization started.\", this.traceObjectId);\n         this.storage.initialize(this.metadata.getContainerEpoch());\n \n         if (this.storage instanceof ChunkedSegmentStorage) {\n             ChunkedSegmentStorage chunkedStorage = (ChunkedSegmentStorage) this.storage;\n \n-            // Initialize storage metadata table segment\n-            ContainerTableExtension tableExtension = getExtension(ContainerTableExtension.class);\n-            String s = NameUtils.getStorageMetadataSegmentName(this.metadata.getContainerId());\n-\n-            val metadata = new TableBasedMetadataStore(s, tableExtension);\n-\n             // Bootstrap\n-            chunkedStorage.bootstrap(this.metadata.getContainerId(), metadata);\n+            return chunkedStorage.bootstrap()\n+                    .thenApplyAsync( v -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ1ODg0OQ=="}, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MTU2OQ==", "bodyText": "done", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520161569", "createdAt": "2020-11-09T22:28:27Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/StreamSegmentContainer.java", "diffHunk": "@@ -190,21 +206,21 @@ private MetadataStore createMetadataStore() {\n      *\n      * @throws Exception\n      */\n-    private void initializeStorage() throws Exception {\n+    private CompletableFuture<Void> initializeStorage() throws Exception {\n+        log.info(\"{}: Storage initialization started.\", this.traceObjectId);\n         this.storage.initialize(this.metadata.getContainerEpoch());\n \n         if (this.storage instanceof ChunkedSegmentStorage) {\n             ChunkedSegmentStorage chunkedStorage = (ChunkedSegmentStorage) this.storage;\n \n-            // Initialize storage metadata table segment\n-            ContainerTableExtension tableExtension = getExtension(ContainerTableExtension.class);\n-            String s = NameUtils.getStorageMetadataSegmentName(this.metadata.getContainerId());\n-\n-            val metadata = new TableBasedMetadataStore(s, tableExtension);\n-\n             // Bootstrap\n-            chunkedStorage.bootstrap(this.metadata.getContainerId(), metadata);\n+            return chunkedStorage.bootstrap()\n+                    .thenApplyAsync( v -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ1ODg0OQ=="}, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTU4MDYxOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/SimpleStorageFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwMDo1Mjo0NVrOHucQpg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjozMDo0MlrOHwENCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ1OTU1OA==", "bodyText": "What exactly is a simple storage adapter?", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r518459558", "createdAt": "2020-11-06T00:52:45Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/SimpleStorageFactory.java", "diffHunk": "@@ -0,0 +1,32 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage;\n+\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+\n+import java.util.concurrent.Executor;\n+\n+/**\n+ * Defines a Factory for Simple Storage Adapters.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MjU2OA==", "bodyText": "fixed", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520162568", "createdAt": "2020-11-09T22:30:42Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/SimpleStorageFactory.java", "diffHunk": "@@ -0,0 +1,32 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage;\n+\n+import io.pravega.segmentstore.storage.metadata.ChunkMetadataStore;\n+\n+import java.util.concurrent.Executor;\n+\n+/**\n+ * Defines a Factory for Simple Storage Adapters.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ1OTU1OA=="}, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTU4Njg2OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/AsyncBaseChunkStorage.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwMDo1NjowMFrOHucUZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjoyNzozNlrOHwEHsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MDUxOA==", "bodyText": "Just put this line of code inside the metrics future callback . No need to create yet another callback.\nIf you do that you can even get rid of the if(log.isTraceEnabled())", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r518460518", "createdAt": "2020-11-06T00:56:00Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/AsyncBaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,650 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ *\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ *\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkedSegmentStorage will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ *\n+ * The implementations in this repository are tested using following test suites.\n+ * <ul>\n+ * <li>SimpleStorageTests</li>\n+ * <li>ChunkedRollingStorageTests</li>\n+ * <li>ChunkStorageTests</li>\n+ * <li>SystemJournalTests</li>\n+ * </ul>\n+ */\n+@Slf4j\n+@Beta\n+public abstract class AsyncBaseChunkStorage implements ChunkStorage {\n+\n+    private final AtomicBoolean closed;\n+\n+    private final Executor executor;\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param executor  An Executor for async operations.\n+     */\n+    public AsyncBaseChunkStorage(Executor executor) {\n+        this.closed = new AtomicBoolean(false);\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return A CompletableFuture that, when completed, will contain True if the object exists, False otherwise.\n+     */\n+    @Override\n+    final public CompletableFuture<Boolean> exists(String chunkName) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        checkChunkName(chunkName);\n+\n+        val traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+        // Call concrete implementation.\n+        val returnFuture = checkExistsAsync(chunkName);\n+        if (log.isTraceEnabled()) {\n+            returnFuture.thenAcceptAsync(retValue -> LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName), executor);\n+        }\n+\n+        return returnFuture;\n+    }\n+\n+    /**\n+     * Creates a new chunk.\n+     *\n+     * @param chunkName Name of the chunk to create.\n+     * @return A CompletableFuture that, when completed, will contain a writable handle for the recently created chunk.\n+     * If the operation failed, it will be completed with the appropriate exception. Notable Exceptions:\n+     * {@link ChunkStorageException} In case of I/O related exceptions.\n+     */\n+    @Override\n+    final public CompletableFuture<ChunkHandle> create(String chunkName) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        checkChunkName(chunkName);\n+\n+        val traceId = LoggerHelpers.traceEnter(log, \"create\", chunkName);\n+        val timer = new Timer();\n+\n+        // Call concrete implementation.\n+        val returnFuture = doCreateAsync(chunkName);\n+        val metricsFuture = returnFuture.thenAcceptAsync(handle -> {\n+            // Record metrics.\n+            val elapsed = timer.getElapsed();\n+            ChunkStorageMetrics.CREATE_LATENCY.reportSuccessEvent(elapsed);\n+            ChunkStorageMetrics.CREATE_COUNT.inc();\n+            log.debug(\"Create - chunk={}, latency={}.\", chunkName, elapsed.toMillis());\n+        }, executor);\n+        if (log.isTraceEnabled()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MDU4MQ==", "bodyText": "Same everywhere below.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r518460581", "createdAt": "2020-11-06T00:56:11Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/AsyncBaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,650 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ *\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ *\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkedSegmentStorage will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ *\n+ * The implementations in this repository are tested using following test suites.\n+ * <ul>\n+ * <li>SimpleStorageTests</li>\n+ * <li>ChunkedRollingStorageTests</li>\n+ * <li>ChunkStorageTests</li>\n+ * <li>SystemJournalTests</li>\n+ * </ul>\n+ */\n+@Slf4j\n+@Beta\n+public abstract class AsyncBaseChunkStorage implements ChunkStorage {\n+\n+    private final AtomicBoolean closed;\n+\n+    private final Executor executor;\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param executor  An Executor for async operations.\n+     */\n+    public AsyncBaseChunkStorage(Executor executor) {\n+        this.closed = new AtomicBoolean(false);\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return A CompletableFuture that, when completed, will contain True if the object exists, False otherwise.\n+     */\n+    @Override\n+    final public CompletableFuture<Boolean> exists(String chunkName) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        checkChunkName(chunkName);\n+\n+        val traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+        // Call concrete implementation.\n+        val returnFuture = checkExistsAsync(chunkName);\n+        if (log.isTraceEnabled()) {\n+            returnFuture.thenAcceptAsync(retValue -> LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName), executor);\n+        }\n+\n+        return returnFuture;\n+    }\n+\n+    /**\n+     * Creates a new chunk.\n+     *\n+     * @param chunkName Name of the chunk to create.\n+     * @return A CompletableFuture that, when completed, will contain a writable handle for the recently created chunk.\n+     * If the operation failed, it will be completed with the appropriate exception. Notable Exceptions:\n+     * {@link ChunkStorageException} In case of I/O related exceptions.\n+     */\n+    @Override\n+    final public CompletableFuture<ChunkHandle> create(String chunkName) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        checkChunkName(chunkName);\n+\n+        val traceId = LoggerHelpers.traceEnter(log, \"create\", chunkName);\n+        val timer = new Timer();\n+\n+        // Call concrete implementation.\n+        val returnFuture = doCreateAsync(chunkName);\n+        val metricsFuture = returnFuture.thenAcceptAsync(handle -> {\n+            // Record metrics.\n+            val elapsed = timer.getElapsed();\n+            ChunkStorageMetrics.CREATE_LATENCY.reportSuccessEvent(elapsed);\n+            ChunkStorageMetrics.CREATE_COUNT.inc();\n+            log.debug(\"Create - chunk={}, latency={}.\", chunkName, elapsed.toMillis());\n+        }, executor);\n+        if (log.isTraceEnabled()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MDUxOA=="}, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MTIwMg==", "bodyText": "done", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520161202", "createdAt": "2020-11-09T22:27:36Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/AsyncBaseChunkStorage.java", "diffHunk": "@@ -0,0 +1,650 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.storage.chunklayer;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import com.google.common.base.Strings;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.LoggerHelpers;\n+import io.pravega.common.Timer;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.InputStream;\n+import java.util.concurrent.Callable;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.CompletionException;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+\n+/**\n+ * Base implementation of {@link ChunkStorage}.\n+ * It implements common functionality that can be used by derived classes.\n+ * Delegates to specific implementations by calling various abstract methods which must be overridden in derived classes.\n+ *\n+ * Below are minimum requirements that any implementation must provide.\n+ * Note that it is the responsibility of storage provider specific implementation to make sure following guarantees are provided even\n+ * though underlying storage may not provide all primitives or guarantees.\n+ * <ul>\n+ * <li>Once an operation is executed and acknowledged as successful then the effects must be permanent and consistent (as opposed to eventually consistent)</li>\n+ * <li>{@link ChunkStorage#create(String)}  and {@link ChunkStorage#delete(ChunkHandle)} are not idempotent.</li>\n+ * <li>{@link ChunkStorage#exists(String)} and {@link ChunkStorage#getInfo(String)} must reflect effects of most recent operation performed.</li>\n+ * </ul>\n+ *\n+ * There are a few different capabilities that ChunkStorage may provide.\n+ * <ul>\n+ * <li> Does {@link ChunkStorage} support appending to existing chunks?\n+ * This is indicated by {@link ChunkStorage#supportsAppend()}. For example S3 compatible Chunk Storage this would return false. </li>\n+ * <li> Does {@link ChunkStorage}  support for concatenating chunks? This is indicated by {@link ChunkStorage#supportsConcat()}.\n+ * If this is true then concat operation concat will be invoked otherwise append functionality is invoked.</li>\n+ * <li>In addition {@link ChunkStorage} may provide ability to truncate chunks at given offsets (either at front end or at tail end). This is indicated by {@link ChunkStorage#supportsTruncation()}. </li>\n+ * </ul>\n+ * There are some obvious constraints - If ChunkStorage supports concat but not natively then it must support append .\n+ *\n+ * For concats, {@link ChunkStorage} supports both native and append, ChunkedSegmentStorage will invoke appropriate method depending on size of target and source chunks. (Eg. ECS)\n+ *\n+ * The implementations in this repository are tested using following test suites.\n+ * <ul>\n+ * <li>SimpleStorageTests</li>\n+ * <li>ChunkedRollingStorageTests</li>\n+ * <li>ChunkStorageTests</li>\n+ * <li>SystemJournalTests</li>\n+ * </ul>\n+ */\n+@Slf4j\n+@Beta\n+public abstract class AsyncBaseChunkStorage implements ChunkStorage {\n+\n+    private final AtomicBoolean closed;\n+\n+    private final Executor executor;\n+\n+    /**\n+     * Constructor.\n+     *\n+     * @param executor  An Executor for async operations.\n+     */\n+    public AsyncBaseChunkStorage(Executor executor) {\n+        this.closed = new AtomicBoolean(false);\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n+    }\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports truncate operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsTruncation();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports append operation on chunks.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsAppend();\n+\n+    /**\n+     * Gets a value indicating whether this Storage implementation supports merge operation either natively or through appends.\n+     *\n+     * @return True or false.\n+     */\n+    @Override\n+    abstract public boolean supportsConcat();\n+\n+    /**\n+     * Determines whether named file/object exists in underlying storage.\n+     *\n+     * @param chunkName Name of the chunk to check.\n+     * @return A CompletableFuture that, when completed, will contain True if the object exists, False otherwise.\n+     */\n+    @Override\n+    final public CompletableFuture<Boolean> exists(String chunkName) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        checkChunkName(chunkName);\n+\n+        val traceId = LoggerHelpers.traceEnter(log, \"exists\", chunkName);\n+        // Call concrete implementation.\n+        val returnFuture = checkExistsAsync(chunkName);\n+        if (log.isTraceEnabled()) {\n+            returnFuture.thenAcceptAsync(retValue -> LoggerHelpers.traceLeave(log, \"exists\", traceId, chunkName), executor);\n+        }\n+\n+        return returnFuture;\n+    }\n+\n+    /**\n+     * Creates a new chunk.\n+     *\n+     * @param chunkName Name of the chunk to create.\n+     * @return A CompletableFuture that, when completed, will contain a writable handle for the recently created chunk.\n+     * If the operation failed, it will be completed with the appropriate exception. Notable Exceptions:\n+     * {@link ChunkStorageException} In case of I/O related exceptions.\n+     */\n+    @Override\n+    final public CompletableFuture<ChunkHandle> create(String chunkName) {\n+        Exceptions.checkNotClosed(this.closed.get(), this);\n+        // Validate parameters\n+        checkChunkName(chunkName);\n+\n+        val traceId = LoggerHelpers.traceEnter(log, \"create\", chunkName);\n+        val timer = new Timer();\n+\n+        // Call concrete implementation.\n+        val returnFuture = doCreateAsync(chunkName);\n+        val metricsFuture = returnFuture.thenAcceptAsync(handle -> {\n+            // Record metrics.\n+            val elapsed = timer.getElapsed();\n+            ChunkStorageMetrics.CREATE_LATENCY.reportSuccessEvent(elapsed);\n+            ChunkStorageMetrics.CREATE_COUNT.inc();\n+            log.debug(\"Create - chunk={}, latency={}.\", chunkName, elapsed.toMillis());\n+        }, executor);\n+        if (log.isTraceEnabled()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MDUxOA=="}, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 152}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTU5NTk1OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwMTowMDozN1rOHucZrQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwMTowMToxOVrOHucaog==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MTg2OQ==", "bodyText": "You can rewrite this as\nif(fenced.get()){\nretval = Futures.failedFuture(new StorageMetadataWritesFencedOutException(...)) ;\n}else{\n .. put the `thenComposeAsync` body in here\n}", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r518461869", "createdAt": "2020-11-06T01:00:37Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 230}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MjExNA==", "bodyText": "This may require a bit more fiddling around with that whenComplete, but at least it should look a bit better.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r518462114", "createdAt": "2020-11-06T01:01:19Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MTg2OQ=="}, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 230}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTU5OTA1OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwMTowMjowOFrOHucbbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjoyNzoxOFrOHwEHEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MjMxOA==", "bodyText": "runAsync if you do not take any args and not return anything. Looks prettier.\nPlease sweep your code to use thenRunAsync, thenAcceptAsync or thenApplyAsync as appropriate.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r518462318", "createdAt": "2020-11-06T01:02:08Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+            if (fenced.get()) {\n+                throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));\n+            }\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Mark keys in transaction as active to prevent their eviction.\n+                    txn.getData().keySet().forEach(this::addToActiveKeySet);\n+\n+                    // Acquire a write lock over segment.\n+                    val tLock = new Timer();\n+                    log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));\n+                    val writeLock = scheduler.getWriteLock(txn.getKeysToLock());\n+                    return writeLock.lock()\n+                            .thenComposeAsync(v0 -> {\n+                                // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+                                // This step is kind of thread safe\n+                                val elapsed = tLock.getElapsed();\n+                                WRITE_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                                log.debug(\"Acquired write lock for {}, wait time: {} ms\",\n+                                        String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                                return loadMissingKeys(txn, skipStoreCheck, txnData);\n+                            }, executor)\n+                            .thenComposeAsync(v1 -> {\n+                                // This check needs to be atomic, with absolutely no possibility of re-entry\n+                                return performCommit(txn, lazyWrite, txnData, modifiedKeys, modifiedValues);\n+                            }, executor)\n+                            .whenCompleteAsync((v2, ex) -> writeLock.unlock(), executor);\n+                }, executor)\n+                .thenRunAsync(() -> {\n+                    //  Step 5 : evict if required.\n+                    txn.setCommitted();\n+                    txnData.clear();\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    // Remove keys from active set.\n+                    txn.getData().keySet().forEach(this::removeFromActiveKeySet);\n+                    COMMIT_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+\n+        // Trigger evict\n+        retValue.thenComposeAsync(v4 -> {\n+            //  Step 6 : evict if required.\n+            return evictIfNeeded();\n+        }, executor);\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        return retValue;\n+    }\n \n-        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n-        // This step is kind of thread safe\n+    /**\n+     * Loads missing keys.\n+     */\n+    private CompletableFuture<Void> loadMissingKeys(MetadataTransaction txn, boolean skipStoreCheck, Map<String, TransactionData> txnData) {\n+        val loadFutures = new ArrayList<CompletableFuture<TransactionData>>();\n         for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-            String key = entry.getKey();\n+            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+            val key = entry.getKey();\n             if (skipStoreCheck || entry.getValue().isPinned()) {\n                 log.trace(\"Skipping loading key from the store key = {}\", key);\n             } else {\n                 // This check is safe to be outside the lock\n-                if (!bufferedTxnData.containsKey(key)) {\n-                    loadFromStore(key);\n+                val dataFromBuffer = bufferedTxnData.get(key);\n+                if (null == dataFromBuffer) {\n+                    loadFutures.add(loadFromStore(txn, key, true));\n                 }\n             }\n         }\n-        // Step 2 : Check whether transaction is safe to commit.\n-        // This check needs to be atomic, with absolutely no possibility of re-entry\n-        synchronized (lock) {\n-            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-                String key = entry.getKey();\n-                val transactionData = entry.getValue();\n-                Preconditions.checkState(null != transactionData.getKey());\n-\n-                // See if this entry was modified in this transaction.\n-                if (transactionData.getVersion() == txn.getVersion()) {\n-                    modifiedKeys.add(key);\n-                    transactionData.setPersisted(false);\n-                    modifiedValues.add(transactionData);\n-                }\n-                // make sure none of the keys used in this transaction have changed.\n-                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n-                if (null != dataFromBuffer) {\n-                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n-                        throw new StorageMetadataVersionMismatchException(\n-                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n-                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+        return Futures.allOf(loadFutures)\n+                .thenApplyAsync(v4 -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 326}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MTA0MQ==", "bodyText": "done", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520161041", "createdAt": "2020-11-09T22:27:18Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+            if (fenced.get()) {\n+                throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));\n+            }\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Mark keys in transaction as active to prevent their eviction.\n+                    txn.getData().keySet().forEach(this::addToActiveKeySet);\n+\n+                    // Acquire a write lock over segment.\n+                    val tLock = new Timer();\n+                    log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));\n+                    val writeLock = scheduler.getWriteLock(txn.getKeysToLock());\n+                    return writeLock.lock()\n+                            .thenComposeAsync(v0 -> {\n+                                // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+                                // This step is kind of thread safe\n+                                val elapsed = tLock.getElapsed();\n+                                WRITE_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                                log.debug(\"Acquired write lock for {}, wait time: {} ms\",\n+                                        String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                                return loadMissingKeys(txn, skipStoreCheck, txnData);\n+                            }, executor)\n+                            .thenComposeAsync(v1 -> {\n+                                // This check needs to be atomic, with absolutely no possibility of re-entry\n+                                return performCommit(txn, lazyWrite, txnData, modifiedKeys, modifiedValues);\n+                            }, executor)\n+                            .whenCompleteAsync((v2, ex) -> writeLock.unlock(), executor);\n+                }, executor)\n+                .thenRunAsync(() -> {\n+                    //  Step 5 : evict if required.\n+                    txn.setCommitted();\n+                    txnData.clear();\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    // Remove keys from active set.\n+                    txn.getData().keySet().forEach(this::removeFromActiveKeySet);\n+                    COMMIT_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+\n+        // Trigger evict\n+        retValue.thenComposeAsync(v4 -> {\n+            //  Step 6 : evict if required.\n+            return evictIfNeeded();\n+        }, executor);\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        return retValue;\n+    }\n \n-        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n-        // This step is kind of thread safe\n+    /**\n+     * Loads missing keys.\n+     */\n+    private CompletableFuture<Void> loadMissingKeys(MetadataTransaction txn, boolean skipStoreCheck, Map<String, TransactionData> txnData) {\n+        val loadFutures = new ArrayList<CompletableFuture<TransactionData>>();\n         for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-            String key = entry.getKey();\n+            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+            val key = entry.getKey();\n             if (skipStoreCheck || entry.getValue().isPinned()) {\n                 log.trace(\"Skipping loading key from the store key = {}\", key);\n             } else {\n                 // This check is safe to be outside the lock\n-                if (!bufferedTxnData.containsKey(key)) {\n-                    loadFromStore(key);\n+                val dataFromBuffer = bufferedTxnData.get(key);\n+                if (null == dataFromBuffer) {\n+                    loadFutures.add(loadFromStore(txn, key, true));\n                 }\n             }\n         }\n-        // Step 2 : Check whether transaction is safe to commit.\n-        // This check needs to be atomic, with absolutely no possibility of re-entry\n-        synchronized (lock) {\n-            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-                String key = entry.getKey();\n-                val transactionData = entry.getValue();\n-                Preconditions.checkState(null != transactionData.getKey());\n-\n-                // See if this entry was modified in this transaction.\n-                if (transactionData.getVersion() == txn.getVersion()) {\n-                    modifiedKeys.add(key);\n-                    transactionData.setPersisted(false);\n-                    modifiedValues.add(transactionData);\n-                }\n-                // make sure none of the keys used in this transaction have changed.\n-                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n-                if (null != dataFromBuffer) {\n-                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n-                        throw new StorageMetadataVersionMismatchException(\n-                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n-                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+        return Futures.allOf(loadFutures)\n+                .thenApplyAsync(v4 -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MjMxOA=="}, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 326}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTYwMDc4OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwMTowMzowMVrOHucchQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMzoyNzoxNFrOHwFowg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MjU5Nw==", "bodyText": "Can you please put a message with all these exceptions. If they fire and you see a stack in the logs you won't know where it came from. Put as much info in these messages as you can. It WILL help you in debugging these in the field.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r518462597", "createdAt": "2020-11-06T01:03:01Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+            if (fenced.get()) {\n+                throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));\n+            }\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Mark keys in transaction as active to prevent their eviction.\n+                    txn.getData().keySet().forEach(this::addToActiveKeySet);\n+\n+                    // Acquire a write lock over segment.\n+                    val tLock = new Timer();\n+                    log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));\n+                    val writeLock = scheduler.getWriteLock(txn.getKeysToLock());\n+                    return writeLock.lock()\n+                            .thenComposeAsync(v0 -> {\n+                                // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+                                // This step is kind of thread safe\n+                                val elapsed = tLock.getElapsed();\n+                                WRITE_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                                log.debug(\"Acquired write lock for {}, wait time: {} ms\",\n+                                        String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                                return loadMissingKeys(txn, skipStoreCheck, txnData);\n+                            }, executor)\n+                            .thenComposeAsync(v1 -> {\n+                                // This check needs to be atomic, with absolutely no possibility of re-entry\n+                                return performCommit(txn, lazyWrite, txnData, modifiedKeys, modifiedValues);\n+                            }, executor)\n+                            .whenCompleteAsync((v2, ex) -> writeLock.unlock(), executor);\n+                }, executor)\n+                .thenRunAsync(() -> {\n+                    //  Step 5 : evict if required.\n+                    txn.setCommitted();\n+                    txnData.clear();\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    // Remove keys from active set.\n+                    txn.getData().keySet().forEach(this::removeFromActiveKeySet);\n+                    COMMIT_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+\n+        // Trigger evict\n+        retValue.thenComposeAsync(v4 -> {\n+            //  Step 6 : evict if required.\n+            return evictIfNeeded();\n+        }, executor);\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        return retValue;\n+    }\n \n-        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n-        // This step is kind of thread safe\n+    /**\n+     * Loads missing keys.\n+     */\n+    private CompletableFuture<Void> loadMissingKeys(MetadataTransaction txn, boolean skipStoreCheck, Map<String, TransactionData> txnData) {\n+        val loadFutures = new ArrayList<CompletableFuture<TransactionData>>();\n         for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-            String key = entry.getKey();\n+            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+            val key = entry.getKey();\n             if (skipStoreCheck || entry.getValue().isPinned()) {\n                 log.trace(\"Skipping loading key from the store key = {}\", key);\n             } else {\n                 // This check is safe to be outside the lock\n-                if (!bufferedTxnData.containsKey(key)) {\n-                    loadFromStore(key);\n+                val dataFromBuffer = bufferedTxnData.get(key);\n+                if (null == dataFromBuffer) {\n+                    loadFutures.add(loadFromStore(txn, key, true));\n                 }\n             }\n         }\n-        // Step 2 : Check whether transaction is safe to commit.\n-        // This check needs to be atomic, with absolutely no possibility of re-entry\n-        synchronized (lock) {\n-            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-                String key = entry.getKey();\n-                val transactionData = entry.getValue();\n-                Preconditions.checkState(null != transactionData.getKey());\n-\n-                // See if this entry was modified in this transaction.\n-                if (transactionData.getVersion() == txn.getVersion()) {\n-                    modifiedKeys.add(key);\n-                    transactionData.setPersisted(false);\n-                    modifiedValues.add(transactionData);\n-                }\n-                // make sure none of the keys used in this transaction have changed.\n-                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n-                if (null != dataFromBuffer) {\n-                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n-                        throw new StorageMetadataVersionMismatchException(\n-                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n-                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+        return Futures.allOf(loadFutures)\n+                .thenApplyAsync(v4 -> {\n+                    // validate everything is alright.\n+                    for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                        val dataFromBuffer = bufferedTxnData.get(entry.getKey());\n+                        if (!(entry.getValue().isPinned())) {\n+                            Preconditions.checkState(activeKeys.contains(entry.getKey()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 331}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE4NjA1MA==", "bodyText": "done.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520186050", "createdAt": "2020-11-09T23:27:14Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+            if (fenced.get()) {\n+                throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));\n+            }\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Mark keys in transaction as active to prevent their eviction.\n+                    txn.getData().keySet().forEach(this::addToActiveKeySet);\n+\n+                    // Acquire a write lock over segment.\n+                    val tLock = new Timer();\n+                    log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));\n+                    val writeLock = scheduler.getWriteLock(txn.getKeysToLock());\n+                    return writeLock.lock()\n+                            .thenComposeAsync(v0 -> {\n+                                // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+                                // This step is kind of thread safe\n+                                val elapsed = tLock.getElapsed();\n+                                WRITE_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                                log.debug(\"Acquired write lock for {}, wait time: {} ms\",\n+                                        String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                                return loadMissingKeys(txn, skipStoreCheck, txnData);\n+                            }, executor)\n+                            .thenComposeAsync(v1 -> {\n+                                // This check needs to be atomic, with absolutely no possibility of re-entry\n+                                return performCommit(txn, lazyWrite, txnData, modifiedKeys, modifiedValues);\n+                            }, executor)\n+                            .whenCompleteAsync((v2, ex) -> writeLock.unlock(), executor);\n+                }, executor)\n+                .thenRunAsync(() -> {\n+                    //  Step 5 : evict if required.\n+                    txn.setCommitted();\n+                    txnData.clear();\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    // Remove keys from active set.\n+                    txn.getData().keySet().forEach(this::removeFromActiveKeySet);\n+                    COMMIT_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+\n+        // Trigger evict\n+        retValue.thenComposeAsync(v4 -> {\n+            //  Step 6 : evict if required.\n+            return evictIfNeeded();\n+        }, executor);\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        return retValue;\n+    }\n \n-        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n-        // This step is kind of thread safe\n+    /**\n+     * Loads missing keys.\n+     */\n+    private CompletableFuture<Void> loadMissingKeys(MetadataTransaction txn, boolean skipStoreCheck, Map<String, TransactionData> txnData) {\n+        val loadFutures = new ArrayList<CompletableFuture<TransactionData>>();\n         for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-            String key = entry.getKey();\n+            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+            val key = entry.getKey();\n             if (skipStoreCheck || entry.getValue().isPinned()) {\n                 log.trace(\"Skipping loading key from the store key = {}\", key);\n             } else {\n                 // This check is safe to be outside the lock\n-                if (!bufferedTxnData.containsKey(key)) {\n-                    loadFromStore(key);\n+                val dataFromBuffer = bufferedTxnData.get(key);\n+                if (null == dataFromBuffer) {\n+                    loadFutures.add(loadFromStore(txn, key, true));\n                 }\n             }\n         }\n-        // Step 2 : Check whether transaction is safe to commit.\n-        // This check needs to be atomic, with absolutely no possibility of re-entry\n-        synchronized (lock) {\n-            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-                String key = entry.getKey();\n-                val transactionData = entry.getValue();\n-                Preconditions.checkState(null != transactionData.getKey());\n-\n-                // See if this entry was modified in this transaction.\n-                if (transactionData.getVersion() == txn.getVersion()) {\n-                    modifiedKeys.add(key);\n-                    transactionData.setPersisted(false);\n-                    modifiedValues.add(transactionData);\n-                }\n-                // make sure none of the keys used in this transaction have changed.\n-                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n-                if (null != dataFromBuffer) {\n-                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n-                        throw new StorageMetadataVersionMismatchException(\n-                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n-                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+        return Futures.allOf(loadFutures)\n+                .thenApplyAsync(v4 -> {\n+                    // validate everything is alright.\n+                    for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                        val dataFromBuffer = bufferedTxnData.get(entry.getKey());\n+                        if (!(entry.getValue().isPinned())) {\n+                            Preconditions.checkState(activeKeys.contains(entry.getKey()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MjU5Nw=="}, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 331}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTYwMTcwOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwMTowMzoyNVrOHucdEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjozMTo0NlrOHwEO9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MjczOA==", "bodyText": "Where is step 1?", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r518462738", "createdAt": "2020-11-06T01:03:25Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+            if (fenced.get()) {\n+                throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));\n+            }\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Mark keys in transaction as active to prevent their eviction.\n+                    txn.getData().keySet().forEach(this::addToActiveKeySet);\n+\n+                    // Acquire a write lock over segment.\n+                    val tLock = new Timer();\n+                    log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));\n+                    val writeLock = scheduler.getWriteLock(txn.getKeysToLock());\n+                    return writeLock.lock()\n+                            .thenComposeAsync(v0 -> {\n+                                // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+                                // This step is kind of thread safe\n+                                val elapsed = tLock.getElapsed();\n+                                WRITE_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                                log.debug(\"Acquired write lock for {}, wait time: {} ms\",\n+                                        String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                                return loadMissingKeys(txn, skipStoreCheck, txnData);\n+                            }, executor)\n+                            .thenComposeAsync(v1 -> {\n+                                // This check needs to be atomic, with absolutely no possibility of re-entry\n+                                return performCommit(txn, lazyWrite, txnData, modifiedKeys, modifiedValues);\n+                            }, executor)\n+                            .whenCompleteAsync((v2, ex) -> writeLock.unlock(), executor);\n+                }, executor)\n+                .thenRunAsync(() -> {\n+                    //  Step 5 : evict if required.\n+                    txn.setCommitted();\n+                    txnData.clear();\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    // Remove keys from active set.\n+                    txn.getData().keySet().forEach(this::removeFromActiveKeySet);\n+                    COMMIT_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+\n+        // Trigger evict\n+        retValue.thenComposeAsync(v4 -> {\n+            //  Step 6 : evict if required.\n+            return evictIfNeeded();\n+        }, executor);\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        return retValue;\n+    }\n \n-        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n-        // This step is kind of thread safe\n+    /**\n+     * Loads missing keys.\n+     */\n+    private CompletableFuture<Void> loadMissingKeys(MetadataTransaction txn, boolean skipStoreCheck, Map<String, TransactionData> txnData) {\n+        val loadFutures = new ArrayList<CompletableFuture<TransactionData>>();\n         for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-            String key = entry.getKey();\n+            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+            val key = entry.getKey();\n             if (skipStoreCheck || entry.getValue().isPinned()) {\n                 log.trace(\"Skipping loading key from the store key = {}\", key);\n             } else {\n                 // This check is safe to be outside the lock\n-                if (!bufferedTxnData.containsKey(key)) {\n-                    loadFromStore(key);\n+                val dataFromBuffer = bufferedTxnData.get(key);\n+                if (null == dataFromBuffer) {\n+                    loadFutures.add(loadFromStore(txn, key, true));\n                 }\n             }\n         }\n-        // Step 2 : Check whether transaction is safe to commit.\n-        // This check needs to be atomic, with absolutely no possibility of re-entry\n-        synchronized (lock) {\n-            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-                String key = entry.getKey();\n-                val transactionData = entry.getValue();\n-                Preconditions.checkState(null != transactionData.getKey());\n-\n-                // See if this entry was modified in this transaction.\n-                if (transactionData.getVersion() == txn.getVersion()) {\n-                    modifiedKeys.add(key);\n-                    transactionData.setPersisted(false);\n-                    modifiedValues.add(transactionData);\n-                }\n-                // make sure none of the keys used in this transaction have changed.\n-                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n-                if (null != dataFromBuffer) {\n-                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n-                        throw new StorageMetadataVersionMismatchException(\n-                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n-                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+        return Futures.allOf(loadFutures)\n+                .thenApplyAsync(v4 -> {\n+                    // validate everything is alright.\n+                    for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                        val dataFromBuffer = bufferedTxnData.get(entry.getKey());\n+                        if (!(entry.getValue().isPinned())) {\n+                            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+                            Preconditions.checkState(null != dataFromBuffer);\n+                            if (!dataFromBuffer.isPinned()) {\n+                                Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                            }\n+                        }\n                     }\n+                    return null;\n+                }, executor);\n+    }\n \n-                    // Pin it if it is already pinned.\n-                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+    /**\n+     * Performs commit.\n+     */\n+    private CompletableFuture<Void> performCommit(MetadataTransaction txn, boolean lazyWrite, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        return CompletableFuture.runAsync(() -> {\n+            // Step 2 : Check whether transaction is safe to commit.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 349}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MzA2MA==", "bodyText": "above line 285", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520163060", "createdAt": "2020-11-09T22:31:46Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+            if (fenced.get()) {\n+                throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));\n+            }\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Mark keys in transaction as active to prevent their eviction.\n+                    txn.getData().keySet().forEach(this::addToActiveKeySet);\n+\n+                    // Acquire a write lock over segment.\n+                    val tLock = new Timer();\n+                    log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));\n+                    val writeLock = scheduler.getWriteLock(txn.getKeysToLock());\n+                    return writeLock.lock()\n+                            .thenComposeAsync(v0 -> {\n+                                // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+                                // This step is kind of thread safe\n+                                val elapsed = tLock.getElapsed();\n+                                WRITE_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                                log.debug(\"Acquired write lock for {}, wait time: {} ms\",\n+                                        String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                                return loadMissingKeys(txn, skipStoreCheck, txnData);\n+                            }, executor)\n+                            .thenComposeAsync(v1 -> {\n+                                // This check needs to be atomic, with absolutely no possibility of re-entry\n+                                return performCommit(txn, lazyWrite, txnData, modifiedKeys, modifiedValues);\n+                            }, executor)\n+                            .whenCompleteAsync((v2, ex) -> writeLock.unlock(), executor);\n+                }, executor)\n+                .thenRunAsync(() -> {\n+                    //  Step 5 : evict if required.\n+                    txn.setCommitted();\n+                    txnData.clear();\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    // Remove keys from active set.\n+                    txn.getData().keySet().forEach(this::removeFromActiveKeySet);\n+                    COMMIT_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+\n+        // Trigger evict\n+        retValue.thenComposeAsync(v4 -> {\n+            //  Step 6 : evict if required.\n+            return evictIfNeeded();\n+        }, executor);\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        return retValue;\n+    }\n \n-        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n-        // This step is kind of thread safe\n+    /**\n+     * Loads missing keys.\n+     */\n+    private CompletableFuture<Void> loadMissingKeys(MetadataTransaction txn, boolean skipStoreCheck, Map<String, TransactionData> txnData) {\n+        val loadFutures = new ArrayList<CompletableFuture<TransactionData>>();\n         for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-            String key = entry.getKey();\n+            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+            val key = entry.getKey();\n             if (skipStoreCheck || entry.getValue().isPinned()) {\n                 log.trace(\"Skipping loading key from the store key = {}\", key);\n             } else {\n                 // This check is safe to be outside the lock\n-                if (!bufferedTxnData.containsKey(key)) {\n-                    loadFromStore(key);\n+                val dataFromBuffer = bufferedTxnData.get(key);\n+                if (null == dataFromBuffer) {\n+                    loadFutures.add(loadFromStore(txn, key, true));\n                 }\n             }\n         }\n-        // Step 2 : Check whether transaction is safe to commit.\n-        // This check needs to be atomic, with absolutely no possibility of re-entry\n-        synchronized (lock) {\n-            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-                String key = entry.getKey();\n-                val transactionData = entry.getValue();\n-                Preconditions.checkState(null != transactionData.getKey());\n-\n-                // See if this entry was modified in this transaction.\n-                if (transactionData.getVersion() == txn.getVersion()) {\n-                    modifiedKeys.add(key);\n-                    transactionData.setPersisted(false);\n-                    modifiedValues.add(transactionData);\n-                }\n-                // make sure none of the keys used in this transaction have changed.\n-                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n-                if (null != dataFromBuffer) {\n-                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n-                        throw new StorageMetadataVersionMismatchException(\n-                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n-                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+        return Futures.allOf(loadFutures)\n+                .thenApplyAsync(v4 -> {\n+                    // validate everything is alright.\n+                    for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                        val dataFromBuffer = bufferedTxnData.get(entry.getKey());\n+                        if (!(entry.getValue().isPinned())) {\n+                            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+                            Preconditions.checkState(null != dataFromBuffer);\n+                            if (!dataFromBuffer.isPinned()) {\n+                                Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                            }\n+                        }\n                     }\n+                    return null;\n+                }, executor);\n+    }\n \n-                    // Pin it if it is already pinned.\n-                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+    /**\n+     * Performs commit.\n+     */\n+    private CompletableFuture<Void> performCommit(MetadataTransaction txn, boolean lazyWrite, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        return CompletableFuture.runAsync(() -> {\n+            // Step 2 : Check whether transaction is safe to commit.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MjczOA=="}, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 349}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTYwMjE5OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwMTowMzozOFrOHucdVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjozMTo1OFrOHwEPWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MjgwNg==", "bodyText": "formatting please", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r518462806", "createdAt": "2020-11-06T01:03:38Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+            if (fenced.get()) {\n+                throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));\n+            }\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Mark keys in transaction as active to prevent their eviction.\n+                    txn.getData().keySet().forEach(this::addToActiveKeySet);\n+\n+                    // Acquire a write lock over segment.\n+                    val tLock = new Timer();\n+                    log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));\n+                    val writeLock = scheduler.getWriteLock(txn.getKeysToLock());\n+                    return writeLock.lock()\n+                            .thenComposeAsync(v0 -> {\n+                                // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+                                // This step is kind of thread safe\n+                                val elapsed = tLock.getElapsed();\n+                                WRITE_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                                log.debug(\"Acquired write lock for {}, wait time: {} ms\",\n+                                        String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                                return loadMissingKeys(txn, skipStoreCheck, txnData);\n+                            }, executor)\n+                            .thenComposeAsync(v1 -> {\n+                                // This check needs to be atomic, with absolutely no possibility of re-entry\n+                                return performCommit(txn, lazyWrite, txnData, modifiedKeys, modifiedValues);\n+                            }, executor)\n+                            .whenCompleteAsync((v2, ex) -> writeLock.unlock(), executor);\n+                }, executor)\n+                .thenRunAsync(() -> {\n+                    //  Step 5 : evict if required.\n+                    txn.setCommitted();\n+                    txnData.clear();\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    // Remove keys from active set.\n+                    txn.getData().keySet().forEach(this::removeFromActiveKeySet);\n+                    COMMIT_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+\n+        // Trigger evict\n+        retValue.thenComposeAsync(v4 -> {\n+            //  Step 6 : evict if required.\n+            return evictIfNeeded();\n+        }, executor);\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        return retValue;\n+    }\n \n-        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n-        // This step is kind of thread safe\n+    /**\n+     * Loads missing keys.\n+     */\n+    private CompletableFuture<Void> loadMissingKeys(MetadataTransaction txn, boolean skipStoreCheck, Map<String, TransactionData> txnData) {\n+        val loadFutures = new ArrayList<CompletableFuture<TransactionData>>();\n         for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-            String key = entry.getKey();\n+            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+            val key = entry.getKey();\n             if (skipStoreCheck || entry.getValue().isPinned()) {\n                 log.trace(\"Skipping loading key from the store key = {}\", key);\n             } else {\n                 // This check is safe to be outside the lock\n-                if (!bufferedTxnData.containsKey(key)) {\n-                    loadFromStore(key);\n+                val dataFromBuffer = bufferedTxnData.get(key);\n+                if (null == dataFromBuffer) {\n+                    loadFutures.add(loadFromStore(txn, key, true));\n                 }\n             }\n         }\n-        // Step 2 : Check whether transaction is safe to commit.\n-        // This check needs to be atomic, with absolutely no possibility of re-entry\n-        synchronized (lock) {\n-            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-                String key = entry.getKey();\n-                val transactionData = entry.getValue();\n-                Preconditions.checkState(null != transactionData.getKey());\n-\n-                // See if this entry was modified in this transaction.\n-                if (transactionData.getVersion() == txn.getVersion()) {\n-                    modifiedKeys.add(key);\n-                    transactionData.setPersisted(false);\n-                    modifiedValues.add(transactionData);\n-                }\n-                // make sure none of the keys used in this transaction have changed.\n-                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n-                if (null != dataFromBuffer) {\n-                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n-                        throw new StorageMetadataVersionMismatchException(\n-                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n-                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+        return Futures.allOf(loadFutures)\n+                .thenApplyAsync(v4 -> {\n+                    // validate everything is alright.\n+                    for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                        val dataFromBuffer = bufferedTxnData.get(entry.getKey());\n+                        if (!(entry.getValue().isPinned())) {\n+                            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+                            Preconditions.checkState(null != dataFromBuffer);\n+                            if (!dataFromBuffer.isPinned()) {\n+                                Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                            }\n+                        }\n                     }\n+                    return null;\n+                }, executor);\n+    }\n \n-                    // Pin it if it is already pinned.\n-                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+    /**\n+     * Performs commit.\n+     */\n+    private CompletableFuture<Void> performCommit(MetadataTransaction txn, boolean lazyWrite, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        return CompletableFuture.runAsync(() -> {\n+            // Step 2 : Check whether transaction is safe to commit.\n+            validateCommit(txn, txnData, modifiedKeys, modifiedValues);\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Step 3: Commit externally.\n+                    // This operation may call external storage.\n+                    return writeToMetadataStore(lazyWrite, modifiedValues);\n+                }, executor)\n+                .thenComposeAsync(v ->", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 357}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MzE2Mg==", "bodyText": "done", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520163162", "createdAt": "2020-11-09T22:31:58Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+            if (fenced.get()) {\n+                throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));\n+            }\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Mark keys in transaction as active to prevent their eviction.\n+                    txn.getData().keySet().forEach(this::addToActiveKeySet);\n+\n+                    // Acquire a write lock over segment.\n+                    val tLock = new Timer();\n+                    log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));\n+                    val writeLock = scheduler.getWriteLock(txn.getKeysToLock());\n+                    return writeLock.lock()\n+                            .thenComposeAsync(v0 -> {\n+                                // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+                                // This step is kind of thread safe\n+                                val elapsed = tLock.getElapsed();\n+                                WRITE_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                                log.debug(\"Acquired write lock for {}, wait time: {} ms\",\n+                                        String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                                return loadMissingKeys(txn, skipStoreCheck, txnData);\n+                            }, executor)\n+                            .thenComposeAsync(v1 -> {\n+                                // This check needs to be atomic, with absolutely no possibility of re-entry\n+                                return performCommit(txn, lazyWrite, txnData, modifiedKeys, modifiedValues);\n+                            }, executor)\n+                            .whenCompleteAsync((v2, ex) -> writeLock.unlock(), executor);\n+                }, executor)\n+                .thenRunAsync(() -> {\n+                    //  Step 5 : evict if required.\n+                    txn.setCommitted();\n+                    txnData.clear();\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    // Remove keys from active set.\n+                    txn.getData().keySet().forEach(this::removeFromActiveKeySet);\n+                    COMMIT_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+\n+        // Trigger evict\n+        retValue.thenComposeAsync(v4 -> {\n+            //  Step 6 : evict if required.\n+            return evictIfNeeded();\n+        }, executor);\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        return retValue;\n+    }\n \n-        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n-        // This step is kind of thread safe\n+    /**\n+     * Loads missing keys.\n+     */\n+    private CompletableFuture<Void> loadMissingKeys(MetadataTransaction txn, boolean skipStoreCheck, Map<String, TransactionData> txnData) {\n+        val loadFutures = new ArrayList<CompletableFuture<TransactionData>>();\n         for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-            String key = entry.getKey();\n+            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+            val key = entry.getKey();\n             if (skipStoreCheck || entry.getValue().isPinned()) {\n                 log.trace(\"Skipping loading key from the store key = {}\", key);\n             } else {\n                 // This check is safe to be outside the lock\n-                if (!bufferedTxnData.containsKey(key)) {\n-                    loadFromStore(key);\n+                val dataFromBuffer = bufferedTxnData.get(key);\n+                if (null == dataFromBuffer) {\n+                    loadFutures.add(loadFromStore(txn, key, true));\n                 }\n             }\n         }\n-        // Step 2 : Check whether transaction is safe to commit.\n-        // This check needs to be atomic, with absolutely no possibility of re-entry\n-        synchronized (lock) {\n-            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-                String key = entry.getKey();\n-                val transactionData = entry.getValue();\n-                Preconditions.checkState(null != transactionData.getKey());\n-\n-                // See if this entry was modified in this transaction.\n-                if (transactionData.getVersion() == txn.getVersion()) {\n-                    modifiedKeys.add(key);\n-                    transactionData.setPersisted(false);\n-                    modifiedValues.add(transactionData);\n-                }\n-                // make sure none of the keys used in this transaction have changed.\n-                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n-                if (null != dataFromBuffer) {\n-                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n-                        throw new StorageMetadataVersionMismatchException(\n-                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n-                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+        return Futures.allOf(loadFutures)\n+                .thenApplyAsync(v4 -> {\n+                    // validate everything is alright.\n+                    for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                        val dataFromBuffer = bufferedTxnData.get(entry.getKey());\n+                        if (!(entry.getValue().isPinned())) {\n+                            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+                            Preconditions.checkState(null != dataFromBuffer);\n+                            if (!dataFromBuffer.isPinned()) {\n+                                Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                            }\n+                        }\n                     }\n+                    return null;\n+                }, executor);\n+    }\n \n-                    // Pin it if it is already pinned.\n-                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+    /**\n+     * Performs commit.\n+     */\n+    private CompletableFuture<Void> performCommit(MetadataTransaction txn, boolean lazyWrite, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        return CompletableFuture.runAsync(() -> {\n+            // Step 2 : Check whether transaction is safe to commit.\n+            validateCommit(txn, txnData, modifiedKeys, modifiedValues);\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Step 3: Commit externally.\n+                    // This operation may call external storage.\n+                    return writeToMetadataStore(lazyWrite, modifiedValues);\n+                }, executor)\n+                .thenComposeAsync(v ->", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MjgwNg=="}, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 357}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTYwMzE3OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwMTowNDoxNVrOHucd8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwMTowNDoxNVrOHucd8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2Mjk2MA==", "bodyText": "You can delete this line. IT will fall through to the end.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r518462960", "createdAt": "2020-11-06T01:04:15Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+            if (fenced.get()) {\n+                throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));\n+            }\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Mark keys in transaction as active to prevent their eviction.\n+                    txn.getData().keySet().forEach(this::addToActiveKeySet);\n+\n+                    // Acquire a write lock over segment.\n+                    val tLock = new Timer();\n+                    log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));\n+                    val writeLock = scheduler.getWriteLock(txn.getKeysToLock());\n+                    return writeLock.lock()\n+                            .thenComposeAsync(v0 -> {\n+                                // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+                                // This step is kind of thread safe\n+                                val elapsed = tLock.getElapsed();\n+                                WRITE_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                                log.debug(\"Acquired write lock for {}, wait time: {} ms\",\n+                                        String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                                return loadMissingKeys(txn, skipStoreCheck, txnData);\n+                            }, executor)\n+                            .thenComposeAsync(v1 -> {\n+                                // This check needs to be atomic, with absolutely no possibility of re-entry\n+                                return performCommit(txn, lazyWrite, txnData, modifiedKeys, modifiedValues);\n+                            }, executor)\n+                            .whenCompleteAsync((v2, ex) -> writeLock.unlock(), executor);\n+                }, executor)\n+                .thenRunAsync(() -> {\n+                    //  Step 5 : evict if required.\n+                    txn.setCommitted();\n+                    txnData.clear();\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    // Remove keys from active set.\n+                    txn.getData().keySet().forEach(this::removeFromActiveKeySet);\n+                    COMMIT_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+\n+        // Trigger evict\n+        retValue.thenComposeAsync(v4 -> {\n+            //  Step 6 : evict if required.\n+            return evictIfNeeded();\n+        }, executor);\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        return retValue;\n+    }\n \n-        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n-        // This step is kind of thread safe\n+    /**\n+     * Loads missing keys.\n+     */\n+    private CompletableFuture<Void> loadMissingKeys(MetadataTransaction txn, boolean skipStoreCheck, Map<String, TransactionData> txnData) {\n+        val loadFutures = new ArrayList<CompletableFuture<TransactionData>>();\n         for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-            String key = entry.getKey();\n+            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+            val key = entry.getKey();\n             if (skipStoreCheck || entry.getValue().isPinned()) {\n                 log.trace(\"Skipping loading key from the store key = {}\", key);\n             } else {\n                 // This check is safe to be outside the lock\n-                if (!bufferedTxnData.containsKey(key)) {\n-                    loadFromStore(key);\n+                val dataFromBuffer = bufferedTxnData.get(key);\n+                if (null == dataFromBuffer) {\n+                    loadFutures.add(loadFromStore(txn, key, true));\n                 }\n             }\n         }\n-        // Step 2 : Check whether transaction is safe to commit.\n-        // This check needs to be atomic, with absolutely no possibility of re-entry\n-        synchronized (lock) {\n-            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-                String key = entry.getKey();\n-                val transactionData = entry.getValue();\n-                Preconditions.checkState(null != transactionData.getKey());\n-\n-                // See if this entry was modified in this transaction.\n-                if (transactionData.getVersion() == txn.getVersion()) {\n-                    modifiedKeys.add(key);\n-                    transactionData.setPersisted(false);\n-                    modifiedValues.add(transactionData);\n-                }\n-                // make sure none of the keys used in this transaction have changed.\n-                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n-                if (null != dataFromBuffer) {\n-                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n-                        throw new StorageMetadataVersionMismatchException(\n-                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n-                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+        return Futures.allOf(loadFutures)\n+                .thenApplyAsync(v4 -> {\n+                    // validate everything is alright.\n+                    for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                        val dataFromBuffer = bufferedTxnData.get(entry.getKey());\n+                        if (!(entry.getValue().isPinned())) {\n+                            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+                            Preconditions.checkState(null != dataFromBuffer);\n+                            if (!dataFromBuffer.isPinned()) {\n+                                Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                            }\n+                        }\n                     }\n+                    return null;\n+                }, executor);\n+    }\n \n-                    // Pin it if it is already pinned.\n-                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+    /**\n+     * Performs commit.\n+     */\n+    private CompletableFuture<Void> performCommit(MetadataTransaction txn, boolean lazyWrite, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        return CompletableFuture.runAsync(() -> {\n+            // Step 2 : Check whether transaction is safe to commit.\n+            validateCommit(txn, txnData, modifiedKeys, modifiedValues);\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Step 3: Commit externally.\n+                    // This operation may call external storage.\n+                    return writeToMetadataStore(lazyWrite, modifiedValues);\n+                }, executor)\n+                .thenComposeAsync(v ->\n+                                executeExternalCommitAction(txn),\n+                        executor)\n+                .thenRunAsync(() -> {\n+                    // If we reach here then it means transaction is safe to commit.\n+                    // Step 4: Update buffer.\n+                    val committedVersion = version.incrementAndGet();\n+                    val toAdd = new HashMap<String, TransactionData>();\n+                    for (String key : modifiedKeys) {\n+                        TransactionData data = txnData.get(key);\n+                        data.setVersion(committedVersion);\n+                        toAdd.put(key, data);\n+                    }\n+                    bufferedTxnData.putAll(toAdd);\n+                    bufferCount.addAndGet(toAdd.size());\n+                }, executor);\n+    }\n \n-                    // Set the database object.\n-                    transactionData.setDbObject(dataFromBuffer.getDbObject());\n-                }\n+    /**\n+     * Writes modified values to the metadata store.\n+     */\n+    private CompletableFuture<Void> writeToMetadataStore(boolean lazyWrite, ArrayList<TransactionData> modifiedValues) {\n+        if (!lazyWrite || (bufferCount.get() > maxEntriesInTxnBuffer)) {\n+            log.trace(\"Persisting all modified keys (except pinned)\");\n+            val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n+            if (toWriteList.size() > 0) {\n+                return writeAll(toWriteList)\n+                        .thenRunAsync(() -> {\n+                            log.trace(\"Done persisting all modified keys\");\n+                            for (val writtenData : toWriteList) {\n+                                // Mark written keys as persisted.\n+                                writtenData.setPersisted(true);\n+                                // Put it in cache.\n+                                cache.put(writtenData.getKey(), writtenData);\n+                            }\n+                        }, executor);\n+            } else {\n+                return CompletableFuture.completedFuture(null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 397}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTYwNDMzOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwMTowNDo1MlrOHucetw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMzoyNjo1MFrOHwFoSQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MzE1OQ==", "bodyText": "Messages. Please do this everywhere else in your code.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r518463159", "createdAt": "2020-11-06T01:04:52Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+            if (fenced.get()) {\n+                throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));\n+            }\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Mark keys in transaction as active to prevent their eviction.\n+                    txn.getData().keySet().forEach(this::addToActiveKeySet);\n+\n+                    // Acquire a write lock over segment.\n+                    val tLock = new Timer();\n+                    log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));\n+                    val writeLock = scheduler.getWriteLock(txn.getKeysToLock());\n+                    return writeLock.lock()\n+                            .thenComposeAsync(v0 -> {\n+                                // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+                                // This step is kind of thread safe\n+                                val elapsed = tLock.getElapsed();\n+                                WRITE_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                                log.debug(\"Acquired write lock for {}, wait time: {} ms\",\n+                                        String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                                return loadMissingKeys(txn, skipStoreCheck, txnData);\n+                            }, executor)\n+                            .thenComposeAsync(v1 -> {\n+                                // This check needs to be atomic, with absolutely no possibility of re-entry\n+                                return performCommit(txn, lazyWrite, txnData, modifiedKeys, modifiedValues);\n+                            }, executor)\n+                            .whenCompleteAsync((v2, ex) -> writeLock.unlock(), executor);\n+                }, executor)\n+                .thenRunAsync(() -> {\n+                    //  Step 5 : evict if required.\n+                    txn.setCommitted();\n+                    txnData.clear();\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    // Remove keys from active set.\n+                    txn.getData().keySet().forEach(this::removeFromActiveKeySet);\n+                    COMMIT_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+\n+        // Trigger evict\n+        retValue.thenComposeAsync(v4 -> {\n+            //  Step 6 : evict if required.\n+            return evictIfNeeded();\n+        }, executor);\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        return retValue;\n+    }\n \n-        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n-        // This step is kind of thread safe\n+    /**\n+     * Loads missing keys.\n+     */\n+    private CompletableFuture<Void> loadMissingKeys(MetadataTransaction txn, boolean skipStoreCheck, Map<String, TransactionData> txnData) {\n+        val loadFutures = new ArrayList<CompletableFuture<TransactionData>>();\n         for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-            String key = entry.getKey();\n+            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+            val key = entry.getKey();\n             if (skipStoreCheck || entry.getValue().isPinned()) {\n                 log.trace(\"Skipping loading key from the store key = {}\", key);\n             } else {\n                 // This check is safe to be outside the lock\n-                if (!bufferedTxnData.containsKey(key)) {\n-                    loadFromStore(key);\n+                val dataFromBuffer = bufferedTxnData.get(key);\n+                if (null == dataFromBuffer) {\n+                    loadFutures.add(loadFromStore(txn, key, true));\n                 }\n             }\n         }\n-        // Step 2 : Check whether transaction is safe to commit.\n-        // This check needs to be atomic, with absolutely no possibility of re-entry\n-        synchronized (lock) {\n-            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-                String key = entry.getKey();\n-                val transactionData = entry.getValue();\n-                Preconditions.checkState(null != transactionData.getKey());\n-\n-                // See if this entry was modified in this transaction.\n-                if (transactionData.getVersion() == txn.getVersion()) {\n-                    modifiedKeys.add(key);\n-                    transactionData.setPersisted(false);\n-                    modifiedValues.add(transactionData);\n-                }\n-                // make sure none of the keys used in this transaction have changed.\n-                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n-                if (null != dataFromBuffer) {\n-                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n-                        throw new StorageMetadataVersionMismatchException(\n-                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n-                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+        return Futures.allOf(loadFutures)\n+                .thenApplyAsync(v4 -> {\n+                    // validate everything is alright.\n+                    for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                        val dataFromBuffer = bufferedTxnData.get(entry.getKey());\n+                        if (!(entry.getValue().isPinned())) {\n+                            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+                            Preconditions.checkState(null != dataFromBuffer);\n+                            if (!dataFromBuffer.isPinned()) {\n+                                Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                            }\n+                        }\n                     }\n+                    return null;\n+                }, executor);\n+    }\n \n-                    // Pin it if it is already pinned.\n-                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+    /**\n+     * Performs commit.\n+     */\n+    private CompletableFuture<Void> performCommit(MetadataTransaction txn, boolean lazyWrite, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        return CompletableFuture.runAsync(() -> {\n+            // Step 2 : Check whether transaction is safe to commit.\n+            validateCommit(txn, txnData, modifiedKeys, modifiedValues);\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Step 3: Commit externally.\n+                    // This operation may call external storage.\n+                    return writeToMetadataStore(lazyWrite, modifiedValues);\n+                }, executor)\n+                .thenComposeAsync(v ->\n+                                executeExternalCommitAction(txn),\n+                        executor)\n+                .thenRunAsync(() -> {\n+                    // If we reach here then it means transaction is safe to commit.\n+                    // Step 4: Update buffer.\n+                    val committedVersion = version.incrementAndGet();\n+                    val toAdd = new HashMap<String, TransactionData>();\n+                    for (String key : modifiedKeys) {\n+                        TransactionData data = txnData.get(key);\n+                        data.setVersion(committedVersion);\n+                        toAdd.put(key, data);\n+                    }\n+                    bufferedTxnData.putAll(toAdd);\n+                    bufferCount.addAndGet(toAdd.size());\n+                }, executor);\n+    }\n \n-                    // Set the database object.\n-                    transactionData.setDbObject(dataFromBuffer.getDbObject());\n-                }\n+    /**\n+     * Writes modified values to the metadata store.\n+     */\n+    private CompletableFuture<Void> writeToMetadataStore(boolean lazyWrite, ArrayList<TransactionData> modifiedValues) {\n+        if (!lazyWrite || (bufferCount.get() > maxEntriesInTxnBuffer)) {\n+            log.trace(\"Persisting all modified keys (except pinned)\");\n+            val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n+            if (toWriteList.size() > 0) {\n+                return writeAll(toWriteList)\n+                        .thenRunAsync(() -> {\n+                            log.trace(\"Done persisting all modified keys\");\n+                            for (val writtenData : toWriteList) {\n+                                // Mark written keys as persisted.\n+                                writtenData.setPersisted(true);\n+                                // Put it in cache.\n+                                cache.put(writtenData.getKey(), writtenData);\n+                            }\n+                        }, executor);\n+            } else {\n+                return CompletableFuture.completedFuture(null);\n             }\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Step 3: Commit externally.\n-            // This operation may call external storage.\n-            if (!lazyWrite || (bufferedTxnData.size() > maxEntriesInTxnBuffer)) {\n-                log.trace(\"Persisting all modified keys (except pinned)\");\n-                val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n-                writeAll(toWriteList);\n-                log.trace(\"Done persisting all modified keys\");\n-\n-                // Mark written keys as persisted.\n-                for (val writtenData : toWriteList) {\n-                    writtenData.setPersisted(true);\n-                }\n+    /**\n+     * Executes external commit step.\n+     */\n+    private CompletableFuture<Void> executeExternalCommitAction(MetadataTransaction txn) {\n+        // Execute external commit step.\n+        try {\n+            if (null != txn.getExternalCommitStep()) {\n+                txn.getExternalCommitStep().call();\n             }\n+        } catch (Exception e) {\n+            log.error(\"Exception during execution of external commit step\", e);\n+            throw new CompletionException(new StorageMetadataException(\"Exception during execution of external commit step\", e));\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Execute external commit step.\n-            try {\n-                if (null != txn.getExternalCommitStep()) {\n-                    txn.getExternalCommitStep().call();\n-                }\n-            } catch (Exception e) {\n-                log.error(\"Exception during execution of external commit step\", e);\n-                throw new StorageMetadataException(\"Exception during execution of external commit step\", e);\n+    private void validateCommit(MetadataTransaction txn, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        for (val entry : txnData.entrySet()) {\n+            val key = entry.getKey();\n+            val transactionData = entry.getValue();\n+            Preconditions.checkState(null != transactionData.getKey());\n+\n+            // See if this entry was modified in this transaction.\n+            if (transactionData.getVersion() == txn.getVersion()) {\n+                modifiedKeys.add(key);\n+                transactionData.setPersisted(false);\n+                modifiedValues.add(transactionData);\n             }\n+            // make sure none of the keys used in this transaction have changed.\n+            val dataFromBuffer = bufferedTxnData.get(key);\n+            if (null != dataFromBuffer) {\n+                if (!dataFromBuffer.isPinned()) {\n+                    Preconditions.checkState(null != dataFromBuffer.getDbObject());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 455}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE4NTkyOQ==", "bodyText": "done.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520185929", "createdAt": "2020-11-09T23:26:50Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+            if (fenced.get()) {\n+                throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));\n+            }\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Mark keys in transaction as active to prevent their eviction.\n+                    txn.getData().keySet().forEach(this::addToActiveKeySet);\n+\n+                    // Acquire a write lock over segment.\n+                    val tLock = new Timer();\n+                    log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));\n+                    val writeLock = scheduler.getWriteLock(txn.getKeysToLock());\n+                    return writeLock.lock()\n+                            .thenComposeAsync(v0 -> {\n+                                // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+                                // This step is kind of thread safe\n+                                val elapsed = tLock.getElapsed();\n+                                WRITE_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                                log.debug(\"Acquired write lock for {}, wait time: {} ms\",\n+                                        String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                                return loadMissingKeys(txn, skipStoreCheck, txnData);\n+                            }, executor)\n+                            .thenComposeAsync(v1 -> {\n+                                // This check needs to be atomic, with absolutely no possibility of re-entry\n+                                return performCommit(txn, lazyWrite, txnData, modifiedKeys, modifiedValues);\n+                            }, executor)\n+                            .whenCompleteAsync((v2, ex) -> writeLock.unlock(), executor);\n+                }, executor)\n+                .thenRunAsync(() -> {\n+                    //  Step 5 : evict if required.\n+                    txn.setCommitted();\n+                    txnData.clear();\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    // Remove keys from active set.\n+                    txn.getData().keySet().forEach(this::removeFromActiveKeySet);\n+                    COMMIT_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+\n+        // Trigger evict\n+        retValue.thenComposeAsync(v4 -> {\n+            //  Step 6 : evict if required.\n+            return evictIfNeeded();\n+        }, executor);\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        return retValue;\n+    }\n \n-        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n-        // This step is kind of thread safe\n+    /**\n+     * Loads missing keys.\n+     */\n+    private CompletableFuture<Void> loadMissingKeys(MetadataTransaction txn, boolean skipStoreCheck, Map<String, TransactionData> txnData) {\n+        val loadFutures = new ArrayList<CompletableFuture<TransactionData>>();\n         for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-            String key = entry.getKey();\n+            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+            val key = entry.getKey();\n             if (skipStoreCheck || entry.getValue().isPinned()) {\n                 log.trace(\"Skipping loading key from the store key = {}\", key);\n             } else {\n                 // This check is safe to be outside the lock\n-                if (!bufferedTxnData.containsKey(key)) {\n-                    loadFromStore(key);\n+                val dataFromBuffer = bufferedTxnData.get(key);\n+                if (null == dataFromBuffer) {\n+                    loadFutures.add(loadFromStore(txn, key, true));\n                 }\n             }\n         }\n-        // Step 2 : Check whether transaction is safe to commit.\n-        // This check needs to be atomic, with absolutely no possibility of re-entry\n-        synchronized (lock) {\n-            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-                String key = entry.getKey();\n-                val transactionData = entry.getValue();\n-                Preconditions.checkState(null != transactionData.getKey());\n-\n-                // See if this entry was modified in this transaction.\n-                if (transactionData.getVersion() == txn.getVersion()) {\n-                    modifiedKeys.add(key);\n-                    transactionData.setPersisted(false);\n-                    modifiedValues.add(transactionData);\n-                }\n-                // make sure none of the keys used in this transaction have changed.\n-                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n-                if (null != dataFromBuffer) {\n-                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n-                        throw new StorageMetadataVersionMismatchException(\n-                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n-                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+        return Futures.allOf(loadFutures)\n+                .thenApplyAsync(v4 -> {\n+                    // validate everything is alright.\n+                    for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                        val dataFromBuffer = bufferedTxnData.get(entry.getKey());\n+                        if (!(entry.getValue().isPinned())) {\n+                            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+                            Preconditions.checkState(null != dataFromBuffer);\n+                            if (!dataFromBuffer.isPinned()) {\n+                                Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                            }\n+                        }\n                     }\n+                    return null;\n+                }, executor);\n+    }\n \n-                    // Pin it if it is already pinned.\n-                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+    /**\n+     * Performs commit.\n+     */\n+    private CompletableFuture<Void> performCommit(MetadataTransaction txn, boolean lazyWrite, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        return CompletableFuture.runAsync(() -> {\n+            // Step 2 : Check whether transaction is safe to commit.\n+            validateCommit(txn, txnData, modifiedKeys, modifiedValues);\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Step 3: Commit externally.\n+                    // This operation may call external storage.\n+                    return writeToMetadataStore(lazyWrite, modifiedValues);\n+                }, executor)\n+                .thenComposeAsync(v ->\n+                                executeExternalCommitAction(txn),\n+                        executor)\n+                .thenRunAsync(() -> {\n+                    // If we reach here then it means transaction is safe to commit.\n+                    // Step 4: Update buffer.\n+                    val committedVersion = version.incrementAndGet();\n+                    val toAdd = new HashMap<String, TransactionData>();\n+                    for (String key : modifiedKeys) {\n+                        TransactionData data = txnData.get(key);\n+                        data.setVersion(committedVersion);\n+                        toAdd.put(key, data);\n+                    }\n+                    bufferedTxnData.putAll(toAdd);\n+                    bufferCount.addAndGet(toAdd.size());\n+                }, executor);\n+    }\n \n-                    // Set the database object.\n-                    transactionData.setDbObject(dataFromBuffer.getDbObject());\n-                }\n+    /**\n+     * Writes modified values to the metadata store.\n+     */\n+    private CompletableFuture<Void> writeToMetadataStore(boolean lazyWrite, ArrayList<TransactionData> modifiedValues) {\n+        if (!lazyWrite || (bufferCount.get() > maxEntriesInTxnBuffer)) {\n+            log.trace(\"Persisting all modified keys (except pinned)\");\n+            val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n+            if (toWriteList.size() > 0) {\n+                return writeAll(toWriteList)\n+                        .thenRunAsync(() -> {\n+                            log.trace(\"Done persisting all modified keys\");\n+                            for (val writtenData : toWriteList) {\n+                                // Mark written keys as persisted.\n+                                writtenData.setPersisted(true);\n+                                // Put it in cache.\n+                                cache.put(writtenData.getKey(), writtenData);\n+                            }\n+                        }, executor);\n+            } else {\n+                return CompletableFuture.completedFuture(null);\n             }\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Step 3: Commit externally.\n-            // This operation may call external storage.\n-            if (!lazyWrite || (bufferedTxnData.size() > maxEntriesInTxnBuffer)) {\n-                log.trace(\"Persisting all modified keys (except pinned)\");\n-                val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n-                writeAll(toWriteList);\n-                log.trace(\"Done persisting all modified keys\");\n-\n-                // Mark written keys as persisted.\n-                for (val writtenData : toWriteList) {\n-                    writtenData.setPersisted(true);\n-                }\n+    /**\n+     * Executes external commit step.\n+     */\n+    private CompletableFuture<Void> executeExternalCommitAction(MetadataTransaction txn) {\n+        // Execute external commit step.\n+        try {\n+            if (null != txn.getExternalCommitStep()) {\n+                txn.getExternalCommitStep().call();\n             }\n+        } catch (Exception e) {\n+            log.error(\"Exception during execution of external commit step\", e);\n+            throw new CompletionException(new StorageMetadataException(\"Exception during execution of external commit step\", e));\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Execute external commit step.\n-            try {\n-                if (null != txn.getExternalCommitStep()) {\n-                    txn.getExternalCommitStep().call();\n-                }\n-            } catch (Exception e) {\n-                log.error(\"Exception during execution of external commit step\", e);\n-                throw new StorageMetadataException(\"Exception during execution of external commit step\", e);\n+    private void validateCommit(MetadataTransaction txn, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        for (val entry : txnData.entrySet()) {\n+            val key = entry.getKey();\n+            val transactionData = entry.getValue();\n+            Preconditions.checkState(null != transactionData.getKey());\n+\n+            // See if this entry was modified in this transaction.\n+            if (transactionData.getVersion() == txn.getVersion()) {\n+                modifiedKeys.add(key);\n+                transactionData.setPersisted(false);\n+                modifiedValues.add(transactionData);\n             }\n+            // make sure none of the keys used in this transaction have changed.\n+            val dataFromBuffer = bufferedTxnData.get(key);\n+            if (null != dataFromBuffer) {\n+                if (!dataFromBuffer.isPinned()) {\n+                    Preconditions.checkState(null != dataFromBuffer.getDbObject());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MzE1OQ=="}, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 455}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTYwNTc3OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwMTowNTozNlrOHucfjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwMTowNTozNlrOHucfjg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MzM3NA==", "bodyText": "You can either declare this as throws or use @SneakyThrows(StorageMetadataVersionMismatchException.class) on the method to avoid having do to this ugly wrapping.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r518463374", "createdAt": "2020-11-06T01:05:36Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+            if (fenced.get()) {\n+                throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));\n+            }\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Mark keys in transaction as active to prevent their eviction.\n+                    txn.getData().keySet().forEach(this::addToActiveKeySet);\n+\n+                    // Acquire a write lock over segment.\n+                    val tLock = new Timer();\n+                    log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));\n+                    val writeLock = scheduler.getWriteLock(txn.getKeysToLock());\n+                    return writeLock.lock()\n+                            .thenComposeAsync(v0 -> {\n+                                // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+                                // This step is kind of thread safe\n+                                val elapsed = tLock.getElapsed();\n+                                WRITE_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                                log.debug(\"Acquired write lock for {}, wait time: {} ms\",\n+                                        String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                                return loadMissingKeys(txn, skipStoreCheck, txnData);\n+                            }, executor)\n+                            .thenComposeAsync(v1 -> {\n+                                // This check needs to be atomic, with absolutely no possibility of re-entry\n+                                return performCommit(txn, lazyWrite, txnData, modifiedKeys, modifiedValues);\n+                            }, executor)\n+                            .whenCompleteAsync((v2, ex) -> writeLock.unlock(), executor);\n+                }, executor)\n+                .thenRunAsync(() -> {\n+                    //  Step 5 : evict if required.\n+                    txn.setCommitted();\n+                    txnData.clear();\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    // Remove keys from active set.\n+                    txn.getData().keySet().forEach(this::removeFromActiveKeySet);\n+                    COMMIT_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+\n+        // Trigger evict\n+        retValue.thenComposeAsync(v4 -> {\n+            //  Step 6 : evict if required.\n+            return evictIfNeeded();\n+        }, executor);\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        return retValue;\n+    }\n \n-        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n-        // This step is kind of thread safe\n+    /**\n+     * Loads missing keys.\n+     */\n+    private CompletableFuture<Void> loadMissingKeys(MetadataTransaction txn, boolean skipStoreCheck, Map<String, TransactionData> txnData) {\n+        val loadFutures = new ArrayList<CompletableFuture<TransactionData>>();\n         for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-            String key = entry.getKey();\n+            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+            val key = entry.getKey();\n             if (skipStoreCheck || entry.getValue().isPinned()) {\n                 log.trace(\"Skipping loading key from the store key = {}\", key);\n             } else {\n                 // This check is safe to be outside the lock\n-                if (!bufferedTxnData.containsKey(key)) {\n-                    loadFromStore(key);\n+                val dataFromBuffer = bufferedTxnData.get(key);\n+                if (null == dataFromBuffer) {\n+                    loadFutures.add(loadFromStore(txn, key, true));\n                 }\n             }\n         }\n-        // Step 2 : Check whether transaction is safe to commit.\n-        // This check needs to be atomic, with absolutely no possibility of re-entry\n-        synchronized (lock) {\n-            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-                String key = entry.getKey();\n-                val transactionData = entry.getValue();\n-                Preconditions.checkState(null != transactionData.getKey());\n-\n-                // See if this entry was modified in this transaction.\n-                if (transactionData.getVersion() == txn.getVersion()) {\n-                    modifiedKeys.add(key);\n-                    transactionData.setPersisted(false);\n-                    modifiedValues.add(transactionData);\n-                }\n-                // make sure none of the keys used in this transaction have changed.\n-                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n-                if (null != dataFromBuffer) {\n-                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n-                        throw new StorageMetadataVersionMismatchException(\n-                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n-                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+        return Futures.allOf(loadFutures)\n+                .thenApplyAsync(v4 -> {\n+                    // validate everything is alright.\n+                    for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                        val dataFromBuffer = bufferedTxnData.get(entry.getKey());\n+                        if (!(entry.getValue().isPinned())) {\n+                            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+                            Preconditions.checkState(null != dataFromBuffer);\n+                            if (!dataFromBuffer.isPinned()) {\n+                                Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                            }\n+                        }\n                     }\n+                    return null;\n+                }, executor);\n+    }\n \n-                    // Pin it if it is already pinned.\n-                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+    /**\n+     * Performs commit.\n+     */\n+    private CompletableFuture<Void> performCommit(MetadataTransaction txn, boolean lazyWrite, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        return CompletableFuture.runAsync(() -> {\n+            // Step 2 : Check whether transaction is safe to commit.\n+            validateCommit(txn, txnData, modifiedKeys, modifiedValues);\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Step 3: Commit externally.\n+                    // This operation may call external storage.\n+                    return writeToMetadataStore(lazyWrite, modifiedValues);\n+                }, executor)\n+                .thenComposeAsync(v ->\n+                                executeExternalCommitAction(txn),\n+                        executor)\n+                .thenRunAsync(() -> {\n+                    // If we reach here then it means transaction is safe to commit.\n+                    // Step 4: Update buffer.\n+                    val committedVersion = version.incrementAndGet();\n+                    val toAdd = new HashMap<String, TransactionData>();\n+                    for (String key : modifiedKeys) {\n+                        TransactionData data = txnData.get(key);\n+                        data.setVersion(committedVersion);\n+                        toAdd.put(key, data);\n+                    }\n+                    bufferedTxnData.putAll(toAdd);\n+                    bufferCount.addAndGet(toAdd.size());\n+                }, executor);\n+    }\n \n-                    // Set the database object.\n-                    transactionData.setDbObject(dataFromBuffer.getDbObject());\n-                }\n+    /**\n+     * Writes modified values to the metadata store.\n+     */\n+    private CompletableFuture<Void> writeToMetadataStore(boolean lazyWrite, ArrayList<TransactionData> modifiedValues) {\n+        if (!lazyWrite || (bufferCount.get() > maxEntriesInTxnBuffer)) {\n+            log.trace(\"Persisting all modified keys (except pinned)\");\n+            val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n+            if (toWriteList.size() > 0) {\n+                return writeAll(toWriteList)\n+                        .thenRunAsync(() -> {\n+                            log.trace(\"Done persisting all modified keys\");\n+                            for (val writtenData : toWriteList) {\n+                                // Mark written keys as persisted.\n+                                writtenData.setPersisted(true);\n+                                // Put it in cache.\n+                                cache.put(writtenData.getKey(), writtenData);\n+                            }\n+                        }, executor);\n+            } else {\n+                return CompletableFuture.completedFuture(null);\n             }\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Step 3: Commit externally.\n-            // This operation may call external storage.\n-            if (!lazyWrite || (bufferedTxnData.size() > maxEntriesInTxnBuffer)) {\n-                log.trace(\"Persisting all modified keys (except pinned)\");\n-                val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n-                writeAll(toWriteList);\n-                log.trace(\"Done persisting all modified keys\");\n-\n-                // Mark written keys as persisted.\n-                for (val writtenData : toWriteList) {\n-                    writtenData.setPersisted(true);\n-                }\n+    /**\n+     * Executes external commit step.\n+     */\n+    private CompletableFuture<Void> executeExternalCommitAction(MetadataTransaction txn) {\n+        // Execute external commit step.\n+        try {\n+            if (null != txn.getExternalCommitStep()) {\n+                txn.getExternalCommitStep().call();\n             }\n+        } catch (Exception e) {\n+            log.error(\"Exception during execution of external commit step\", e);\n+            throw new CompletionException(new StorageMetadataException(\"Exception during execution of external commit step\", e));\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Execute external commit step.\n-            try {\n-                if (null != txn.getExternalCommitStep()) {\n-                    txn.getExternalCommitStep().call();\n-                }\n-            } catch (Exception e) {\n-                log.error(\"Exception during execution of external commit step\", e);\n-                throw new StorageMetadataException(\"Exception during execution of external commit step\", e);\n+    private void validateCommit(MetadataTransaction txn, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        for (val entry : txnData.entrySet()) {\n+            val key = entry.getKey();\n+            val transactionData = entry.getValue();\n+            Preconditions.checkState(null != transactionData.getKey());\n+\n+            // See if this entry was modified in this transaction.\n+            if (transactionData.getVersion() == txn.getVersion()) {\n+                modifiedKeys.add(key);\n+                transactionData.setPersisted(false);\n+                modifiedValues.add(transactionData);\n             }\n+            // make sure none of the keys used in this transaction have changed.\n+            val dataFromBuffer = bufferedTxnData.get(key);\n+            if (null != dataFromBuffer) {\n+                if (!dataFromBuffer.isPinned()) {\n+                    Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                }\n+                if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n+                    throw new CompletionException(new StorageMetadataVersionMismatchException(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 458}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTYwNjUxOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwMTowNjowMVrOHucf-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjoyNjozOFrOHwEFxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MzQ4Mw==", "bodyText": "Yes. Why, oh why are we here?\n(How is this useful in a production log?)", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r518463483", "createdAt": "2020-11-06T01:06:01Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+            if (fenced.get()) {\n+                throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));\n+            }\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Mark keys in transaction as active to prevent their eviction.\n+                    txn.getData().keySet().forEach(this::addToActiveKeySet);\n+\n+                    // Acquire a write lock over segment.\n+                    val tLock = new Timer();\n+                    log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));\n+                    val writeLock = scheduler.getWriteLock(txn.getKeysToLock());\n+                    return writeLock.lock()\n+                            .thenComposeAsync(v0 -> {\n+                                // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+                                // This step is kind of thread safe\n+                                val elapsed = tLock.getElapsed();\n+                                WRITE_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                                log.debug(\"Acquired write lock for {}, wait time: {} ms\",\n+                                        String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                                return loadMissingKeys(txn, skipStoreCheck, txnData);\n+                            }, executor)\n+                            .thenComposeAsync(v1 -> {\n+                                // This check needs to be atomic, with absolutely no possibility of re-entry\n+                                return performCommit(txn, lazyWrite, txnData, modifiedKeys, modifiedValues);\n+                            }, executor)\n+                            .whenCompleteAsync((v2, ex) -> writeLock.unlock(), executor);\n+                }, executor)\n+                .thenRunAsync(() -> {\n+                    //  Step 5 : evict if required.\n+                    txn.setCommitted();\n+                    txnData.clear();\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    // Remove keys from active set.\n+                    txn.getData().keySet().forEach(this::removeFromActiveKeySet);\n+                    COMMIT_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+\n+        // Trigger evict\n+        retValue.thenComposeAsync(v4 -> {\n+            //  Step 6 : evict if required.\n+            return evictIfNeeded();\n+        }, executor);\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        return retValue;\n+    }\n \n-        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n-        // This step is kind of thread safe\n+    /**\n+     * Loads missing keys.\n+     */\n+    private CompletableFuture<Void> loadMissingKeys(MetadataTransaction txn, boolean skipStoreCheck, Map<String, TransactionData> txnData) {\n+        val loadFutures = new ArrayList<CompletableFuture<TransactionData>>();\n         for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-            String key = entry.getKey();\n+            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+            val key = entry.getKey();\n             if (skipStoreCheck || entry.getValue().isPinned()) {\n                 log.trace(\"Skipping loading key from the store key = {}\", key);\n             } else {\n                 // This check is safe to be outside the lock\n-                if (!bufferedTxnData.containsKey(key)) {\n-                    loadFromStore(key);\n+                val dataFromBuffer = bufferedTxnData.get(key);\n+                if (null == dataFromBuffer) {\n+                    loadFutures.add(loadFromStore(txn, key, true));\n                 }\n             }\n         }\n-        // Step 2 : Check whether transaction is safe to commit.\n-        // This check needs to be atomic, with absolutely no possibility of re-entry\n-        synchronized (lock) {\n-            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-                String key = entry.getKey();\n-                val transactionData = entry.getValue();\n-                Preconditions.checkState(null != transactionData.getKey());\n-\n-                // See if this entry was modified in this transaction.\n-                if (transactionData.getVersion() == txn.getVersion()) {\n-                    modifiedKeys.add(key);\n-                    transactionData.setPersisted(false);\n-                    modifiedValues.add(transactionData);\n-                }\n-                // make sure none of the keys used in this transaction have changed.\n-                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n-                if (null != dataFromBuffer) {\n-                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n-                        throw new StorageMetadataVersionMismatchException(\n-                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n-                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+        return Futures.allOf(loadFutures)\n+                .thenApplyAsync(v4 -> {\n+                    // validate everything is alright.\n+                    for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                        val dataFromBuffer = bufferedTxnData.get(entry.getKey());\n+                        if (!(entry.getValue().isPinned())) {\n+                            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+                            Preconditions.checkState(null != dataFromBuffer);\n+                            if (!dataFromBuffer.isPinned()) {\n+                                Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                            }\n+                        }\n                     }\n+                    return null;\n+                }, executor);\n+    }\n \n-                    // Pin it if it is already pinned.\n-                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+    /**\n+     * Performs commit.\n+     */\n+    private CompletableFuture<Void> performCommit(MetadataTransaction txn, boolean lazyWrite, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        return CompletableFuture.runAsync(() -> {\n+            // Step 2 : Check whether transaction is safe to commit.\n+            validateCommit(txn, txnData, modifiedKeys, modifiedValues);\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Step 3: Commit externally.\n+                    // This operation may call external storage.\n+                    return writeToMetadataStore(lazyWrite, modifiedValues);\n+                }, executor)\n+                .thenComposeAsync(v ->\n+                                executeExternalCommitAction(txn),\n+                        executor)\n+                .thenRunAsync(() -> {\n+                    // If we reach here then it means transaction is safe to commit.\n+                    // Step 4: Update buffer.\n+                    val committedVersion = version.incrementAndGet();\n+                    val toAdd = new HashMap<String, TransactionData>();\n+                    for (String key : modifiedKeys) {\n+                        TransactionData data = txnData.get(key);\n+                        data.setVersion(committedVersion);\n+                        toAdd.put(key, data);\n+                    }\n+                    bufferedTxnData.putAll(toAdd);\n+                    bufferCount.addAndGet(toAdd.size());\n+                }, executor);\n+    }\n \n-                    // Set the database object.\n-                    transactionData.setDbObject(dataFromBuffer.getDbObject());\n-                }\n+    /**\n+     * Writes modified values to the metadata store.\n+     */\n+    private CompletableFuture<Void> writeToMetadataStore(boolean lazyWrite, ArrayList<TransactionData> modifiedValues) {\n+        if (!lazyWrite || (bufferCount.get() > maxEntriesInTxnBuffer)) {\n+            log.trace(\"Persisting all modified keys (except pinned)\");\n+            val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n+            if (toWriteList.size() > 0) {\n+                return writeAll(toWriteList)\n+                        .thenRunAsync(() -> {\n+                            log.trace(\"Done persisting all modified keys\");\n+                            for (val writtenData : toWriteList) {\n+                                // Mark written keys as persisted.\n+                                writtenData.setPersisted(true);\n+                                // Put it in cache.\n+                                cache.put(writtenData.getKey(), writtenData);\n+                            }\n+                        }, executor);\n+            } else {\n+                return CompletableFuture.completedFuture(null);\n             }\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Step 3: Commit externally.\n-            // This operation may call external storage.\n-            if (!lazyWrite || (bufferedTxnData.size() > maxEntriesInTxnBuffer)) {\n-                log.trace(\"Persisting all modified keys (except pinned)\");\n-                val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n-                writeAll(toWriteList);\n-                log.trace(\"Done persisting all modified keys\");\n-\n-                // Mark written keys as persisted.\n-                for (val writtenData : toWriteList) {\n-                    writtenData.setPersisted(true);\n-                }\n+    /**\n+     * Executes external commit step.\n+     */\n+    private CompletableFuture<Void> executeExternalCommitAction(MetadataTransaction txn) {\n+        // Execute external commit step.\n+        try {\n+            if (null != txn.getExternalCommitStep()) {\n+                txn.getExternalCommitStep().call();\n             }\n+        } catch (Exception e) {\n+            log.error(\"Exception during execution of external commit step\", e);\n+            throw new CompletionException(new StorageMetadataException(\"Exception during execution of external commit step\", e));\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Execute external commit step.\n-            try {\n-                if (null != txn.getExternalCommitStep()) {\n-                    txn.getExternalCommitStep().call();\n-                }\n-            } catch (Exception e) {\n-                log.error(\"Exception during execution of external commit step\", e);\n-                throw new StorageMetadataException(\"Exception during execution of external commit step\", e);\n+    private void validateCommit(MetadataTransaction txn, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        for (val entry : txnData.entrySet()) {\n+            val key = entry.getKey();\n+            val transactionData = entry.getValue();\n+            Preconditions.checkState(null != transactionData.getKey());\n+\n+            // See if this entry was modified in this transaction.\n+            if (transactionData.getVersion() == txn.getVersion()) {\n+                modifiedKeys.add(key);\n+                transactionData.setPersisted(false);\n+                modifiedValues.add(transactionData);\n             }\n+            // make sure none of the keys used in this transaction have changed.\n+            val dataFromBuffer = bufferedTxnData.get(key);\n+            if (null != dataFromBuffer) {\n+                if (!dataFromBuffer.isPinned()) {\n+                    Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                }\n+                if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n+                    throw new CompletionException(new StorageMetadataVersionMismatchException(\n+                            String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n+                                    key, dataFromBuffer.getVersion(), txnData.get(key).getVersion())));\n+                }\n+\n+                // Pin it if it is already pinned.\n+                transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n \n-            // If we reach here then it means transaction is safe to commit.\n-            // Step 4: Insert\n-            long committedVersion = version.incrementAndGet();\n-            HashMap<String, TransactionData> toAdd = new HashMap<String, TransactionData>();\n-            for (String key : modifiedKeys) {\n-                TransactionData data = txnData.get(key);\n-                data.setVersion(committedVersion);\n-                toAdd.put(key, data);\n+                // Set the database object.\n+                transactionData.setDbObject(dataFromBuffer.getDbObject());\n+            } else {\n+                Preconditions.checkState(entry.getValue().isPinned(), \"Why are we here??\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 477}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MDcwOA==", "bodyText": ":) fixed", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520160708", "createdAt": "2020-11-09T22:26:38Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+            if (fenced.get()) {\n+                throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));\n+            }\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Mark keys in transaction as active to prevent their eviction.\n+                    txn.getData().keySet().forEach(this::addToActiveKeySet);\n+\n+                    // Acquire a write lock over segment.\n+                    val tLock = new Timer();\n+                    log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));\n+                    val writeLock = scheduler.getWriteLock(txn.getKeysToLock());\n+                    return writeLock.lock()\n+                            .thenComposeAsync(v0 -> {\n+                                // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+                                // This step is kind of thread safe\n+                                val elapsed = tLock.getElapsed();\n+                                WRITE_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                                log.debug(\"Acquired write lock for {}, wait time: {} ms\",\n+                                        String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                                return loadMissingKeys(txn, skipStoreCheck, txnData);\n+                            }, executor)\n+                            .thenComposeAsync(v1 -> {\n+                                // This check needs to be atomic, with absolutely no possibility of re-entry\n+                                return performCommit(txn, lazyWrite, txnData, modifiedKeys, modifiedValues);\n+                            }, executor)\n+                            .whenCompleteAsync((v2, ex) -> writeLock.unlock(), executor);\n+                }, executor)\n+                .thenRunAsync(() -> {\n+                    //  Step 5 : evict if required.\n+                    txn.setCommitted();\n+                    txnData.clear();\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    // Remove keys from active set.\n+                    txn.getData().keySet().forEach(this::removeFromActiveKeySet);\n+                    COMMIT_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+\n+        // Trigger evict\n+        retValue.thenComposeAsync(v4 -> {\n+            //  Step 6 : evict if required.\n+            return evictIfNeeded();\n+        }, executor);\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        return retValue;\n+    }\n \n-        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n-        // This step is kind of thread safe\n+    /**\n+     * Loads missing keys.\n+     */\n+    private CompletableFuture<Void> loadMissingKeys(MetadataTransaction txn, boolean skipStoreCheck, Map<String, TransactionData> txnData) {\n+        val loadFutures = new ArrayList<CompletableFuture<TransactionData>>();\n         for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-            String key = entry.getKey();\n+            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+            val key = entry.getKey();\n             if (skipStoreCheck || entry.getValue().isPinned()) {\n                 log.trace(\"Skipping loading key from the store key = {}\", key);\n             } else {\n                 // This check is safe to be outside the lock\n-                if (!bufferedTxnData.containsKey(key)) {\n-                    loadFromStore(key);\n+                val dataFromBuffer = bufferedTxnData.get(key);\n+                if (null == dataFromBuffer) {\n+                    loadFutures.add(loadFromStore(txn, key, true));\n                 }\n             }\n         }\n-        // Step 2 : Check whether transaction is safe to commit.\n-        // This check needs to be atomic, with absolutely no possibility of re-entry\n-        synchronized (lock) {\n-            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-                String key = entry.getKey();\n-                val transactionData = entry.getValue();\n-                Preconditions.checkState(null != transactionData.getKey());\n-\n-                // See if this entry was modified in this transaction.\n-                if (transactionData.getVersion() == txn.getVersion()) {\n-                    modifiedKeys.add(key);\n-                    transactionData.setPersisted(false);\n-                    modifiedValues.add(transactionData);\n-                }\n-                // make sure none of the keys used in this transaction have changed.\n-                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n-                if (null != dataFromBuffer) {\n-                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n-                        throw new StorageMetadataVersionMismatchException(\n-                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n-                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+        return Futures.allOf(loadFutures)\n+                .thenApplyAsync(v4 -> {\n+                    // validate everything is alright.\n+                    for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                        val dataFromBuffer = bufferedTxnData.get(entry.getKey());\n+                        if (!(entry.getValue().isPinned())) {\n+                            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+                            Preconditions.checkState(null != dataFromBuffer);\n+                            if (!dataFromBuffer.isPinned()) {\n+                                Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                            }\n+                        }\n                     }\n+                    return null;\n+                }, executor);\n+    }\n \n-                    // Pin it if it is already pinned.\n-                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+    /**\n+     * Performs commit.\n+     */\n+    private CompletableFuture<Void> performCommit(MetadataTransaction txn, boolean lazyWrite, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        return CompletableFuture.runAsync(() -> {\n+            // Step 2 : Check whether transaction is safe to commit.\n+            validateCommit(txn, txnData, modifiedKeys, modifiedValues);\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Step 3: Commit externally.\n+                    // This operation may call external storage.\n+                    return writeToMetadataStore(lazyWrite, modifiedValues);\n+                }, executor)\n+                .thenComposeAsync(v ->\n+                                executeExternalCommitAction(txn),\n+                        executor)\n+                .thenRunAsync(() -> {\n+                    // If we reach here then it means transaction is safe to commit.\n+                    // Step 4: Update buffer.\n+                    val committedVersion = version.incrementAndGet();\n+                    val toAdd = new HashMap<String, TransactionData>();\n+                    for (String key : modifiedKeys) {\n+                        TransactionData data = txnData.get(key);\n+                        data.setVersion(committedVersion);\n+                        toAdd.put(key, data);\n+                    }\n+                    bufferedTxnData.putAll(toAdd);\n+                    bufferCount.addAndGet(toAdd.size());\n+                }, executor);\n+    }\n \n-                    // Set the database object.\n-                    transactionData.setDbObject(dataFromBuffer.getDbObject());\n-                }\n+    /**\n+     * Writes modified values to the metadata store.\n+     */\n+    private CompletableFuture<Void> writeToMetadataStore(boolean lazyWrite, ArrayList<TransactionData> modifiedValues) {\n+        if (!lazyWrite || (bufferCount.get() > maxEntriesInTxnBuffer)) {\n+            log.trace(\"Persisting all modified keys (except pinned)\");\n+            val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n+            if (toWriteList.size() > 0) {\n+                return writeAll(toWriteList)\n+                        .thenRunAsync(() -> {\n+                            log.trace(\"Done persisting all modified keys\");\n+                            for (val writtenData : toWriteList) {\n+                                // Mark written keys as persisted.\n+                                writtenData.setPersisted(true);\n+                                // Put it in cache.\n+                                cache.put(writtenData.getKey(), writtenData);\n+                            }\n+                        }, executor);\n+            } else {\n+                return CompletableFuture.completedFuture(null);\n             }\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Step 3: Commit externally.\n-            // This operation may call external storage.\n-            if (!lazyWrite || (bufferedTxnData.size() > maxEntriesInTxnBuffer)) {\n-                log.trace(\"Persisting all modified keys (except pinned)\");\n-                val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n-                writeAll(toWriteList);\n-                log.trace(\"Done persisting all modified keys\");\n-\n-                // Mark written keys as persisted.\n-                for (val writtenData : toWriteList) {\n-                    writtenData.setPersisted(true);\n-                }\n+    /**\n+     * Executes external commit step.\n+     */\n+    private CompletableFuture<Void> executeExternalCommitAction(MetadataTransaction txn) {\n+        // Execute external commit step.\n+        try {\n+            if (null != txn.getExternalCommitStep()) {\n+                txn.getExternalCommitStep().call();\n             }\n+        } catch (Exception e) {\n+            log.error(\"Exception during execution of external commit step\", e);\n+            throw new CompletionException(new StorageMetadataException(\"Exception during execution of external commit step\", e));\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Execute external commit step.\n-            try {\n-                if (null != txn.getExternalCommitStep()) {\n-                    txn.getExternalCommitStep().call();\n-                }\n-            } catch (Exception e) {\n-                log.error(\"Exception during execution of external commit step\", e);\n-                throw new StorageMetadataException(\"Exception during execution of external commit step\", e);\n+    private void validateCommit(MetadataTransaction txn, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        for (val entry : txnData.entrySet()) {\n+            val key = entry.getKey();\n+            val transactionData = entry.getValue();\n+            Preconditions.checkState(null != transactionData.getKey());\n+\n+            // See if this entry was modified in this transaction.\n+            if (transactionData.getVersion() == txn.getVersion()) {\n+                modifiedKeys.add(key);\n+                transactionData.setPersisted(false);\n+                modifiedValues.add(transactionData);\n             }\n+            // make sure none of the keys used in this transaction have changed.\n+            val dataFromBuffer = bufferedTxnData.get(key);\n+            if (null != dataFromBuffer) {\n+                if (!dataFromBuffer.isPinned()) {\n+                    Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                }\n+                if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n+                    throw new CompletionException(new StorageMetadataVersionMismatchException(\n+                            String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n+                                    key, dataFromBuffer.getVersion(), txnData.get(key).getVersion())));\n+                }\n+\n+                // Pin it if it is already pinned.\n+                transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n \n-            // If we reach here then it means transaction is safe to commit.\n-            // Step 4: Insert\n-            long committedVersion = version.incrementAndGet();\n-            HashMap<String, TransactionData> toAdd = new HashMap<String, TransactionData>();\n-            for (String key : modifiedKeys) {\n-                TransactionData data = txnData.get(key);\n-                data.setVersion(committedVersion);\n-                toAdd.put(key, data);\n+                // Set the database object.\n+                transactionData.setDbObject(dataFromBuffer.getDbObject());\n+            } else {\n+                Preconditions.checkState(entry.getValue().isPinned(), \"Why are we here??\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MzQ4Mw=="}, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 477}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTYwNzU3OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwMTowNjo0MlrOHucgsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMzoyNjoyOVrOHwFnzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MzY2NA==", "bodyText": "count ?", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r518463664", "createdAt": "2020-11-06T01:06:42Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+            if (fenced.get()) {\n+                throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));\n+            }\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Mark keys in transaction as active to prevent their eviction.\n+                    txn.getData().keySet().forEach(this::addToActiveKeySet);\n+\n+                    // Acquire a write lock over segment.\n+                    val tLock = new Timer();\n+                    log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));\n+                    val writeLock = scheduler.getWriteLock(txn.getKeysToLock());\n+                    return writeLock.lock()\n+                            .thenComposeAsync(v0 -> {\n+                                // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+                                // This step is kind of thread safe\n+                                val elapsed = tLock.getElapsed();\n+                                WRITE_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                                log.debug(\"Acquired write lock for {}, wait time: {} ms\",\n+                                        String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                                return loadMissingKeys(txn, skipStoreCheck, txnData);\n+                            }, executor)\n+                            .thenComposeAsync(v1 -> {\n+                                // This check needs to be atomic, with absolutely no possibility of re-entry\n+                                return performCommit(txn, lazyWrite, txnData, modifiedKeys, modifiedValues);\n+                            }, executor)\n+                            .whenCompleteAsync((v2, ex) -> writeLock.unlock(), executor);\n+                }, executor)\n+                .thenRunAsync(() -> {\n+                    //  Step 5 : evict if required.\n+                    txn.setCommitted();\n+                    txnData.clear();\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    // Remove keys from active set.\n+                    txn.getData().keySet().forEach(this::removeFromActiveKeySet);\n+                    COMMIT_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+\n+        // Trigger evict\n+        retValue.thenComposeAsync(v4 -> {\n+            //  Step 6 : evict if required.\n+            return evictIfNeeded();\n+        }, executor);\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        return retValue;\n+    }\n \n-        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n-        // This step is kind of thread safe\n+    /**\n+     * Loads missing keys.\n+     */\n+    private CompletableFuture<Void> loadMissingKeys(MetadataTransaction txn, boolean skipStoreCheck, Map<String, TransactionData> txnData) {\n+        val loadFutures = new ArrayList<CompletableFuture<TransactionData>>();\n         for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-            String key = entry.getKey();\n+            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+            val key = entry.getKey();\n             if (skipStoreCheck || entry.getValue().isPinned()) {\n                 log.trace(\"Skipping loading key from the store key = {}\", key);\n             } else {\n                 // This check is safe to be outside the lock\n-                if (!bufferedTxnData.containsKey(key)) {\n-                    loadFromStore(key);\n+                val dataFromBuffer = bufferedTxnData.get(key);\n+                if (null == dataFromBuffer) {\n+                    loadFutures.add(loadFromStore(txn, key, true));\n                 }\n             }\n         }\n-        // Step 2 : Check whether transaction is safe to commit.\n-        // This check needs to be atomic, with absolutely no possibility of re-entry\n-        synchronized (lock) {\n-            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-                String key = entry.getKey();\n-                val transactionData = entry.getValue();\n-                Preconditions.checkState(null != transactionData.getKey());\n-\n-                // See if this entry was modified in this transaction.\n-                if (transactionData.getVersion() == txn.getVersion()) {\n-                    modifiedKeys.add(key);\n-                    transactionData.setPersisted(false);\n-                    modifiedValues.add(transactionData);\n-                }\n-                // make sure none of the keys used in this transaction have changed.\n-                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n-                if (null != dataFromBuffer) {\n-                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n-                        throw new StorageMetadataVersionMismatchException(\n-                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n-                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+        return Futures.allOf(loadFutures)\n+                .thenApplyAsync(v4 -> {\n+                    // validate everything is alright.\n+                    for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                        val dataFromBuffer = bufferedTxnData.get(entry.getKey());\n+                        if (!(entry.getValue().isPinned())) {\n+                            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+                            Preconditions.checkState(null != dataFromBuffer);\n+                            if (!dataFromBuffer.isPinned()) {\n+                                Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                            }\n+                        }\n                     }\n+                    return null;\n+                }, executor);\n+    }\n \n-                    // Pin it if it is already pinned.\n-                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+    /**\n+     * Performs commit.\n+     */\n+    private CompletableFuture<Void> performCommit(MetadataTransaction txn, boolean lazyWrite, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        return CompletableFuture.runAsync(() -> {\n+            // Step 2 : Check whether transaction is safe to commit.\n+            validateCommit(txn, txnData, modifiedKeys, modifiedValues);\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Step 3: Commit externally.\n+                    // This operation may call external storage.\n+                    return writeToMetadataStore(lazyWrite, modifiedValues);\n+                }, executor)\n+                .thenComposeAsync(v ->\n+                                executeExternalCommitAction(txn),\n+                        executor)\n+                .thenRunAsync(() -> {\n+                    // If we reach here then it means transaction is safe to commit.\n+                    // Step 4: Update buffer.\n+                    val committedVersion = version.incrementAndGet();\n+                    val toAdd = new HashMap<String, TransactionData>();\n+                    for (String key : modifiedKeys) {\n+                        TransactionData data = txnData.get(key);\n+                        data.setVersion(committedVersion);\n+                        toAdd.put(key, data);\n+                    }\n+                    bufferedTxnData.putAll(toAdd);\n+                    bufferCount.addAndGet(toAdd.size());\n+                }, executor);\n+    }\n \n-                    // Set the database object.\n-                    transactionData.setDbObject(dataFromBuffer.getDbObject());\n-                }\n+    /**\n+     * Writes modified values to the metadata store.\n+     */\n+    private CompletableFuture<Void> writeToMetadataStore(boolean lazyWrite, ArrayList<TransactionData> modifiedValues) {\n+        if (!lazyWrite || (bufferCount.get() > maxEntriesInTxnBuffer)) {\n+            log.trace(\"Persisting all modified keys (except pinned)\");\n+            val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n+            if (toWriteList.size() > 0) {\n+                return writeAll(toWriteList)\n+                        .thenRunAsync(() -> {\n+                            log.trace(\"Done persisting all modified keys\");\n+                            for (val writtenData : toWriteList) {\n+                                // Mark written keys as persisted.\n+                                writtenData.setPersisted(true);\n+                                // Put it in cache.\n+                                cache.put(writtenData.getKey(), writtenData);\n+                            }\n+                        }, executor);\n+            } else {\n+                return CompletableFuture.completedFuture(null);\n             }\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Step 3: Commit externally.\n-            // This operation may call external storage.\n-            if (!lazyWrite || (bufferedTxnData.size() > maxEntriesInTxnBuffer)) {\n-                log.trace(\"Persisting all modified keys (except pinned)\");\n-                val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n-                writeAll(toWriteList);\n-                log.trace(\"Done persisting all modified keys\");\n-\n-                // Mark written keys as persisted.\n-                for (val writtenData : toWriteList) {\n-                    writtenData.setPersisted(true);\n-                }\n+    /**\n+     * Executes external commit step.\n+     */\n+    private CompletableFuture<Void> executeExternalCommitAction(MetadataTransaction txn) {\n+        // Execute external commit step.\n+        try {\n+            if (null != txn.getExternalCommitStep()) {\n+                txn.getExternalCommitStep().call();\n             }\n+        } catch (Exception e) {\n+            log.error(\"Exception during execution of external commit step\", e);\n+            throw new CompletionException(new StorageMetadataException(\"Exception during execution of external commit step\", e));\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Execute external commit step.\n-            try {\n-                if (null != txn.getExternalCommitStep()) {\n-                    txn.getExternalCommitStep().call();\n-                }\n-            } catch (Exception e) {\n-                log.error(\"Exception during execution of external commit step\", e);\n-                throw new StorageMetadataException(\"Exception during execution of external commit step\", e);\n+    private void validateCommit(MetadataTransaction txn, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        for (val entry : txnData.entrySet()) {\n+            val key = entry.getKey();\n+            val transactionData = entry.getValue();\n+            Preconditions.checkState(null != transactionData.getKey());\n+\n+            // See if this entry was modified in this transaction.\n+            if (transactionData.getVersion() == txn.getVersion()) {\n+                modifiedKeys.add(key);\n+                transactionData.setPersisted(false);\n+                modifiedValues.add(transactionData);\n             }\n+            // make sure none of the keys used in this transaction have changed.\n+            val dataFromBuffer = bufferedTxnData.get(key);\n+            if (null != dataFromBuffer) {\n+                if (!dataFromBuffer.isPinned()) {\n+                    Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                }\n+                if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n+                    throw new CompletionException(new StorageMetadataVersionMismatchException(\n+                            String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n+                                    key, dataFromBuffer.getVersion(), txnData.get(key).getVersion())));\n+                }\n+\n+                // Pin it if it is already pinned.\n+                transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n \n-            // If we reach here then it means transaction is safe to commit.\n-            // Step 4: Insert\n-            long committedVersion = version.incrementAndGet();\n-            HashMap<String, TransactionData> toAdd = new HashMap<String, TransactionData>();\n-            for (String key : modifiedKeys) {\n-                TransactionData data = txnData.get(key);\n-                data.setVersion(committedVersion);\n-                toAdd.put(key, data);\n+                // Set the database object.\n+                transactionData.setDbObject(dataFromBuffer.getDbObject());\n+            } else {\n+                Preconditions.checkState(entry.getValue().isPinned(), \"Why are we here??\");\n             }\n-            bufferedTxnData.putAll(toAdd);\n         }\n+    }\n \n-        //  Step 5 : evict if required.\n-        if (bufferedTxnData.size() > maxEntriesInTxnBuffer) {\n-            bufferedTxnData.entrySet().removeIf(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned());\n+    /**\n+     * Evict entries if needed.\n+     * Only evict keys that are persisted, not pinned or active.\n+     */\n+    private CompletableFuture<Void> evictIfNeeded() {\n+        if (isEvictionRunning.compareAndSet(false, true)) {\n+            val limit = 1 + maxEntriesInTxnBuffer / CACHE_EVICTION_RATIO;\n+            if (bufferCount.get() > maxEntriesInTxnBuffer) {\n+                val toEvict = bufferedTxnData.entrySet().parallelStream()\n+                        .filter(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned()\n+                                && !activeKeys.contains(entry.getKey()))\n+                        .map(Map.Entry::getKey)\n+                        .limit(limit)\n+                        .collect(Collectors.toList());\n+                int i = 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 500}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE4NTgwNA==", "bodyText": "done", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520185804", "createdAt": "2020-11-09T23:26:29Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+            if (fenced.get()) {\n+                throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));\n+            }\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Mark keys in transaction as active to prevent their eviction.\n+                    txn.getData().keySet().forEach(this::addToActiveKeySet);\n+\n+                    // Acquire a write lock over segment.\n+                    val tLock = new Timer();\n+                    log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));\n+                    val writeLock = scheduler.getWriteLock(txn.getKeysToLock());\n+                    return writeLock.lock()\n+                            .thenComposeAsync(v0 -> {\n+                                // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+                                // This step is kind of thread safe\n+                                val elapsed = tLock.getElapsed();\n+                                WRITE_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                                log.debug(\"Acquired write lock for {}, wait time: {} ms\",\n+                                        String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                                return loadMissingKeys(txn, skipStoreCheck, txnData);\n+                            }, executor)\n+                            .thenComposeAsync(v1 -> {\n+                                // This check needs to be atomic, with absolutely no possibility of re-entry\n+                                return performCommit(txn, lazyWrite, txnData, modifiedKeys, modifiedValues);\n+                            }, executor)\n+                            .whenCompleteAsync((v2, ex) -> writeLock.unlock(), executor);\n+                }, executor)\n+                .thenRunAsync(() -> {\n+                    //  Step 5 : evict if required.\n+                    txn.setCommitted();\n+                    txnData.clear();\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    // Remove keys from active set.\n+                    txn.getData().keySet().forEach(this::removeFromActiveKeySet);\n+                    COMMIT_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+\n+        // Trigger evict\n+        retValue.thenComposeAsync(v4 -> {\n+            //  Step 6 : evict if required.\n+            return evictIfNeeded();\n+        }, executor);\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        return retValue;\n+    }\n \n-        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n-        // This step is kind of thread safe\n+    /**\n+     * Loads missing keys.\n+     */\n+    private CompletableFuture<Void> loadMissingKeys(MetadataTransaction txn, boolean skipStoreCheck, Map<String, TransactionData> txnData) {\n+        val loadFutures = new ArrayList<CompletableFuture<TransactionData>>();\n         for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-            String key = entry.getKey();\n+            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+            val key = entry.getKey();\n             if (skipStoreCheck || entry.getValue().isPinned()) {\n                 log.trace(\"Skipping loading key from the store key = {}\", key);\n             } else {\n                 // This check is safe to be outside the lock\n-                if (!bufferedTxnData.containsKey(key)) {\n-                    loadFromStore(key);\n+                val dataFromBuffer = bufferedTxnData.get(key);\n+                if (null == dataFromBuffer) {\n+                    loadFutures.add(loadFromStore(txn, key, true));\n                 }\n             }\n         }\n-        // Step 2 : Check whether transaction is safe to commit.\n-        // This check needs to be atomic, with absolutely no possibility of re-entry\n-        synchronized (lock) {\n-            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-                String key = entry.getKey();\n-                val transactionData = entry.getValue();\n-                Preconditions.checkState(null != transactionData.getKey());\n-\n-                // See if this entry was modified in this transaction.\n-                if (transactionData.getVersion() == txn.getVersion()) {\n-                    modifiedKeys.add(key);\n-                    transactionData.setPersisted(false);\n-                    modifiedValues.add(transactionData);\n-                }\n-                // make sure none of the keys used in this transaction have changed.\n-                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n-                if (null != dataFromBuffer) {\n-                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n-                        throw new StorageMetadataVersionMismatchException(\n-                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n-                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+        return Futures.allOf(loadFutures)\n+                .thenApplyAsync(v4 -> {\n+                    // validate everything is alright.\n+                    for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                        val dataFromBuffer = bufferedTxnData.get(entry.getKey());\n+                        if (!(entry.getValue().isPinned())) {\n+                            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+                            Preconditions.checkState(null != dataFromBuffer);\n+                            if (!dataFromBuffer.isPinned()) {\n+                                Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                            }\n+                        }\n                     }\n+                    return null;\n+                }, executor);\n+    }\n \n-                    // Pin it if it is already pinned.\n-                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+    /**\n+     * Performs commit.\n+     */\n+    private CompletableFuture<Void> performCommit(MetadataTransaction txn, boolean lazyWrite, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        return CompletableFuture.runAsync(() -> {\n+            // Step 2 : Check whether transaction is safe to commit.\n+            validateCommit(txn, txnData, modifiedKeys, modifiedValues);\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Step 3: Commit externally.\n+                    // This operation may call external storage.\n+                    return writeToMetadataStore(lazyWrite, modifiedValues);\n+                }, executor)\n+                .thenComposeAsync(v ->\n+                                executeExternalCommitAction(txn),\n+                        executor)\n+                .thenRunAsync(() -> {\n+                    // If we reach here then it means transaction is safe to commit.\n+                    // Step 4: Update buffer.\n+                    val committedVersion = version.incrementAndGet();\n+                    val toAdd = new HashMap<String, TransactionData>();\n+                    for (String key : modifiedKeys) {\n+                        TransactionData data = txnData.get(key);\n+                        data.setVersion(committedVersion);\n+                        toAdd.put(key, data);\n+                    }\n+                    bufferedTxnData.putAll(toAdd);\n+                    bufferCount.addAndGet(toAdd.size());\n+                }, executor);\n+    }\n \n-                    // Set the database object.\n-                    transactionData.setDbObject(dataFromBuffer.getDbObject());\n-                }\n+    /**\n+     * Writes modified values to the metadata store.\n+     */\n+    private CompletableFuture<Void> writeToMetadataStore(boolean lazyWrite, ArrayList<TransactionData> modifiedValues) {\n+        if (!lazyWrite || (bufferCount.get() > maxEntriesInTxnBuffer)) {\n+            log.trace(\"Persisting all modified keys (except pinned)\");\n+            val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n+            if (toWriteList.size() > 0) {\n+                return writeAll(toWriteList)\n+                        .thenRunAsync(() -> {\n+                            log.trace(\"Done persisting all modified keys\");\n+                            for (val writtenData : toWriteList) {\n+                                // Mark written keys as persisted.\n+                                writtenData.setPersisted(true);\n+                                // Put it in cache.\n+                                cache.put(writtenData.getKey(), writtenData);\n+                            }\n+                        }, executor);\n+            } else {\n+                return CompletableFuture.completedFuture(null);\n             }\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Step 3: Commit externally.\n-            // This operation may call external storage.\n-            if (!lazyWrite || (bufferedTxnData.size() > maxEntriesInTxnBuffer)) {\n-                log.trace(\"Persisting all modified keys (except pinned)\");\n-                val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n-                writeAll(toWriteList);\n-                log.trace(\"Done persisting all modified keys\");\n-\n-                // Mark written keys as persisted.\n-                for (val writtenData : toWriteList) {\n-                    writtenData.setPersisted(true);\n-                }\n+    /**\n+     * Executes external commit step.\n+     */\n+    private CompletableFuture<Void> executeExternalCommitAction(MetadataTransaction txn) {\n+        // Execute external commit step.\n+        try {\n+            if (null != txn.getExternalCommitStep()) {\n+                txn.getExternalCommitStep().call();\n             }\n+        } catch (Exception e) {\n+            log.error(\"Exception during execution of external commit step\", e);\n+            throw new CompletionException(new StorageMetadataException(\"Exception during execution of external commit step\", e));\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Execute external commit step.\n-            try {\n-                if (null != txn.getExternalCommitStep()) {\n-                    txn.getExternalCommitStep().call();\n-                }\n-            } catch (Exception e) {\n-                log.error(\"Exception during execution of external commit step\", e);\n-                throw new StorageMetadataException(\"Exception during execution of external commit step\", e);\n+    private void validateCommit(MetadataTransaction txn, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        for (val entry : txnData.entrySet()) {\n+            val key = entry.getKey();\n+            val transactionData = entry.getValue();\n+            Preconditions.checkState(null != transactionData.getKey());\n+\n+            // See if this entry was modified in this transaction.\n+            if (transactionData.getVersion() == txn.getVersion()) {\n+                modifiedKeys.add(key);\n+                transactionData.setPersisted(false);\n+                modifiedValues.add(transactionData);\n             }\n+            // make sure none of the keys used in this transaction have changed.\n+            val dataFromBuffer = bufferedTxnData.get(key);\n+            if (null != dataFromBuffer) {\n+                if (!dataFromBuffer.isPinned()) {\n+                    Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                }\n+                if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n+                    throw new CompletionException(new StorageMetadataVersionMismatchException(\n+                            String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n+                                    key, dataFromBuffer.getVersion(), txnData.get(key).getVersion())));\n+                }\n+\n+                // Pin it if it is already pinned.\n+                transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n \n-            // If we reach here then it means transaction is safe to commit.\n-            // Step 4: Insert\n-            long committedVersion = version.incrementAndGet();\n-            HashMap<String, TransactionData> toAdd = new HashMap<String, TransactionData>();\n-            for (String key : modifiedKeys) {\n-                TransactionData data = txnData.get(key);\n-                data.setVersion(committedVersion);\n-                toAdd.put(key, data);\n+                // Set the database object.\n+                transactionData.setDbObject(dataFromBuffer.getDbObject());\n+            } else {\n+                Preconditions.checkState(entry.getValue().isPinned(), \"Why are we here??\");\n             }\n-            bufferedTxnData.putAll(toAdd);\n         }\n+    }\n \n-        //  Step 5 : evict if required.\n-        if (bufferedTxnData.size() > maxEntriesInTxnBuffer) {\n-            bufferedTxnData.entrySet().removeIf(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned());\n+    /**\n+     * Evict entries if needed.\n+     * Only evict keys that are persisted, not pinned or active.\n+     */\n+    private CompletableFuture<Void> evictIfNeeded() {\n+        if (isEvictionRunning.compareAndSet(false, true)) {\n+            val limit = 1 + maxEntriesInTxnBuffer / CACHE_EVICTION_RATIO;\n+            if (bufferCount.get() > maxEntriesInTxnBuffer) {\n+                val toEvict = bufferedTxnData.entrySet().parallelStream()\n+                        .filter(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned()\n+                                && !activeKeys.contains(entry.getKey()))\n+                        .map(Map.Entry::getKey)\n+                        .limit(limit)\n+                        .collect(Collectors.toList());\n+                int i = 0;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MzY2NA=="}, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 500}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTYwODA5OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwMTowNjo1NFrOHucg9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMzoyNjoyMlrOHwFnqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MzczNA==", "bodyText": "Maybe log the number of evicted entries?", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r518463734", "createdAt": "2020-11-06T01:06:54Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+            if (fenced.get()) {\n+                throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));\n+            }\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Mark keys in transaction as active to prevent their eviction.\n+                    txn.getData().keySet().forEach(this::addToActiveKeySet);\n+\n+                    // Acquire a write lock over segment.\n+                    val tLock = new Timer();\n+                    log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));\n+                    val writeLock = scheduler.getWriteLock(txn.getKeysToLock());\n+                    return writeLock.lock()\n+                            .thenComposeAsync(v0 -> {\n+                                // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+                                // This step is kind of thread safe\n+                                val elapsed = tLock.getElapsed();\n+                                WRITE_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                                log.debug(\"Acquired write lock for {}, wait time: {} ms\",\n+                                        String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                                return loadMissingKeys(txn, skipStoreCheck, txnData);\n+                            }, executor)\n+                            .thenComposeAsync(v1 -> {\n+                                // This check needs to be atomic, with absolutely no possibility of re-entry\n+                                return performCommit(txn, lazyWrite, txnData, modifiedKeys, modifiedValues);\n+                            }, executor)\n+                            .whenCompleteAsync((v2, ex) -> writeLock.unlock(), executor);\n+                }, executor)\n+                .thenRunAsync(() -> {\n+                    //  Step 5 : evict if required.\n+                    txn.setCommitted();\n+                    txnData.clear();\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    // Remove keys from active set.\n+                    txn.getData().keySet().forEach(this::removeFromActiveKeySet);\n+                    COMMIT_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+\n+        // Trigger evict\n+        retValue.thenComposeAsync(v4 -> {\n+            //  Step 6 : evict if required.\n+            return evictIfNeeded();\n+        }, executor);\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        return retValue;\n+    }\n \n-        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n-        // This step is kind of thread safe\n+    /**\n+     * Loads missing keys.\n+     */\n+    private CompletableFuture<Void> loadMissingKeys(MetadataTransaction txn, boolean skipStoreCheck, Map<String, TransactionData> txnData) {\n+        val loadFutures = new ArrayList<CompletableFuture<TransactionData>>();\n         for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-            String key = entry.getKey();\n+            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+            val key = entry.getKey();\n             if (skipStoreCheck || entry.getValue().isPinned()) {\n                 log.trace(\"Skipping loading key from the store key = {}\", key);\n             } else {\n                 // This check is safe to be outside the lock\n-                if (!bufferedTxnData.containsKey(key)) {\n-                    loadFromStore(key);\n+                val dataFromBuffer = bufferedTxnData.get(key);\n+                if (null == dataFromBuffer) {\n+                    loadFutures.add(loadFromStore(txn, key, true));\n                 }\n             }\n         }\n-        // Step 2 : Check whether transaction is safe to commit.\n-        // This check needs to be atomic, with absolutely no possibility of re-entry\n-        synchronized (lock) {\n-            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-                String key = entry.getKey();\n-                val transactionData = entry.getValue();\n-                Preconditions.checkState(null != transactionData.getKey());\n-\n-                // See if this entry was modified in this transaction.\n-                if (transactionData.getVersion() == txn.getVersion()) {\n-                    modifiedKeys.add(key);\n-                    transactionData.setPersisted(false);\n-                    modifiedValues.add(transactionData);\n-                }\n-                // make sure none of the keys used in this transaction have changed.\n-                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n-                if (null != dataFromBuffer) {\n-                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n-                        throw new StorageMetadataVersionMismatchException(\n-                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n-                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+        return Futures.allOf(loadFutures)\n+                .thenApplyAsync(v4 -> {\n+                    // validate everything is alright.\n+                    for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                        val dataFromBuffer = bufferedTxnData.get(entry.getKey());\n+                        if (!(entry.getValue().isPinned())) {\n+                            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+                            Preconditions.checkState(null != dataFromBuffer);\n+                            if (!dataFromBuffer.isPinned()) {\n+                                Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                            }\n+                        }\n                     }\n+                    return null;\n+                }, executor);\n+    }\n \n-                    // Pin it if it is already pinned.\n-                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+    /**\n+     * Performs commit.\n+     */\n+    private CompletableFuture<Void> performCommit(MetadataTransaction txn, boolean lazyWrite, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        return CompletableFuture.runAsync(() -> {\n+            // Step 2 : Check whether transaction is safe to commit.\n+            validateCommit(txn, txnData, modifiedKeys, modifiedValues);\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Step 3: Commit externally.\n+                    // This operation may call external storage.\n+                    return writeToMetadataStore(lazyWrite, modifiedValues);\n+                }, executor)\n+                .thenComposeAsync(v ->\n+                                executeExternalCommitAction(txn),\n+                        executor)\n+                .thenRunAsync(() -> {\n+                    // If we reach here then it means transaction is safe to commit.\n+                    // Step 4: Update buffer.\n+                    val committedVersion = version.incrementAndGet();\n+                    val toAdd = new HashMap<String, TransactionData>();\n+                    for (String key : modifiedKeys) {\n+                        TransactionData data = txnData.get(key);\n+                        data.setVersion(committedVersion);\n+                        toAdd.put(key, data);\n+                    }\n+                    bufferedTxnData.putAll(toAdd);\n+                    bufferCount.addAndGet(toAdd.size());\n+                }, executor);\n+    }\n \n-                    // Set the database object.\n-                    transactionData.setDbObject(dataFromBuffer.getDbObject());\n-                }\n+    /**\n+     * Writes modified values to the metadata store.\n+     */\n+    private CompletableFuture<Void> writeToMetadataStore(boolean lazyWrite, ArrayList<TransactionData> modifiedValues) {\n+        if (!lazyWrite || (bufferCount.get() > maxEntriesInTxnBuffer)) {\n+            log.trace(\"Persisting all modified keys (except pinned)\");\n+            val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n+            if (toWriteList.size() > 0) {\n+                return writeAll(toWriteList)\n+                        .thenRunAsync(() -> {\n+                            log.trace(\"Done persisting all modified keys\");\n+                            for (val writtenData : toWriteList) {\n+                                // Mark written keys as persisted.\n+                                writtenData.setPersisted(true);\n+                                // Put it in cache.\n+                                cache.put(writtenData.getKey(), writtenData);\n+                            }\n+                        }, executor);\n+            } else {\n+                return CompletableFuture.completedFuture(null);\n             }\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Step 3: Commit externally.\n-            // This operation may call external storage.\n-            if (!lazyWrite || (bufferedTxnData.size() > maxEntriesInTxnBuffer)) {\n-                log.trace(\"Persisting all modified keys (except pinned)\");\n-                val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n-                writeAll(toWriteList);\n-                log.trace(\"Done persisting all modified keys\");\n-\n-                // Mark written keys as persisted.\n-                for (val writtenData : toWriteList) {\n-                    writtenData.setPersisted(true);\n-                }\n+    /**\n+     * Executes external commit step.\n+     */\n+    private CompletableFuture<Void> executeExternalCommitAction(MetadataTransaction txn) {\n+        // Execute external commit step.\n+        try {\n+            if (null != txn.getExternalCommitStep()) {\n+                txn.getExternalCommitStep().call();\n             }\n+        } catch (Exception e) {\n+            log.error(\"Exception during execution of external commit step\", e);\n+            throw new CompletionException(new StorageMetadataException(\"Exception during execution of external commit step\", e));\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Execute external commit step.\n-            try {\n-                if (null != txn.getExternalCommitStep()) {\n-                    txn.getExternalCommitStep().call();\n-                }\n-            } catch (Exception e) {\n-                log.error(\"Exception during execution of external commit step\", e);\n-                throw new StorageMetadataException(\"Exception during execution of external commit step\", e);\n+    private void validateCommit(MetadataTransaction txn, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        for (val entry : txnData.entrySet()) {\n+            val key = entry.getKey();\n+            val transactionData = entry.getValue();\n+            Preconditions.checkState(null != transactionData.getKey());\n+\n+            // See if this entry was modified in this transaction.\n+            if (transactionData.getVersion() == txn.getVersion()) {\n+                modifiedKeys.add(key);\n+                transactionData.setPersisted(false);\n+                modifiedValues.add(transactionData);\n             }\n+            // make sure none of the keys used in this transaction have changed.\n+            val dataFromBuffer = bufferedTxnData.get(key);\n+            if (null != dataFromBuffer) {\n+                if (!dataFromBuffer.isPinned()) {\n+                    Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                }\n+                if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n+                    throw new CompletionException(new StorageMetadataVersionMismatchException(\n+                            String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n+                                    key, dataFromBuffer.getVersion(), txnData.get(key).getVersion())));\n+                }\n+\n+                // Pin it if it is already pinned.\n+                transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n \n-            // If we reach here then it means transaction is safe to commit.\n-            // Step 4: Insert\n-            long committedVersion = version.incrementAndGet();\n-            HashMap<String, TransactionData> toAdd = new HashMap<String, TransactionData>();\n-            for (String key : modifiedKeys) {\n-                TransactionData data = txnData.get(key);\n-                data.setVersion(committedVersion);\n-                toAdd.put(key, data);\n+                // Set the database object.\n+                transactionData.setDbObject(dataFromBuffer.getDbObject());\n+            } else {\n+                Preconditions.checkState(entry.getValue().isPinned(), \"Why are we here??\");\n             }\n-            bufferedTxnData.putAll(toAdd);\n         }\n+    }\n \n-        //  Step 5 : evict if required.\n-        if (bufferedTxnData.size() > maxEntriesInTxnBuffer) {\n-            bufferedTxnData.entrySet().removeIf(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned());\n+    /**\n+     * Evict entries if needed.\n+     * Only evict keys that are persisted, not pinned or active.\n+     */\n+    private CompletableFuture<Void> evictIfNeeded() {\n+        if (isEvictionRunning.compareAndSet(false, true)) {\n+            val limit = 1 + maxEntriesInTxnBuffer / CACHE_EVICTION_RATIO;\n+            if (bufferCount.get() > maxEntriesInTxnBuffer) {\n+                val toEvict = bufferedTxnData.entrySet().parallelStream()\n+                        .filter(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned()\n+                                && !activeKeys.contains(entry.getKey()))\n+                        .map(Map.Entry::getKey)\n+                        .limit(limit)\n+                        .collect(Collectors.toList());\n+                int i = 0;\n+                for (val key : toEvict) {\n+                    // synchronize so that we don't accidentally delete a key that becomes active after check here.\n+                    synchronized (evictionLock) {\n+                        if (0 == activeKeys.count(key)) {\n+                            // Synchronization prevents error when key becomes active between the check and remove.\n+                            // Move the key to cache\n+                            cache.put(key, bufferedTxnData.get(key));\n+                            // Remove from buffer.\n+                            bufferedTxnData.remove(key);\n+                            i++;\n+                        }\n+                    }\n+                }\n+                bufferCount.addAndGet(-1 * i);\n+            }\n+            isEvictionRunning.set(false);\n+            log.debug(\"Entries evicted from transaction buffer.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 517}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE4NTc3MA==", "bodyText": "done", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520185770", "createdAt": "2020-11-09T23:26:22Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+            if (fenced.get()) {\n+                throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));\n+            }\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Mark keys in transaction as active to prevent their eviction.\n+                    txn.getData().keySet().forEach(this::addToActiveKeySet);\n+\n+                    // Acquire a write lock over segment.\n+                    val tLock = new Timer();\n+                    log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));\n+                    val writeLock = scheduler.getWriteLock(txn.getKeysToLock());\n+                    return writeLock.lock()\n+                            .thenComposeAsync(v0 -> {\n+                                // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+                                // This step is kind of thread safe\n+                                val elapsed = tLock.getElapsed();\n+                                WRITE_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                                log.debug(\"Acquired write lock for {}, wait time: {} ms\",\n+                                        String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                                return loadMissingKeys(txn, skipStoreCheck, txnData);\n+                            }, executor)\n+                            .thenComposeAsync(v1 -> {\n+                                // This check needs to be atomic, with absolutely no possibility of re-entry\n+                                return performCommit(txn, lazyWrite, txnData, modifiedKeys, modifiedValues);\n+                            }, executor)\n+                            .whenCompleteAsync((v2, ex) -> writeLock.unlock(), executor);\n+                }, executor)\n+                .thenRunAsync(() -> {\n+                    //  Step 5 : evict if required.\n+                    txn.setCommitted();\n+                    txnData.clear();\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    // Remove keys from active set.\n+                    txn.getData().keySet().forEach(this::removeFromActiveKeySet);\n+                    COMMIT_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+\n+        // Trigger evict\n+        retValue.thenComposeAsync(v4 -> {\n+            //  Step 6 : evict if required.\n+            return evictIfNeeded();\n+        }, executor);\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        return retValue;\n+    }\n \n-        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n-        // This step is kind of thread safe\n+    /**\n+     * Loads missing keys.\n+     */\n+    private CompletableFuture<Void> loadMissingKeys(MetadataTransaction txn, boolean skipStoreCheck, Map<String, TransactionData> txnData) {\n+        val loadFutures = new ArrayList<CompletableFuture<TransactionData>>();\n         for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-            String key = entry.getKey();\n+            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+            val key = entry.getKey();\n             if (skipStoreCheck || entry.getValue().isPinned()) {\n                 log.trace(\"Skipping loading key from the store key = {}\", key);\n             } else {\n                 // This check is safe to be outside the lock\n-                if (!bufferedTxnData.containsKey(key)) {\n-                    loadFromStore(key);\n+                val dataFromBuffer = bufferedTxnData.get(key);\n+                if (null == dataFromBuffer) {\n+                    loadFutures.add(loadFromStore(txn, key, true));\n                 }\n             }\n         }\n-        // Step 2 : Check whether transaction is safe to commit.\n-        // This check needs to be atomic, with absolutely no possibility of re-entry\n-        synchronized (lock) {\n-            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-                String key = entry.getKey();\n-                val transactionData = entry.getValue();\n-                Preconditions.checkState(null != transactionData.getKey());\n-\n-                // See if this entry was modified in this transaction.\n-                if (transactionData.getVersion() == txn.getVersion()) {\n-                    modifiedKeys.add(key);\n-                    transactionData.setPersisted(false);\n-                    modifiedValues.add(transactionData);\n-                }\n-                // make sure none of the keys used in this transaction have changed.\n-                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n-                if (null != dataFromBuffer) {\n-                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n-                        throw new StorageMetadataVersionMismatchException(\n-                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n-                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+        return Futures.allOf(loadFutures)\n+                .thenApplyAsync(v4 -> {\n+                    // validate everything is alright.\n+                    for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                        val dataFromBuffer = bufferedTxnData.get(entry.getKey());\n+                        if (!(entry.getValue().isPinned())) {\n+                            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+                            Preconditions.checkState(null != dataFromBuffer);\n+                            if (!dataFromBuffer.isPinned()) {\n+                                Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                            }\n+                        }\n                     }\n+                    return null;\n+                }, executor);\n+    }\n \n-                    // Pin it if it is already pinned.\n-                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+    /**\n+     * Performs commit.\n+     */\n+    private CompletableFuture<Void> performCommit(MetadataTransaction txn, boolean lazyWrite, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        return CompletableFuture.runAsync(() -> {\n+            // Step 2 : Check whether transaction is safe to commit.\n+            validateCommit(txn, txnData, modifiedKeys, modifiedValues);\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Step 3: Commit externally.\n+                    // This operation may call external storage.\n+                    return writeToMetadataStore(lazyWrite, modifiedValues);\n+                }, executor)\n+                .thenComposeAsync(v ->\n+                                executeExternalCommitAction(txn),\n+                        executor)\n+                .thenRunAsync(() -> {\n+                    // If we reach here then it means transaction is safe to commit.\n+                    // Step 4: Update buffer.\n+                    val committedVersion = version.incrementAndGet();\n+                    val toAdd = new HashMap<String, TransactionData>();\n+                    for (String key : modifiedKeys) {\n+                        TransactionData data = txnData.get(key);\n+                        data.setVersion(committedVersion);\n+                        toAdd.put(key, data);\n+                    }\n+                    bufferedTxnData.putAll(toAdd);\n+                    bufferCount.addAndGet(toAdd.size());\n+                }, executor);\n+    }\n \n-                    // Set the database object.\n-                    transactionData.setDbObject(dataFromBuffer.getDbObject());\n-                }\n+    /**\n+     * Writes modified values to the metadata store.\n+     */\n+    private CompletableFuture<Void> writeToMetadataStore(boolean lazyWrite, ArrayList<TransactionData> modifiedValues) {\n+        if (!lazyWrite || (bufferCount.get() > maxEntriesInTxnBuffer)) {\n+            log.trace(\"Persisting all modified keys (except pinned)\");\n+            val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n+            if (toWriteList.size() > 0) {\n+                return writeAll(toWriteList)\n+                        .thenRunAsync(() -> {\n+                            log.trace(\"Done persisting all modified keys\");\n+                            for (val writtenData : toWriteList) {\n+                                // Mark written keys as persisted.\n+                                writtenData.setPersisted(true);\n+                                // Put it in cache.\n+                                cache.put(writtenData.getKey(), writtenData);\n+                            }\n+                        }, executor);\n+            } else {\n+                return CompletableFuture.completedFuture(null);\n             }\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Step 3: Commit externally.\n-            // This operation may call external storage.\n-            if (!lazyWrite || (bufferedTxnData.size() > maxEntriesInTxnBuffer)) {\n-                log.trace(\"Persisting all modified keys (except pinned)\");\n-                val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n-                writeAll(toWriteList);\n-                log.trace(\"Done persisting all modified keys\");\n-\n-                // Mark written keys as persisted.\n-                for (val writtenData : toWriteList) {\n-                    writtenData.setPersisted(true);\n-                }\n+    /**\n+     * Executes external commit step.\n+     */\n+    private CompletableFuture<Void> executeExternalCommitAction(MetadataTransaction txn) {\n+        // Execute external commit step.\n+        try {\n+            if (null != txn.getExternalCommitStep()) {\n+                txn.getExternalCommitStep().call();\n             }\n+        } catch (Exception e) {\n+            log.error(\"Exception during execution of external commit step\", e);\n+            throw new CompletionException(new StorageMetadataException(\"Exception during execution of external commit step\", e));\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Execute external commit step.\n-            try {\n-                if (null != txn.getExternalCommitStep()) {\n-                    txn.getExternalCommitStep().call();\n-                }\n-            } catch (Exception e) {\n-                log.error(\"Exception during execution of external commit step\", e);\n-                throw new StorageMetadataException(\"Exception during execution of external commit step\", e);\n+    private void validateCommit(MetadataTransaction txn, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        for (val entry : txnData.entrySet()) {\n+            val key = entry.getKey();\n+            val transactionData = entry.getValue();\n+            Preconditions.checkState(null != transactionData.getKey());\n+\n+            // See if this entry was modified in this transaction.\n+            if (transactionData.getVersion() == txn.getVersion()) {\n+                modifiedKeys.add(key);\n+                transactionData.setPersisted(false);\n+                modifiedValues.add(transactionData);\n             }\n+            // make sure none of the keys used in this transaction have changed.\n+            val dataFromBuffer = bufferedTxnData.get(key);\n+            if (null != dataFromBuffer) {\n+                if (!dataFromBuffer.isPinned()) {\n+                    Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                }\n+                if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n+                    throw new CompletionException(new StorageMetadataVersionMismatchException(\n+                            String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n+                                    key, dataFromBuffer.getVersion(), txnData.get(key).getVersion())));\n+                }\n+\n+                // Pin it if it is already pinned.\n+                transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n \n-            // If we reach here then it means transaction is safe to commit.\n-            // Step 4: Insert\n-            long committedVersion = version.incrementAndGet();\n-            HashMap<String, TransactionData> toAdd = new HashMap<String, TransactionData>();\n-            for (String key : modifiedKeys) {\n-                TransactionData data = txnData.get(key);\n-                data.setVersion(committedVersion);\n-                toAdd.put(key, data);\n+                // Set the database object.\n+                transactionData.setDbObject(dataFromBuffer.getDbObject());\n+            } else {\n+                Preconditions.checkState(entry.getValue().isPinned(), \"Why are we here??\");\n             }\n-            bufferedTxnData.putAll(toAdd);\n         }\n+    }\n \n-        //  Step 5 : evict if required.\n-        if (bufferedTxnData.size() > maxEntriesInTxnBuffer) {\n-            bufferedTxnData.entrySet().removeIf(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned());\n+    /**\n+     * Evict entries if needed.\n+     * Only evict keys that are persisted, not pinned or active.\n+     */\n+    private CompletableFuture<Void> evictIfNeeded() {\n+        if (isEvictionRunning.compareAndSet(false, true)) {\n+            val limit = 1 + maxEntriesInTxnBuffer / CACHE_EVICTION_RATIO;\n+            if (bufferCount.get() > maxEntriesInTxnBuffer) {\n+                val toEvict = bufferedTxnData.entrySet().parallelStream()\n+                        .filter(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned()\n+                                && !activeKeys.contains(entry.getKey()))\n+                        .map(Map.Entry::getKey)\n+                        .limit(limit)\n+                        .collect(Collectors.toList());\n+                int i = 0;\n+                for (val key : toEvict) {\n+                    // synchronize so that we don't accidentally delete a key that becomes active after check here.\n+                    synchronized (evictionLock) {\n+                        if (0 == activeKeys.count(key)) {\n+                            // Synchronization prevents error when key becomes active between the check and remove.\n+                            // Move the key to cache\n+                            cache.put(key, bufferedTxnData.get(key));\n+                            // Remove from buffer.\n+                            bufferedTxnData.remove(key);\n+                            i++;\n+                        }\n+                    }\n+                }\n+                bufferCount.addAndGet(-1 * i);\n+            }\n+            isEvictionRunning.set(false);\n+            log.debug(\"Entries evicted from transaction buffer.\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2MzczNA=="}, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 517}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTYwODg1OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwMTowNzoyM1rOHuchfA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjoyNTo1OFrOHwEEfw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2Mzg2OA==", "bodyText": "Why do you return a CompletableFuture? Make your method return void if it's not async.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r518463868", "createdAt": "2020-11-06T01:07:23Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+            if (fenced.get()) {\n+                throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));\n+            }\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Mark keys in transaction as active to prevent their eviction.\n+                    txn.getData().keySet().forEach(this::addToActiveKeySet);\n+\n+                    // Acquire a write lock over segment.\n+                    val tLock = new Timer();\n+                    log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));\n+                    val writeLock = scheduler.getWriteLock(txn.getKeysToLock());\n+                    return writeLock.lock()\n+                            .thenComposeAsync(v0 -> {\n+                                // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+                                // This step is kind of thread safe\n+                                val elapsed = tLock.getElapsed();\n+                                WRITE_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                                log.debug(\"Acquired write lock for {}, wait time: {} ms\",\n+                                        String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                                return loadMissingKeys(txn, skipStoreCheck, txnData);\n+                            }, executor)\n+                            .thenComposeAsync(v1 -> {\n+                                // This check needs to be atomic, with absolutely no possibility of re-entry\n+                                return performCommit(txn, lazyWrite, txnData, modifiedKeys, modifiedValues);\n+                            }, executor)\n+                            .whenCompleteAsync((v2, ex) -> writeLock.unlock(), executor);\n+                }, executor)\n+                .thenRunAsync(() -> {\n+                    //  Step 5 : evict if required.\n+                    txn.setCommitted();\n+                    txnData.clear();\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    // Remove keys from active set.\n+                    txn.getData().keySet().forEach(this::removeFromActiveKeySet);\n+                    COMMIT_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+\n+        // Trigger evict\n+        retValue.thenComposeAsync(v4 -> {\n+            //  Step 6 : evict if required.\n+            return evictIfNeeded();\n+        }, executor);\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        return retValue;\n+    }\n \n-        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n-        // This step is kind of thread safe\n+    /**\n+     * Loads missing keys.\n+     */\n+    private CompletableFuture<Void> loadMissingKeys(MetadataTransaction txn, boolean skipStoreCheck, Map<String, TransactionData> txnData) {\n+        val loadFutures = new ArrayList<CompletableFuture<TransactionData>>();\n         for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-            String key = entry.getKey();\n+            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+            val key = entry.getKey();\n             if (skipStoreCheck || entry.getValue().isPinned()) {\n                 log.trace(\"Skipping loading key from the store key = {}\", key);\n             } else {\n                 // This check is safe to be outside the lock\n-                if (!bufferedTxnData.containsKey(key)) {\n-                    loadFromStore(key);\n+                val dataFromBuffer = bufferedTxnData.get(key);\n+                if (null == dataFromBuffer) {\n+                    loadFutures.add(loadFromStore(txn, key, true));\n                 }\n             }\n         }\n-        // Step 2 : Check whether transaction is safe to commit.\n-        // This check needs to be atomic, with absolutely no possibility of re-entry\n-        synchronized (lock) {\n-            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-                String key = entry.getKey();\n-                val transactionData = entry.getValue();\n-                Preconditions.checkState(null != transactionData.getKey());\n-\n-                // See if this entry was modified in this transaction.\n-                if (transactionData.getVersion() == txn.getVersion()) {\n-                    modifiedKeys.add(key);\n-                    transactionData.setPersisted(false);\n-                    modifiedValues.add(transactionData);\n-                }\n-                // make sure none of the keys used in this transaction have changed.\n-                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n-                if (null != dataFromBuffer) {\n-                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n-                        throw new StorageMetadataVersionMismatchException(\n-                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n-                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+        return Futures.allOf(loadFutures)\n+                .thenApplyAsync(v4 -> {\n+                    // validate everything is alright.\n+                    for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                        val dataFromBuffer = bufferedTxnData.get(entry.getKey());\n+                        if (!(entry.getValue().isPinned())) {\n+                            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+                            Preconditions.checkState(null != dataFromBuffer);\n+                            if (!dataFromBuffer.isPinned()) {\n+                                Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                            }\n+                        }\n                     }\n+                    return null;\n+                }, executor);\n+    }\n \n-                    // Pin it if it is already pinned.\n-                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+    /**\n+     * Performs commit.\n+     */\n+    private CompletableFuture<Void> performCommit(MetadataTransaction txn, boolean lazyWrite, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        return CompletableFuture.runAsync(() -> {\n+            // Step 2 : Check whether transaction is safe to commit.\n+            validateCommit(txn, txnData, modifiedKeys, modifiedValues);\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Step 3: Commit externally.\n+                    // This operation may call external storage.\n+                    return writeToMetadataStore(lazyWrite, modifiedValues);\n+                }, executor)\n+                .thenComposeAsync(v ->\n+                                executeExternalCommitAction(txn),\n+                        executor)\n+                .thenRunAsync(() -> {\n+                    // If we reach here then it means transaction is safe to commit.\n+                    // Step 4: Update buffer.\n+                    val committedVersion = version.incrementAndGet();\n+                    val toAdd = new HashMap<String, TransactionData>();\n+                    for (String key : modifiedKeys) {\n+                        TransactionData data = txnData.get(key);\n+                        data.setVersion(committedVersion);\n+                        toAdd.put(key, data);\n+                    }\n+                    bufferedTxnData.putAll(toAdd);\n+                    bufferCount.addAndGet(toAdd.size());\n+                }, executor);\n+    }\n \n-                    // Set the database object.\n-                    transactionData.setDbObject(dataFromBuffer.getDbObject());\n-                }\n+    /**\n+     * Writes modified values to the metadata store.\n+     */\n+    private CompletableFuture<Void> writeToMetadataStore(boolean lazyWrite, ArrayList<TransactionData> modifiedValues) {\n+        if (!lazyWrite || (bufferCount.get() > maxEntriesInTxnBuffer)) {\n+            log.trace(\"Persisting all modified keys (except pinned)\");\n+            val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n+            if (toWriteList.size() > 0) {\n+                return writeAll(toWriteList)\n+                        .thenRunAsync(() -> {\n+                            log.trace(\"Done persisting all modified keys\");\n+                            for (val writtenData : toWriteList) {\n+                                // Mark written keys as persisted.\n+                                writtenData.setPersisted(true);\n+                                // Put it in cache.\n+                                cache.put(writtenData.getKey(), writtenData);\n+                            }\n+                        }, executor);\n+            } else {\n+                return CompletableFuture.completedFuture(null);\n             }\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Step 3: Commit externally.\n-            // This operation may call external storage.\n-            if (!lazyWrite || (bufferedTxnData.size() > maxEntriesInTxnBuffer)) {\n-                log.trace(\"Persisting all modified keys (except pinned)\");\n-                val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n-                writeAll(toWriteList);\n-                log.trace(\"Done persisting all modified keys\");\n-\n-                // Mark written keys as persisted.\n-                for (val writtenData : toWriteList) {\n-                    writtenData.setPersisted(true);\n-                }\n+    /**\n+     * Executes external commit step.\n+     */\n+    private CompletableFuture<Void> executeExternalCommitAction(MetadataTransaction txn) {\n+        // Execute external commit step.\n+        try {\n+            if (null != txn.getExternalCommitStep()) {\n+                txn.getExternalCommitStep().call();\n             }\n+        } catch (Exception e) {\n+            log.error(\"Exception during execution of external commit step\", e);\n+            throw new CompletionException(new StorageMetadataException(\"Exception during execution of external commit step\", e));\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Execute external commit step.\n-            try {\n-                if (null != txn.getExternalCommitStep()) {\n-                    txn.getExternalCommitStep().call();\n-                }\n-            } catch (Exception e) {\n-                log.error(\"Exception during execution of external commit step\", e);\n-                throw new StorageMetadataException(\"Exception during execution of external commit step\", e);\n+    private void validateCommit(MetadataTransaction txn, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        for (val entry : txnData.entrySet()) {\n+            val key = entry.getKey();\n+            val transactionData = entry.getValue();\n+            Preconditions.checkState(null != transactionData.getKey());\n+\n+            // See if this entry was modified in this transaction.\n+            if (transactionData.getVersion() == txn.getVersion()) {\n+                modifiedKeys.add(key);\n+                transactionData.setPersisted(false);\n+                modifiedValues.add(transactionData);\n             }\n+            // make sure none of the keys used in this transaction have changed.\n+            val dataFromBuffer = bufferedTxnData.get(key);\n+            if (null != dataFromBuffer) {\n+                if (!dataFromBuffer.isPinned()) {\n+                    Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                }\n+                if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n+                    throw new CompletionException(new StorageMetadataVersionMismatchException(\n+                            String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n+                                    key, dataFromBuffer.getVersion(), txnData.get(key).getVersion())));\n+                }\n+\n+                // Pin it if it is already pinned.\n+                transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n \n-            // If we reach here then it means transaction is safe to commit.\n-            // Step 4: Insert\n-            long committedVersion = version.incrementAndGet();\n-            HashMap<String, TransactionData> toAdd = new HashMap<String, TransactionData>();\n-            for (String key : modifiedKeys) {\n-                TransactionData data = txnData.get(key);\n-                data.setVersion(committedVersion);\n-                toAdd.put(key, data);\n+                // Set the database object.\n+                transactionData.setDbObject(dataFromBuffer.getDbObject());\n+            } else {\n+                Preconditions.checkState(entry.getValue().isPinned(), \"Why are we here??\");\n             }\n-            bufferedTxnData.putAll(toAdd);\n         }\n+    }\n \n-        //  Step 5 : evict if required.\n-        if (bufferedTxnData.size() > maxEntriesInTxnBuffer) {\n-            bufferedTxnData.entrySet().removeIf(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned());\n+    /**\n+     * Evict entries if needed.\n+     * Only evict keys that are persisted, not pinned or active.\n+     */\n+    private CompletableFuture<Void> evictIfNeeded() {\n+        if (isEvictionRunning.compareAndSet(false, true)) {\n+            val limit = 1 + maxEntriesInTxnBuffer / CACHE_EVICTION_RATIO;\n+            if (bufferCount.get() > maxEntriesInTxnBuffer) {\n+                val toEvict = bufferedTxnData.entrySet().parallelStream()\n+                        .filter(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned()\n+                                && !activeKeys.contains(entry.getKey()))\n+                        .map(Map.Entry::getKey)\n+                        .limit(limit)\n+                        .collect(Collectors.toList());\n+                int i = 0;\n+                for (val key : toEvict) {\n+                    // synchronize so that we don't accidentally delete a key that becomes active after check here.\n+                    synchronized (evictionLock) {\n+                        if (0 == activeKeys.count(key)) {\n+                            // Synchronization prevents error when key becomes active between the check and remove.\n+                            // Move the key to cache\n+                            cache.put(key, bufferedTxnData.get(key));\n+                            // Remove from buffer.\n+                            bufferedTxnData.remove(key);\n+                            i++;\n+                        }\n+                    }\n+                }\n+                bufferCount.addAndGet(-1 * i);\n+            }\n+            isEvictionRunning.set(false);\n+            log.debug(\"Entries evicted from transaction buffer.\");\n         }\n-\n-        //  Step 6: finally clear\n-        txnData.clear();\n+        return CompletableFuture.completedFuture(null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 522}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MDM4Mw==", "bodyText": "fixed", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520160383", "createdAt": "2020-11-09T22:25:58Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+            if (fenced.get()) {\n+                throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));\n+            }\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Mark keys in transaction as active to prevent their eviction.\n+                    txn.getData().keySet().forEach(this::addToActiveKeySet);\n+\n+                    // Acquire a write lock over segment.\n+                    val tLock = new Timer();\n+                    log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));\n+                    val writeLock = scheduler.getWriteLock(txn.getKeysToLock());\n+                    return writeLock.lock()\n+                            .thenComposeAsync(v0 -> {\n+                                // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+                                // This step is kind of thread safe\n+                                val elapsed = tLock.getElapsed();\n+                                WRITE_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                                log.debug(\"Acquired write lock for {}, wait time: {} ms\",\n+                                        String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                                return loadMissingKeys(txn, skipStoreCheck, txnData);\n+                            }, executor)\n+                            .thenComposeAsync(v1 -> {\n+                                // This check needs to be atomic, with absolutely no possibility of re-entry\n+                                return performCommit(txn, lazyWrite, txnData, modifiedKeys, modifiedValues);\n+                            }, executor)\n+                            .whenCompleteAsync((v2, ex) -> writeLock.unlock(), executor);\n+                }, executor)\n+                .thenRunAsync(() -> {\n+                    //  Step 5 : evict if required.\n+                    txn.setCommitted();\n+                    txnData.clear();\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    // Remove keys from active set.\n+                    txn.getData().keySet().forEach(this::removeFromActiveKeySet);\n+                    COMMIT_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+\n+        // Trigger evict\n+        retValue.thenComposeAsync(v4 -> {\n+            //  Step 6 : evict if required.\n+            return evictIfNeeded();\n+        }, executor);\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        return retValue;\n+    }\n \n-        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n-        // This step is kind of thread safe\n+    /**\n+     * Loads missing keys.\n+     */\n+    private CompletableFuture<Void> loadMissingKeys(MetadataTransaction txn, boolean skipStoreCheck, Map<String, TransactionData> txnData) {\n+        val loadFutures = new ArrayList<CompletableFuture<TransactionData>>();\n         for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-            String key = entry.getKey();\n+            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+            val key = entry.getKey();\n             if (skipStoreCheck || entry.getValue().isPinned()) {\n                 log.trace(\"Skipping loading key from the store key = {}\", key);\n             } else {\n                 // This check is safe to be outside the lock\n-                if (!bufferedTxnData.containsKey(key)) {\n-                    loadFromStore(key);\n+                val dataFromBuffer = bufferedTxnData.get(key);\n+                if (null == dataFromBuffer) {\n+                    loadFutures.add(loadFromStore(txn, key, true));\n                 }\n             }\n         }\n-        // Step 2 : Check whether transaction is safe to commit.\n-        // This check needs to be atomic, with absolutely no possibility of re-entry\n-        synchronized (lock) {\n-            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-                String key = entry.getKey();\n-                val transactionData = entry.getValue();\n-                Preconditions.checkState(null != transactionData.getKey());\n-\n-                // See if this entry was modified in this transaction.\n-                if (transactionData.getVersion() == txn.getVersion()) {\n-                    modifiedKeys.add(key);\n-                    transactionData.setPersisted(false);\n-                    modifiedValues.add(transactionData);\n-                }\n-                // make sure none of the keys used in this transaction have changed.\n-                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n-                if (null != dataFromBuffer) {\n-                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n-                        throw new StorageMetadataVersionMismatchException(\n-                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n-                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+        return Futures.allOf(loadFutures)\n+                .thenApplyAsync(v4 -> {\n+                    // validate everything is alright.\n+                    for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                        val dataFromBuffer = bufferedTxnData.get(entry.getKey());\n+                        if (!(entry.getValue().isPinned())) {\n+                            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+                            Preconditions.checkState(null != dataFromBuffer);\n+                            if (!dataFromBuffer.isPinned()) {\n+                                Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                            }\n+                        }\n                     }\n+                    return null;\n+                }, executor);\n+    }\n \n-                    // Pin it if it is already pinned.\n-                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+    /**\n+     * Performs commit.\n+     */\n+    private CompletableFuture<Void> performCommit(MetadataTransaction txn, boolean lazyWrite, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        return CompletableFuture.runAsync(() -> {\n+            // Step 2 : Check whether transaction is safe to commit.\n+            validateCommit(txn, txnData, modifiedKeys, modifiedValues);\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Step 3: Commit externally.\n+                    // This operation may call external storage.\n+                    return writeToMetadataStore(lazyWrite, modifiedValues);\n+                }, executor)\n+                .thenComposeAsync(v ->\n+                                executeExternalCommitAction(txn),\n+                        executor)\n+                .thenRunAsync(() -> {\n+                    // If we reach here then it means transaction is safe to commit.\n+                    // Step 4: Update buffer.\n+                    val committedVersion = version.incrementAndGet();\n+                    val toAdd = new HashMap<String, TransactionData>();\n+                    for (String key : modifiedKeys) {\n+                        TransactionData data = txnData.get(key);\n+                        data.setVersion(committedVersion);\n+                        toAdd.put(key, data);\n+                    }\n+                    bufferedTxnData.putAll(toAdd);\n+                    bufferCount.addAndGet(toAdd.size());\n+                }, executor);\n+    }\n \n-                    // Set the database object.\n-                    transactionData.setDbObject(dataFromBuffer.getDbObject());\n-                }\n+    /**\n+     * Writes modified values to the metadata store.\n+     */\n+    private CompletableFuture<Void> writeToMetadataStore(boolean lazyWrite, ArrayList<TransactionData> modifiedValues) {\n+        if (!lazyWrite || (bufferCount.get() > maxEntriesInTxnBuffer)) {\n+            log.trace(\"Persisting all modified keys (except pinned)\");\n+            val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n+            if (toWriteList.size() > 0) {\n+                return writeAll(toWriteList)\n+                        .thenRunAsync(() -> {\n+                            log.trace(\"Done persisting all modified keys\");\n+                            for (val writtenData : toWriteList) {\n+                                // Mark written keys as persisted.\n+                                writtenData.setPersisted(true);\n+                                // Put it in cache.\n+                                cache.put(writtenData.getKey(), writtenData);\n+                            }\n+                        }, executor);\n+            } else {\n+                return CompletableFuture.completedFuture(null);\n             }\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Step 3: Commit externally.\n-            // This operation may call external storage.\n-            if (!lazyWrite || (bufferedTxnData.size() > maxEntriesInTxnBuffer)) {\n-                log.trace(\"Persisting all modified keys (except pinned)\");\n-                val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n-                writeAll(toWriteList);\n-                log.trace(\"Done persisting all modified keys\");\n-\n-                // Mark written keys as persisted.\n-                for (val writtenData : toWriteList) {\n-                    writtenData.setPersisted(true);\n-                }\n+    /**\n+     * Executes external commit step.\n+     */\n+    private CompletableFuture<Void> executeExternalCommitAction(MetadataTransaction txn) {\n+        // Execute external commit step.\n+        try {\n+            if (null != txn.getExternalCommitStep()) {\n+                txn.getExternalCommitStep().call();\n             }\n+        } catch (Exception e) {\n+            log.error(\"Exception during execution of external commit step\", e);\n+            throw new CompletionException(new StorageMetadataException(\"Exception during execution of external commit step\", e));\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Execute external commit step.\n-            try {\n-                if (null != txn.getExternalCommitStep()) {\n-                    txn.getExternalCommitStep().call();\n-                }\n-            } catch (Exception e) {\n-                log.error(\"Exception during execution of external commit step\", e);\n-                throw new StorageMetadataException(\"Exception during execution of external commit step\", e);\n+    private void validateCommit(MetadataTransaction txn, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        for (val entry : txnData.entrySet()) {\n+            val key = entry.getKey();\n+            val transactionData = entry.getValue();\n+            Preconditions.checkState(null != transactionData.getKey());\n+\n+            // See if this entry was modified in this transaction.\n+            if (transactionData.getVersion() == txn.getVersion()) {\n+                modifiedKeys.add(key);\n+                transactionData.setPersisted(false);\n+                modifiedValues.add(transactionData);\n             }\n+            // make sure none of the keys used in this transaction have changed.\n+            val dataFromBuffer = bufferedTxnData.get(key);\n+            if (null != dataFromBuffer) {\n+                if (!dataFromBuffer.isPinned()) {\n+                    Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                }\n+                if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n+                    throw new CompletionException(new StorageMetadataVersionMismatchException(\n+                            String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n+                                    key, dataFromBuffer.getVersion(), txnData.get(key).getVersion())));\n+                }\n+\n+                // Pin it if it is already pinned.\n+                transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n \n-            // If we reach here then it means transaction is safe to commit.\n-            // Step 4: Insert\n-            long committedVersion = version.incrementAndGet();\n-            HashMap<String, TransactionData> toAdd = new HashMap<String, TransactionData>();\n-            for (String key : modifiedKeys) {\n-                TransactionData data = txnData.get(key);\n-                data.setVersion(committedVersion);\n-                toAdd.put(key, data);\n+                // Set the database object.\n+                transactionData.setDbObject(dataFromBuffer.getDbObject());\n+            } else {\n+                Preconditions.checkState(entry.getValue().isPinned(), \"Why are we here??\");\n             }\n-            bufferedTxnData.putAll(toAdd);\n         }\n+    }\n \n-        //  Step 5 : evict if required.\n-        if (bufferedTxnData.size() > maxEntriesInTxnBuffer) {\n-            bufferedTxnData.entrySet().removeIf(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned());\n+    /**\n+     * Evict entries if needed.\n+     * Only evict keys that are persisted, not pinned or active.\n+     */\n+    private CompletableFuture<Void> evictIfNeeded() {\n+        if (isEvictionRunning.compareAndSet(false, true)) {\n+            val limit = 1 + maxEntriesInTxnBuffer / CACHE_EVICTION_RATIO;\n+            if (bufferCount.get() > maxEntriesInTxnBuffer) {\n+                val toEvict = bufferedTxnData.entrySet().parallelStream()\n+                        .filter(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned()\n+                                && !activeKeys.contains(entry.getKey()))\n+                        .map(Map.Entry::getKey)\n+                        .limit(limit)\n+                        .collect(Collectors.toList());\n+                int i = 0;\n+                for (val key : toEvict) {\n+                    // synchronize so that we don't accidentally delete a key that becomes active after check here.\n+                    synchronized (evictionLock) {\n+                        if (0 == activeKeys.count(key)) {\n+                            // Synchronization prevents error when key becomes active between the check and remove.\n+                            // Move the key to cache\n+                            cache.put(key, bufferedTxnData.get(key));\n+                            // Remove from buffer.\n+                            bufferedTxnData.remove(key);\n+                            i++;\n+                        }\n+                    }\n+                }\n+                bufferCount.addAndGet(-1 * i);\n+            }\n+            isEvictionRunning.set(false);\n+            log.debug(\"Entries evicted from transaction buffer.\");\n         }\n-\n-        //  Step 6: finally clear\n-        txnData.clear();\n+        return CompletableFuture.completedFuture(null);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2Mzg2OA=="}, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 522}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTYwOTcwOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwMTowNzo1NVrOHuciDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjoyNTo0MVrOHwED-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2NDAxMw==", "bodyText": "I assume that some subclass will override this?", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r518464013", "createdAt": "2020-11-06T01:07:55Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+            if (fenced.get()) {\n+                throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));\n+            }\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Mark keys in transaction as active to prevent their eviction.\n+                    txn.getData().keySet().forEach(this::addToActiveKeySet);\n+\n+                    // Acquire a write lock over segment.\n+                    val tLock = new Timer();\n+                    log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));\n+                    val writeLock = scheduler.getWriteLock(txn.getKeysToLock());\n+                    return writeLock.lock()\n+                            .thenComposeAsync(v0 -> {\n+                                // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+                                // This step is kind of thread safe\n+                                val elapsed = tLock.getElapsed();\n+                                WRITE_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                                log.debug(\"Acquired write lock for {}, wait time: {} ms\",\n+                                        String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                                return loadMissingKeys(txn, skipStoreCheck, txnData);\n+                            }, executor)\n+                            .thenComposeAsync(v1 -> {\n+                                // This check needs to be atomic, with absolutely no possibility of re-entry\n+                                return performCommit(txn, lazyWrite, txnData, modifiedKeys, modifiedValues);\n+                            }, executor)\n+                            .whenCompleteAsync((v2, ex) -> writeLock.unlock(), executor);\n+                }, executor)\n+                .thenRunAsync(() -> {\n+                    //  Step 5 : evict if required.\n+                    txn.setCommitted();\n+                    txnData.clear();\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    // Remove keys from active set.\n+                    txn.getData().keySet().forEach(this::removeFromActiveKeySet);\n+                    COMMIT_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+\n+        // Trigger evict\n+        retValue.thenComposeAsync(v4 -> {\n+            //  Step 6 : evict if required.\n+            return evictIfNeeded();\n+        }, executor);\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        return retValue;\n+    }\n \n-        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n-        // This step is kind of thread safe\n+    /**\n+     * Loads missing keys.\n+     */\n+    private CompletableFuture<Void> loadMissingKeys(MetadataTransaction txn, boolean skipStoreCheck, Map<String, TransactionData> txnData) {\n+        val loadFutures = new ArrayList<CompletableFuture<TransactionData>>();\n         for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-            String key = entry.getKey();\n+            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+            val key = entry.getKey();\n             if (skipStoreCheck || entry.getValue().isPinned()) {\n                 log.trace(\"Skipping loading key from the store key = {}\", key);\n             } else {\n                 // This check is safe to be outside the lock\n-                if (!bufferedTxnData.containsKey(key)) {\n-                    loadFromStore(key);\n+                val dataFromBuffer = bufferedTxnData.get(key);\n+                if (null == dataFromBuffer) {\n+                    loadFutures.add(loadFromStore(txn, key, true));\n                 }\n             }\n         }\n-        // Step 2 : Check whether transaction is safe to commit.\n-        // This check needs to be atomic, with absolutely no possibility of re-entry\n-        synchronized (lock) {\n-            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-                String key = entry.getKey();\n-                val transactionData = entry.getValue();\n-                Preconditions.checkState(null != transactionData.getKey());\n-\n-                // See if this entry was modified in this transaction.\n-                if (transactionData.getVersion() == txn.getVersion()) {\n-                    modifiedKeys.add(key);\n-                    transactionData.setPersisted(false);\n-                    modifiedValues.add(transactionData);\n-                }\n-                // make sure none of the keys used in this transaction have changed.\n-                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n-                if (null != dataFromBuffer) {\n-                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n-                        throw new StorageMetadataVersionMismatchException(\n-                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n-                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+        return Futures.allOf(loadFutures)\n+                .thenApplyAsync(v4 -> {\n+                    // validate everything is alright.\n+                    for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                        val dataFromBuffer = bufferedTxnData.get(entry.getKey());\n+                        if (!(entry.getValue().isPinned())) {\n+                            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+                            Preconditions.checkState(null != dataFromBuffer);\n+                            if (!dataFromBuffer.isPinned()) {\n+                                Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                            }\n+                        }\n                     }\n+                    return null;\n+                }, executor);\n+    }\n \n-                    // Pin it if it is already pinned.\n-                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+    /**\n+     * Performs commit.\n+     */\n+    private CompletableFuture<Void> performCommit(MetadataTransaction txn, boolean lazyWrite, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        return CompletableFuture.runAsync(() -> {\n+            // Step 2 : Check whether transaction is safe to commit.\n+            validateCommit(txn, txnData, modifiedKeys, modifiedValues);\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Step 3: Commit externally.\n+                    // This operation may call external storage.\n+                    return writeToMetadataStore(lazyWrite, modifiedValues);\n+                }, executor)\n+                .thenComposeAsync(v ->\n+                                executeExternalCommitAction(txn),\n+                        executor)\n+                .thenRunAsync(() -> {\n+                    // If we reach here then it means transaction is safe to commit.\n+                    // Step 4: Update buffer.\n+                    val committedVersion = version.incrementAndGet();\n+                    val toAdd = new HashMap<String, TransactionData>();\n+                    for (String key : modifiedKeys) {\n+                        TransactionData data = txnData.get(key);\n+                        data.setVersion(committedVersion);\n+                        toAdd.put(key, data);\n+                    }\n+                    bufferedTxnData.putAll(toAdd);\n+                    bufferCount.addAndGet(toAdd.size());\n+                }, executor);\n+    }\n \n-                    // Set the database object.\n-                    transactionData.setDbObject(dataFromBuffer.getDbObject());\n-                }\n+    /**\n+     * Writes modified values to the metadata store.\n+     */\n+    private CompletableFuture<Void> writeToMetadataStore(boolean lazyWrite, ArrayList<TransactionData> modifiedValues) {\n+        if (!lazyWrite || (bufferCount.get() > maxEntriesInTxnBuffer)) {\n+            log.trace(\"Persisting all modified keys (except pinned)\");\n+            val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n+            if (toWriteList.size() > 0) {\n+                return writeAll(toWriteList)\n+                        .thenRunAsync(() -> {\n+                            log.trace(\"Done persisting all modified keys\");\n+                            for (val writtenData : toWriteList) {\n+                                // Mark written keys as persisted.\n+                                writtenData.setPersisted(true);\n+                                // Put it in cache.\n+                                cache.put(writtenData.getKey(), writtenData);\n+                            }\n+                        }, executor);\n+            } else {\n+                return CompletableFuture.completedFuture(null);\n             }\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Step 3: Commit externally.\n-            // This operation may call external storage.\n-            if (!lazyWrite || (bufferedTxnData.size() > maxEntriesInTxnBuffer)) {\n-                log.trace(\"Persisting all modified keys (except pinned)\");\n-                val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n-                writeAll(toWriteList);\n-                log.trace(\"Done persisting all modified keys\");\n-\n-                // Mark written keys as persisted.\n-                for (val writtenData : toWriteList) {\n-                    writtenData.setPersisted(true);\n-                }\n+    /**\n+     * Executes external commit step.\n+     */\n+    private CompletableFuture<Void> executeExternalCommitAction(MetadataTransaction txn) {\n+        // Execute external commit step.\n+        try {\n+            if (null != txn.getExternalCommitStep()) {\n+                txn.getExternalCommitStep().call();\n             }\n+        } catch (Exception e) {\n+            log.error(\"Exception during execution of external commit step\", e);\n+            throw new CompletionException(new StorageMetadataException(\"Exception during execution of external commit step\", e));\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Execute external commit step.\n-            try {\n-                if (null != txn.getExternalCommitStep()) {\n-                    txn.getExternalCommitStep().call();\n-                }\n-            } catch (Exception e) {\n-                log.error(\"Exception during execution of external commit step\", e);\n-                throw new StorageMetadataException(\"Exception during execution of external commit step\", e);\n+    private void validateCommit(MetadataTransaction txn, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        for (val entry : txnData.entrySet()) {\n+            val key = entry.getKey();\n+            val transactionData = entry.getValue();\n+            Preconditions.checkState(null != transactionData.getKey());\n+\n+            // See if this entry was modified in this transaction.\n+            if (transactionData.getVersion() == txn.getVersion()) {\n+                modifiedKeys.add(key);\n+                transactionData.setPersisted(false);\n+                modifiedValues.add(transactionData);\n             }\n+            // make sure none of the keys used in this transaction have changed.\n+            val dataFromBuffer = bufferedTxnData.get(key);\n+            if (null != dataFromBuffer) {\n+                if (!dataFromBuffer.isPinned()) {\n+                    Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                }\n+                if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n+                    throw new CompletionException(new StorageMetadataVersionMismatchException(\n+                            String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n+                                    key, dataFromBuffer.getVersion(), txnData.get(key).getVersion())));\n+                }\n+\n+                // Pin it if it is already pinned.\n+                transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n \n-            // If we reach here then it means transaction is safe to commit.\n-            // Step 4: Insert\n-            long committedVersion = version.incrementAndGet();\n-            HashMap<String, TransactionData> toAdd = new HashMap<String, TransactionData>();\n-            for (String key : modifiedKeys) {\n-                TransactionData data = txnData.get(key);\n-                data.setVersion(committedVersion);\n-                toAdd.put(key, data);\n+                // Set the database object.\n+                transactionData.setDbObject(dataFromBuffer.getDbObject());\n+            } else {\n+                Preconditions.checkState(entry.getValue().isPinned(), \"Why are we here??\");\n             }\n-            bufferedTxnData.putAll(toAdd);\n         }\n+    }\n \n-        //  Step 5 : evict if required.\n-        if (bufferedTxnData.size() > maxEntriesInTxnBuffer) {\n-            bufferedTxnData.entrySet().removeIf(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned());\n+    /**\n+     * Evict entries if needed.\n+     * Only evict keys that are persisted, not pinned or active.\n+     */\n+    private CompletableFuture<Void> evictIfNeeded() {\n+        if (isEvictionRunning.compareAndSet(false, true)) {\n+            val limit = 1 + maxEntriesInTxnBuffer / CACHE_EVICTION_RATIO;\n+            if (bufferCount.get() > maxEntriesInTxnBuffer) {\n+                val toEvict = bufferedTxnData.entrySet().parallelStream()\n+                        .filter(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned()\n+                                && !activeKeys.contains(entry.getKey()))\n+                        .map(Map.Entry::getKey)\n+                        .limit(limit)\n+                        .collect(Collectors.toList());\n+                int i = 0;\n+                for (val key : toEvict) {\n+                    // synchronize so that we don't accidentally delete a key that becomes active after check here.\n+                    synchronized (evictionLock) {\n+                        if (0 == activeKeys.count(key)) {\n+                            // Synchronization prevents error when key becomes active between the check and remove.\n+                            // Move the key to cache\n+                            cache.put(key, bufferedTxnData.get(key));\n+                            // Remove from buffer.\n+                            bufferedTxnData.remove(key);\n+                            i++;\n+                        }\n+                    }\n+                }\n+                bufferCount.addAndGet(-1 * i);\n+            }\n+            isEvictionRunning.set(false);\n+            log.debug(\"Entries evicted from transaction buffer.\");\n         }\n-\n-        //  Step 6: finally clear\n-        txnData.clear();\n+        return CompletableFuture.completedFuture(null);\n     }\n \n     /**\n      * Aborts given transaction.\n      *\n      * @param txn transaction to abort.\n-     * @throws StorageMetadataException If there are any errors.\n+     *            throws StorageMetadataException If there are any errors.\n      */\n-    public void abort(MetadataTransaction txn) throws StorageMetadataException {\n+    public CompletableFuture<Void> abort(MetadataTransaction txn) {\n         // Do nothing\n+        return CompletableFuture.completedFuture(null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 535}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MDI1MQ==", "bodyText": "yes", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520160251", "createdAt": "2020-11-09T22:25:41Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+            if (fenced.get()) {\n+                throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));\n+            }\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Mark keys in transaction as active to prevent their eviction.\n+                    txn.getData().keySet().forEach(this::addToActiveKeySet);\n+\n+                    // Acquire a write lock over segment.\n+                    val tLock = new Timer();\n+                    log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));\n+                    val writeLock = scheduler.getWriteLock(txn.getKeysToLock());\n+                    return writeLock.lock()\n+                            .thenComposeAsync(v0 -> {\n+                                // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+                                // This step is kind of thread safe\n+                                val elapsed = tLock.getElapsed();\n+                                WRITE_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                                log.debug(\"Acquired write lock for {}, wait time: {} ms\",\n+                                        String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                                return loadMissingKeys(txn, skipStoreCheck, txnData);\n+                            }, executor)\n+                            .thenComposeAsync(v1 -> {\n+                                // This check needs to be atomic, with absolutely no possibility of re-entry\n+                                return performCommit(txn, lazyWrite, txnData, modifiedKeys, modifiedValues);\n+                            }, executor)\n+                            .whenCompleteAsync((v2, ex) -> writeLock.unlock(), executor);\n+                }, executor)\n+                .thenRunAsync(() -> {\n+                    //  Step 5 : evict if required.\n+                    txn.setCommitted();\n+                    txnData.clear();\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    // Remove keys from active set.\n+                    txn.getData().keySet().forEach(this::removeFromActiveKeySet);\n+                    COMMIT_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+\n+        // Trigger evict\n+        retValue.thenComposeAsync(v4 -> {\n+            //  Step 6 : evict if required.\n+            return evictIfNeeded();\n+        }, executor);\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        return retValue;\n+    }\n \n-        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n-        // This step is kind of thread safe\n+    /**\n+     * Loads missing keys.\n+     */\n+    private CompletableFuture<Void> loadMissingKeys(MetadataTransaction txn, boolean skipStoreCheck, Map<String, TransactionData> txnData) {\n+        val loadFutures = new ArrayList<CompletableFuture<TransactionData>>();\n         for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-            String key = entry.getKey();\n+            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+            val key = entry.getKey();\n             if (skipStoreCheck || entry.getValue().isPinned()) {\n                 log.trace(\"Skipping loading key from the store key = {}\", key);\n             } else {\n                 // This check is safe to be outside the lock\n-                if (!bufferedTxnData.containsKey(key)) {\n-                    loadFromStore(key);\n+                val dataFromBuffer = bufferedTxnData.get(key);\n+                if (null == dataFromBuffer) {\n+                    loadFutures.add(loadFromStore(txn, key, true));\n                 }\n             }\n         }\n-        // Step 2 : Check whether transaction is safe to commit.\n-        // This check needs to be atomic, with absolutely no possibility of re-entry\n-        synchronized (lock) {\n-            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-                String key = entry.getKey();\n-                val transactionData = entry.getValue();\n-                Preconditions.checkState(null != transactionData.getKey());\n-\n-                // See if this entry was modified in this transaction.\n-                if (transactionData.getVersion() == txn.getVersion()) {\n-                    modifiedKeys.add(key);\n-                    transactionData.setPersisted(false);\n-                    modifiedValues.add(transactionData);\n-                }\n-                // make sure none of the keys used in this transaction have changed.\n-                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n-                if (null != dataFromBuffer) {\n-                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n-                        throw new StorageMetadataVersionMismatchException(\n-                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n-                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+        return Futures.allOf(loadFutures)\n+                .thenApplyAsync(v4 -> {\n+                    // validate everything is alright.\n+                    for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                        val dataFromBuffer = bufferedTxnData.get(entry.getKey());\n+                        if (!(entry.getValue().isPinned())) {\n+                            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+                            Preconditions.checkState(null != dataFromBuffer);\n+                            if (!dataFromBuffer.isPinned()) {\n+                                Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                            }\n+                        }\n                     }\n+                    return null;\n+                }, executor);\n+    }\n \n-                    // Pin it if it is already pinned.\n-                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+    /**\n+     * Performs commit.\n+     */\n+    private CompletableFuture<Void> performCommit(MetadataTransaction txn, boolean lazyWrite, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        return CompletableFuture.runAsync(() -> {\n+            // Step 2 : Check whether transaction is safe to commit.\n+            validateCommit(txn, txnData, modifiedKeys, modifiedValues);\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Step 3: Commit externally.\n+                    // This operation may call external storage.\n+                    return writeToMetadataStore(lazyWrite, modifiedValues);\n+                }, executor)\n+                .thenComposeAsync(v ->\n+                                executeExternalCommitAction(txn),\n+                        executor)\n+                .thenRunAsync(() -> {\n+                    // If we reach here then it means transaction is safe to commit.\n+                    // Step 4: Update buffer.\n+                    val committedVersion = version.incrementAndGet();\n+                    val toAdd = new HashMap<String, TransactionData>();\n+                    for (String key : modifiedKeys) {\n+                        TransactionData data = txnData.get(key);\n+                        data.setVersion(committedVersion);\n+                        toAdd.put(key, data);\n+                    }\n+                    bufferedTxnData.putAll(toAdd);\n+                    bufferCount.addAndGet(toAdd.size());\n+                }, executor);\n+    }\n \n-                    // Set the database object.\n-                    transactionData.setDbObject(dataFromBuffer.getDbObject());\n-                }\n+    /**\n+     * Writes modified values to the metadata store.\n+     */\n+    private CompletableFuture<Void> writeToMetadataStore(boolean lazyWrite, ArrayList<TransactionData> modifiedValues) {\n+        if (!lazyWrite || (bufferCount.get() > maxEntriesInTxnBuffer)) {\n+            log.trace(\"Persisting all modified keys (except pinned)\");\n+            val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n+            if (toWriteList.size() > 0) {\n+                return writeAll(toWriteList)\n+                        .thenRunAsync(() -> {\n+                            log.trace(\"Done persisting all modified keys\");\n+                            for (val writtenData : toWriteList) {\n+                                // Mark written keys as persisted.\n+                                writtenData.setPersisted(true);\n+                                // Put it in cache.\n+                                cache.put(writtenData.getKey(), writtenData);\n+                            }\n+                        }, executor);\n+            } else {\n+                return CompletableFuture.completedFuture(null);\n             }\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Step 3: Commit externally.\n-            // This operation may call external storage.\n-            if (!lazyWrite || (bufferedTxnData.size() > maxEntriesInTxnBuffer)) {\n-                log.trace(\"Persisting all modified keys (except pinned)\");\n-                val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n-                writeAll(toWriteList);\n-                log.trace(\"Done persisting all modified keys\");\n-\n-                // Mark written keys as persisted.\n-                for (val writtenData : toWriteList) {\n-                    writtenData.setPersisted(true);\n-                }\n+    /**\n+     * Executes external commit step.\n+     */\n+    private CompletableFuture<Void> executeExternalCommitAction(MetadataTransaction txn) {\n+        // Execute external commit step.\n+        try {\n+            if (null != txn.getExternalCommitStep()) {\n+                txn.getExternalCommitStep().call();\n             }\n+        } catch (Exception e) {\n+            log.error(\"Exception during execution of external commit step\", e);\n+            throw new CompletionException(new StorageMetadataException(\"Exception during execution of external commit step\", e));\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Execute external commit step.\n-            try {\n-                if (null != txn.getExternalCommitStep()) {\n-                    txn.getExternalCommitStep().call();\n-                }\n-            } catch (Exception e) {\n-                log.error(\"Exception during execution of external commit step\", e);\n-                throw new StorageMetadataException(\"Exception during execution of external commit step\", e);\n+    private void validateCommit(MetadataTransaction txn, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        for (val entry : txnData.entrySet()) {\n+            val key = entry.getKey();\n+            val transactionData = entry.getValue();\n+            Preconditions.checkState(null != transactionData.getKey());\n+\n+            // See if this entry was modified in this transaction.\n+            if (transactionData.getVersion() == txn.getVersion()) {\n+                modifiedKeys.add(key);\n+                transactionData.setPersisted(false);\n+                modifiedValues.add(transactionData);\n             }\n+            // make sure none of the keys used in this transaction have changed.\n+            val dataFromBuffer = bufferedTxnData.get(key);\n+            if (null != dataFromBuffer) {\n+                if (!dataFromBuffer.isPinned()) {\n+                    Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                }\n+                if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n+                    throw new CompletionException(new StorageMetadataVersionMismatchException(\n+                            String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n+                                    key, dataFromBuffer.getVersion(), txnData.get(key).getVersion())));\n+                }\n+\n+                // Pin it if it is already pinned.\n+                transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n \n-            // If we reach here then it means transaction is safe to commit.\n-            // Step 4: Insert\n-            long committedVersion = version.incrementAndGet();\n-            HashMap<String, TransactionData> toAdd = new HashMap<String, TransactionData>();\n-            for (String key : modifiedKeys) {\n-                TransactionData data = txnData.get(key);\n-                data.setVersion(committedVersion);\n-                toAdd.put(key, data);\n+                // Set the database object.\n+                transactionData.setDbObject(dataFromBuffer.getDbObject());\n+            } else {\n+                Preconditions.checkState(entry.getValue().isPinned(), \"Why are we here??\");\n             }\n-            bufferedTxnData.putAll(toAdd);\n         }\n+    }\n \n-        //  Step 5 : evict if required.\n-        if (bufferedTxnData.size() > maxEntriesInTxnBuffer) {\n-            bufferedTxnData.entrySet().removeIf(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned());\n+    /**\n+     * Evict entries if needed.\n+     * Only evict keys that are persisted, not pinned or active.\n+     */\n+    private CompletableFuture<Void> evictIfNeeded() {\n+        if (isEvictionRunning.compareAndSet(false, true)) {\n+            val limit = 1 + maxEntriesInTxnBuffer / CACHE_EVICTION_RATIO;\n+            if (bufferCount.get() > maxEntriesInTxnBuffer) {\n+                val toEvict = bufferedTxnData.entrySet().parallelStream()\n+                        .filter(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned()\n+                                && !activeKeys.contains(entry.getKey()))\n+                        .map(Map.Entry::getKey)\n+                        .limit(limit)\n+                        .collect(Collectors.toList());\n+                int i = 0;\n+                for (val key : toEvict) {\n+                    // synchronize so that we don't accidentally delete a key that becomes active after check here.\n+                    synchronized (evictionLock) {\n+                        if (0 == activeKeys.count(key)) {\n+                            // Synchronization prevents error when key becomes active between the check and remove.\n+                            // Move the key to cache\n+                            cache.put(key, bufferedTxnData.get(key));\n+                            // Remove from buffer.\n+                            bufferedTxnData.remove(key);\n+                            i++;\n+                        }\n+                    }\n+                }\n+                bufferCount.addAndGet(-1 * i);\n+            }\n+            isEvictionRunning.set(false);\n+            log.debug(\"Entries evicted from transaction buffer.\");\n         }\n-\n-        //  Step 6: finally clear\n-        txnData.clear();\n+        return CompletableFuture.completedFuture(null);\n     }\n \n     /**\n      * Aborts given transaction.\n      *\n      * @param txn transaction to abort.\n-     * @throws StorageMetadataException If there are any errors.\n+     *            throws StorageMetadataException If there are any errors.\n      */\n-    public void abort(MetadataTransaction txn) throws StorageMetadataException {\n+    public CompletableFuture<Void> abort(MetadataTransaction txn) {\n         // Do nothing\n+        return CompletableFuture.completedFuture(null);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2NDAxMw=="}, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 535}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTYxMjE2OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwMTowOToxN1rOHucjiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjoyNTozMFrOHwEDrg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2NDM5Mg==", "bodyText": "Ouch. This String.join will ALWAYS be evaluated even if debug logging is off. Do not do this. Just pass in the collection and the logger will convert that to something nice.\nPlease fix this elsewhere if you did similar things. I see some below.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r518464392", "createdAt": "2020-11-06T01:09:17Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+            if (fenced.get()) {\n+                throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));\n+            }\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Mark keys in transaction as active to prevent their eviction.\n+                    txn.getData().keySet().forEach(this::addToActiveKeySet);\n+\n+                    // Acquire a write lock over segment.\n+                    val tLock = new Timer();\n+                    log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));\n+                    val writeLock = scheduler.getWriteLock(txn.getKeysToLock());\n+                    return writeLock.lock()\n+                            .thenComposeAsync(v0 -> {\n+                                // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+                                // This step is kind of thread safe\n+                                val elapsed = tLock.getElapsed();\n+                                WRITE_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                                log.debug(\"Acquired write lock for {}, wait time: {} ms\",\n+                                        String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                                return loadMissingKeys(txn, skipStoreCheck, txnData);\n+                            }, executor)\n+                            .thenComposeAsync(v1 -> {\n+                                // This check needs to be atomic, with absolutely no possibility of re-entry\n+                                return performCommit(txn, lazyWrite, txnData, modifiedKeys, modifiedValues);\n+                            }, executor)\n+                            .whenCompleteAsync((v2, ex) -> writeLock.unlock(), executor);\n+                }, executor)\n+                .thenRunAsync(() -> {\n+                    //  Step 5 : evict if required.\n+                    txn.setCommitted();\n+                    txnData.clear();\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    // Remove keys from active set.\n+                    txn.getData().keySet().forEach(this::removeFromActiveKeySet);\n+                    COMMIT_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+\n+        // Trigger evict\n+        retValue.thenComposeAsync(v4 -> {\n+            //  Step 6 : evict if required.\n+            return evictIfNeeded();\n+        }, executor);\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        return retValue;\n+    }\n \n-        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n-        // This step is kind of thread safe\n+    /**\n+     * Loads missing keys.\n+     */\n+    private CompletableFuture<Void> loadMissingKeys(MetadataTransaction txn, boolean skipStoreCheck, Map<String, TransactionData> txnData) {\n+        val loadFutures = new ArrayList<CompletableFuture<TransactionData>>();\n         for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-            String key = entry.getKey();\n+            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+            val key = entry.getKey();\n             if (skipStoreCheck || entry.getValue().isPinned()) {\n                 log.trace(\"Skipping loading key from the store key = {}\", key);\n             } else {\n                 // This check is safe to be outside the lock\n-                if (!bufferedTxnData.containsKey(key)) {\n-                    loadFromStore(key);\n+                val dataFromBuffer = bufferedTxnData.get(key);\n+                if (null == dataFromBuffer) {\n+                    loadFutures.add(loadFromStore(txn, key, true));\n                 }\n             }\n         }\n-        // Step 2 : Check whether transaction is safe to commit.\n-        // This check needs to be atomic, with absolutely no possibility of re-entry\n-        synchronized (lock) {\n-            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-                String key = entry.getKey();\n-                val transactionData = entry.getValue();\n-                Preconditions.checkState(null != transactionData.getKey());\n-\n-                // See if this entry was modified in this transaction.\n-                if (transactionData.getVersion() == txn.getVersion()) {\n-                    modifiedKeys.add(key);\n-                    transactionData.setPersisted(false);\n-                    modifiedValues.add(transactionData);\n-                }\n-                // make sure none of the keys used in this transaction have changed.\n-                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n-                if (null != dataFromBuffer) {\n-                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n-                        throw new StorageMetadataVersionMismatchException(\n-                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n-                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+        return Futures.allOf(loadFutures)\n+                .thenApplyAsync(v4 -> {\n+                    // validate everything is alright.\n+                    for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                        val dataFromBuffer = bufferedTxnData.get(entry.getKey());\n+                        if (!(entry.getValue().isPinned())) {\n+                            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+                            Preconditions.checkState(null != dataFromBuffer);\n+                            if (!dataFromBuffer.isPinned()) {\n+                                Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                            }\n+                        }\n                     }\n+                    return null;\n+                }, executor);\n+    }\n \n-                    // Pin it if it is already pinned.\n-                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+    /**\n+     * Performs commit.\n+     */\n+    private CompletableFuture<Void> performCommit(MetadataTransaction txn, boolean lazyWrite, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        return CompletableFuture.runAsync(() -> {\n+            // Step 2 : Check whether transaction is safe to commit.\n+            validateCommit(txn, txnData, modifiedKeys, modifiedValues);\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Step 3: Commit externally.\n+                    // This operation may call external storage.\n+                    return writeToMetadataStore(lazyWrite, modifiedValues);\n+                }, executor)\n+                .thenComposeAsync(v ->\n+                                executeExternalCommitAction(txn),\n+                        executor)\n+                .thenRunAsync(() -> {\n+                    // If we reach here then it means transaction is safe to commit.\n+                    // Step 4: Update buffer.\n+                    val committedVersion = version.incrementAndGet();\n+                    val toAdd = new HashMap<String, TransactionData>();\n+                    for (String key : modifiedKeys) {\n+                        TransactionData data = txnData.get(key);\n+                        data.setVersion(committedVersion);\n+                        toAdd.put(key, data);\n+                    }\n+                    bufferedTxnData.putAll(toAdd);\n+                    bufferCount.addAndGet(toAdd.size());\n+                }, executor);\n+    }\n \n-                    // Set the database object.\n-                    transactionData.setDbObject(dataFromBuffer.getDbObject());\n-                }\n+    /**\n+     * Writes modified values to the metadata store.\n+     */\n+    private CompletableFuture<Void> writeToMetadataStore(boolean lazyWrite, ArrayList<TransactionData> modifiedValues) {\n+        if (!lazyWrite || (bufferCount.get() > maxEntriesInTxnBuffer)) {\n+            log.trace(\"Persisting all modified keys (except pinned)\");\n+            val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n+            if (toWriteList.size() > 0) {\n+                return writeAll(toWriteList)\n+                        .thenRunAsync(() -> {\n+                            log.trace(\"Done persisting all modified keys\");\n+                            for (val writtenData : toWriteList) {\n+                                // Mark written keys as persisted.\n+                                writtenData.setPersisted(true);\n+                                // Put it in cache.\n+                                cache.put(writtenData.getKey(), writtenData);\n+                            }\n+                        }, executor);\n+            } else {\n+                return CompletableFuture.completedFuture(null);\n             }\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Step 3: Commit externally.\n-            // This operation may call external storage.\n-            if (!lazyWrite || (bufferedTxnData.size() > maxEntriesInTxnBuffer)) {\n-                log.trace(\"Persisting all modified keys (except pinned)\");\n-                val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n-                writeAll(toWriteList);\n-                log.trace(\"Done persisting all modified keys\");\n-\n-                // Mark written keys as persisted.\n-                for (val writtenData : toWriteList) {\n-                    writtenData.setPersisted(true);\n-                }\n+    /**\n+     * Executes external commit step.\n+     */\n+    private CompletableFuture<Void> executeExternalCommitAction(MetadataTransaction txn) {\n+        // Execute external commit step.\n+        try {\n+            if (null != txn.getExternalCommitStep()) {\n+                txn.getExternalCommitStep().call();\n             }\n+        } catch (Exception e) {\n+            log.error(\"Exception during execution of external commit step\", e);\n+            throw new CompletionException(new StorageMetadataException(\"Exception during execution of external commit step\", e));\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Execute external commit step.\n-            try {\n-                if (null != txn.getExternalCommitStep()) {\n-                    txn.getExternalCommitStep().call();\n-                }\n-            } catch (Exception e) {\n-                log.error(\"Exception during execution of external commit step\", e);\n-                throw new StorageMetadataException(\"Exception during execution of external commit step\", e);\n+    private void validateCommit(MetadataTransaction txn, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        for (val entry : txnData.entrySet()) {\n+            val key = entry.getKey();\n+            val transactionData = entry.getValue();\n+            Preconditions.checkState(null != transactionData.getKey());\n+\n+            // See if this entry was modified in this transaction.\n+            if (transactionData.getVersion() == txn.getVersion()) {\n+                modifiedKeys.add(key);\n+                transactionData.setPersisted(false);\n+                modifiedValues.add(transactionData);\n             }\n+            // make sure none of the keys used in this transaction have changed.\n+            val dataFromBuffer = bufferedTxnData.get(key);\n+            if (null != dataFromBuffer) {\n+                if (!dataFromBuffer.isPinned()) {\n+                    Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                }\n+                if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n+                    throw new CompletionException(new StorageMetadataVersionMismatchException(\n+                            String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n+                                    key, dataFromBuffer.getVersion(), txnData.get(key).getVersion())));\n+                }\n+\n+                // Pin it if it is already pinned.\n+                transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n \n-            // If we reach here then it means transaction is safe to commit.\n-            // Step 4: Insert\n-            long committedVersion = version.incrementAndGet();\n-            HashMap<String, TransactionData> toAdd = new HashMap<String, TransactionData>();\n-            for (String key : modifiedKeys) {\n-                TransactionData data = txnData.get(key);\n-                data.setVersion(committedVersion);\n-                toAdd.put(key, data);\n+                // Set the database object.\n+                transactionData.setDbObject(dataFromBuffer.getDbObject());\n+            } else {\n+                Preconditions.checkState(entry.getValue().isPinned(), \"Why are we here??\");\n             }\n-            bufferedTxnData.putAll(toAdd);\n         }\n+    }\n \n-        //  Step 5 : evict if required.\n-        if (bufferedTxnData.size() > maxEntriesInTxnBuffer) {\n-            bufferedTxnData.entrySet().removeIf(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned());\n+    /**\n+     * Evict entries if needed.\n+     * Only evict keys that are persisted, not pinned or active.\n+     */\n+    private CompletableFuture<Void> evictIfNeeded() {\n+        if (isEvictionRunning.compareAndSet(false, true)) {\n+            val limit = 1 + maxEntriesInTxnBuffer / CACHE_EVICTION_RATIO;\n+            if (bufferCount.get() > maxEntriesInTxnBuffer) {\n+                val toEvict = bufferedTxnData.entrySet().parallelStream()\n+                        .filter(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned()\n+                                && !activeKeys.contains(entry.getKey()))\n+                        .map(Map.Entry::getKey)\n+                        .limit(limit)\n+                        .collect(Collectors.toList());\n+                int i = 0;\n+                for (val key : toEvict) {\n+                    // synchronize so that we don't accidentally delete a key that becomes active after check here.\n+                    synchronized (evictionLock) {\n+                        if (0 == activeKeys.count(key)) {\n+                            // Synchronization prevents error when key becomes active between the check and remove.\n+                            // Move the key to cache\n+                            cache.put(key, bufferedTxnData.get(key));\n+                            // Remove from buffer.\n+                            bufferedTxnData.remove(key);\n+                            i++;\n+                        }\n+                    }\n+                }\n+                bufferCount.addAndGet(-1 * i);\n+            }\n+            isEvictionRunning.set(false);\n+            log.debug(\"Entries evicted from transaction buffer.\");\n         }\n-\n-        //  Step 6: finally clear\n-        txnData.clear();\n+        return CompletableFuture.completedFuture(null);\n     }\n \n     /**\n      * Aborts given transaction.\n      *\n      * @param txn transaction to abort.\n-     * @throws StorageMetadataException If there are any errors.\n+     *            throws StorageMetadataException If there are any errors.\n      */\n-    public void abort(MetadataTransaction txn) throws StorageMetadataException {\n+    public CompletableFuture<Void> abort(MetadataTransaction txn) {\n         // Do nothing\n+        return CompletableFuture.completedFuture(null);\n     }\n \n     /**\n      * Retrieves the metadata for given key.\n      *\n      * @param txn Transaction.\n      * @param key key to use to retrieve metadata.\n-     * @return Metadata for given key. Null if key was not found.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return A CompletableFuture that, when completed, will contain metadata for given key. Null if key was not found.\n+     * @throws CompletionException If the operation failed, it will be completed with the appropriate exception. Notable Exceptions:\n+     *                             {@link StorageMetadataException} Exception related to storage metadata operations.\n      */\n     @Override\n-    public StorageMetadata get(MetadataTransaction txn, String key) throws StorageMetadataException {\n+    public CompletableFuture<StorageMetadata> get(MetadataTransaction txn, String key) {\n         Preconditions.checkArgument(null != txn);\n-        TransactionData dataFromBuffer = null;\n         if (null == key) {\n-            return null;\n+            return CompletableFuture.completedFuture(null);\n         }\n-        StorageMetadata retValue = null;\n+        val t = new Timer();\n+        val txnData = txn.getData();\n \n-        Map<String, TransactionData> txnData = txn.getData();\n+        // Record is found in transaction data itself.\n         TransactionData data = txnData.get(key);\n+        if (null != data) {\n+            GET_LATENCY.reportSuccessEvent(t.getElapsed());\n+            return CompletableFuture.completedFuture(data.getValue());\n+        }\n \n-        // Search in the buffer.\n-        if (null == data) {\n-            synchronized (lock) {\n-                dataFromBuffer = bufferedTxnData.get(key);\n-            }\n-            // If we did not find in buffer then load it from store\n-            if (null == dataFromBuffer) {\n-                // NOTE: This call to read MUST be outside the lock, it is most likely cause re-entry.\n-                loadFromStore(key);\n-                dataFromBuffer = bufferedTxnData.get(key);\n-                Preconditions.checkState(null != dataFromBuffer);\n-            }\n+        // Prevent the key from getting evicted.\n+        addToActiveKeySet(key);\n+\n+        // Try to find it in buffer. Access buffer using reader lock.\n+        val tLock = new Timer();\n+        log.debug(\"Acquiring read lock for {}\", key);\n+        val readLock = scheduler.getReadLock(txn.getKeysToLock());\n+        return readLock.lock()\n+                .thenApplyAsync(v -> {\n+                    val elapsed = tLock.getElapsed();\n+                    READ_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                    log.debug(\"Acquired read lock for {}, wait time: {} ms\",\n+                            String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                    return bufferedTxnData.get(key);\n+                }, executor)\n+                .thenApplyAsync(dataFromBuffer -> {\n+                    if (dataFromBuffer != null) {\n+                        // Make sure it is a deep copy.\n+                        val retValue = dataFromBuffer.getValue();\n+                        if (null != retValue) {\n+                            return retValue.deepCopy();\n+                        }\n+                        return null;\n+                    }\n+                    return null;\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> readLock.unlock(), executor)\n+                .thenComposeAsync(retValue -> {\n+                    if (retValue != null) {\n+                        return CompletableFuture.completedFuture(retValue);\n+                    }\n+                    // We did not find it in the buffer either.\n+                    // Try to find it in store.\n+                    return loadFromStore(txn, key, false)\n+                            .thenApplyAsync(TransactionData::getValue, executor);\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    removeFromActiveKeySet(key);\n+                    GET_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+    }\n+\n+    private void removeFromActiveKeySet(String key) {\n+        // No need to synchronize as activeKeys is already a ConcurrentHashMultiset.\n+        // In case of any race with eviction logic, the key will simply be evicted next iteration.\n+        // This is not incorrect and the race should be rare.\n+        activeKeys.remove(key);\n+    }\n \n-            if (null != dataFromBuffer && null != dataFromBuffer.getValue()) {\n-                // Make copy.\n-                data = dataFromBuffer.toBuilder()\n-                        .key(key)\n-                        .value(dataFromBuffer.getValue().deepCopy())\n-                        .build();\n-                txnData.put(key, data);\n+    private void addToActiveKeySet(String key) {\n+        // No need to synchronize if the eviction is not running as activeKeys is ConcurrentHashMultiset\n+        if (isEvictionRunning.get()) {\n+            // However this is required when eviction is happening in background because eviction code checks the count\n+            // and should evict key only if the count is zero.\n+            // These two steps are not atomic hence the use of synchronized in this narrow case to prevent race.\n+            synchronized (evictionLock) {\n+                activeKeys.add(key);\n             }\n+        } else {\n+            activeKeys.add(key);\n         }\n+    }\n \n-        if (data != null) {\n-            retValue = data.getValue();\n+    /**\n+     * Loads value from store.\n+     */\n+    private CompletableFuture<TransactionData> loadFromStore(MetadataTransaction txn, String key, boolean isReenterant) {\n+        log.trace(\"Loading key from the store key = {}\", key);\n+        return readFromStore(key)\n+                .thenApplyAsync(this::makeCopyForBuffer, executor)\n+                .thenComposeAsync(copyForBuffer -> {\n+                    Preconditions.checkState(null != copyForBuffer);\n+                    Preconditions.checkState(null != copyForBuffer.getDbObject());\n+                    if (!isReenterant) {\n+                        val t = new Timer();\n+                        log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 666}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MDE3NA==", "bodyText": "gone", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520160174", "createdAt": "2020-11-09T22:25:30Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/BaseMetadataStore.java", "diffHunk": "@@ -120,295 +139,572 @@\n     /**\n      * Buffer for reading and writing transaction data entries to underlying KV store.\n      * This allows lazy storing and avoiding unnecessary load for recently/frequently updated key value pairs.\n+     * Note that entries in this buffer should not be evicted while transaction using them are in flight.\n      */\n-    @GuardedBy(\"lock\")\n     private final ConcurrentHashMap<String, TransactionData> bufferedTxnData;\n \n+    /**\n+     * Set of active records from commits that are in-flight. These records should not be evicted until the active commits finish.\n+     */\n+    private final ConcurrentHashMultiset<String> activeKeys;\n+\n+    /**\n+     * Cache for reading and writing transaction data entries to underlying KV store.\n+     */\n+    private final Cache<String, TransactionData> cache;\n+\n+    /**\n+     * {@link MultiKeyReaderWriterScheduler} instance.\n+     */\n+    private final MultiKeyReaderWriterScheduler scheduler = new MultiKeyReaderWriterScheduler();\n+\n+    /**\n+     * Storage executor object.\n+     */\n+    @Getter(AccessLevel.PROTECTED)\n+    private final Executor executor;\n+\n     /**\n      * Maximum number of metadata entries to keep in recent transaction buffer.\n      */\n     @Getter\n     @Setter\n     int maxEntriesInTxnBuffer = MAX_ENTRIES_IN_TXN_BUFFER;\n \n+    /**\n+     * Maximum number of metadata entries to keep in recent transaction buffer.\n+     */\n+    @Getter\n+    @Setter\n+    int maxEntriesInCache = MAX_ENTRIES_IN_CACHE;\n+\n+    /**\n+     * Keep count of records in buffer. ConcurrentHashMap.size() is an expensive operation.\n+     */\n+    private final AtomicInteger bufferCount = new AtomicInteger(0);\n+\n+    /**\n+     * Flag to keep track of whether the eviction is currently running.\n+     */\n+    private final AtomicBoolean isEvictionRunning = new AtomicBoolean();\n+\n+    /**\n+     * Lock object to synchronize on during eviction.\n+     */\n+    private final Object evictionLock = new Object();\n+\n     /**\n      * Constructs a BaseMetadataStore object.\n+     *\n+     * @param executor Executor to use for async operations.\n      */\n-    public BaseMetadataStore() {\n+    public BaseMetadataStore(Executor executor) {\n         version = new AtomicLong(System.currentTimeMillis()); // Start with unique number.\n         fenced = new AtomicBoolean(false);\n         bufferedTxnData = new ConcurrentHashMap<>(); // Don't think we need anything fancy here. But we'll measure and see.\n+        activeKeys = ConcurrentHashMultiset.create();\n+        cache = CacheBuilder.newBuilder()\n+                .maximumSize(maxEntriesInCache)\n+                .build();\n+        this.executor = Preconditions.checkNotNull(executor, \"executor\");\n     }\n \n     /**\n      * Begins a new transaction.\n      *\n+     * @param keysToLock Array of keys to lock for this transaction.\n      * @return Returns a new instance of MetadataTransaction.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    public MetadataTransaction beginTransaction() throws StorageMetadataException {\n-        // Each transaction gets a unique number which is monotinically increasing.\n-        return new MetadataTransaction(this, version.incrementAndGet());\n+    public MetadataTransaction beginTransaction(String... keysToLock) {\n+        // Each transaction gets a unique number which is monotonically increasing.\n+        return new MetadataTransaction(this, version.incrementAndGet(), keysToLock);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite) throws StorageMetadataException {\n-        commit(txn, lazyWrite, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite) {\n+        return commit(txn, lazyWrite, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn transaction to commit.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn) throws StorageMetadataException {\n-        commit(txn, false, false);\n+    public CompletableFuture<Void> commit(MetadataTransaction txn) {\n+        return commit(txn, false, false);\n     }\n \n     /**\n      * Commits given transaction.\n      *\n      * @param txn       transaction to commit.\n      * @param lazyWrite true if data can be written lazily.\n-     * @throws StorageMetadataException StorageMetadataVersionMismatchException if transaction can not be commited.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed.\n+     * If the operation failed, it will contain the cause of the failure. Notable exceptions:\n+     * {@link StorageMetadataException} if transaction can not be committed.\n      */\n     @Override\n-    public void commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) throws StorageMetadataException {\n+    public CompletableFuture<Void> commit(MetadataTransaction txn, boolean lazyWrite, boolean skipStoreCheck) {\n         Preconditions.checkArgument(null != txn);\n-        if (fenced.get()) {\n-            throw new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\");\n-        }\n-\n-        Map<String, TransactionData> txnData = txn.getData();\n+        val txnData = txn.getData();\n+\n+        val modifiedKeys = new ArrayList<String>();\n+        val modifiedValues = new ArrayList<TransactionData>();\n+        val t = new Timer();\n+        val retValue = CompletableFuture.runAsync(() -> {\n+            if (fenced.get()) {\n+                throw new CompletionException(new StorageMetadataWritesFencedOutException(\"Transaction writer is fenced off.\"));\n+            }\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Mark keys in transaction as active to prevent their eviction.\n+                    txn.getData().keySet().forEach(this::addToActiveKeySet);\n+\n+                    // Acquire a write lock over segment.\n+                    val tLock = new Timer();\n+                    log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));\n+                    val writeLock = scheduler.getWriteLock(txn.getKeysToLock());\n+                    return writeLock.lock()\n+                            .thenComposeAsync(v0 -> {\n+                                // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n+                                // This step is kind of thread safe\n+                                val elapsed = tLock.getElapsed();\n+                                WRITE_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                                log.debug(\"Acquired write lock for {}, wait time: {} ms\",\n+                                        String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                                return loadMissingKeys(txn, skipStoreCheck, txnData);\n+                            }, executor)\n+                            .thenComposeAsync(v1 -> {\n+                                // This check needs to be atomic, with absolutely no possibility of re-entry\n+                                return performCommit(txn, lazyWrite, txnData, modifiedKeys, modifiedValues);\n+                            }, executor)\n+                            .whenCompleteAsync((v2, ex) -> writeLock.unlock(), executor);\n+                }, executor)\n+                .thenRunAsync(() -> {\n+                    //  Step 5 : evict if required.\n+                    txn.setCommitted();\n+                    txnData.clear();\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    // Remove keys from active set.\n+                    txn.getData().keySet().forEach(this::removeFromActiveKeySet);\n+                    COMMIT_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+\n+        // Trigger evict\n+        retValue.thenComposeAsync(v4 -> {\n+            //  Step 6 : evict if required.\n+            return evictIfNeeded();\n+        }, executor);\n \n-        ArrayList<String> modifiedKeys = new ArrayList<>();\n-        ArrayList<TransactionData> modifiedValues = new ArrayList<>();\n+        return retValue;\n+    }\n \n-        // Step 1 : If bufferedTxnData data was flushed, then read it back from external source and re-insert in bufferedTxnData buffer.\n-        // This step is kind of thread safe\n+    /**\n+     * Loads missing keys.\n+     */\n+    private CompletableFuture<Void> loadMissingKeys(MetadataTransaction txn, boolean skipStoreCheck, Map<String, TransactionData> txnData) {\n+        val loadFutures = new ArrayList<CompletableFuture<TransactionData>>();\n         for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-            String key = entry.getKey();\n+            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+            val key = entry.getKey();\n             if (skipStoreCheck || entry.getValue().isPinned()) {\n                 log.trace(\"Skipping loading key from the store key = {}\", key);\n             } else {\n                 // This check is safe to be outside the lock\n-                if (!bufferedTxnData.containsKey(key)) {\n-                    loadFromStore(key);\n+                val dataFromBuffer = bufferedTxnData.get(key);\n+                if (null == dataFromBuffer) {\n+                    loadFutures.add(loadFromStore(txn, key, true));\n                 }\n             }\n         }\n-        // Step 2 : Check whether transaction is safe to commit.\n-        // This check needs to be atomic, with absolutely no possibility of re-entry\n-        synchronized (lock) {\n-            for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n-                String key = entry.getKey();\n-                val transactionData = entry.getValue();\n-                Preconditions.checkState(null != transactionData.getKey());\n-\n-                // See if this entry was modified in this transaction.\n-                if (transactionData.getVersion() == txn.getVersion()) {\n-                    modifiedKeys.add(key);\n-                    transactionData.setPersisted(false);\n-                    modifiedValues.add(transactionData);\n-                }\n-                // make sure none of the keys used in this transaction have changed.\n-                TransactionData dataFromBuffer = bufferedTxnData.get(key);\n-                if (null != dataFromBuffer) {\n-                    if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n-                        throw new StorageMetadataVersionMismatchException(\n-                                String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n-                                        key, dataFromBuffer.getVersion(), txnData.get(key).getVersion()));\n+        return Futures.allOf(loadFutures)\n+                .thenApplyAsync(v4 -> {\n+                    // validate everything is alright.\n+                    for (Map.Entry<String, TransactionData> entry : txnData.entrySet()) {\n+                        val dataFromBuffer = bufferedTxnData.get(entry.getKey());\n+                        if (!(entry.getValue().isPinned())) {\n+                            Preconditions.checkState(activeKeys.contains(entry.getKey()));\n+                            Preconditions.checkState(null != dataFromBuffer);\n+                            if (!dataFromBuffer.isPinned()) {\n+                                Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                            }\n+                        }\n                     }\n+                    return null;\n+                }, executor);\n+    }\n \n-                    // Pin it if it is already pinned.\n-                    transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n+    /**\n+     * Performs commit.\n+     */\n+    private CompletableFuture<Void> performCommit(MetadataTransaction txn, boolean lazyWrite, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        return CompletableFuture.runAsync(() -> {\n+            // Step 2 : Check whether transaction is safe to commit.\n+            validateCommit(txn, txnData, modifiedKeys, modifiedValues);\n+        }, executor)\n+                .thenComposeAsync(v -> {\n+                    // Step 3: Commit externally.\n+                    // This operation may call external storage.\n+                    return writeToMetadataStore(lazyWrite, modifiedValues);\n+                }, executor)\n+                .thenComposeAsync(v ->\n+                                executeExternalCommitAction(txn),\n+                        executor)\n+                .thenRunAsync(() -> {\n+                    // If we reach here then it means transaction is safe to commit.\n+                    // Step 4: Update buffer.\n+                    val committedVersion = version.incrementAndGet();\n+                    val toAdd = new HashMap<String, TransactionData>();\n+                    for (String key : modifiedKeys) {\n+                        TransactionData data = txnData.get(key);\n+                        data.setVersion(committedVersion);\n+                        toAdd.put(key, data);\n+                    }\n+                    bufferedTxnData.putAll(toAdd);\n+                    bufferCount.addAndGet(toAdd.size());\n+                }, executor);\n+    }\n \n-                    // Set the database object.\n-                    transactionData.setDbObject(dataFromBuffer.getDbObject());\n-                }\n+    /**\n+     * Writes modified values to the metadata store.\n+     */\n+    private CompletableFuture<Void> writeToMetadataStore(boolean lazyWrite, ArrayList<TransactionData> modifiedValues) {\n+        if (!lazyWrite || (bufferCount.get() > maxEntriesInTxnBuffer)) {\n+            log.trace(\"Persisting all modified keys (except pinned)\");\n+            val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n+            if (toWriteList.size() > 0) {\n+                return writeAll(toWriteList)\n+                        .thenRunAsync(() -> {\n+                            log.trace(\"Done persisting all modified keys\");\n+                            for (val writtenData : toWriteList) {\n+                                // Mark written keys as persisted.\n+                                writtenData.setPersisted(true);\n+                                // Put it in cache.\n+                                cache.put(writtenData.getKey(), writtenData);\n+                            }\n+                        }, executor);\n+            } else {\n+                return CompletableFuture.completedFuture(null);\n             }\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Step 3: Commit externally.\n-            // This operation may call external storage.\n-            if (!lazyWrite || (bufferedTxnData.size() > maxEntriesInTxnBuffer)) {\n-                log.trace(\"Persisting all modified keys (except pinned)\");\n-                val toWriteList = modifiedValues.stream().filter(entry -> !entry.isPinned()).collect(Collectors.toList());\n-                writeAll(toWriteList);\n-                log.trace(\"Done persisting all modified keys\");\n-\n-                // Mark written keys as persisted.\n-                for (val writtenData : toWriteList) {\n-                    writtenData.setPersisted(true);\n-                }\n+    /**\n+     * Executes external commit step.\n+     */\n+    private CompletableFuture<Void> executeExternalCommitAction(MetadataTransaction txn) {\n+        // Execute external commit step.\n+        try {\n+            if (null != txn.getExternalCommitStep()) {\n+                txn.getExternalCommitStep().call();\n             }\n+        } catch (Exception e) {\n+            log.error(\"Exception during execution of external commit step\", e);\n+            throw new CompletionException(new StorageMetadataException(\"Exception during execution of external commit step\", e));\n+        }\n+        return CompletableFuture.completedFuture(null);\n+    }\n \n-            // Execute external commit step.\n-            try {\n-                if (null != txn.getExternalCommitStep()) {\n-                    txn.getExternalCommitStep().call();\n-                }\n-            } catch (Exception e) {\n-                log.error(\"Exception during execution of external commit step\", e);\n-                throw new StorageMetadataException(\"Exception during execution of external commit step\", e);\n+    private void validateCommit(MetadataTransaction txn, Map<String, TransactionData> txnData, ArrayList<String> modifiedKeys, ArrayList<TransactionData> modifiedValues) {\n+        for (val entry : txnData.entrySet()) {\n+            val key = entry.getKey();\n+            val transactionData = entry.getValue();\n+            Preconditions.checkState(null != transactionData.getKey());\n+\n+            // See if this entry was modified in this transaction.\n+            if (transactionData.getVersion() == txn.getVersion()) {\n+                modifiedKeys.add(key);\n+                transactionData.setPersisted(false);\n+                modifiedValues.add(transactionData);\n             }\n+            // make sure none of the keys used in this transaction have changed.\n+            val dataFromBuffer = bufferedTxnData.get(key);\n+            if (null != dataFromBuffer) {\n+                if (!dataFromBuffer.isPinned()) {\n+                    Preconditions.checkState(null != dataFromBuffer.getDbObject());\n+                }\n+                if (dataFromBuffer.getVersion() > transactionData.getVersion()) {\n+                    throw new CompletionException(new StorageMetadataVersionMismatchException(\n+                            String.format(\"Transaction uses stale data. Key version changed key:%s buffer:%s transaction:%s\",\n+                                    key, dataFromBuffer.getVersion(), txnData.get(key).getVersion())));\n+                }\n+\n+                // Pin it if it is already pinned.\n+                transactionData.setPinned(transactionData.isPinned() || dataFromBuffer.isPinned());\n \n-            // If we reach here then it means transaction is safe to commit.\n-            // Step 4: Insert\n-            long committedVersion = version.incrementAndGet();\n-            HashMap<String, TransactionData> toAdd = new HashMap<String, TransactionData>();\n-            for (String key : modifiedKeys) {\n-                TransactionData data = txnData.get(key);\n-                data.setVersion(committedVersion);\n-                toAdd.put(key, data);\n+                // Set the database object.\n+                transactionData.setDbObject(dataFromBuffer.getDbObject());\n+            } else {\n+                Preconditions.checkState(entry.getValue().isPinned(), \"Why are we here??\");\n             }\n-            bufferedTxnData.putAll(toAdd);\n         }\n+    }\n \n-        //  Step 5 : evict if required.\n-        if (bufferedTxnData.size() > maxEntriesInTxnBuffer) {\n-            bufferedTxnData.entrySet().removeIf(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned());\n+    /**\n+     * Evict entries if needed.\n+     * Only evict keys that are persisted, not pinned or active.\n+     */\n+    private CompletableFuture<Void> evictIfNeeded() {\n+        if (isEvictionRunning.compareAndSet(false, true)) {\n+            val limit = 1 + maxEntriesInTxnBuffer / CACHE_EVICTION_RATIO;\n+            if (bufferCount.get() > maxEntriesInTxnBuffer) {\n+                val toEvict = bufferedTxnData.entrySet().parallelStream()\n+                        .filter(entry -> entry.getValue().isPersisted() && !entry.getValue().isPinned()\n+                                && !activeKeys.contains(entry.getKey()))\n+                        .map(Map.Entry::getKey)\n+                        .limit(limit)\n+                        .collect(Collectors.toList());\n+                int i = 0;\n+                for (val key : toEvict) {\n+                    // synchronize so that we don't accidentally delete a key that becomes active after check here.\n+                    synchronized (evictionLock) {\n+                        if (0 == activeKeys.count(key)) {\n+                            // Synchronization prevents error when key becomes active between the check and remove.\n+                            // Move the key to cache\n+                            cache.put(key, bufferedTxnData.get(key));\n+                            // Remove from buffer.\n+                            bufferedTxnData.remove(key);\n+                            i++;\n+                        }\n+                    }\n+                }\n+                bufferCount.addAndGet(-1 * i);\n+            }\n+            isEvictionRunning.set(false);\n+            log.debug(\"Entries evicted from transaction buffer.\");\n         }\n-\n-        //  Step 6: finally clear\n-        txnData.clear();\n+        return CompletableFuture.completedFuture(null);\n     }\n \n     /**\n      * Aborts given transaction.\n      *\n      * @param txn transaction to abort.\n-     * @throws StorageMetadataException If there are any errors.\n+     *            throws StorageMetadataException If there are any errors.\n      */\n-    public void abort(MetadataTransaction txn) throws StorageMetadataException {\n+    public CompletableFuture<Void> abort(MetadataTransaction txn) {\n         // Do nothing\n+        return CompletableFuture.completedFuture(null);\n     }\n \n     /**\n      * Retrieves the metadata for given key.\n      *\n      * @param txn Transaction.\n      * @param key key to use to retrieve metadata.\n-     * @return Metadata for given key. Null if key was not found.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n+     * @return A CompletableFuture that, when completed, will contain metadata for given key. Null if key was not found.\n+     * @throws CompletionException If the operation failed, it will be completed with the appropriate exception. Notable Exceptions:\n+     *                             {@link StorageMetadataException} Exception related to storage metadata operations.\n      */\n     @Override\n-    public StorageMetadata get(MetadataTransaction txn, String key) throws StorageMetadataException {\n+    public CompletableFuture<StorageMetadata> get(MetadataTransaction txn, String key) {\n         Preconditions.checkArgument(null != txn);\n-        TransactionData dataFromBuffer = null;\n         if (null == key) {\n-            return null;\n+            return CompletableFuture.completedFuture(null);\n         }\n-        StorageMetadata retValue = null;\n+        val t = new Timer();\n+        val txnData = txn.getData();\n \n-        Map<String, TransactionData> txnData = txn.getData();\n+        // Record is found in transaction data itself.\n         TransactionData data = txnData.get(key);\n+        if (null != data) {\n+            GET_LATENCY.reportSuccessEvent(t.getElapsed());\n+            return CompletableFuture.completedFuture(data.getValue());\n+        }\n \n-        // Search in the buffer.\n-        if (null == data) {\n-            synchronized (lock) {\n-                dataFromBuffer = bufferedTxnData.get(key);\n-            }\n-            // If we did not find in buffer then load it from store\n-            if (null == dataFromBuffer) {\n-                // NOTE: This call to read MUST be outside the lock, it is most likely cause re-entry.\n-                loadFromStore(key);\n-                dataFromBuffer = bufferedTxnData.get(key);\n-                Preconditions.checkState(null != dataFromBuffer);\n-            }\n+        // Prevent the key from getting evicted.\n+        addToActiveKeySet(key);\n+\n+        // Try to find it in buffer. Access buffer using reader lock.\n+        val tLock = new Timer();\n+        log.debug(\"Acquiring read lock for {}\", key);\n+        val readLock = scheduler.getReadLock(txn.getKeysToLock());\n+        return readLock.lock()\n+                .thenApplyAsync(v -> {\n+                    val elapsed = tLock.getElapsed();\n+                    READ_LOCK_LATENCY.reportSuccessEvent(t.getElapsed());\n+                    log.debug(\"Acquired read lock for {}, wait time: {} ms\",\n+                            String.join(\", \", txn.getKeysToLock()), elapsed.toMillis());\n+                    return bufferedTxnData.get(key);\n+                }, executor)\n+                .thenApplyAsync(dataFromBuffer -> {\n+                    if (dataFromBuffer != null) {\n+                        // Make sure it is a deep copy.\n+                        val retValue = dataFromBuffer.getValue();\n+                        if (null != retValue) {\n+                            return retValue.deepCopy();\n+                        }\n+                        return null;\n+                    }\n+                    return null;\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> readLock.unlock(), executor)\n+                .thenComposeAsync(retValue -> {\n+                    if (retValue != null) {\n+                        return CompletableFuture.completedFuture(retValue);\n+                    }\n+                    // We did not find it in the buffer either.\n+                    // Try to find it in store.\n+                    return loadFromStore(txn, key, false)\n+                            .thenApplyAsync(TransactionData::getValue, executor);\n+                }, executor)\n+                .whenCompleteAsync((v, ex) -> {\n+                    removeFromActiveKeySet(key);\n+                    GET_LATENCY.reportSuccessEvent(t.getElapsed());\n+                }, executor);\n+    }\n+\n+    private void removeFromActiveKeySet(String key) {\n+        // No need to synchronize as activeKeys is already a ConcurrentHashMultiset.\n+        // In case of any race with eviction logic, the key will simply be evicted next iteration.\n+        // This is not incorrect and the race should be rare.\n+        activeKeys.remove(key);\n+    }\n \n-            if (null != dataFromBuffer && null != dataFromBuffer.getValue()) {\n-                // Make copy.\n-                data = dataFromBuffer.toBuilder()\n-                        .key(key)\n-                        .value(dataFromBuffer.getValue().deepCopy())\n-                        .build();\n-                txnData.put(key, data);\n+    private void addToActiveKeySet(String key) {\n+        // No need to synchronize if the eviction is not running as activeKeys is ConcurrentHashMultiset\n+        if (isEvictionRunning.get()) {\n+            // However this is required when eviction is happening in background because eviction code checks the count\n+            // and should evict key only if the count is zero.\n+            // These two steps are not atomic hence the use of synchronized in this narrow case to prevent race.\n+            synchronized (evictionLock) {\n+                activeKeys.add(key);\n             }\n+        } else {\n+            activeKeys.add(key);\n         }\n+    }\n \n-        if (data != null) {\n-            retValue = data.getValue();\n+    /**\n+     * Loads value from store.\n+     */\n+    private CompletableFuture<TransactionData> loadFromStore(MetadataTransaction txn, String key, boolean isReenterant) {\n+        log.trace(\"Loading key from the store key = {}\", key);\n+        return readFromStore(key)\n+                .thenApplyAsync(this::makeCopyForBuffer, executor)\n+                .thenComposeAsync(copyForBuffer -> {\n+                    Preconditions.checkState(null != copyForBuffer);\n+                    Preconditions.checkState(null != copyForBuffer.getDbObject());\n+                    if (!isReenterant) {\n+                        val t = new Timer();\n+                        log.debug(\"Acquiring write lock for {}\", String.join(\", \", txn.getKeysToLock()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2NDM5Mg=="}, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 666}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTYxNDkyOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/TableBasedMetadataStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwMToxMDozMFrOHuclIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjoyNToyMFrOHwEDaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2NDgwMA==", "bodyText": "retValue -> entries ?\nGive your args a meaningful name.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r518464800", "createdAt": "2020-11-06T01:10:30Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/TableBasedMetadataStore.java", "diffHunk": "@@ -69,95 +77,110 @@ public TableBasedMetadataStore(String tableName, TableStore tableStore) {\n      *\n      * @param key Key for the metadata record.\n      * @return Associated {@link io.pravega.segmentstore.storage.metadata.BaseMetadataStore.TransactionData}.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    protected TransactionData read(String key) throws StorageMetadataException {\n-        ensureInitialized();\n-        List<BufferView> keys = new ArrayList<>();\n+    protected CompletableFuture<TransactionData> read(String key) {\n+        val keys = new ArrayList<BufferView>();\n         keys.add(new ByteArraySegment(key.getBytes(Charsets.UTF_8)));\n-        try {\n-            List<TableEntry> retValue = this.tableStore.get(tableName, keys, timeout).get();\n-            Preconditions.checkState(retValue.size() == 1, \"Unexpected number of values returned.\");\n-            TableEntry entry = retValue.get(0);\n-            if (null != entry) {\n-                val arr = entry.getValue();\n-                TransactionData txnData = serializer.deserialize(arr);\n-                txnData.setDbObject(entry.getKey().getVersion());\n-                txnData.setPersisted(true);\n-                return txnData;\n-            }\n-        } catch (IllegalStateException e) {\n-            throw e;\n-        } catch (Exception e) {\n-            throw new StorageMetadataException(\"Error while reading\", e);\n-        }\n-\n-        return TransactionData.builder()\n-                .key(key)\n-                .persisted(true)\n-                .dbObject(TableKey.NOT_EXISTS)\n-                .build();\n+        val t = new Timer();\n+        return ensureInitialized()\n+                .thenComposeAsync(v -> this.tableStore.get(tableName, keys, timeout)\n+                        .thenApplyAsync(retValue -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MDEwNQ==", "bodyText": "done", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520160105", "createdAt": "2020-11-09T22:25:20Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/TableBasedMetadataStore.java", "diffHunk": "@@ -69,95 +77,110 @@ public TableBasedMetadataStore(String tableName, TableStore tableStore) {\n      *\n      * @param key Key for the metadata record.\n      * @return Associated {@link io.pravega.segmentstore.storage.metadata.BaseMetadataStore.TransactionData}.\n-     * @throws StorageMetadataException Exception related to storage metadata operations.\n      */\n     @Override\n-    protected TransactionData read(String key) throws StorageMetadataException {\n-        ensureInitialized();\n-        List<BufferView> keys = new ArrayList<>();\n+    protected CompletableFuture<TransactionData> read(String key) {\n+        val keys = new ArrayList<BufferView>();\n         keys.add(new ByteArraySegment(key.getBytes(Charsets.UTF_8)));\n-        try {\n-            List<TableEntry> retValue = this.tableStore.get(tableName, keys, timeout).get();\n-            Preconditions.checkState(retValue.size() == 1, \"Unexpected number of values returned.\");\n-            TableEntry entry = retValue.get(0);\n-            if (null != entry) {\n-                val arr = entry.getValue();\n-                TransactionData txnData = serializer.deserialize(arr);\n-                txnData.setDbObject(entry.getKey().getVersion());\n-                txnData.setPersisted(true);\n-                return txnData;\n-            }\n-        } catch (IllegalStateException e) {\n-            throw e;\n-        } catch (Exception e) {\n-            throw new StorageMetadataException(\"Error while reading\", e);\n-        }\n-\n-        return TransactionData.builder()\n-                .key(key)\n-                .persisted(true)\n-                .dbObject(TableKey.NOT_EXISTS)\n-                .build();\n+        val t = new Timer();\n+        return ensureInitialized()\n+                .thenComposeAsync(v -> this.tableStore.get(tableName, keys, timeout)\n+                        .thenApplyAsync(retValue -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2NDgwMA=="}, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0OTYxNzUxOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/TableBasedMetadataStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNlQwMToxMTo0OFrOHucmrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wOVQyMjoyNToxMlrOHwEDKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2NTE5Ng==", "bodyText": "debug. I don't need to see this upon every container boot.\nThe one above where you create is OK to be info. It should only happen once.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r518465196", "createdAt": "2020-11-06T01:11:48Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/TableBasedMetadataStore.java", "diffHunk": "@@ -167,29 +190,36 @@ private StorageMetadataException handleException(Throwable e) throws StorageMeta\n         return new StorageMetadataException(\"Transaction failed\", e);\n     }\n \n-    private void ensureInitialized() {\n+    private CompletableFuture<Void> ensureInitialized() {\n         if (!isTableInitialized.get()) {\n             // Storage Metadata Segment is a System, Internal Segment. It must also be designated as Critical since the\n             // Segment Store may not function properly without it performing well. The Critical designation will cause\n             // all of its \"modify\" operations to bypass any ingestion pipeline throttling and be expedited for processing.\n             val segmentType = SegmentType.builder().tableSegment().system().critical().internal().build();\n-            try {\n-                this.tableStore.createSegment(tableName, segmentType, timeout).join();\n-                log.info(\"Created table segment {}\", tableName);\n-            } catch (CompletionException e) {\n-                if (e.getCause() instanceof StreamSegmentExistsException) {\n-                    log.info(\"Table segment {} already exists.\", tableName);\n-                }\n-            }\n-            isTableInitialized.set(true);\n+            return this.tableStore.createSegment(tableName, segmentType, timeout)\n+                    .thenRunAsync(() -> {\n+                        log.info(\"Created table segment {}\", tableName);\n+                        isTableInitialized.set(true);\n+                    }, getExecutor())\n+                    .exceptionally(e -> {\n+                        val ex = Exceptions.unwrap(e);\n+                        if (e.getCause() instanceof StreamSegmentExistsException) {\n+                            log.info(\"Table segment {} already exists.\", tableName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 264}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDE2MDA0MQ==", "bodyText": "done.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520160041", "createdAt": "2020-11-09T22:25:12Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/metadata/TableBasedMetadataStore.java", "diffHunk": "@@ -167,29 +190,36 @@ private StorageMetadataException handleException(Throwable e) throws StorageMeta\n         return new StorageMetadataException(\"Transaction failed\", e);\n     }\n \n-    private void ensureInitialized() {\n+    private CompletableFuture<Void> ensureInitialized() {\n         if (!isTableInitialized.get()) {\n             // Storage Metadata Segment is a System, Internal Segment. It must also be designated as Critical since the\n             // Segment Store may not function properly without it performing well. The Critical designation will cause\n             // all of its \"modify\" operations to bypass any ingestion pipeline throttling and be expedited for processing.\n             val segmentType = SegmentType.builder().tableSegment().system().critical().internal().build();\n-            try {\n-                this.tableStore.createSegment(tableName, segmentType, timeout).join();\n-                log.info(\"Created table segment {}\", tableName);\n-            } catch (CompletionException e) {\n-                if (e.getCause() instanceof StreamSegmentExistsException) {\n-                    log.info(\"Table segment {} already exists.\", tableName);\n-                }\n-            }\n-            isTableInitialized.set(true);\n+            return this.tableStore.createSegment(tableName, segmentType, timeout)\n+                    .thenRunAsync(() -> {\n+                        log.info(\"Created table segment {}\", tableName);\n+                        isTableInitialized.set(true);\n+                    }, getExecutor())\n+                    .exceptionally(e -> {\n+                        val ex = Exceptions.unwrap(e);\n+                        if (e.getCause() instanceof StreamSegmentExistsException) {\n+                            log.info(\"Table segment {} already exists.\", tableName);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxODQ2NTE5Ng=="}, "originalCommit": {"oid": "a20fa7dac8368d64152b32f47aba6f5d675401c6"}, "originalPosition": 264}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NTY5NzkwOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/AsyncBaseChunkStorage.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQyMjo0MDowMVrOHwybTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQyMjo0MDoxMFrOHwybkA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkxOTg4NA==", "bodyText": "this check is not needed. log.trace will be a no-op in that case. The whole point of doing the check originally was not to add yet another callback to the Future. In this refactored code, it is no longer an issue.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520919884", "createdAt": "2020-11-10T22:40:01Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/AsyncBaseChunkStorage.java", "diffHunk": "@@ -142,16 +142,16 @@ public AsyncBaseChunkStorage(Executor executor) {\n \n         // Call concrete implementation.\n         val returnFuture = doCreateAsync(chunkName);\n-        val metricsFuture = returnFuture.thenAcceptAsync(handle -> {\n+        returnFuture.thenAcceptAsync(handle -> {\n             // Record metrics.\n             val elapsed = timer.getElapsed();\n             ChunkStorageMetrics.CREATE_LATENCY.reportSuccessEvent(elapsed);\n             ChunkStorageMetrics.CREATE_COUNT.inc();\n             log.debug(\"Create - chunk={}, latency={}.\", chunkName, elapsed.toMillis());\n+            if (log.isTraceEnabled()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkxOTk1Mg==", "bodyText": "same below", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520919952", "createdAt": "2020-11-10T22:40:10Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/AsyncBaseChunkStorage.java", "diffHunk": "@@ -142,16 +142,16 @@ public AsyncBaseChunkStorage(Executor executor) {\n \n         // Call concrete implementation.\n         val returnFuture = doCreateAsync(chunkName);\n-        val metricsFuture = returnFuture.thenAcceptAsync(handle -> {\n+        returnFuture.thenAcceptAsync(handle -> {\n             // Record metrics.\n             val elapsed = timer.getElapsed();\n             ChunkStorageMetrics.CREATE_LATENCY.reportSuccessEvent(elapsed);\n             ChunkStorageMetrics.CREATE_COUNT.inc();\n             log.debug(\"Create - chunk={}, latency={}.\", chunkName, elapsed.toMillis());\n+            if (log.isTraceEnabled()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkxOTg4NA=="}, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NTcwNjExOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQyMjo0Mjo1M1rOHwygTA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMTozMTo0OVrOHw3IGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyMTE2NA==", "bodyText": "Same thing with log.debug. DEBUG logging is not a common occurrence, so only add a callback if debug logging is enabled.\nPlus, there is absolutely no reason to do ...Async on these ones. These one-liners will eat up more CPU resources doing context switching than what they actually do need to execute. Just turn this one into thenRun() with no executor. It will be executed on the same thread the previous callback finished.\nWhile this doesn't matter in this particular case, since this is a very infrequent operation, for others it does matter and we don't want to flood the threadpool with useless callbacks like this one.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520921164", "createdAt": "2020-11-10T22:42:53Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -173,11 +178,8 @@ public ChunkedSegmentStorage(int containerId, ChunkStorage chunkStorage, ChunkMe\n         this.logPrefix = String.format(\"ChunkedSegmentStorage[%d]\", containerId);\n \n         // Now bootstrap\n-        log.info(\"{} STORAGE BOOT: Started.\", logPrefix);\n-        return this.systemJournal.bootstrap(epoch).thenApplyAsync(v -> {\n-            log.info(\"{} STORAGE BOOT: Ended.\", logPrefix);\n-            return null;\n-        }, executor);\n+        log.debug(\"{} STORAGE BOOT: Started.\", logPrefix);\n+        return this.systemJournal.bootstrap(epoch).thenRunAsync(() -> log.debug(\"{} STORAGE BOOT: Ended.\", logPrefix), executor);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk5Njg4OQ==", "bodyText": "fixed.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520996889", "createdAt": "2020-11-11T01:31:49Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -173,11 +178,8 @@ public ChunkedSegmentStorage(int containerId, ChunkStorage chunkStorage, ChunkMe\n         this.logPrefix = String.format(\"ChunkedSegmentStorage[%d]\", containerId);\n \n         // Now bootstrap\n-        log.info(\"{} STORAGE BOOT: Started.\", logPrefix);\n-        return this.systemJournal.bootstrap(epoch).thenApplyAsync(v -> {\n-            log.info(\"{} STORAGE BOOT: Ended.\", logPrefix);\n-            return null;\n-        }, executor);\n+        log.debug(\"{} STORAGE BOOT: Started.\", logPrefix);\n+        return this.systemJournal.bootstrap(epoch).thenRunAsync(() -> log.debug(\"{} STORAGE BOOT: Ended.\", logPrefix), executor);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyMTE2NA=="}, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NTcxMDUwOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQyMjo0NDoyN1rOHwyi0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDo0NDozNFrOHw1OLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyMTgwOQ==", "bodyText": "You can rewrite this as CompletableFuture.runAsync. Why do you create a completed future just to have an async continuation?", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520921809", "createdAt": "2020-11-10T22:44:27Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -681,17 +684,24 @@ public void close() {\n      * */\n     private <R> CompletableFuture<R> executeSerialized(Callable<CompletableFuture<R>> operation, String... segmentNames) {\n         Exceptions.checkNotClosed(this.closed.get(), this);\n-        return this.taskProcessor.add(Arrays.asList(segmentNames), () -> executeAsync(operation));\n+        return this.taskProcessor.add(Arrays.asList(segmentNames), () -> executeExclusive(operation, segmentNames));\n     }\n \n     /**\n-     * Executes the given Callable and returns its result.\n+     * Executes the given Callable asynchronously and exclusively.\n+     * It returns a CompletableFuture that will be completed with the result.\n+     * The operations are not allowed to be concurrent.\n      *\n-     * @param operation The Callable to execute.\n+     * @param operation    The Callable to execute.\n      * @param <R>       Return type of the operation.\n-     * @return CompletableFuture<R> of the return type of the operation.\n-     */\n-    private <R> CompletableFuture<R> executeAsync(Callable<CompletableFuture<R>> operation) {\n+     * @param segmentNames The names of the Segments involved in this operation (for sequencing purposes).\n+     * @return A CompletableFuture that, when completed, will contain the result of the operation.\n+     * If the operation failed, it will contain the cause of the failure.\n+     * */\n+    private <R> CompletableFuture<R> executeExclusive(Callable<CompletableFuture<R>> operation, String... segmentNames) {\n+        val shouldRelease = new AtomicBoolean(false);\n+        acquire(segmentNames);\n+        shouldRelease.set(true);\n         return CompletableFuture.completedFuture(null).thenComposeAsync(v -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 156}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk2NTY3Ng==", "bodyText": "There is no composeAsync equivalent to supplyAsync", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520965676", "createdAt": "2020-11-11T00:44:34Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -681,17 +684,24 @@ public void close() {\n      * */\n     private <R> CompletableFuture<R> executeSerialized(Callable<CompletableFuture<R>> operation, String... segmentNames) {\n         Exceptions.checkNotClosed(this.closed.get(), this);\n-        return this.taskProcessor.add(Arrays.asList(segmentNames), () -> executeAsync(operation));\n+        return this.taskProcessor.add(Arrays.asList(segmentNames), () -> executeExclusive(operation, segmentNames));\n     }\n \n     /**\n-     * Executes the given Callable and returns its result.\n+     * Executes the given Callable asynchronously and exclusively.\n+     * It returns a CompletableFuture that will be completed with the result.\n+     * The operations are not allowed to be concurrent.\n      *\n-     * @param operation The Callable to execute.\n+     * @param operation    The Callable to execute.\n      * @param <R>       Return type of the operation.\n-     * @return CompletableFuture<R> of the return type of the operation.\n-     */\n-    private <R> CompletableFuture<R> executeAsync(Callable<CompletableFuture<R>> operation) {\n+     * @param segmentNames The names of the Segments involved in this operation (for sequencing purposes).\n+     * @return A CompletableFuture that, when completed, will contain the result of the operation.\n+     * If the operation failed, it will contain the cause of the failure.\n+     * */\n+    private <R> CompletableFuture<R> executeExclusive(Callable<CompletableFuture<R>> operation, String... segmentNames) {\n+        val shouldRelease = new AtomicBoolean(false);\n+        acquire(segmentNames);\n+        shouldRelease.set(true);\n         return CompletableFuture.completedFuture(null).thenComposeAsync(v -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyMTgwOQ=="}, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 156}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NTcxMTk4OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQyMjo0NDo1N1rOHwyjwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDo0MDoxN1rOHw1Iyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyMjA1MA==", "bodyText": "This is a good example of when Async is not needed.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520922050", "createdAt": "2020-11-10T22:44:57Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -701,9 +711,48 @@ public void close() {\n             } catch (Exception e) {\n                 throw new CompletionException(e);\n             }\n+        }, this.executor)\n+        .whenCompleteAsync((v, e) -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk0MzIyMg==", "bodyText": "In general any whenCompleteAsync without executor will eat up processing from core thread pool and also there is no guarantee that it won't run on fork join pool. (The documentation say this can run on fork join pool).\nStorage I/O thread pool is very IO heavy - these few lines and waiting on IO is all these threads are doing.\nPlus it is using the same thread pool every where so there is actually no context switching.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520943222", "createdAt": "2020-11-10T23:38:25Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -701,9 +711,48 @@ public void close() {\n             } catch (Exception e) {\n                 throw new CompletionException(e);\n             }\n+        }, this.executor)\n+        .whenCompleteAsync((v, e) -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyMjA1MA=="}, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk2NDI5OA==", "bodyText": "http://hg.openjdk.java.net/jdk8/jdk8/jdk/file/687fd7c7986d/src/share/classes/java/util/concurrent/CompletableFuture.java#l2372 This stuff scared me.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520964298", "createdAt": "2020-11-11T00:40:17Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -701,9 +711,48 @@ public void close() {\n             } catch (Exception e) {\n                 throw new CompletionException(e);\n             }\n+        }, this.executor)\n+        .whenCompleteAsync((v, e) -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyMjA1MA=="}, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 164}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NTcxMjc0OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQyMjo0NTowN1rOHwykJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQyMzo0MDozMlrOHwz5mQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyMjE1MA==", "bodyText": "When is this ever not true?", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520922150", "createdAt": "2020-11-10T22:45:07Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -701,9 +711,48 @@ public void close() {\n             } catch (Exception e) {\n                 throw new CompletionException(e);\n             }\n+        }, this.executor)\n+        .whenCompleteAsync((v, e) -> {\n+            if (shouldRelease.get()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 165}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk0NDAyNQ==", "bodyText": "when we throw and exception because of Concurrent access . We don't want it to clear up the entry for inflight valid call and unintentionally allow concurrent calls.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520944025", "createdAt": "2020-11-10T23:40:32Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ChunkedSegmentStorage.java", "diffHunk": "@@ -701,9 +711,48 @@ public void close() {\n             } catch (Exception e) {\n                 throw new CompletionException(e);\n             }\n+        }, this.executor)\n+        .whenCompleteAsync((v, e) -> {\n+            if (shouldRelease.get()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyMjE1MA=="}, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 165}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NTcxNjg3OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQyMjo0NjozOVrOHwymuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDo0NToxNFrOHw1PGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyMjgxMQ==", "bodyText": "This is another example of when Async is expected to cause more perf harm than not.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520922811", "createdAt": "2020-11-10T22:46:39Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -145,21 +145,19 @@\n                             } else {\n                                 f = CompletableFuture.completedFuture(null);\n                             }\n-                            return f.thenApplyAsync(vv -> {\n+                            return f.thenRunAsync(() -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk2NTkxMw==", "bodyText": "same as above.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520965913", "createdAt": "2020-11-11T00:45:14Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -145,21 +145,19 @@\n                             } else {\n                                 f = CompletableFuture.completedFuture(null);\n                             }\n-                            return f.thenApplyAsync(vv -> {\n+                            return f.thenRunAsync(() -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyMjgxMQ=="}, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NTcyMzAyOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQyMjo0ODo0NFrOHwyqQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQyMzo0MDo0NVrOHwz54g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyMzcxNA==", "bodyText": "And here", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520923714", "createdAt": "2020-11-10T22:48:44Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -145,21 +145,19 @@\n                             } else {\n                                 f = CompletableFuture.completedFuture(null);\n                             }\n-                            return f.thenApplyAsync(vv -> {\n+                            return f.thenRunAsync(() -> {\n                                 // Move on to next place in list where we can concat if we are done with append based concatenations.\n                                 if (!useAppend) {\n                                     targetChunkName = nextChunkName;\n                                 }\n                                 // Toggle\n                                 useAppend = !useAppend;\n-                                return null;\n                             }, chunkedSegmentStorage.getExecutor());\n                         }, chunkedSegmentStorage.getExecutor()),\n                 chunkedSegmentStorage.getExecutor())\n-                .thenApplyAsync(vv -> {\n+                .thenRunAsync(() -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 17}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk0NDA5OA==", "bodyText": "same as above", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520944098", "createdAt": "2020-11-10T23:40:45Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -145,21 +145,19 @@\n                             } else {\n                                 f = CompletableFuture.completedFuture(null);\n                             }\n-                            return f.thenApplyAsync(vv -> {\n+                            return f.thenRunAsync(() -> {\n                                 // Move on to next place in list where we can concat if we are done with append based concatenations.\n                                 if (!useAppend) {\n                                     targetChunkName = nextChunkName;\n                                 }\n                                 // Toggle\n                                 useAppend = !useAppend;\n-                                return null;\n                             }, chunkedSegmentStorage.getExecutor());\n                         }, chunkedSegmentStorage.getExecutor()),\n                 chunkedSegmentStorage.getExecutor())\n-                .thenApplyAsync(vv -> {\n+                .thenRunAsync(() -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyMzcxNA=="}, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 17}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NTcyMzcwOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQyMjo0OTowMFrOHwyqtA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQyMzo0MTowMlrOHwz6QQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyMzgyOA==", "bodyText": "and here", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520923828", "createdAt": "2020-11-10T22:49:00Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -250,9 +246,8 @@\n                     bytesToRead = Math.toIntExact(arg.getLength());\n \n                     return copyBytes(writeHandle, arg)\n-                            .thenApplyAsync(v -> {\n+                            .thenRunAsync(() -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk0NDE5Mw==", "bodyText": "same as above", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520944193", "createdAt": "2020-11-10T23:41:02Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -250,9 +246,8 @@\n                     bytesToRead = Math.toIntExact(arg.getLength());\n \n                     return copyBytes(writeHandle, arg)\n-                            .thenApplyAsync(v -> {\n+                            .thenRunAsync(() -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyMzgyOA=="}, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NTcyNjU3OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQyMjo1MDowMlrOHwysdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDo0NToyOFrOHw1PZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyNDI3Nw==", "bodyText": "and here", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520924277", "createdAt": "2020-11-10T22:50:02Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -269,9 +264,8 @@\n                                 bytesToRead -= size;\n                                 readAtOffset += size;\n                                 return chunkedSegmentStorage.getChunkStorage().write(writeHandle, writeAtOffset, size, new ByteArrayInputStream(buffer, 0, size))\n-                                        .thenApplyAsync(written -> {\n+                                        .thenAcceptAsync(written -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk2NTk5MA==", "bodyText": "same as above", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520965990", "createdAt": "2020-11-11T00:45:28Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -269,9 +264,8 @@\n                                 bytesToRead -= size;\n                                 readAtOffset += size;\n                                 return chunkedSegmentStorage.getChunkStorage().write(writeHandle, writeAtOffset, size, new ByteArrayInputStream(buffer, 0, size))\n-                                        .thenApplyAsync(written -> {\n+                                        .thenAcceptAsync(written -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyNDI3Nw=="}, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NTcyNzY3OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQyMjo1MDoyMlrOHwytHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMTozMDo1M1rOHw3FqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyNDQ0Nw==", "bodyText": "This is not thread safe. Use AtomicLong.\nWhile volatile is atomic with respect to get and set, it is not with respect to modification. Your addition is a read-modify-update operation which is not atomic.\nPlease fix this everywhere in your code.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520924447", "createdAt": "2020-11-10T22:50:22Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -269,9 +264,8 @@\n                                 bytesToRead -= size;\n                                 readAtOffset += size;\n                                 return chunkedSegmentStorage.getChunkStorage().write(writeHandle, writeAtOffset, size, new ByteArrayInputStream(buffer, 0, size))\n-                                        .thenApplyAsync(written -> {\n+                                        .thenAcceptAsync(written -> {\n                                             writeAtOffset += written;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk2NzA1Mg==", "bodyText": "Based to application logic they are thread safe as only one lambda from this method executes at a time.  However to avoid future bugs and confusion I'll fix where values are modified. (based on code warning in IDE)", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520967052", "createdAt": "2020-11-11T00:47:39Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -269,9 +264,8 @@\n                                 bytesToRead -= size;\n                                 readAtOffset += size;\n                                 return chunkedSegmentStorage.getChunkStorage().write(writeHandle, writeAtOffset, size, new ByteArrayInputStream(buffer, 0, size))\n-                                        .thenApplyAsync(written -> {\n+                                        .thenAcceptAsync(written -> {\n                                             writeAtOffset += written;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyNDQ0Nw=="}, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk5NjI2NA==", "bodyText": "ok updated.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520996264", "createdAt": "2020-11-11T01:30:53Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/DefragmentOperation.java", "diffHunk": "@@ -269,9 +264,8 @@\n                                 bytesToRead -= size;\n                                 readAtOffset += size;\n                                 return chunkedSegmentStorage.getChunkStorage().write(writeHandle, writeAtOffset, size, new ByteArrayInputStream(buffer, 0, size))\n-                                        .thenApplyAsync(written -> {\n+                                        .thenAcceptAsync(written -> {\n                                             writeAtOffset += written;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyNDQ0Nw=="}, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NTczMzY3OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadOperation.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQyMjo1MjoyNFrOHwywlQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMDo0MjozN1rOHw1L2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyNTMzMw==", "bodyText": "and here", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520925333", "createdAt": "2020-11-10T22:52:24Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadOperation.java", "diffHunk": "@@ -145,12 +144,11 @@ private void logEnd() {\n                                                 bytesToRead,\n                                                 buffer,\n                                                 currentBufferOffset)\n-                                                .thenApplyAsync(bytesRead -> {\n+                                                .thenAcceptAsync(bytesRead -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk0NDY3OQ==", "bodyText": "same as above", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520944679", "createdAt": "2020-11-10T23:42:19Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadOperation.java", "diffHunk": "@@ -145,12 +144,11 @@ private void logEnd() {\n                                                 bytesToRead,\n                                                 buffer,\n                                                 currentBufferOffset)\n-                                                .thenApplyAsync(bytesRead -> {\n+                                                .thenAcceptAsync(bytesRead -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyNTMzMw=="}, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk2NTA4MQ==", "bodyText": "same as above", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520965081", "createdAt": "2020-11-11T00:42:37Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/ReadOperation.java", "diffHunk": "@@ -145,12 +144,11 @@ private void logEnd() {\n                                                 bytesToRead,\n                                                 buffer,\n                                                 currentBufferOffset)\n-                                                .thenApplyAsync(bytesRead -> {\n+                                                .thenAcceptAsync(bytesRead -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyNTMzMw=="}, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI2NTczNTcyOnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/TruncateOperation.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMFQyMjo1MzoxMlrOHwyxzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQwMTozMTowMVrOHw3GBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyNTY0Ng==", "bodyText": "FYI, Java allows you to compress this syntax into thenRunAsync(this::postCommit, executor)", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520925646", "createdAt": "2020-11-10T22:53:12Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/TruncateOperation.java", "diffHunk": "@@ -90,9 +90,8 @@\n                                         return commit(txn)\n                                                 .handleAsync(this::handleException, chunkedSegmentStorage.getExecutor())\n                                                 .thenComposeAsync(vv ->\n-                                                                chunkedSegmentStorage.collectGarbage(chunksToDelete).thenApplyAsync(vvv -> {\n+                                                                chunkedSegmentStorage.collectGarbage(chunksToDelete).thenRunAsync(() -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDk5NjM1OA==", "bodyText": "fixed.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r520996358", "createdAt": "2020-11-11T01:31:01Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/TruncateOperation.java", "diffHunk": "@@ -90,9 +90,8 @@\n                                         return commit(txn)\n                                                 .handleAsync(this::handleException, chunkedSegmentStorage.getExecutor())\n                                                 .thenComposeAsync(vv ->\n-                                                                chunkedSegmentStorage.collectGarbage(chunksToDelete).thenApplyAsync(vvv -> {\n+                                                                chunkedSegmentStorage.collectGarbage(chunksToDelete).thenRunAsync(() -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMDkyNTY0Ng=="}, "originalCommit": {"oid": "26167f653eb6ccaef2b11021bbbac5538a137129"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI3MDE3NDY5OnYy", "diffSide": "RIGHT", "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/WriteOperation.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQyMDo0NzowMVrOHxdsbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xMVQyMTo0Njo1NlrOHxfepg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTYyODc4MA==", "bodyText": "Do you really need to run this on another thread? thenRun will suffice.\nEverywhere else too.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r521628780", "createdAt": "2020-11-11T20:47:01Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/WriteOperation.java", "diffHunk": "@@ -188,30 +187,28 @@ private void collectGarbage() {\n \n         // if layout did not change then commit with lazyWrite.\n         return txn.commit(!didSegmentLayoutChange && chunkedSegmentStorage.getConfig().isLazyCommitEnabled())\n-                .thenRunAsync(() -> {\n-                    isCommitted = true;\n-                }, chunkedSegmentStorage.getExecutor());\n+                .thenRunAsync(() -> isCommitted = true, chunkedSegmentStorage.getExecutor());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "38c5a9883187d142407cd7dba6e53bbb2fdc3078"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTY1NzU2Ng==", "bodyText": "thenRun will potentially run on callers thread which may or may not include segment store core threads.\nI do want Storage code to always run on storage pool.\nWith thread pools the number of threads are fixed and we are not switching threads explicitly here. Framework is free to schedule any task on any free thread- including then thenRun. It uses variant of work stealing which will greedily execute next available task.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r521657566", "createdAt": "2020-11-11T21:46:00Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/WriteOperation.java", "diffHunk": "@@ -188,30 +187,28 @@ private void collectGarbage() {\n \n         // if layout did not change then commit with lazyWrite.\n         return txn.commit(!didSegmentLayoutChange && chunkedSegmentStorage.getConfig().isLazyCommitEnabled())\n-                .thenRunAsync(() -> {\n-                    isCommitted = true;\n-                }, chunkedSegmentStorage.getExecutor());\n+                .thenRunAsync(() -> isCommitted = true, chunkedSegmentStorage.getExecutor());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTYyODc4MA=="}, "originalCommit": {"oid": "38c5a9883187d142407cd7dba6e53bbb2fdc3078"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTY1ODAyMg==", "bodyText": "There is no extra overhead here.", "url": "https://github.com/pravega/pravega/pull/5200#discussion_r521658022", "createdAt": "2020-11-11T21:46:56Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/storage/src/main/java/io/pravega/segmentstore/storage/chunklayer/WriteOperation.java", "diffHunk": "@@ -188,30 +187,28 @@ private void collectGarbage() {\n \n         // if layout did not change then commit with lazyWrite.\n         return txn.commit(!didSegmentLayoutChange && chunkedSegmentStorage.getConfig().isLazyCommitEnabled())\n-                .thenRunAsync(() -> {\n-                    isCommitted = true;\n-                }, chunkedSegmentStorage.getExecutor());\n+                .thenRunAsync(() -> isCommitted = true, chunkedSegmentStorage.getExecutor());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMTYyODc4MA=="}, "originalCommit": {"oid": "38c5a9883187d142407cd7dba6e53bbb2fdc3078"}, "originalPosition": 76}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4707, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}