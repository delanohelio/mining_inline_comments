{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDMyNDAyNzQ4", "number": 4863, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwNToxNDozOFrOEEizGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQwMzo1Mjo0M1rOEFYaRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczMTk5ODk5OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/host/src/main/java/io/pravega/segmentstore/server/host/stat/AutoScaleProcessor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwNToxNDozOFrOGiOxTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQwNToxNDozOFrOGiOxTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODU0Njc2Nw==", "bodyText": "we can use io.pravega.common.concurrent.Futures#isSuccessful here.", "url": "https://github.com/pravega/pravega/pull/4863#discussion_r438546767", "createdAt": "2020-06-11T05:14:38Z", "author": {"login": "shrids"}, "path": "segmentstore/server/host/src/main/java/io/pravega/segmentstore/server/host/stat/AutoScaleProcessor.java", "diffHunk": "@@ -116,48 +122,51 @@\n                 }, executor))\n                 .build();\n \n+        this.executor = executor;\n+        \n         // Even if there is no activity, keep cleaning up the cache so that scale down can be triggered.\n         // caches do not perform clean up if there is no activity. This is because they do not maintain their\n         // own background thread.\n         this.cacheCleanup = executor.scheduleAtFixedRate(cache::cleanUp, 0, configuration.getCacheCleanup().getSeconds(), TimeUnit.SECONDS);\n-        if (clientFactory != null) {\n-            bootstrapRequestWriters(clientFactory, executor);\n-        }\n     }\n \n     @Override\n+    @Synchronized\n     public void close() {\n-        val w = this.writer.get();\n-        if (w != null) {\n-            w.close();\n-            this.writer.set(null);\n+        if (writer != null) {\n+            writer.cancel(true);\n+\n+            if (!writer.isCancelled() && !writer.isCompletedExceptionally()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2025f3c22dea3a605b1ed613731a51793220dc6d"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNDI5MzU3OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/host/src/main/java/io/pravega/segmentstore/server/host/stat/AutoScaleProcessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNjozNTozOVrOGilmww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQwNDoyNzo0MlrOGi2qFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODkyMDg5OQ==", "bodyText": "Non-final fields make it look like something's not right. Let's see if we can make this better. Would this work?\n\nMake this final and remove the \"GuardedBy\"\nIn the getWriter method, do exactly what you're doing, but when the retry loop is done, set the result on this future, whether it be the writer or an exception. You can use Futures.completeAfter (I may forget the actual name).\n\nThis way we don't need a new lock and we can make this field final. CompletableFuture internally is thread safe so it takes care of any concurrency issues for us.", "url": "https://github.com/pravega/pravega/pull/4863#discussion_r438920899", "createdAt": "2020-06-11T16:35:39Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/host/src/main/java/io/pravega/segmentstore/server/host/stat/AutoScaleProcessor.java", "diffHunk": "@@ -61,10 +66,12 @@\n \n     private final EventStreamClientFactory clientFactory;\n     private final Cache<String, Pair<Long, Long>> cache;\n-    private final AtomicReference<EventStreamWriter<AutoScaleEvent>> writer;\n+    @GuardedBy(\"$lock\")\n+    private CompletableFuture<EventStreamWriter<AutoScaleEvent>> writer;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039847398915fd9278c3d5c8cbf6adeff7ee4834"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTIwMDI3OQ==", "bodyText": "We can avoid non final writer. But the objective is to make the bootstrap of writer be triggered exactly once which will eventually complete the future.\nIn existing code, the bootstrap was being invoked from the constructor.\nBut now we want to do it lazily, upon first request. But there could be multiple concurrent requests.\nSo we need some syncrhonization to make sure exactly one of them end up attempting to create a writer.\nSo syncrhonization is really for that. And instead of using a non-final boolean which is synchronized (or atomic boolean) i had chosen to do with writerFuture = null.\nI have an alternate idea though -- which would still require using atomic boolean.. let me make and push that change and see if that is better.", "url": "https://github.com/pravega/pravega/pull/4863#discussion_r439200279", "createdAt": "2020-06-12T04:27:42Z", "author": {"login": "shiveshr"}, "path": "segmentstore/server/host/src/main/java/io/pravega/segmentstore/server/host/stat/AutoScaleProcessor.java", "diffHunk": "@@ -61,10 +66,12 @@\n \n     private final EventStreamClientFactory clientFactory;\n     private final Cache<String, Pair<Long, Long>> cache;\n-    private final AtomicReference<EventStreamWriter<AutoScaleEvent>> writer;\n+    @GuardedBy(\"$lock\")\n+    private CompletableFuture<EventStreamWriter<AutoScaleEvent>> writer;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODkyMDg5OQ=="}, "originalCommit": {"oid": "039847398915fd9278c3d5c8cbf6adeff7ee4834"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNDI5Njc1OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/host/src/main/java/io/pravega/segmentstore/server/host/stat/AutoScaleProcessor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNjozNjoyOFrOGilouQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNjozNjoyOFrOGilouQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODkyMTQwMQ==", "bodyText": "Writer should always be non-null.", "url": "https://github.com/pravega/pravega/pull/4863#discussion_r438921401", "createdAt": "2020-06-11T16:36:28Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/host/src/main/java/io/pravega/segmentstore/server/host/stat/AutoScaleProcessor.java", "diffHunk": "@@ -116,48 +123,51 @@\n                 }, executor))\n                 .build();\n \n+        this.executor = executor;\n+        \n         // Even if there is no activity, keep cleaning up the cache so that scale down can be triggered.\n         // caches do not perform clean up if there is no activity. This is because they do not maintain their\n         // own background thread.\n         this.cacheCleanup = executor.scheduleAtFixedRate(cache::cleanUp, 0, configuration.getCacheCleanup().getSeconds(), TimeUnit.SECONDS);\n-        if (clientFactory != null) {\n-            bootstrapRequestWriters(clientFactory, executor);\n-        }\n     }\n \n     @Override\n+    @Synchronized\n     public void close() {\n-        val w = this.writer.get();\n-        if (w != null) {\n-            w.close();\n-            this.writer.set(null);\n+        if (writer != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039847398915fd9278c3d5c8cbf6adeff7ee4834"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNDMwMjE5OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/host/src/main/java/io/pravega/segmentstore/server/host/stat/AutoScaleProcessor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNjozNzo1NVrOGilsEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNjozNzo1NVrOGilsEw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODkyMjI1OQ==", "bodyText": "This indefinite retry won't work well in case we fail to bootstrap and we decide to shut down. It will likely leave this running forever. You should include a condition that can cancel it. My recommendation would be to check this.writer.isDone (it will be true when either a writer has been set or when it got completed with an exception).", "url": "https://github.com/pravega/pravega/pull/4863#discussion_r438922259", "createdAt": "2020-06-11T16:37:55Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/host/src/main/java/io/pravega/segmentstore/server/host/stat/AutoScaleProcessor.java", "diffHunk": "@@ -116,48 +123,51 @@\n                 }, executor))\n                 .build();\n \n+        this.executor = executor;\n+        \n         // Even if there is no activity, keep cleaning up the cache so that scale down can be triggered.\n         // caches do not perform clean up if there is no activity. This is because they do not maintain their\n         // own background thread.\n         this.cacheCleanup = executor.scheduleAtFixedRate(cache::cleanUp, 0, configuration.getCacheCleanup().getSeconds(), TimeUnit.SECONDS);\n-        if (clientFactory != null) {\n-            bootstrapRequestWriters(clientFactory, executor);\n-        }\n     }\n \n     @Override\n+    @Synchronized\n     public void close() {\n-        val w = this.writer.get();\n-        if (w != null) {\n-            w.close();\n-            this.writer.set(null);\n+        if (writer != null) {\n+            writer.cancel(true);\n+\n+            if (Futures.isSuccessful(writer)) {\n+                val w = this.writer.join();\n+                if (w != null) {\n+                    w.close();\n+                }\n+            }\n         }\n \n         this.clientFactory.close();\n         this.cacheCleanup.cancel(true);\n     }\n \n-    private void bootstrapRequestWriters(EventStreamClientFactory clientFactory, ScheduledExecutorService executor) {\n-        // Starting with initial delay, in case request stream has not been created, to give it time to start\n-        // However, we have this wrapped in consumeFailure which means the creation of writer will be retried.\n-        // We are introducing a delay to avoid exceptions in the log in case creation of writer is attempted before\n-        // creation of requeststream.\n-        executor.schedule(\n-                () -> Retry.indefinitelyWithExpBackoff(100, 10, 10000, this::handleBootstrapException)\n-                        .runInExecutor(() -> bootstrapOnce(clientFactory), executor),\n-                10, TimeUnit.SECONDS);\n+    @Synchronized\n+    private CompletableFuture<EventStreamWriter<AutoScaleEvent>> getWriter() {\n+        if (writer == null) {\n+            AtomicReference<EventStreamWriter<AutoScaleEvent>> w = new AtomicReference<>();\n+            EventWriterConfig writerConfig = EventWriterConfig.builder().build();\n+            writer = Retry.indefinitelyWithExpBackoff(100, 10, 10000, this::handleBootstrapException)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039847398915fd9278c3d5c8cbf6adeff7ee4834"}, "originalPosition": 113}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjczNDMwNTkyOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/host/src/main/java/io/pravega/segmentstore/server/host/stat/AutoScaleProcessor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNjozODo1NFrOGilubw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMVQxNjozODo1NFrOGilubw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODkyMjg2Mw==", "bodyText": "Please revert this. This will result in too much garbage in the segment store logs.", "url": "https://github.com/pravega/pravega/pull/4863#discussion_r438922863", "createdAt": "2020-06-11T16:38:54Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/host/src/main/java/io/pravega/segmentstore/server/host/stat/AutoScaleProcessor.java", "diffHunk": "@@ -192,113 +202,99 @@ static boolean hasTlsEnabled(@NonNull final URI controllerURI) {\n         return uriScheme.equals(\"tls\") || uriScheme.equals(\"pravegas\");\n     }\n \n-    private boolean isInitialized() {\n-        return this.writer.get() != null;\n-    }\n-\n     private void triggerScaleUp(String streamSegmentName, int numOfSplits) {\n-        if (isInitialized()) {\n-            Pair<Long, Long> pair = cache.getIfPresent(streamSegmentName);\n-            long lastRequestTs = 0;\n+        Pair<Long, Long> pair = cache.getIfPresent(streamSegmentName);\n+        long lastRequestTs = 0;\n \n-            if (pair != null && pair.getKey() != null) {\n-                lastRequestTs = pair.getKey();\n-            }\n+        if (pair != null && pair.getKey() != null) {\n+            lastRequestTs = pair.getKey();\n+        }\n \n-            long timestamp = System.currentTimeMillis();\n-            long requestId = requestIdGenerator.get();\n-            if (timestamp - lastRequestTs > configuration.getMuteDuration().toMillis()) {\n-                log.info(requestId, \"sending request for scale up for {}\", streamSegmentName);\n+        long timestamp = getTimeMillis();\n+        long requestId = requestIdGenerator.get();\n+        if (timestamp - lastRequestTs > configuration.getMuteDuration().toMillis()) {\n+            log.info(requestId, \"sending request for scale up for {}\", streamSegmentName);\n \n-                Segment segment = Segment.fromScopedName(streamSegmentName);\n-                AutoScaleEvent event = new AutoScaleEvent(segment.getScope(), segment.getStreamName(), segment.getSegmentId(),\n-                        AutoScaleEvent.UP, timestamp, numOfSplits, false, requestId);\n-                // Mute scale for timestamp for both scale up and down\n-                writeRequest(event, () -> cache.put(streamSegmentName, new ImmutablePair<>(timestamp, timestamp)));\n-            }\n+            Segment segment = Segment.fromScopedName(streamSegmentName);\n+            AutoScaleEvent event = new AutoScaleEvent(segment.getScope(), segment.getStreamName(), segment.getSegmentId(),\n+                    AutoScaleEvent.UP, timestamp, numOfSplits, false, requestId);\n+            // Mute scale for timestamp for both scale up and down\n+            writeRequest(event, () -> cache.put(streamSegmentName, new ImmutablePair<>(timestamp, timestamp)));\n         }\n     }\n \n     private void triggerScaleDown(String streamSegmentName, boolean silent) {\n-        if (isInitialized()) {\n-            Pair<Long, Long> pair = cache.getIfPresent(streamSegmentName);\n-            long lastRequestTs = 0;\n+        Pair<Long, Long> pair = cache.getIfPresent(streamSegmentName);\n+        long lastRequestTs = 0;\n \n-            if (pair != null && pair.getValue() != null) {\n-                lastRequestTs = pair.getValue();\n-            }\n+        if (pair != null && pair.getValue() != null) {\n+            lastRequestTs = pair.getValue();\n+        }\n \n-            long timestamp = System.currentTimeMillis();\n-            long requestId = requestIdGenerator.get();\n-            if (timestamp - lastRequestTs > configuration.getMuteDuration().toMillis()) {\n-                log.info(requestId, \"sending request for scale down for {}\", streamSegmentName);\n-\n-                Segment segment = Segment.fromScopedName(streamSegmentName);\n-                AutoScaleEvent event = new AutoScaleEvent(segment.getScope(), segment.getStreamName(), segment.getSegmentId(),\n-                        AutoScaleEvent.DOWN, timestamp, 0, silent, requestId);\n-                writeRequest(event, () -> {\n-                    if (!silent) {\n-                        // mute only scale downs\n-                        cache.put(streamSegmentName, new ImmutablePair<>(0L, timestamp));\n-                    }\n-                });\n-            }\n+        long timestamp = getTimeMillis();\n+        long requestId = requestIdGenerator.get();\n+        if (timestamp - lastRequestTs > configuration.getMuteDuration().toMillis()) {\n+            log.info(requestId, \"sending request for scale down for {}\", streamSegmentName);\n+\n+            Segment segment = Segment.fromScopedName(streamSegmentName);\n+            AutoScaleEvent event = new AutoScaleEvent(segment.getScope(), segment.getStreamName(), segment.getSegmentId(),\n+                    AutoScaleEvent.DOWN, timestamp, 0, silent, requestId);\n+            writeRequest(event, () -> {\n+                if (!silent) {\n+                    // mute only scale downs\n+                    cache.put(streamSegmentName, new ImmutablePair<>(0L, timestamp));\n+                }\n+            });\n         }\n     }\n \n     private void writeRequest(AutoScaleEvent event, Runnable successCallback) {\n-        val writer = this.writer.get();\n-        if (writer == null) {\n-            log.warn(event.getRequestId(), \"Writer not bootstrapped; unable to post Scale Event {}.\", event);\n-        } else {\n-            writer.writeEvent(event.getKey(), event)\n+        getWriter().thenCompose(w -> w.writeEvent(event.getKey(), event)\n                     .whenComplete((r, e) -> {\n                         if (e != null) {\n                             log.error(event.getRequestId(), \"Unable to post Scale Event to RequestStream '{}'.\",\n                                     this.configuration.getInternalRequestStream(), e);\n                         } else {\n-                            log.debug(event.getRequestId(), \"Scale Event posted successfully: {}.\", event);\n+                            log.info(event.getRequestId(), \"Scale Event posted successfully: {}.\", event);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "039847398915fd9278c3d5c8cbf6adeff7ee4834"}, "originalPosition": 240}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc0MDc4Mjc4OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/host/src/main/java/io/pravega/segmentstore/server/host/stat/AutoScaleProcessor.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQwMzo1Mjo0M1rOGjidgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xNVQwMzo1Mjo0M1rOGjidgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTkxNzk1NQ==", "bodyText": "If startInitWriter is false then we would be logging this line repeatedly.\nThis would clutter the logs with WARN messages. (including standalone) Can we avoid it?", "url": "https://github.com/pravega/pravega/pull/4863#discussion_r439917955", "createdAt": "2020-06-15T03:52:43Z", "author": {"login": "shrids"}, "path": "segmentstore/server/host/src/main/java/io/pravega/segmentstore/server/host/stat/AutoScaleProcessor.java", "diffHunk": "@@ -127,35 +131,44 @@\n \n     @Override\n     public void close() {\n-        val w = this.writer.get();\n-        if (w != null) {\n-            w.close();\n-            this.writer.set(null);\n+        writer.cancel(true);\n+\n+        if (Futures.isSuccessful(writer)) {\n+            val w = this.writer.join();\n+            if (w != null) {\n+                w.close();\n+            }\n         }\n \n-        this.clientFactory.close();\n+        if (clientFactory != null) {\n+            this.clientFactory.close();\n+        }\n         this.cacheCleanup.cancel(true);\n     }\n-\n+    \n     private void bootstrapRequestWriters(EventStreamClientFactory clientFactory, ScheduledExecutorService executor) {\n-        // Starting with initial delay, in case request stream has not been created, to give it time to start\n-        // However, we have this wrapped in consumeFailure which means the creation of writer will be retried.\n-        // We are introducing a delay to avoid exceptions in the log in case creation of writer is attempted before\n-        // creation of requeststream.\n-        executor.schedule(\n-                () -> Retry.indefinitelyWithExpBackoff(100, 10, 10000, this::handleBootstrapException)\n-                        .runInExecutor(() -> bootstrapOnce(clientFactory), executor),\n-                10, TimeUnit.SECONDS);\n+        AtomicReference<EventStreamWriter<AutoScaleEvent>> w = new AtomicReference<>();\n+\n+        Futures.completeAfter(() -> Retry.indefinitelyWithExpBackoff(100, 10, 10000, this::handleBootstrapException)\n+                                         .runInExecutor(() -> bootstrapOnce(clientFactory, w), \n+                                                 executor).thenApply(v -> w.get()), writer);\n     }\n \n     private void handleBootstrapException(Throwable e) {\n         log.warn(\"Unable to create writer for requeststream: {}.\", LoggerHelpers.exceptionSummary(log, e));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3a901ef2f428b7984db5b358b4a0463d2f77ed48"}, "originalPosition": 93}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4379, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}