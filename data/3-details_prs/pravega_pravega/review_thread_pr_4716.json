{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDA2ODI4NTEw", "number": 4716, "reviewThreads": {"totalCount": 130, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo1Mzo1OVrOEX0AmQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMDo0OTo1N1rOEZRUZQ==", "hasNextPage": false, "hasPreviousPage": true}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDA0ODI1OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo1Mzo1OVrOG_0Ldg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo1Mzo1OVrOG_0Ldg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2ODM3NA==", "bodyText": "Isn't there a RANDOM.nextBoolean()?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469568374", "createdAt": "2020-08-12T21:53:59Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "diffHunk": "@@ -0,0 +1,315 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.mocks.InMemoryDurableDataLogFactory;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorage;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorageFactory;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.ByteArrayInputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+/**\n+ * Tests for DebugStreamSegmentContainer class.\n+ */\n+@Slf4j\n+public class DebugStreamSegmentContainerTests extends ThreadPooledTestSuite {\n+    private static final int MIN_SEGMENT_LENGTH = 0; // Used in randomly generating the length for a segment\n+    private static final int MAX_SEGMENT_LENGTH = 10100; // Used in randomly generating the length for a segment\n+    private static final int CONTAINER_ID = 1234567;\n+    private static final int EXPECTED_PINNED_SEGMENT_COUNT = 1;\n+    private static final int MAX_DATA_LOG_APPEND_SIZE = 100 * 1024;\n+    private static final int TEST_TIMEOUT_MILLIS = 60 * 1000;\n+    private static final Duration TIMEOUT = Duration.ofMillis(TEST_TIMEOUT_MILLIS);\n+    private static final Random RANDOM = new Random(1234);\n+    private static final int THREAD_POOL_COUNT = 30;\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, 10 * 60)\n+            .build();\n+\n+    private static final DurableLogConfig DEFAULT_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10 * 1024 * 1024L)\n+            .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+            .build();\n+\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig DEFAULT_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+            .build();\n+    private static final ContainerConfig CONTAINER_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+            .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 200 + EXPECTED_PINNED_SEGMENT_COUNT)\n+            .build();\n+    private ScheduledExecutorService executorService;\n+\n+    @Rule\n+    public Timeout globalTimeout = Timeout.millis(TEST_TIMEOUT_MILLIS);\n+\n+    @Before\n+    public void setUp() {\n+        this.executorService = executorService();\n+    }\n+\n+    @After\n+    public void tearDown() {\n+        this.executorService.shutdown();\n+    }\n+\n+    protected int getThreadPoolSize() {\n+        return THREAD_POOL_COUNT;\n+    }\n+\n+    /**\n+     * It tests the ability to register an existing segment(segment existing only in Long-Term Storage) using debug\n+     * segment container. Method registerSegment in {@link DebugStreamSegmentContainer} is tested here.\n+     * The test starts a debug segment container and creates some segments using it and then verifies if the segments\n+     * were created successfully.\n+     */\n+    @Test\n+    public void testRegisterExistingSegment() {\n+        int maxSegmentCount = 100;\n+        final int createdSegmentCount = maxSegmentCount * 2;\n+\n+        // Sets up dataLogFactory, readIndexFactory, attributeIndexFactory etc for the DebugSegmentContainer.\n+        @Cleanup\n+        TestContext context = createContext(executorService);\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DEFAULT_DURABLE_LOG_CONFIG, context.dataLogFactory, executorService());\n+        // Starts a DebugSegmentContainer.\n+        @Cleanup\n+        MetadataCleanupContainer localContainer = new MetadataCleanupContainer(CONTAINER_ID, CONTAINER_CONFIG, localDurableLogFactory,\n+                context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, context.storageFactory,\n+                context.getDefaultExtensions(), executorService());\n+        localContainer.startAsync().awaitRunning();\n+        log.info(\"Started debug segment container.\");\n+\n+        // Record details(name, length & sealed status) of each segment to be created.\n+        ArrayList<String> segments = new ArrayList<>();\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        long[] segmentLengths = new long[createdSegmentCount];\n+        boolean[] segmentSealedStatus = new boolean[createdSegmentCount];\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            segmentLengths[i] = MIN_SEGMENT_LENGTH + RANDOM.nextInt(MAX_SEGMENT_LENGTH - MIN_SEGMENT_LENGTH);\n+            segmentSealedStatus[i] = RANDOM.nextBoolean();\n+            String name = \"Segment_\" + i;\n+            segments.add(name);\n+            futures.add(localContainer.registerSegment(name, segmentLengths[i], segmentSealedStatus[i]));\n+        }\n+        // Creates all the segments.\n+        Futures.allOf(futures).join();\n+        log.info(\"Created the segments using debug segment container.\");\n+\n+        // Verify the Segments are still there with their length & sealed status.\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            SegmentProperties props = localContainer.getStreamSegmentInfo(segments.get(i), TIMEOUT).join();\n+            Assert.assertEquals(\"Segment length mismatch \", segmentLengths[i], props.getLength());\n+            Assert.assertEquals(\"Segment sealed status mismatch\", segmentSealedStatus[i], props.isSealed());\n+        }\n+        localContainer.stopAsync().awaitTerminated();\n+    }\n+\n+    /**\n+     * Use a storage instance to create segments. Lists the segments from the storage and and then recreates them using\n+     * debug segment containers. Before re-creating(or registering), the segments are mapped to their respective debug\n+     * segment container. Once registered, segment's properties are matched to verify if the test was successful or not.\n+     */\n+    @Test\n+    public void testEndToEnd() throws Exception {\n+        // Segments are mapped to four different containers.\n+        int containerCount = 4;\n+        int segmentsToCreateCount = 50;\n+\n+        // Create a storage.\n+        @Cleanup\n+        val baseStorage = new InMemoryStorage();\n+        @Cleanup\n+        val s = new RollingStorage(baseStorage, new SegmentRollingPolicy(1));\n+        s.initialize(1);\n+        log.info(\"Created a storage instance\");\n+\n+        // Record details(name, container Id & sealed status) of each segment to be created.\n+        Set<String> sealedSegments = new HashSet<>();\n+        byte[] data = \"data\".getBytes();\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, ArrayList<String>> segmentByContainers = new HashMap<>();\n+\n+        // Create segments and get their container Ids, sealed status and names to verify.\n+        for (int i = 0; i < segmentsToCreateCount; i++) {\n+            String segmentName = \"segment-\" + RANDOM.nextInt();\n+\n+            // Use segmentName to map to different containers.\n+            int containerId = segToConMapper.getContainerId(segmentName);\n+            ArrayList<String> segmentsList = segmentByContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentByContainers.put(containerId, segmentsList);\n+            }\n+            segmentByContainers.get(containerId).add(segmentName);\n+\n+            // Create segments, write data and randomly seal some of them.\n+            val wh1 = s.create(segmentName);\n+            // Write data.\n+            s.write(wh1, 0, new ByteArrayInputStream(data), data.length);\n+            if (RANDOM.nextInt(2) == 1) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 224}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDA1NTQ4OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo1NjoyOVrOG_0PqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNjowOTowM1rOHAorig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2OTQ0OQ==", "bodyText": "testSegmentRestoration or similar. Just do not call it endToEndDebugSegmentContainer since it tells a reader nothing about what it's doing. Same with the java doc.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469569449", "createdAt": "2020-08-12T21:56:29Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "diffHunk": "@@ -145,7 +176,78 @@ protected boolean appendAfterMerging() {\n         return true;\n     }\n \n-    //endregion\n+    /**\n+     * End to end test to verify DebugSegmentContainer process.\n+     * @throws Exception If an exception occurred.\n+     */\n+    public void endToEndDebugSegmentContainer() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyODU1NA==", "bodyText": "Ok. Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470428554", "createdAt": "2020-08-14T06:09:03Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "diffHunk": "@@ -145,7 +176,78 @@ protected boolean appendAfterMerging() {\n         return true;\n     }\n \n-    //endregion\n+    /**\n+     * End to end test to verify DebugSegmentContainer process.\n+     * @throws Exception If an exception occurred.\n+     */\n+    public void endToEndDebugSegmentContainer() throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2OTQ0OQ=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDA1NTg2OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo1NjozOVrOG_0P5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNjowOTo0NFrOHAosTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2OTUwOA==", "bodyText": "You do not need to hold this in a variable.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469569508", "createdAt": "2020-08-12T21:56:39Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "diffHunk": "@@ -145,7 +176,78 @@ protected boolean appendAfterMerging() {\n         return true;\n     }\n \n-    //endregion\n+    /**\n+     * End to end test to verify DebugSegmentContainer process.\n+     * @throws Exception If an exception occurred.\n+     */\n+    public void endToEndDebugSegmentContainer() throws Exception {\n+        ScheduledExecutorService executorService = executorService();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyODc1MA==", "bodyText": "Removed.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470428750", "createdAt": "2020-08-14T06:09:44Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "diffHunk": "@@ -145,7 +176,78 @@ protected boolean appendAfterMerging() {\n         return true;\n     }\n \n-    //endregion\n+    /**\n+     * End to end test to verify DebugSegmentContainer process.\n+     * @throws Exception If an exception occurred.\n+     */\n+    public void endToEndDebugSegmentContainer() throws Exception {\n+        ScheduledExecutorService executorService = executorService();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2OTUwOA=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDA1OTg4OnYy", "diffSide": "RIGHT", "path": "shared/protocol/src/main/java/io/pravega/shared/NameUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo1ODowM1rOG_0SLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNjowOTo1NFrOHAosdg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU3MDA5Mw==", "bodyText": "Attribute Segment", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469570093", "createdAt": "2020-08-12T21:58:03Z", "author": {"login": "andreipaduroiu"}, "path": "shared/protocol/src/main/java/io/pravega/shared/NameUtils.java", "diffHunk": "@@ -190,6 +190,16 @@ public static String extractPrimaryStreamSegmentName(String streamSegmentName) {\n         return streamSegmentName.substring(0, endOfStreamNamePos);\n     }\n \n+    /**\n+     * Checks whether the given name is an attribute Segment or not.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyODc5MA==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470428790", "createdAt": "2020-08-14T06:09:54Z", "author": {"login": "ManishKumarKeshri"}, "path": "shared/protocol/src/main/java/io/pravega/shared/NameUtils.java", "diffHunk": "@@ -190,6 +190,16 @@ public static String extractPrimaryStreamSegmentName(String streamSegmentName) {\n         return streamSegmentName.substring(0, endOfStreamNamePos);\n     }\n \n+    /**\n+     * Checks whether the given name is an attribute Segment or not.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU3MDA5Mw=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDA2MDIyOnYy", "diffSide": "RIGHT", "path": "shared/protocol/src/main/java/io/pravega/shared/NameUtils.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo1ODowNlrOG_0SUw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNjoxMDo0N1rOHAothw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU3MDEzMQ==", "bodyText": "There are several places in this file where this can be used. Please find them and update them to make use of your new method.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469570131", "createdAt": "2020-08-12T21:58:06Z", "author": {"login": "andreipaduroiu"}, "path": "shared/protocol/src/main/java/io/pravega/shared/NameUtils.java", "diffHunk": "@@ -190,6 +190,16 @@ public static String extractPrimaryStreamSegmentName(String streamSegmentName) {\n         return streamSegmentName.substring(0, endOfStreamNamePos);\n     }\n \n+    /**\n+     * Checks whether the given name is an attribute Segment or not.\n+     *\n+     * @param segmentName   The name of the segment.\n+     * @return              True if the segment is an attribute Segment, false otherwise.\n+     */\n+    public static boolean isAttributeSegment(String segmentName) {\n+        return segmentName.endsWith(ATTRIBUTE_SUFFIX);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyOTA2Mw==", "bodyText": "It was in one place in the same file. Updated it.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470429063", "createdAt": "2020-08-14T06:10:47Z", "author": {"login": "ManishKumarKeshri"}, "path": "shared/protocol/src/main/java/io/pravega/shared/NameUtils.java", "diffHunk": "@@ -190,6 +190,16 @@ public static String extractPrimaryStreamSegmentName(String streamSegmentName) {\n         return streamSegmentName.substring(0, endOfStreamNamePos);\n     }\n \n+    /**\n+     * Checks whether the given name is an attribute Segment or not.\n+     *\n+     * @param segmentName   The name of the segment.\n+     * @return              True if the segment is an attribute Segment, false otherwise.\n+     */\n+    public static boolean isAttributeSegment(String segmentName) {\n+        return segmentName.endsWith(ATTRIBUTE_SUFFIX);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU3MDEzMQ=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDA2MTg3OnYy", "diffSide": "RIGHT", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo1ODo0OVrOG_0TTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNjoxMzoxNVrOHAowZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU3MDM4MQ==", "bodyText": "Should we make this with more than just 1 container?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469570381", "createdAt": "2020-08-12T21:58:49Z", "author": {"login": "andreipaduroiu"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,563 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.connection.impl.ConnectionFactory;\n+import io.pravega.client.connection.impl.ConnectionPool;\n+import io.pravega.client.connection.impl.ConnectionPoolImpl;\n+import io.pravega.client.connection.impl.SocketConnectionFactoryImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.server.containers.SegmentsRecovery;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.SegmentsTracker;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainerTests;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorageFactory;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.net.URI;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ * What test does, step by step:\n+ * 1. Starts Pravega locally with just one segment container.\n+ * 2. Writes 300 events to two different segments.\n+ * 3. Waits for all segments created to be flushed to the long term storage.\n+ * 4. Shuts down the controller, segment store and bookeeper/zookeeper.\n+ * 5. Deletes container metadata segment and its attribute segment from the old LTS.\n+ * 5. Starts debug segment container using a new bookeeper/zookeeper and the old LTS.\n+ * 6. Re-creates the container metadata segment in Tier1 and let's it flushed to the LTS.\n+ * 7. Starts segment store and controller.\n+ * 8. Reads all 600 events again.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(100 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyOTc5OA==", "bodyText": "I have written integration tests for multiple containers, readers stall while reading and then recovery and then resume from the same point, and watermarking. Will create PRs for them, once this is merged.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470429798", "createdAt": "2020-08-14T06:13:15Z", "author": {"login": "ManishKumarKeshri"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,563 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.connection.impl.ConnectionFactory;\n+import io.pravega.client.connection.impl.ConnectionPool;\n+import io.pravega.client.connection.impl.ConnectionPoolImpl;\n+import io.pravega.client.connection.impl.SocketConnectionFactoryImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.server.containers.SegmentsRecovery;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.SegmentsTracker;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainerTests;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorageFactory;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.net.URI;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ * What test does, step by step:\n+ * 1. Starts Pravega locally with just one segment container.\n+ * 2. Writes 300 events to two different segments.\n+ * 3. Waits for all segments created to be flushed to the long term storage.\n+ * 4. Shuts down the controller, segment store and bookeeper/zookeeper.\n+ * 5. Deletes container metadata segment and its attribute segment from the old LTS.\n+ * 5. Starts debug segment container using a new bookeeper/zookeeper and the old LTS.\n+ * 6. Re-creates the container metadata segment in Tier1 and let's it flushed to the LTS.\n+ * 7. Starts segment store and controller.\n+ * 8. Reads all 600 events again.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(100 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU3MDM4MQ=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MjM5OTQ5OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMDozMjo0N1rOHBCi8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMzoxMzoxNlrOHBFgNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg1MjMzOA==", "bodyText": "What is this map holding ?\nmetadataSegmentsByContainer The name seems confusing.\nSomething like existingSegmetsMap (or similar) will convey the intent better.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470852338", "createdAt": "2020-08-14T20:32:47Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<Integer, Set<String>> metadataSegmentsByContainer = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "883da1068638d9dae5e87aea87c92ade5260e15a"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkwMDc4OQ==", "bodyText": "Ok. Changed to existingSegmentsMap.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470900789", "createdAt": "2020-08-14T23:13:16Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<Integer, Set<String>> metadataSegmentsByContainer = new HashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg1MjMzOA=="}, "originalCommit": {"oid": "883da1068638d9dae5e87aea87c92ade5260e15a"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MjQwMDg5OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMDozMzoyOVrOHBCj9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMzoxMjozNVrOHBFfsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg1MjU5Ng==", "bodyText": "May be extract this into a method", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470852596", "createdAt": "2020-08-14T20:33:29Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<Integer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        val args = IteratorArgs.builder().fetchTimeout(TIMEOUT).build();\n+        for (val debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            Preconditions.checkNotNull(debugStreamSegmentContainerEntry.getValue());\n+            val tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            val keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), args).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k ->\n+                    metadataSegments.addAll(k.getEntries().stream()\n+                            .map(entry -> entry.getKey().toString())\n+                            .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getKey(), metadataSegments);\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "883da1068638d9dae5e87aea87c92ade5260e15a"}, "originalPosition": 92}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkwMDY1OA==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470900658", "createdAt": "2020-08-14T23:12:35Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<Integer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        val args = IteratorArgs.builder().fetchTimeout(TIMEOUT).build();\n+        for (val debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            Preconditions.checkNotNull(debugStreamSegmentContainerEntry.getValue());\n+            val tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            val keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), args).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k ->\n+                    metadataSegments.addAll(k.getEntries().stream()\n+                            .map(entry -> entry.getKey().toString())\n+                            .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getKey(), metadataSegments);\n+        }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg1MjU5Ng=="}, "originalCommit": {"oid": "883da1068638d9dae5e87aea87c92ade5260e15a"}, "originalPosition": 92}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MjQwMzYzOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMDozNDozN1rOHBClrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMzoxMjoyNVrOHBFflQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg1MzAzNg==", "bodyText": "May be\nfor (val currentSegment : ...) { \n....\n}", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470853036", "createdAt": "2020-08-14T20:34:37Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<Integer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        val args = IteratorArgs.builder().fetchTimeout(TIMEOUT).build();\n+        for (val debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            Preconditions.checkNotNull(debugStreamSegmentContainerEntry.getValue());\n+            val tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            val keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), args).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k ->\n+                    metadataSegments.addAll(k.getEntries().stream()\n+                            .map(entry -> entry.getKey().toString())\n+                            .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getKey(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "883da1068638d9dae5e87aea87c92ade5260e15a"}, "originalPosition": 103}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkwMDYyOQ==", "bodyText": "OK.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470900629", "createdAt": "2020-08-14T23:12:25Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<Integer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        val args = IteratorArgs.builder().fetchTimeout(TIMEOUT).build();\n+        for (val debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            Preconditions.checkNotNull(debugStreamSegmentContainerEntry.getValue());\n+            val tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            val keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), args).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k ->\n+                    metadataSegments.addAll(k.getEntries().stream()\n+                            .map(entry -> entry.getKey().toString())\n+                            .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getKey(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg1MzAzNg=="}, "originalCommit": {"oid": "883da1068638d9dae5e87aea87c92ade5260e15a"}, "originalPosition": 103}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MjQyMjA2OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMDo0MjoxOFrOHBCwpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMjo0NTowMlrOHBFIiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg1NTg0Nw==", "bodyText": "Do you have test that tests exception thrown from here ?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470855847", "createdAt": "2020-08-14T20:42:18Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<Integer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        val args = IteratorArgs.builder().fetchTimeout(TIMEOUT).build();\n+        for (val debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            Preconditions.checkNotNull(debugStreamSegmentContainerEntry.getValue());\n+            val tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            val keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), args).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k ->\n+                    metadataSegments.addAll(k.getEntries().stream()\n+                            .map(entry -> entry.getKey().toString())\n+                            .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getKey(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            metadataSegmentsByContainer.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        for (val metadataSegmentsSetEntry : metadataSegmentsByContainer.entrySet()) {\n+            for (String segmentName : metadataSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in the storage.\", segmentName);\n+                debugStreamSegmentContainers.get(metadataSegmentsSetEntry.getKey()).deleteStreamSegment(segmentName, TIMEOUT).join();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * This method takes a {@link DebugStreamSegmentContainer} instance and a {@link SegmentProperties} object as arguments\n+     * and takes one of the following actions:\n+     * 1. If the segment is present in the container metadata and its length or sealed status or both doesn't match with the\n+     * given {@link SegmentProperties}, then it is deleted from there and registered using the properties from the given\n+     * {@link SegmentProperties} instance.\n+     * 2. If the segment is absent in the container metadata, then it is registered using the properties from the given\n+     * {@link SegmentProperties}.\n+     * @param container         A {@link DebugStreamSegmentContainer} instance for registering the given segment and checking\n+     *                          its existence in the container metadata.\n+     * @param storageSegment    A {@link SegmentProperties} instance which has properties of the segment to be registered.\n+     * @return                  CompletableFuture which when completed will have the segment registered on to the container\n+     *                          metadata.\n+     */\n+    private static CompletableFuture<Void> recoverSegment(DebugStreamSegmentContainer container, SegmentProperties storageSegment) {\n+        Preconditions.checkNotNull(container);\n+        Preconditions.checkNotNull(storageSegment);\n+        long segmentLength = storageSegment.getLength();\n+        boolean isSealed = storageSegment.isSealed();\n+        String segmentName = storageSegment.getName();\n+\n+        log.info(\"Registering: {}, {}, {}.\", segmentName, segmentLength, isSealed);\n+        return Futures.exceptionallyComposeExpecting(\n+                container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                                container.metadataStore.deleteSegment(segmentName, TIMEOUT)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "883da1068638d9dae5e87aea87c92ade5260e15a"}, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg5NDcyOQ==", "bodyText": "Almost all segments in the integration test through exception that it is not present in the container metadata.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470894729", "createdAt": "2020-08-14T22:45:02Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<Integer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        val args = IteratorArgs.builder().fetchTimeout(TIMEOUT).build();\n+        for (val debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            Preconditions.checkNotNull(debugStreamSegmentContainerEntry.getValue());\n+            val tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            val keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), args).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k ->\n+                    metadataSegments.addAll(k.getEntries().stream()\n+                            .map(entry -> entry.getKey().toString())\n+                            .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getKey(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            metadataSegmentsByContainer.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        for (val metadataSegmentsSetEntry : metadataSegmentsByContainer.entrySet()) {\n+            for (String segmentName : metadataSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in the storage.\", segmentName);\n+                debugStreamSegmentContainers.get(metadataSegmentsSetEntry.getKey()).deleteStreamSegment(segmentName, TIMEOUT).join();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * This method takes a {@link DebugStreamSegmentContainer} instance and a {@link SegmentProperties} object as arguments\n+     * and takes one of the following actions:\n+     * 1. If the segment is present in the container metadata and its length or sealed status or both doesn't match with the\n+     * given {@link SegmentProperties}, then it is deleted from there and registered using the properties from the given\n+     * {@link SegmentProperties} instance.\n+     * 2. If the segment is absent in the container metadata, then it is registered using the properties from the given\n+     * {@link SegmentProperties}.\n+     * @param container         A {@link DebugStreamSegmentContainer} instance for registering the given segment and checking\n+     *                          its existence in the container metadata.\n+     * @param storageSegment    A {@link SegmentProperties} instance which has properties of the segment to be registered.\n+     * @return                  CompletableFuture which when completed will have the segment registered on to the container\n+     *                          metadata.\n+     */\n+    private static CompletableFuture<Void> recoverSegment(DebugStreamSegmentContainer container, SegmentProperties storageSegment) {\n+        Preconditions.checkNotNull(container);\n+        Preconditions.checkNotNull(storageSegment);\n+        long segmentLength = storageSegment.getLength();\n+        boolean isSealed = storageSegment.isSealed();\n+        String segmentName = storageSegment.getName();\n+\n+        log.info(\"Registering: {}, {}, {}.\", segmentName, segmentLength, isSealed);\n+        return Futures.exceptionallyComposeExpecting(\n+                container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                                container.metadataStore.deleteSegment(segmentName, TIMEOUT)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg1NTg0Nw=="}, "originalCommit": {"oid": "883da1068638d9dae5e87aea87c92ade5260e15a"}, "originalPosition": 149}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MjQyNjI5OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMDo0NDoxNFrOHBCzVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMzoxMTo1M1rOHBFfLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg1NjUzMw==", "bodyText": "to avoid confusion please rename it to deleteSegmentFromStorage.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470856533", "createdAt": "2020-08-14T20:44:14Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<Integer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        val args = IteratorArgs.builder().fetchTimeout(TIMEOUT).build();\n+        for (val debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            Preconditions.checkNotNull(debugStreamSegmentContainerEntry.getValue());\n+            val tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            val keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), args).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k ->\n+                    metadataSegments.addAll(k.getEntries().stream()\n+                            .map(entry -> entry.getKey().toString())\n+                            .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getKey(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            metadataSegmentsByContainer.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        for (val metadataSegmentsSetEntry : metadataSegmentsByContainer.entrySet()) {\n+            for (String segmentName : metadataSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in the storage.\", segmentName);\n+                debugStreamSegmentContainers.get(metadataSegmentsSetEntry.getKey()).deleteStreamSegment(segmentName, TIMEOUT).join();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * This method takes a {@link DebugStreamSegmentContainer} instance and a {@link SegmentProperties} object as arguments\n+     * and takes one of the following actions:\n+     * 1. If the segment is present in the container metadata and its length or sealed status or both doesn't match with the\n+     * given {@link SegmentProperties}, then it is deleted from there and registered using the properties from the given\n+     * {@link SegmentProperties} instance.\n+     * 2. If the segment is absent in the container metadata, then it is registered using the properties from the given\n+     * {@link SegmentProperties}.\n+     * @param container         A {@link DebugStreamSegmentContainer} instance for registering the given segment and checking\n+     *                          its existence in the container metadata.\n+     * @param storageSegment    A {@link SegmentProperties} instance which has properties of the segment to be registered.\n+     * @return                  CompletableFuture which when completed will have the segment registered on to the container\n+     *                          metadata.\n+     */\n+    private static CompletableFuture<Void> recoverSegment(DebugStreamSegmentContainer container, SegmentProperties storageSegment) {\n+        Preconditions.checkNotNull(container);\n+        Preconditions.checkNotNull(storageSegment);\n+        long segmentLength = storageSegment.getLength();\n+        boolean isSealed = storageSegment.isSealed();\n+        String segmentName = storageSegment.getName();\n+\n+        log.info(\"Registering: {}, {}, {}.\", segmentName, segmentLength, isSealed);\n+        return Futures.exceptionallyComposeExpecting(\n+                container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                                container.metadataStore.deleteSegment(segmentName, TIMEOUT)\n+                                        .thenAccept(x -> container.registerSegment(segmentName, segmentLength, isSealed));\n+                            }\n+                        }), ex -> Exceptions.unwrap(ex) instanceof StreamSegmentNotExistsException,\n+                () -> container.registerSegment(segmentName, segmentLength, isSealed));\n+    }\n+\n+    /**\n+     * Deletes container metadata segment and its Attribute segment from the {@link Storage} for the given container Id.\n+     * @param storage       A {@link Storage} instance to delete the segments from.\n+     * @param containerId   Id of the container for which the segments has to be deleted.\n+     */\n+    public static void deleteContainerMetadataAndAttributeSegments(Storage storage, int containerId) {\n+        Preconditions.checkNotNull(storage);\n+        String metadataSegmentName = NameUtils.getMetadataSegmentName(containerId);\n+        String attributeSegmentName = NameUtils.getAttributeSegmentName(metadataSegmentName);\n+        deleteSegment(storage, metadataSegmentName);\n+        deleteSegment(storage, attributeSegmentName);\n+    }\n+\n+    /**\n+     * Deletes the segment with given name from the given {@link Storage} instance.\n+     * @param storage       A {@link Storage} instance to delete the segments from.\n+     * @param segmentName   Name of the segment to be deleted.\n+     */\n+    private static void deleteSegment(Storage storage, String segmentName) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "883da1068638d9dae5e87aea87c92ade5260e15a"}, "originalPosition": 174}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkwMDUyNA==", "bodyText": "Ok.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470900524", "createdAt": "2020-08-14T23:11:53Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<Integer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        val args = IteratorArgs.builder().fetchTimeout(TIMEOUT).build();\n+        for (val debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            Preconditions.checkNotNull(debugStreamSegmentContainerEntry.getValue());\n+            val tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            val keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), args).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k ->\n+                    metadataSegments.addAll(k.getEntries().stream()\n+                            .map(entry -> entry.getKey().toString())\n+                            .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getKey(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            metadataSegmentsByContainer.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        for (val metadataSegmentsSetEntry : metadataSegmentsByContainer.entrySet()) {\n+            for (String segmentName : metadataSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in the storage.\", segmentName);\n+                debugStreamSegmentContainers.get(metadataSegmentsSetEntry.getKey()).deleteStreamSegment(segmentName, TIMEOUT).join();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * This method takes a {@link DebugStreamSegmentContainer} instance and a {@link SegmentProperties} object as arguments\n+     * and takes one of the following actions:\n+     * 1. If the segment is present in the container metadata and its length or sealed status or both doesn't match with the\n+     * given {@link SegmentProperties}, then it is deleted from there and registered using the properties from the given\n+     * {@link SegmentProperties} instance.\n+     * 2. If the segment is absent in the container metadata, then it is registered using the properties from the given\n+     * {@link SegmentProperties}.\n+     * @param container         A {@link DebugStreamSegmentContainer} instance for registering the given segment and checking\n+     *                          its existence in the container metadata.\n+     * @param storageSegment    A {@link SegmentProperties} instance which has properties of the segment to be registered.\n+     * @return                  CompletableFuture which when completed will have the segment registered on to the container\n+     *                          metadata.\n+     */\n+    private static CompletableFuture<Void> recoverSegment(DebugStreamSegmentContainer container, SegmentProperties storageSegment) {\n+        Preconditions.checkNotNull(container);\n+        Preconditions.checkNotNull(storageSegment);\n+        long segmentLength = storageSegment.getLength();\n+        boolean isSealed = storageSegment.isSealed();\n+        String segmentName = storageSegment.getName();\n+\n+        log.info(\"Registering: {}, {}, {}.\", segmentName, segmentLength, isSealed);\n+        return Futures.exceptionallyComposeExpecting(\n+                container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                                container.metadataStore.deleteSegment(segmentName, TIMEOUT)\n+                                        .thenAccept(x -> container.registerSegment(segmentName, segmentLength, isSealed));\n+                            }\n+                        }), ex -> Exceptions.unwrap(ex) instanceof StreamSegmentNotExistsException,\n+                () -> container.registerSegment(segmentName, segmentLength, isSealed));\n+    }\n+\n+    /**\n+     * Deletes container metadata segment and its Attribute segment from the {@link Storage} for the given container Id.\n+     * @param storage       A {@link Storage} instance to delete the segments from.\n+     * @param containerId   Id of the container for which the segments has to be deleted.\n+     */\n+    public static void deleteContainerMetadataAndAttributeSegments(Storage storage, int containerId) {\n+        Preconditions.checkNotNull(storage);\n+        String metadataSegmentName = NameUtils.getMetadataSegmentName(containerId);\n+        String attributeSegmentName = NameUtils.getAttributeSegmentName(metadataSegmentName);\n+        deleteSegment(storage, metadataSegmentName);\n+        deleteSegment(storage, attributeSegmentName);\n+    }\n+\n+    /**\n+     * Deletes the segment with given name from the given {@link Storage} instance.\n+     * @param storage       A {@link Storage} instance to delete the segments from.\n+     * @param segmentName   Name of the segment to be deleted.\n+     */\n+    private static void deleteSegment(Storage storage, String segmentName) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg1NjUzMw=="}, "originalCommit": {"oid": "883da1068638d9dae5e87aea87c92ade5260e15a"}, "originalPosition": 174}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MjQzNDQwOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/MetadataStore.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMDo0Nzo0MlrOHBC4RQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMDo0Nzo0MlrOHBC4RQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg1Nzc5Nw==", "bodyText": "java doc would be helpful here. Someone not familiar with it may not understand what this is for/what it does.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470857797", "createdAt": "2020-08-14T20:47:42Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/MetadataStore.java", "diffHunk": "@@ -704,6 +704,18 @@ static SegmentInfo newSegment(String name, Collection<AttributeUpdate> attribute\n                     .build();\n         }\n \n+        static ArrayView recoveredSegment(String streamSegmentName, long length, boolean isSealed) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "883da1068638d9dae5e87aea87c92ade5260e15a"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MjQzODk5OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/StreamSegmentContainer.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMDo0OTozNlrOHBC7HA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMzowMTowNVrOHBFWHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg1ODUyNA==", "bodyText": "Seems like nothing really changed in this file.\nPlease revert the irrelevant change (or  just another make change to match exactly how it was before.)", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470858524", "createdAt": "2020-08-14T20:49:36Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/StreamSegmentContainer.java", "diffHunk": "@@ -98,13 +98,13 @@\n     private static final RetryAndThrowConditionally CACHE_ATTRIBUTES_RETRY = Retry.withExpBackoff(50, 2, 10, 1000)\n             .retryWhen(ex -> ex instanceof BadAttributeUpdateException);\n     protected final StreamSegmentContainerMetadata metadata;\n+    protected final MetadataStore metadataStore;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "883da1068638d9dae5e87aea87c92ade5260e15a"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg5ODIwNQ==", "bodyText": "Class member metadataStore is used in ContainerRecoveryUtils and DebugStreamSegmentContainer.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470898205", "createdAt": "2020-08-14T23:01:05Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/StreamSegmentContainer.java", "diffHunk": "@@ -98,13 +98,13 @@\n     private static final RetryAndThrowConditionally CACHE_ATTRIBUTES_RETRY = Retry.withExpBackoff(50, 2, 10, 1000)\n             .retryWhen(ex -> ex instanceof BadAttributeUpdateException);\n     protected final StreamSegmentContainerMetadata metadata;\n+    protected final MetadataStore metadataStore;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg1ODUyNA=="}, "originalCommit": {"oid": "883da1068638d9dae5e87aea87c92ade5260e15a"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MjQ1NDc2OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/SegmentStoreWithSegmentTracker.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMDo1NToyOVrOHBDEPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMzowNjo0M1rOHBFa8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg2MDg2Mg==", "bodyText": "Nit: missing javadoc", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470860862", "createdAt": "2020-08-14T20:55:29Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/SegmentStoreWithSegmentTracker.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.BufferView;\n+import io.pravega.segmentstore.contracts.AttributeUpdate;\n+import io.pravega.segmentstore.contracts.MergeStreamSegmentResult;\n+import io.pravega.segmentstore.contracts.ReadResult;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableEntry;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.contracts.tables.TableStore;\n+import lombok.AccessLevel;\n+import lombok.Getter;\n+\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+/**\n+ * A wrapper class to StreamSegmentStore and TableStore to track the segments being created or deleted. The list of segments\n+ * obtained during this process is used in RestoreBackUpDataRecoveryTest to wait for segments to be flushed to the long term storage.\n+ */\n+public class SegmentStoreWithSegmentTracker implements StreamSegmentStore, TableStore {\n+    private final StreamSegmentStore streamSegmentStore;\n+    private final TableStore tableStore;\n+\n+    @Getter(AccessLevel.PUBLIC)\n+    private final ConcurrentHashMap<String, Boolean> segments;\n+\n+    public SegmentStoreWithSegmentTracker(StreamSegmentStore streamSegmentStore, TableStore tableStore) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "883da1068638d9dae5e87aea87c92ade5260e15a"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg5OTQ0Mw==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470899443", "createdAt": "2020-08-14T23:06:43Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/SegmentStoreWithSegmentTracker.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.BufferView;\n+import io.pravega.segmentstore.contracts.AttributeUpdate;\n+import io.pravega.segmentstore.contracts.MergeStreamSegmentResult;\n+import io.pravega.segmentstore.contracts.ReadResult;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableEntry;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.contracts.tables.TableStore;\n+import lombok.AccessLevel;\n+import lombok.Getter;\n+\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+/**\n+ * A wrapper class to StreamSegmentStore and TableStore to track the segments being created or deleted. The list of segments\n+ * obtained during this process is used in RestoreBackUpDataRecoveryTest to wait for segments to be flushed to the long term storage.\n+ */\n+public class SegmentStoreWithSegmentTracker implements StreamSegmentStore, TableStore {\n+    private final StreamSegmentStore streamSegmentStore;\n+    private final TableStore tableStore;\n+\n+    @Getter(AccessLevel.PUBLIC)\n+    private final ConcurrentHashMap<String, Boolean> segments;\n+\n+    public SegmentStoreWithSegmentTracker(StreamSegmentStore streamSegmentStore, TableStore tableStore) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg2MDg2Mg=="}, "originalCommit": {"oid": "883da1068638d9dae5e87aea87c92ade5260e15a"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MjUxNjczOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/store/ServiceBuilder.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMToyMDowOFrOHBDo8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMzoxMTo0MFrOHBFe7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg3MDI1Ng==", "bodyText": "You don't need this method , why not just call createStorageFactory directly ?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470870256", "createdAt": "2020-08-14T21:20:08Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/store/ServiceBuilder.java", "diffHunk": "@@ -241,6 +241,14 @@ public void initialize() throws DurableDataLogException {\n         getSingleton(this.containerManager, this.segmentContainerManagerCreator).initialize();\n     }\n \n+    /**\n+     * To get the storageFactory after a ServiceBuilder has been initialized.\n+     * @return StorageFactory instance used to initialize ServiceBuilder.\n+     */\n+    public StorageFactory getStorageFactory() {\n+        return createStorageFactory();\n+    }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "883da1068638d9dae5e87aea87c92ade5260e15a"}, "originalPosition": 11}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkwMDQ2Mw==", "bodyText": "Ok. Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470900463", "createdAt": "2020-08-14T23:11:40Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/store/ServiceBuilder.java", "diffHunk": "@@ -241,6 +241,14 @@ public void initialize() throws DurableDataLogException {\n         getSingleton(this.containerManager, this.segmentContainerManagerCreator).initialize();\n     }\n \n+    /**\n+     * To get the storageFactory after a ServiceBuilder has been initialized.\n+     * @return StorageFactory instance used to initialize ServiceBuilder.\n+     */\n+    public StorageFactory getStorageFactory() {\n+        return createStorageFactory();\n+    }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg3MDI1Ng=="}, "originalCommit": {"oid": "883da1068638d9dae5e87aea87c92ade5260e15a"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MjUyMDMzOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMToyMTozOVrOHBDrAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMzowNjo1NFrOHBFbGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg3MDc4NQ==", "bodyText": "This method should return CompletableFuture.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470870785", "createdAt": "2020-08-14T21:21:39Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<Integer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        val args = IteratorArgs.builder().fetchTimeout(TIMEOUT).build();\n+        for (val debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            Preconditions.checkNotNull(debugStreamSegmentContainerEntry.getValue());\n+            val tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            val keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), args).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k ->\n+                    metadataSegments.addAll(k.getEntries().stream()\n+                            .map(entry -> entry.getKey().toString())\n+                            .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getKey(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            metadataSegmentsByContainer.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        for (val metadataSegmentsSetEntry : metadataSegmentsByContainer.entrySet()) {\n+            for (String segmentName : metadataSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in the storage.\", segmentName);\n+                debugStreamSegmentContainers.get(metadataSegmentsSetEntry.getKey()).deleteStreamSegment(segmentName, TIMEOUT).join();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * This method takes a {@link DebugStreamSegmentContainer} instance and a {@link SegmentProperties} object as arguments\n+     * and takes one of the following actions:\n+     * 1. If the segment is present in the container metadata and its length or sealed status or both doesn't match with the\n+     * given {@link SegmentProperties}, then it is deleted from there and registered using the properties from the given\n+     * {@link SegmentProperties} instance.\n+     * 2. If the segment is absent in the container metadata, then it is registered using the properties from the given\n+     * {@link SegmentProperties}.\n+     * @param container         A {@link DebugStreamSegmentContainer} instance for registering the given segment and checking\n+     *                          its existence in the container metadata.\n+     * @param storageSegment    A {@link SegmentProperties} instance which has properties of the segment to be registered.\n+     * @return                  CompletableFuture which when completed will have the segment registered on to the container\n+     *                          metadata.\n+     */\n+    private static CompletableFuture<Void> recoverSegment(DebugStreamSegmentContainer container, SegmentProperties storageSegment) {\n+        Preconditions.checkNotNull(container);\n+        Preconditions.checkNotNull(storageSegment);\n+        long segmentLength = storageSegment.getLength();\n+        boolean isSealed = storageSegment.isSealed();\n+        String segmentName = storageSegment.getName();\n+\n+        log.info(\"Registering: {}, {}, {}.\", segmentName, segmentLength, isSealed);\n+        return Futures.exceptionallyComposeExpecting(\n+                container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                                container.metadataStore.deleteSegment(segmentName, TIMEOUT)\n+                                        .thenAccept(x -> container.registerSegment(segmentName, segmentLength, isSealed));\n+                            }\n+                        }), ex -> Exceptions.unwrap(ex) instanceof StreamSegmentNotExistsException,\n+                () -> container.registerSegment(segmentName, segmentLength, isSealed));\n+    }\n+\n+    /**\n+     * Deletes container metadata segment and its Attribute segment from the {@link Storage} for the given container Id.\n+     * @param storage       A {@link Storage} instance to delete the segments from.\n+     * @param containerId   Id of the container for which the segments has to be deleted.\n+     */\n+    public static void deleteContainerMetadataAndAttributeSegments(Storage storage, int containerId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "883da1068638d9dae5e87aea87c92ade5260e15a"}, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg5OTQ4Mg==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470899482", "createdAt": "2020-08-14T23:06:54Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<Integer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        val args = IteratorArgs.builder().fetchTimeout(TIMEOUT).build();\n+        for (val debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            Preconditions.checkNotNull(debugStreamSegmentContainerEntry.getValue());\n+            val tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            val keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), args).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k ->\n+                    metadataSegments.addAll(k.getEntries().stream()\n+                            .map(entry -> entry.getKey().toString())\n+                            .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getKey(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            metadataSegmentsByContainer.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        for (val metadataSegmentsSetEntry : metadataSegmentsByContainer.entrySet()) {\n+            for (String segmentName : metadataSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in the storage.\", segmentName);\n+                debugStreamSegmentContainers.get(metadataSegmentsSetEntry.getKey()).deleteStreamSegment(segmentName, TIMEOUT).join();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * This method takes a {@link DebugStreamSegmentContainer} instance and a {@link SegmentProperties} object as arguments\n+     * and takes one of the following actions:\n+     * 1. If the segment is present in the container metadata and its length or sealed status or both doesn't match with the\n+     * given {@link SegmentProperties}, then it is deleted from there and registered using the properties from the given\n+     * {@link SegmentProperties} instance.\n+     * 2. If the segment is absent in the container metadata, then it is registered using the properties from the given\n+     * {@link SegmentProperties}.\n+     * @param container         A {@link DebugStreamSegmentContainer} instance for registering the given segment and checking\n+     *                          its existence in the container metadata.\n+     * @param storageSegment    A {@link SegmentProperties} instance which has properties of the segment to be registered.\n+     * @return                  CompletableFuture which when completed will have the segment registered on to the container\n+     *                          metadata.\n+     */\n+    private static CompletableFuture<Void> recoverSegment(DebugStreamSegmentContainer container, SegmentProperties storageSegment) {\n+        Preconditions.checkNotNull(container);\n+        Preconditions.checkNotNull(storageSegment);\n+        long segmentLength = storageSegment.getLength();\n+        boolean isSealed = storageSegment.isSealed();\n+        String segmentName = storageSegment.getName();\n+\n+        log.info(\"Registering: {}, {}, {}.\", segmentName, segmentLength, isSealed);\n+        return Futures.exceptionallyComposeExpecting(\n+                container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                                container.metadataStore.deleteSegment(segmentName, TIMEOUT)\n+                                        .thenAccept(x -> container.registerSegment(segmentName, segmentLength, isSealed));\n+                            }\n+                        }), ex -> Exceptions.unwrap(ex) instanceof StreamSegmentNotExistsException,\n+                () -> container.registerSegment(segmentName, segmentLength, isSealed));\n+    }\n+\n+    /**\n+     * Deletes container metadata segment and its Attribute segment from the {@link Storage} for the given container Id.\n+     * @param storage       A {@link Storage} instance to delete the segments from.\n+     * @param containerId   Id of the container for which the segments has to be deleted.\n+     */\n+    public static void deleteContainerMetadataAndAttributeSegments(Storage storage, int containerId) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg3MDc4NQ=="}, "originalCommit": {"oid": "883da1068638d9dae5e87aea87c92ade5260e15a"}, "originalPosition": 161}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MjUyMDgyOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMToyMTo1MFrOHBDrSQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMzowNzowNFrOHBFbOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg3MDg1Nw==", "bodyText": "This method should return CompletableFuture.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470870857", "createdAt": "2020-08-14T21:21:50Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<Integer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        val args = IteratorArgs.builder().fetchTimeout(TIMEOUT).build();\n+        for (val debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            Preconditions.checkNotNull(debugStreamSegmentContainerEntry.getValue());\n+            val tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            val keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), args).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k ->\n+                    metadataSegments.addAll(k.getEntries().stream()\n+                            .map(entry -> entry.getKey().toString())\n+                            .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getKey(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            metadataSegmentsByContainer.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        for (val metadataSegmentsSetEntry : metadataSegmentsByContainer.entrySet()) {\n+            for (String segmentName : metadataSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in the storage.\", segmentName);\n+                debugStreamSegmentContainers.get(metadataSegmentsSetEntry.getKey()).deleteStreamSegment(segmentName, TIMEOUT).join();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * This method takes a {@link DebugStreamSegmentContainer} instance and a {@link SegmentProperties} object as arguments\n+     * and takes one of the following actions:\n+     * 1. If the segment is present in the container metadata and its length or sealed status or both doesn't match with the\n+     * given {@link SegmentProperties}, then it is deleted from there and registered using the properties from the given\n+     * {@link SegmentProperties} instance.\n+     * 2. If the segment is absent in the container metadata, then it is registered using the properties from the given\n+     * {@link SegmentProperties}.\n+     * @param container         A {@link DebugStreamSegmentContainer} instance for registering the given segment and checking\n+     *                          its existence in the container metadata.\n+     * @param storageSegment    A {@link SegmentProperties} instance which has properties of the segment to be registered.\n+     * @return                  CompletableFuture which when completed will have the segment registered on to the container\n+     *                          metadata.\n+     */\n+    private static CompletableFuture<Void> recoverSegment(DebugStreamSegmentContainer container, SegmentProperties storageSegment) {\n+        Preconditions.checkNotNull(container);\n+        Preconditions.checkNotNull(storageSegment);\n+        long segmentLength = storageSegment.getLength();\n+        boolean isSealed = storageSegment.isSealed();\n+        String segmentName = storageSegment.getName();\n+\n+        log.info(\"Registering: {}, {}, {}.\", segmentName, segmentLength, isSealed);\n+        return Futures.exceptionallyComposeExpecting(\n+                container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                                container.metadataStore.deleteSegment(segmentName, TIMEOUT)\n+                                        .thenAccept(x -> container.registerSegment(segmentName, segmentLength, isSealed));\n+                            }\n+                        }), ex -> Exceptions.unwrap(ex) instanceof StreamSegmentNotExistsException,\n+                () -> container.registerSegment(segmentName, segmentLength, isSealed));\n+    }\n+\n+    /**\n+     * Deletes container metadata segment and its Attribute segment from the {@link Storage} for the given container Id.\n+     * @param storage       A {@link Storage} instance to delete the segments from.\n+     * @param containerId   Id of the container for which the segments has to be deleted.\n+     */\n+    public static void deleteContainerMetadataAndAttributeSegments(Storage storage, int containerId) {\n+        Preconditions.checkNotNull(storage);\n+        String metadataSegmentName = NameUtils.getMetadataSegmentName(containerId);\n+        String attributeSegmentName = NameUtils.getAttributeSegmentName(metadataSegmentName);\n+        deleteSegment(storage, metadataSegmentName);\n+        deleteSegment(storage, attributeSegmentName);\n+    }\n+\n+    /**\n+     * Deletes the segment with given name from the given {@link Storage} instance.\n+     * @param storage       A {@link Storage} instance to delete the segments from.\n+     * @param segmentName   Name of the segment to be deleted.\n+     */\n+    private static void deleteSegment(Storage storage, String segmentName) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "883da1068638d9dae5e87aea87c92ade5260e15a"}, "originalPosition": 174}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg5OTUxNQ==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470899515", "createdAt": "2020-08-14T23:07:04Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,185 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<Integer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        val args = IteratorArgs.builder().fetchTimeout(TIMEOUT).build();\n+        for (val debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            Preconditions.checkNotNull(debugStreamSegmentContainerEntry.getValue());\n+            val tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            val keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), args).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k ->\n+                    metadataSegments.addAll(k.getEntries().stream()\n+                            .map(entry -> entry.getKey().toString())\n+                            .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getKey(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            metadataSegmentsByContainer.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        for (val metadataSegmentsSetEntry : metadataSegmentsByContainer.entrySet()) {\n+            for (String segmentName : metadataSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in the storage.\", segmentName);\n+                debugStreamSegmentContainers.get(metadataSegmentsSetEntry.getKey()).deleteStreamSegment(segmentName, TIMEOUT).join();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * This method takes a {@link DebugStreamSegmentContainer} instance and a {@link SegmentProperties} object as arguments\n+     * and takes one of the following actions:\n+     * 1. If the segment is present in the container metadata and its length or sealed status or both doesn't match with the\n+     * given {@link SegmentProperties}, then it is deleted from there and registered using the properties from the given\n+     * {@link SegmentProperties} instance.\n+     * 2. If the segment is absent in the container metadata, then it is registered using the properties from the given\n+     * {@link SegmentProperties}.\n+     * @param container         A {@link DebugStreamSegmentContainer} instance for registering the given segment and checking\n+     *                          its existence in the container metadata.\n+     * @param storageSegment    A {@link SegmentProperties} instance which has properties of the segment to be registered.\n+     * @return                  CompletableFuture which when completed will have the segment registered on to the container\n+     *                          metadata.\n+     */\n+    private static CompletableFuture<Void> recoverSegment(DebugStreamSegmentContainer container, SegmentProperties storageSegment) {\n+        Preconditions.checkNotNull(container);\n+        Preconditions.checkNotNull(storageSegment);\n+        long segmentLength = storageSegment.getLength();\n+        boolean isSealed = storageSegment.isSealed();\n+        String segmentName = storageSegment.getName();\n+\n+        log.info(\"Registering: {}, {}, {}.\", segmentName, segmentLength, isSealed);\n+        return Futures.exceptionallyComposeExpecting(\n+                container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                                container.metadataStore.deleteSegment(segmentName, TIMEOUT)\n+                                        .thenAccept(x -> container.registerSegment(segmentName, segmentLength, isSealed));\n+                            }\n+                        }), ex -> Exceptions.unwrap(ex) instanceof StreamSegmentNotExistsException,\n+                () -> container.registerSegment(segmentName, segmentLength, isSealed));\n+    }\n+\n+    /**\n+     * Deletes container metadata segment and its Attribute segment from the {@link Storage} for the given container Id.\n+     * @param storage       A {@link Storage} instance to delete the segments from.\n+     * @param containerId   Id of the container for which the segments has to be deleted.\n+     */\n+    public static void deleteContainerMetadataAndAttributeSegments(Storage storage, int containerId) {\n+        Preconditions.checkNotNull(storage);\n+        String metadataSegmentName = NameUtils.getMetadataSegmentName(containerId);\n+        String attributeSegmentName = NameUtils.getAttributeSegmentName(metadataSegmentName);\n+        deleteSegment(storage, metadataSegmentName);\n+        deleteSegment(storage, attributeSegmentName);\n+    }\n+\n+    /**\n+     * Deletes the segment with given name from the given {@link Storage} instance.\n+     * @param storage       A {@link Storage} instance to delete the segments from.\n+     * @param segmentName   Name of the segment to be deleted.\n+     */\n+    private static void deleteSegment(Storage storage, String segmentName) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg3MDg1Nw=="}, "originalCommit": {"oid": "883da1068638d9dae5e87aea87c92ade5260e15a"}, "originalPosition": 174}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0MjUzMjQ3OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMToyNjo1MFrOHBDx8w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMzoxMToyOFrOHBFe1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg3MjU2Mw==", "bodyText": "Is this needed?\nDelete constants not used.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470872563", "createdAt": "2020-08-14T21:26:50Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "diffHunk": "@@ -0,0 +1,304 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.mocks.InMemoryDurableDataLogFactory;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorage;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorageFactory;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.ByteArrayInputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+import static io.pravega.segmentstore.server.containers.ContainerRecoveryUtils.recoverAllSegments;\n+\n+/**\n+ * Tests for DebugStreamSegmentContainer class.\n+ */\n+@Slf4j\n+public class DebugStreamSegmentContainerTests extends ThreadPooledTestSuite {\n+    private static final int MIN_SEGMENT_LENGTH = 0; // Used in randomly generating the length for a segment\n+    private static final int MAX_SEGMENT_LENGTH = 10100; // Used in randomly generating the length for a segment\n+    private static final int CONTAINER_ID = 1234567;\n+    private static final int EXPECTED_PINNED_SEGMENT_COUNT = 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "883da1068638d9dae5e87aea87c92ade5260e15a"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkwMDQzNw==", "bodyText": "Deleted.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470900437", "createdAt": "2020-08-14T23:11:28Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "diffHunk": "@@ -0,0 +1,304 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.mocks.InMemoryDurableDataLogFactory;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorage;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorageFactory;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.ByteArrayInputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+import static io.pravega.segmentstore.server.containers.ContainerRecoveryUtils.recoverAllSegments;\n+\n+/**\n+ * Tests for DebugStreamSegmentContainer class.\n+ */\n+@Slf4j\n+public class DebugStreamSegmentContainerTests extends ThreadPooledTestSuite {\n+    private static final int MIN_SEGMENT_LENGTH = 0; // Used in randomly generating the length for a segment\n+    private static final int MAX_SEGMENT_LENGTH = 10100; // Used in randomly generating the length for a segment\n+    private static final int CONTAINER_ID = 1234567;\n+    private static final int EXPECTED_PINNED_SEGMENT_COUNT = 1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg3MjU2Mw=="}, "originalCommit": {"oid": "883da1068638d9dae5e87aea87c92ade5260e15a"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0OTMxMTIxOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMDozNjo0M1rOHB_UAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxOToyMTowOFrOHDVy3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg0NzkzOQ==", "bodyText": "Replace this with:\nRecovers Segments from the given Storage instance. This is done by:\n1. Listing all Segments from the given Storage instance and partitioning them by their assigned Container Id using the standard {@link StreamSegmentMapper}.\n2. Filtering out all shadow Segments (such as Attribute Segments).\n3. Registering all remaining (external) Segments to the owning Container's {@link MetadataStore}.\n\nThe {@link DebugStreamSegmentContainer} instance(s) that are provided to this method may have some segments already present in their respective {@link MetadataStore}. \n\nAfter the method successfully completes, the following are true:\n- Only the segments which exist in the {@link Storage} will remain in the Container's {@link MetadataStore}.\n- If a Segment exists both in the Container's {@link MetadataStore} and in {@link Storage}, then the information that exists in {@link Storage} (length, sealed) will prevail.\n      \nIf the method fails during execution, the appropriate exception is thrown and the Containers' {@link MetadataStores} may be left in an inconsistent state", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r471847939", "createdAt": "2020-08-18T00:36:43Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzI2NDg2MA==", "bodyText": "Replaced.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r473264860", "createdAt": "2020-08-19T19:21:08Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg0NzkzOQ=="}, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0OTMxOTkzOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMDo0MToxNVrOHB_Y5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxOToyMToyMFrOHDVzWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg0OTE4OA==", "bodyText": "For all these methods, do this for the exceptions:\n\nHave them declare that they throw Exception.\nDo not document InterruptedException or ExecutionException.\nFor TimeoutException and IOException please have appropriate descriptions. Required for Futures.get is not of any use for the user.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r471849188", "createdAt": "2020-08-18T00:41:15Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzI2NDk4NQ==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r473264985", "createdAt": "2020-08-19T19:21:20Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg0OTE4OA=="}, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0OTMyNjU1OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMDo0NDo1NFrOHB_crA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxOToyMTozN1rOHDVz9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg1MDE1Ng==", "bodyText": "Here you use join but in other methods you use get(Timeout). Please be consistent. If you already use get(Timeout) in one place, you might as well use it everywhere.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r471850156", "createdAt": "2020-08-18T00:44:54Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Get all segments in the container metadata for each debug segment container instance.\n+        Map<Integer, Set<String>> existingSegmentsMap = getExistingSegments(debugStreamSegmentContainers, executorService);\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            val currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            existingSegmentsMap.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzI2NTE0MA==", "bodyText": "Using get now.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r473265140", "createdAt": "2020-08-19T19:21:37Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Get all segments in the container metadata for each debug segment container instance.\n+        Map<Integer, Set<String>> existingSegmentsMap = getExistingSegments(debugStreamSegmentContainers, executorService);\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            val currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            existingSegmentsMap.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg1MDE1Ng=="}, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 100}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0OTMyNzk2OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMDo0NTozNFrOHB_ddA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxOToyMjowNVrOHDV02w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg1MDM1Ng==", "bodyText": "Loop through all the given containers and verify that they are all there (i.e., no duplicate IDs and that all of the expected Ids are provided). If you get 4 containers, you should expect ids 0 through 3.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r471850356", "createdAt": "2020-08-18T00:45:34Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg1MDQ0Mg==", "bodyText": "You can do this as a helper method and use a HashSet", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r471850442", "createdAt": "2020-08-18T00:45:52Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg1MDM1Ng=="}, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzI2NTM3MQ==", "bodyText": "Done. Implemented another method.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r473265371", "createdAt": "2020-08-19T19:22:05Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg1MDM1Ng=="}, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 74}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0OTMyOTEyOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMDo0NjoxMFrOHB_eEQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxOToyMjoxNFrOHDV1KA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg1MDUxMw==", "bodyText": "get", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r471850513", "createdAt": "2020-08-18T00:46:10Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Get all segments in the container metadata for each debug segment container instance.\n+        Map<Integer, Set<String>> existingSegmentsMap = getExistingSegments(debugStreamSegmentContainers, executorService);\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            val currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            existingSegmentsMap.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        futures.clear();\n+        // Delete segments which only exist in the container metadata, not in storage.\n+        for (val existingSegmentsSetEntry : existingSegmentsMap.entrySet()) {\n+            for (String segmentName : existingSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in the storage.\", segmentName);\n+                futures.add(debugStreamSegmentContainers.get(existingSegmentsSetEntry.getKey()).deleteStreamSegment(segmentName, TIMEOUT));\n+            }\n+        }\n+        Futures.allOf(futures).join();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzI2NTQ0OA==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r473265448", "createdAt": "2020-08-19T19:22:14Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Get all segments in the container metadata for each debug segment container instance.\n+        Map<Integer, Set<String>> existingSegmentsMap = getExistingSegments(debugStreamSegmentContainers, executorService);\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            val currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            existingSegmentsMap.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        futures.clear();\n+        // Delete segments which only exist in the container metadata, not in storage.\n+        for (val existingSegmentsSetEntry : existingSegmentsMap.entrySet()) {\n+            for (String segmentName : existingSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in the storage.\", segmentName);\n+                futures.add(debugStreamSegmentContainers.get(existingSegmentsSetEntry.getKey()).deleteStreamSegment(segmentName, TIMEOUT));\n+            }\n+        }\n+        Futures.allOf(futures).join();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg1MDUxMw=="}, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0OTMyOTU1OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMDo0NjoyOFrOHB_eWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxOToyMjoyM1rOHDV1hA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg1MDU4Nw==", "bodyText": "Same comment about exceptions as above.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r471850587", "createdAt": "2020-08-18T00:46:28Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Get all segments in the container metadata for each debug segment container instance.\n+        Map<Integer, Set<String>> existingSegmentsMap = getExistingSegments(debugStreamSegmentContainers, executorService);\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            val currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            existingSegmentsMap.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        futures.clear();\n+        // Delete segments which only exist in the container metadata, not in storage.\n+        for (val existingSegmentsSetEntry : existingSegmentsMap.entrySet()) {\n+            for (String segmentName : existingSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in the storage.\", segmentName);\n+                futures.add(debugStreamSegmentContainers.get(existingSegmentsSetEntry.getKey()).deleteStreamSegment(segmentName, TIMEOUT));\n+            }\n+        }\n+        Futures.allOf(futures).join();\n+    }\n+\n+    /**\n+     * The method lists all segments present in the container metadata segments of the given {@link DebugStreamSegmentContainer}\n+     * instances, stores their names by container Id in a map and returns it.\n+     * @param containerMap              A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                  representing the containers to list the segments from.\n+     * @param executorService           A thread pool for execution.\n+     * @return                          A Map of Container Ids to segment names representing all segments present in the\n+     *                                  container metadata segment of a Container.\n+     * @throws InterruptedException     Required for Futures.get()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzI2NTU0MA==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r473265540", "createdAt": "2020-08-19T19:22:23Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Get all segments in the container metadata for each debug segment container instance.\n+        Map<Integer, Set<String>> existingSegmentsMap = getExistingSegments(debugStreamSegmentContainers, executorService);\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            val currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            existingSegmentsMap.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        futures.clear();\n+        // Delete segments which only exist in the container metadata, not in storage.\n+        for (val existingSegmentsSetEntry : existingSegmentsMap.entrySet()) {\n+            for (String segmentName : existingSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in the storage.\", segmentName);\n+                futures.add(debugStreamSegmentContainers.get(existingSegmentsSetEntry.getKey()).deleteStreamSegment(segmentName, TIMEOUT));\n+            }\n+        }\n+        Futures.allOf(futures).join();\n+    }\n+\n+    /**\n+     * The method lists all segments present in the container metadata segments of the given {@link DebugStreamSegmentContainer}\n+     * instances, stores their names by container Id in a map and returns it.\n+     * @param containerMap              A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                  representing the containers to list the segments from.\n+     * @param executorService           A thread pool for execution.\n+     * @return                          A Map of Container Ids to segment names representing all segments present in the\n+     *                                  container metadata segment of a Container.\n+     * @throws InterruptedException     Required for Futures.get()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg1MDU4Nw=="}, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 121}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0OTMzMTY3OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMDo0Nzo0NFrOHB_fjQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxOToyMjozN1rOHDV2Ew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg1MDg5Mw==", "bodyText": "I am pretty sure that Futures.exceptionallyComposeExpecting unwraps it for you. Please check, and if so, simplify this line.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r471850893", "createdAt": "2020-08-18T00:47:44Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Get all segments in the container metadata for each debug segment container instance.\n+        Map<Integer, Set<String>> existingSegmentsMap = getExistingSegments(debugStreamSegmentContainers, executorService);\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            val currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            existingSegmentsMap.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        futures.clear();\n+        // Delete segments which only exist in the container metadata, not in storage.\n+        for (val existingSegmentsSetEntry : existingSegmentsMap.entrySet()) {\n+            for (String segmentName : existingSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in the storage.\", segmentName);\n+                futures.add(debugStreamSegmentContainers.get(existingSegmentsSetEntry.getKey()).deleteStreamSegment(segmentName, TIMEOUT));\n+            }\n+        }\n+        Futures.allOf(futures).join();\n+    }\n+\n+    /**\n+     * The method lists all segments present in the container metadata segments of the given {@link DebugStreamSegmentContainer}\n+     * instances, stores their names by container Id in a map and returns it.\n+     * @param containerMap              A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                  representing the containers to list the segments from.\n+     * @param executorService           A thread pool for execution.\n+     * @return                          A Map of Container Ids to segment names representing all segments present in the\n+     *                                  container metadata segment of a Container.\n+     * @throws InterruptedException     Required for Futures.get()\n+     * @throws ExecutionException       Required for Futures.get()\n+     * @throws TimeoutException         Required for Futures.get()\n+     */\n+    private static Map<Integer, Set<String>> getExistingSegments(Map<Integer, DebugStreamSegmentContainer> containerMap,\n+                                                                 ExecutorService executorService)\n+            throws InterruptedException, ExecutionException, TimeoutException {\n+        Map<Integer, Set<String>> metadataSegmentsMap = new HashMap<>();\n+        val args = IteratorArgs.builder().fetchTimeout(TIMEOUT).build();\n+\n+        // Get all segments for each container entry\n+        for (val containerEntry : containerMap.entrySet()) {\n+            Preconditions.checkNotNull(containerEntry.getValue());\n+            val tableExtension = containerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            val keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    containerEntry.getKey()), args).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+            // Store the segments in a set\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k ->\n+                    metadataSegments.addAll(k.getEntries().stream()\n+                            .map(entry -> entry.getKey().toString())\n+                            .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsMap.put(containerEntry.getKey(), metadataSegments);\n+        }\n+        return metadataSegmentsMap;\n+    }\n+\n+    /**\n+     * This method takes a {@link DebugStreamSegmentContainer} instance and a {@link SegmentProperties} object as arguments\n+     * and takes one of the following actions:\n+     * 1. If the segment is present in the container metadata and its length or sealed status or both doesn't match with the\n+     * given {@link SegmentProperties}, then it is deleted from there and registered using the properties from the given\n+     * {@link SegmentProperties} instance.\n+     * 2. If the segment is absent in the container metadata, then it is registered using the properties from the given\n+     * {@link SegmentProperties}.\n+     * @param container         A {@link DebugStreamSegmentContainer} instance for registering the given segment and checking\n+     *                          its existence in the container metadata.\n+     * @param storageSegment    A {@link SegmentProperties} instance which has properties of the segment to be registered.\n+     * @return                  CompletableFuture which when completed will have the segment registered on to the container\n+     *                          metadata.\n+     */\n+    private static CompletableFuture<Void> recoverSegment(DebugStreamSegmentContainer container, SegmentProperties storageSegment) {\n+        Preconditions.checkNotNull(container);\n+        Preconditions.checkNotNull(storageSegment);\n+        long segmentLength = storageSegment.getLength();\n+        boolean isSealed = storageSegment.isSealed();\n+        String segmentName = storageSegment.getName();\n+\n+        log.info(\"Registering: {}, {}, {}.\", segmentName, segmentLength, isSealed);\n+        return Futures.exceptionallyComposeExpecting(\n+                container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                                container.metadataStore.deleteSegment(segmentName, TIMEOUT)\n+                                        .thenAccept(x -> container.registerSegment(segmentName, segmentLength, isSealed));\n+                            }\n+                        }), ex -> Exceptions.unwrap(ex) instanceof StreamSegmentNotExistsException,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 178}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzI2NTY4Mw==", "bodyText": "Yeah. It does. Thanks.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r473265683", "createdAt": "2020-08-19T19:22:37Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Get all segments in the container metadata for each debug segment container instance.\n+        Map<Integer, Set<String>> existingSegmentsMap = getExistingSegments(debugStreamSegmentContainers, executorService);\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            val currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            existingSegmentsMap.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        futures.clear();\n+        // Delete segments which only exist in the container metadata, not in storage.\n+        for (val existingSegmentsSetEntry : existingSegmentsMap.entrySet()) {\n+            for (String segmentName : existingSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in the storage.\", segmentName);\n+                futures.add(debugStreamSegmentContainers.get(existingSegmentsSetEntry.getKey()).deleteStreamSegment(segmentName, TIMEOUT));\n+            }\n+        }\n+        Futures.allOf(futures).join();\n+    }\n+\n+    /**\n+     * The method lists all segments present in the container metadata segments of the given {@link DebugStreamSegmentContainer}\n+     * instances, stores their names by container Id in a map and returns it.\n+     * @param containerMap              A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                  representing the containers to list the segments from.\n+     * @param executorService           A thread pool for execution.\n+     * @return                          A Map of Container Ids to segment names representing all segments present in the\n+     *                                  container metadata segment of a Container.\n+     * @throws InterruptedException     Required for Futures.get()\n+     * @throws ExecutionException       Required for Futures.get()\n+     * @throws TimeoutException         Required for Futures.get()\n+     */\n+    private static Map<Integer, Set<String>> getExistingSegments(Map<Integer, DebugStreamSegmentContainer> containerMap,\n+                                                                 ExecutorService executorService)\n+            throws InterruptedException, ExecutionException, TimeoutException {\n+        Map<Integer, Set<String>> metadataSegmentsMap = new HashMap<>();\n+        val args = IteratorArgs.builder().fetchTimeout(TIMEOUT).build();\n+\n+        // Get all segments for each container entry\n+        for (val containerEntry : containerMap.entrySet()) {\n+            Preconditions.checkNotNull(containerEntry.getValue());\n+            val tableExtension = containerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            val keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    containerEntry.getKey()), args).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+            // Store the segments in a set\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k ->\n+                    metadataSegments.addAll(k.getEntries().stream()\n+                            .map(entry -> entry.getKey().toString())\n+                            .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsMap.put(containerEntry.getKey(), metadataSegments);\n+        }\n+        return metadataSegmentsMap;\n+    }\n+\n+    /**\n+     * This method takes a {@link DebugStreamSegmentContainer} instance and a {@link SegmentProperties} object as arguments\n+     * and takes one of the following actions:\n+     * 1. If the segment is present in the container metadata and its length or sealed status or both doesn't match with the\n+     * given {@link SegmentProperties}, then it is deleted from there and registered using the properties from the given\n+     * {@link SegmentProperties} instance.\n+     * 2. If the segment is absent in the container metadata, then it is registered using the properties from the given\n+     * {@link SegmentProperties}.\n+     * @param container         A {@link DebugStreamSegmentContainer} instance for registering the given segment and checking\n+     *                          its existence in the container metadata.\n+     * @param storageSegment    A {@link SegmentProperties} instance which has properties of the segment to be registered.\n+     * @return                  CompletableFuture which when completed will have the segment registered on to the container\n+     *                          metadata.\n+     */\n+    private static CompletableFuture<Void> recoverSegment(DebugStreamSegmentContainer container, SegmentProperties storageSegment) {\n+        Preconditions.checkNotNull(container);\n+        Preconditions.checkNotNull(storageSegment);\n+        long segmentLength = storageSegment.getLength();\n+        boolean isSealed = storageSegment.isSealed();\n+        String segmentName = storageSegment.getName();\n+\n+        log.info(\"Registering: {}, {}, {}.\", segmentName, segmentLength, isSealed);\n+        return Futures.exceptionallyComposeExpecting(\n+                container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                                container.metadataStore.deleteSegment(segmentName, TIMEOUT)\n+                                        .thenAccept(x -> container.registerSegment(segmentName, segmentLength, isSealed));\n+                            }\n+                        }), ex -> Exceptions.unwrap(ex) instanceof StreamSegmentNotExistsException,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg1MDg5Mw=="}, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 178}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0OTMzMzA5OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMDo0ODoyOVrOHB_gUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxOToyMzo1M1rOHDV44A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg1MTA5MA==", "bodyText": "This is an async operation. Where are you waiting for it?\nChange the enclosing thenAccept into a thenCompose and return it.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r471851090", "createdAt": "2020-08-18T00:48:29Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Get all segments in the container metadata for each debug segment container instance.\n+        Map<Integer, Set<String>> existingSegmentsMap = getExistingSegments(debugStreamSegmentContainers, executorService);\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            val currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            existingSegmentsMap.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        futures.clear();\n+        // Delete segments which only exist in the container metadata, not in storage.\n+        for (val existingSegmentsSetEntry : existingSegmentsMap.entrySet()) {\n+            for (String segmentName : existingSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in the storage.\", segmentName);\n+                futures.add(debugStreamSegmentContainers.get(existingSegmentsSetEntry.getKey()).deleteStreamSegment(segmentName, TIMEOUT));\n+            }\n+        }\n+        Futures.allOf(futures).join();\n+    }\n+\n+    /**\n+     * The method lists all segments present in the container metadata segments of the given {@link DebugStreamSegmentContainer}\n+     * instances, stores their names by container Id in a map and returns it.\n+     * @param containerMap              A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                  representing the containers to list the segments from.\n+     * @param executorService           A thread pool for execution.\n+     * @return                          A Map of Container Ids to segment names representing all segments present in the\n+     *                                  container metadata segment of a Container.\n+     * @throws InterruptedException     Required for Futures.get()\n+     * @throws ExecutionException       Required for Futures.get()\n+     * @throws TimeoutException         Required for Futures.get()\n+     */\n+    private static Map<Integer, Set<String>> getExistingSegments(Map<Integer, DebugStreamSegmentContainer> containerMap,\n+                                                                 ExecutorService executorService)\n+            throws InterruptedException, ExecutionException, TimeoutException {\n+        Map<Integer, Set<String>> metadataSegmentsMap = new HashMap<>();\n+        val args = IteratorArgs.builder().fetchTimeout(TIMEOUT).build();\n+\n+        // Get all segments for each container entry\n+        for (val containerEntry : containerMap.entrySet()) {\n+            Preconditions.checkNotNull(containerEntry.getValue());\n+            val tableExtension = containerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            val keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    containerEntry.getKey()), args).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+            // Store the segments in a set\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k ->\n+                    metadataSegments.addAll(k.getEntries().stream()\n+                            .map(entry -> entry.getKey().toString())\n+                            .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsMap.put(containerEntry.getKey(), metadataSegments);\n+        }\n+        return metadataSegmentsMap;\n+    }\n+\n+    /**\n+     * This method takes a {@link DebugStreamSegmentContainer} instance and a {@link SegmentProperties} object as arguments\n+     * and takes one of the following actions:\n+     * 1. If the segment is present in the container metadata and its length or sealed status or both doesn't match with the\n+     * given {@link SegmentProperties}, then it is deleted from there and registered using the properties from the given\n+     * {@link SegmentProperties} instance.\n+     * 2. If the segment is absent in the container metadata, then it is registered using the properties from the given\n+     * {@link SegmentProperties}.\n+     * @param container         A {@link DebugStreamSegmentContainer} instance for registering the given segment and checking\n+     *                          its existence in the container metadata.\n+     * @param storageSegment    A {@link SegmentProperties} instance which has properties of the segment to be registered.\n+     * @return                  CompletableFuture which when completed will have the segment registered on to the container\n+     *                          metadata.\n+     */\n+    private static CompletableFuture<Void> recoverSegment(DebugStreamSegmentContainer container, SegmentProperties storageSegment) {\n+        Preconditions.checkNotNull(container);\n+        Preconditions.checkNotNull(storageSegment);\n+        long segmentLength = storageSegment.getLength();\n+        boolean isSealed = storageSegment.isSealed();\n+        String segmentName = storageSegment.getName();\n+\n+        log.info(\"Registering: {}, {}, {}.\", segmentName, segmentLength, isSealed);\n+        return Futures.exceptionallyComposeExpecting(\n+                container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                                container.metadataStore.deleteSegment(segmentName, TIMEOUT)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 175}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzI2NjQwMA==", "bodyText": "Changed. Now returning it.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r473266400", "createdAt": "2020-08-19T19:23:53Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Get all segments in the container metadata for each debug segment container instance.\n+        Map<Integer, Set<String>> existingSegmentsMap = getExistingSegments(debugStreamSegmentContainers, executorService);\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            val currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            existingSegmentsMap.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        futures.clear();\n+        // Delete segments which only exist in the container metadata, not in storage.\n+        for (val existingSegmentsSetEntry : existingSegmentsMap.entrySet()) {\n+            for (String segmentName : existingSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in the storage.\", segmentName);\n+                futures.add(debugStreamSegmentContainers.get(existingSegmentsSetEntry.getKey()).deleteStreamSegment(segmentName, TIMEOUT));\n+            }\n+        }\n+        Futures.allOf(futures).join();\n+    }\n+\n+    /**\n+     * The method lists all segments present in the container metadata segments of the given {@link DebugStreamSegmentContainer}\n+     * instances, stores their names by container Id in a map and returns it.\n+     * @param containerMap              A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                  representing the containers to list the segments from.\n+     * @param executorService           A thread pool for execution.\n+     * @return                          A Map of Container Ids to segment names representing all segments present in the\n+     *                                  container metadata segment of a Container.\n+     * @throws InterruptedException     Required for Futures.get()\n+     * @throws ExecutionException       Required for Futures.get()\n+     * @throws TimeoutException         Required for Futures.get()\n+     */\n+    private static Map<Integer, Set<String>> getExistingSegments(Map<Integer, DebugStreamSegmentContainer> containerMap,\n+                                                                 ExecutorService executorService)\n+            throws InterruptedException, ExecutionException, TimeoutException {\n+        Map<Integer, Set<String>> metadataSegmentsMap = new HashMap<>();\n+        val args = IteratorArgs.builder().fetchTimeout(TIMEOUT).build();\n+\n+        // Get all segments for each container entry\n+        for (val containerEntry : containerMap.entrySet()) {\n+            Preconditions.checkNotNull(containerEntry.getValue());\n+            val tableExtension = containerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            val keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    containerEntry.getKey()), args).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+            // Store the segments in a set\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k ->\n+                    metadataSegments.addAll(k.getEntries().stream()\n+                            .map(entry -> entry.getKey().toString())\n+                            .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsMap.put(containerEntry.getKey(), metadataSegments);\n+        }\n+        return metadataSegmentsMap;\n+    }\n+\n+    /**\n+     * This method takes a {@link DebugStreamSegmentContainer} instance and a {@link SegmentProperties} object as arguments\n+     * and takes one of the following actions:\n+     * 1. If the segment is present in the container metadata and its length or sealed status or both doesn't match with the\n+     * given {@link SegmentProperties}, then it is deleted from there and registered using the properties from the given\n+     * {@link SegmentProperties} instance.\n+     * 2. If the segment is absent in the container metadata, then it is registered using the properties from the given\n+     * {@link SegmentProperties}.\n+     * @param container         A {@link DebugStreamSegmentContainer} instance for registering the given segment and checking\n+     *                          its existence in the container metadata.\n+     * @param storageSegment    A {@link SegmentProperties} instance which has properties of the segment to be registered.\n+     * @return                  CompletableFuture which when completed will have the segment registered on to the container\n+     *                          metadata.\n+     */\n+    private static CompletableFuture<Void> recoverSegment(DebugStreamSegmentContainer container, SegmentProperties storageSegment) {\n+        Preconditions.checkNotNull(container);\n+        Preconditions.checkNotNull(storageSegment);\n+        long segmentLength = storageSegment.getLength();\n+        boolean isSealed = storageSegment.isSealed();\n+        String segmentName = storageSegment.getName();\n+\n+        log.info(\"Registering: {}, {}, {}.\", segmentName, segmentLength, isSealed);\n+        return Futures.exceptionallyComposeExpecting(\n+                container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                                container.metadataStore.deleteSegment(segmentName, TIMEOUT)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg1MTA5MA=="}, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 175}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0OTMzMzQzOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMDo0ODo0NVrOHB_giA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQxOToyNToxMlrOHE4-wA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg1MTE0NA==", "bodyText": "This is also an async operation.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r471851144", "createdAt": "2020-08-18T00:48:45Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Get all segments in the container metadata for each debug segment container instance.\n+        Map<Integer, Set<String>> existingSegmentsMap = getExistingSegments(debugStreamSegmentContainers, executorService);\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            val currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            existingSegmentsMap.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        futures.clear();\n+        // Delete segments which only exist in the container metadata, not in storage.\n+        for (val existingSegmentsSetEntry : existingSegmentsMap.entrySet()) {\n+            for (String segmentName : existingSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in the storage.\", segmentName);\n+                futures.add(debugStreamSegmentContainers.get(existingSegmentsSetEntry.getKey()).deleteStreamSegment(segmentName, TIMEOUT));\n+            }\n+        }\n+        Futures.allOf(futures).join();\n+    }\n+\n+    /**\n+     * The method lists all segments present in the container metadata segments of the given {@link DebugStreamSegmentContainer}\n+     * instances, stores their names by container Id in a map and returns it.\n+     * @param containerMap              A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                  representing the containers to list the segments from.\n+     * @param executorService           A thread pool for execution.\n+     * @return                          A Map of Container Ids to segment names representing all segments present in the\n+     *                                  container metadata segment of a Container.\n+     * @throws InterruptedException     Required for Futures.get()\n+     * @throws ExecutionException       Required for Futures.get()\n+     * @throws TimeoutException         Required for Futures.get()\n+     */\n+    private static Map<Integer, Set<String>> getExistingSegments(Map<Integer, DebugStreamSegmentContainer> containerMap,\n+                                                                 ExecutorService executorService)\n+            throws InterruptedException, ExecutionException, TimeoutException {\n+        Map<Integer, Set<String>> metadataSegmentsMap = new HashMap<>();\n+        val args = IteratorArgs.builder().fetchTimeout(TIMEOUT).build();\n+\n+        // Get all segments for each container entry\n+        for (val containerEntry : containerMap.entrySet()) {\n+            Preconditions.checkNotNull(containerEntry.getValue());\n+            val tableExtension = containerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            val keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    containerEntry.getKey()), args).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+            // Store the segments in a set\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k ->\n+                    metadataSegments.addAll(k.getEntries().stream()\n+                            .map(entry -> entry.getKey().toString())\n+                            .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsMap.put(containerEntry.getKey(), metadataSegments);\n+        }\n+        return metadataSegmentsMap;\n+    }\n+\n+    /**\n+     * This method takes a {@link DebugStreamSegmentContainer} instance and a {@link SegmentProperties} object as arguments\n+     * and takes one of the following actions:\n+     * 1. If the segment is present in the container metadata and its length or sealed status or both doesn't match with the\n+     * given {@link SegmentProperties}, then it is deleted from there and registered using the properties from the given\n+     * {@link SegmentProperties} instance.\n+     * 2. If the segment is absent in the container metadata, then it is registered using the properties from the given\n+     * {@link SegmentProperties}.\n+     * @param container         A {@link DebugStreamSegmentContainer} instance for registering the given segment and checking\n+     *                          its existence in the container metadata.\n+     * @param storageSegment    A {@link SegmentProperties} instance which has properties of the segment to be registered.\n+     * @return                  CompletableFuture which when completed will have the segment registered on to the container\n+     *                          metadata.\n+     */\n+    private static CompletableFuture<Void> recoverSegment(DebugStreamSegmentContainer container, SegmentProperties storageSegment) {\n+        Preconditions.checkNotNull(container);\n+        Preconditions.checkNotNull(storageSegment);\n+        long segmentLength = storageSegment.getLength();\n+        boolean isSealed = storageSegment.isSealed();\n+        String segmentName = storageSegment.getName();\n+\n+        log.info(\"Registering: {}, {}, {}.\", segmentName, segmentLength, isSealed);\n+        return Futures.exceptionallyComposeExpecting(\n+                container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                                container.metadataStore.deleteSegment(segmentName, TIMEOUT)\n+                                        .thenAccept(x -> container.registerSegment(segmentName, segmentLength, isSealed));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 176}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzI2NzQ2MQ==", "bodyText": "Now returning it.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r473267461", "createdAt": "2020-08-19T19:25:59Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Get all segments in the container metadata for each debug segment container instance.\n+        Map<Integer, Set<String>> existingSegmentsMap = getExistingSegments(debugStreamSegmentContainers, executorService);\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            val currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            existingSegmentsMap.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        futures.clear();\n+        // Delete segments which only exist in the container metadata, not in storage.\n+        for (val existingSegmentsSetEntry : existingSegmentsMap.entrySet()) {\n+            for (String segmentName : existingSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in the storage.\", segmentName);\n+                futures.add(debugStreamSegmentContainers.get(existingSegmentsSetEntry.getKey()).deleteStreamSegment(segmentName, TIMEOUT));\n+            }\n+        }\n+        Futures.allOf(futures).join();\n+    }\n+\n+    /**\n+     * The method lists all segments present in the container metadata segments of the given {@link DebugStreamSegmentContainer}\n+     * instances, stores their names by container Id in a map and returns it.\n+     * @param containerMap              A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                  representing the containers to list the segments from.\n+     * @param executorService           A thread pool for execution.\n+     * @return                          A Map of Container Ids to segment names representing all segments present in the\n+     *                                  container metadata segment of a Container.\n+     * @throws InterruptedException     Required for Futures.get()\n+     * @throws ExecutionException       Required for Futures.get()\n+     * @throws TimeoutException         Required for Futures.get()\n+     */\n+    private static Map<Integer, Set<String>> getExistingSegments(Map<Integer, DebugStreamSegmentContainer> containerMap,\n+                                                                 ExecutorService executorService)\n+            throws InterruptedException, ExecutionException, TimeoutException {\n+        Map<Integer, Set<String>> metadataSegmentsMap = new HashMap<>();\n+        val args = IteratorArgs.builder().fetchTimeout(TIMEOUT).build();\n+\n+        // Get all segments for each container entry\n+        for (val containerEntry : containerMap.entrySet()) {\n+            Preconditions.checkNotNull(containerEntry.getValue());\n+            val tableExtension = containerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            val keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    containerEntry.getKey()), args).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+            // Store the segments in a set\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k ->\n+                    metadataSegments.addAll(k.getEntries().stream()\n+                            .map(entry -> entry.getKey().toString())\n+                            .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsMap.put(containerEntry.getKey(), metadataSegments);\n+        }\n+        return metadataSegmentsMap;\n+    }\n+\n+    /**\n+     * This method takes a {@link DebugStreamSegmentContainer} instance and a {@link SegmentProperties} object as arguments\n+     * and takes one of the following actions:\n+     * 1. If the segment is present in the container metadata and its length or sealed status or both doesn't match with the\n+     * given {@link SegmentProperties}, then it is deleted from there and registered using the properties from the given\n+     * {@link SegmentProperties} instance.\n+     * 2. If the segment is absent in the container metadata, then it is registered using the properties from the given\n+     * {@link SegmentProperties}.\n+     * @param container         A {@link DebugStreamSegmentContainer} instance for registering the given segment and checking\n+     *                          its existence in the container metadata.\n+     * @param storageSegment    A {@link SegmentProperties} instance which has properties of the segment to be registered.\n+     * @return                  CompletableFuture which when completed will have the segment registered on to the container\n+     *                          metadata.\n+     */\n+    private static CompletableFuture<Void> recoverSegment(DebugStreamSegmentContainer container, SegmentProperties storageSegment) {\n+        Preconditions.checkNotNull(container);\n+        Preconditions.checkNotNull(storageSegment);\n+        long segmentLength = storageSegment.getLength();\n+        boolean isSealed = storageSegment.isSealed();\n+        String segmentName = storageSegment.getName();\n+\n+        log.info(\"Registering: {}, {}, {}.\", segmentName, segmentLength, isSealed);\n+        return Futures.exceptionallyComposeExpecting(\n+                container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                                container.metadataStore.deleteSegment(segmentName, TIMEOUT)\n+                                        .thenAccept(x -> container.registerSegment(segmentName, segmentLength, isSealed));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg1MTE0NA=="}, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 176}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDg4OTkyMA==", "bodyText": "Isn't container.registerSegment async? You need to return it too and change thenAccept to thenCompose", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r474889920", "createdAt": "2020-08-21T19:25:12Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Get all segments in the container metadata for each debug segment container instance.\n+        Map<Integer, Set<String>> existingSegmentsMap = getExistingSegments(debugStreamSegmentContainers, executorService);\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            val currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            existingSegmentsMap.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        futures.clear();\n+        // Delete segments which only exist in the container metadata, not in storage.\n+        for (val existingSegmentsSetEntry : existingSegmentsMap.entrySet()) {\n+            for (String segmentName : existingSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in the storage.\", segmentName);\n+                futures.add(debugStreamSegmentContainers.get(existingSegmentsSetEntry.getKey()).deleteStreamSegment(segmentName, TIMEOUT));\n+            }\n+        }\n+        Futures.allOf(futures).join();\n+    }\n+\n+    /**\n+     * The method lists all segments present in the container metadata segments of the given {@link DebugStreamSegmentContainer}\n+     * instances, stores their names by container Id in a map and returns it.\n+     * @param containerMap              A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                  representing the containers to list the segments from.\n+     * @param executorService           A thread pool for execution.\n+     * @return                          A Map of Container Ids to segment names representing all segments present in the\n+     *                                  container metadata segment of a Container.\n+     * @throws InterruptedException     Required for Futures.get()\n+     * @throws ExecutionException       Required for Futures.get()\n+     * @throws TimeoutException         Required for Futures.get()\n+     */\n+    private static Map<Integer, Set<String>> getExistingSegments(Map<Integer, DebugStreamSegmentContainer> containerMap,\n+                                                                 ExecutorService executorService)\n+            throws InterruptedException, ExecutionException, TimeoutException {\n+        Map<Integer, Set<String>> metadataSegmentsMap = new HashMap<>();\n+        val args = IteratorArgs.builder().fetchTimeout(TIMEOUT).build();\n+\n+        // Get all segments for each container entry\n+        for (val containerEntry : containerMap.entrySet()) {\n+            Preconditions.checkNotNull(containerEntry.getValue());\n+            val tableExtension = containerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            val keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    containerEntry.getKey()), args).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+            // Store the segments in a set\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k ->\n+                    metadataSegments.addAll(k.getEntries().stream()\n+                            .map(entry -> entry.getKey().toString())\n+                            .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsMap.put(containerEntry.getKey(), metadataSegments);\n+        }\n+        return metadataSegmentsMap;\n+    }\n+\n+    /**\n+     * This method takes a {@link DebugStreamSegmentContainer} instance and a {@link SegmentProperties} object as arguments\n+     * and takes one of the following actions:\n+     * 1. If the segment is present in the container metadata and its length or sealed status or both doesn't match with the\n+     * given {@link SegmentProperties}, then it is deleted from there and registered using the properties from the given\n+     * {@link SegmentProperties} instance.\n+     * 2. If the segment is absent in the container metadata, then it is registered using the properties from the given\n+     * {@link SegmentProperties}.\n+     * @param container         A {@link DebugStreamSegmentContainer} instance for registering the given segment and checking\n+     *                          its existence in the container metadata.\n+     * @param storageSegment    A {@link SegmentProperties} instance which has properties of the segment to be registered.\n+     * @return                  CompletableFuture which when completed will have the segment registered on to the container\n+     *                          metadata.\n+     */\n+    private static CompletableFuture<Void> recoverSegment(DebugStreamSegmentContainer container, SegmentProperties storageSegment) {\n+        Preconditions.checkNotNull(container);\n+        Preconditions.checkNotNull(storageSegment);\n+        long segmentLength = storageSegment.getLength();\n+        boolean isSealed = storageSegment.isSealed();\n+        String segmentName = storageSegment.getName();\n+\n+        log.info(\"Registering: {}, {}, {}.\", segmentName, segmentLength, isSealed);\n+        return Futures.exceptionallyComposeExpecting(\n+                container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                                container.metadataStore.deleteSegment(segmentName, TIMEOUT)\n+                                        .thenAccept(x -> container.registerSegment(segmentName, segmentLength, isSealed));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg1MTE0NA=="}, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 176}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0OTMzMzY1OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMDo0ODo1NVrOHB_gsw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxOTo0MToxMVrOHDWbBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg1MTE4Nw==", "bodyText": "and this.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r471851187", "createdAt": "2020-08-18T00:48:55Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Get all segments in the container metadata for each debug segment container instance.\n+        Map<Integer, Set<String>> existingSegmentsMap = getExistingSegments(debugStreamSegmentContainers, executorService);\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            val currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            existingSegmentsMap.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        futures.clear();\n+        // Delete segments which only exist in the container metadata, not in storage.\n+        for (val existingSegmentsSetEntry : existingSegmentsMap.entrySet()) {\n+            for (String segmentName : existingSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in the storage.\", segmentName);\n+                futures.add(debugStreamSegmentContainers.get(existingSegmentsSetEntry.getKey()).deleteStreamSegment(segmentName, TIMEOUT));\n+            }\n+        }\n+        Futures.allOf(futures).join();\n+    }\n+\n+    /**\n+     * The method lists all segments present in the container metadata segments of the given {@link DebugStreamSegmentContainer}\n+     * instances, stores their names by container Id in a map and returns it.\n+     * @param containerMap              A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                  representing the containers to list the segments from.\n+     * @param executorService           A thread pool for execution.\n+     * @return                          A Map of Container Ids to segment names representing all segments present in the\n+     *                                  container metadata segment of a Container.\n+     * @throws InterruptedException     Required for Futures.get()\n+     * @throws ExecutionException       Required for Futures.get()\n+     * @throws TimeoutException         Required for Futures.get()\n+     */\n+    private static Map<Integer, Set<String>> getExistingSegments(Map<Integer, DebugStreamSegmentContainer> containerMap,\n+                                                                 ExecutorService executorService)\n+            throws InterruptedException, ExecutionException, TimeoutException {\n+        Map<Integer, Set<String>> metadataSegmentsMap = new HashMap<>();\n+        val args = IteratorArgs.builder().fetchTimeout(TIMEOUT).build();\n+\n+        // Get all segments for each container entry\n+        for (val containerEntry : containerMap.entrySet()) {\n+            Preconditions.checkNotNull(containerEntry.getValue());\n+            val tableExtension = containerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            val keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    containerEntry.getKey()), args).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+            // Store the segments in a set\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k ->\n+                    metadataSegments.addAll(k.getEntries().stream()\n+                            .map(entry -> entry.getKey().toString())\n+                            .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsMap.put(containerEntry.getKey(), metadataSegments);\n+        }\n+        return metadataSegmentsMap;\n+    }\n+\n+    /**\n+     * This method takes a {@link DebugStreamSegmentContainer} instance and a {@link SegmentProperties} object as arguments\n+     * and takes one of the following actions:\n+     * 1. If the segment is present in the container metadata and its length or sealed status or both doesn't match with the\n+     * given {@link SegmentProperties}, then it is deleted from there and registered using the properties from the given\n+     * {@link SegmentProperties} instance.\n+     * 2. If the segment is absent in the container metadata, then it is registered using the properties from the given\n+     * {@link SegmentProperties}.\n+     * @param container         A {@link DebugStreamSegmentContainer} instance for registering the given segment and checking\n+     *                          its existence in the container metadata.\n+     * @param storageSegment    A {@link SegmentProperties} instance which has properties of the segment to be registered.\n+     * @return                  CompletableFuture which when completed will have the segment registered on to the container\n+     *                          metadata.\n+     */\n+    private static CompletableFuture<Void> recoverSegment(DebugStreamSegmentContainer container, SegmentProperties storageSegment) {\n+        Preconditions.checkNotNull(container);\n+        Preconditions.checkNotNull(storageSegment);\n+        long segmentLength = storageSegment.getLength();\n+        boolean isSealed = storageSegment.isSealed();\n+        String segmentName = storageSegment.getName();\n+\n+        log.info(\"Registering: {}, {}, {}.\", segmentName, segmentLength, isSealed);\n+        return Futures.exceptionallyComposeExpecting(\n+                container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                                container.metadataStore.deleteSegment(segmentName, TIMEOUT)\n+                                        .thenAccept(x -> container.registerSegment(segmentName, segmentLength, isSealed));\n+                            }\n+                        }), ex -> Exceptions.unwrap(ex) instanceof StreamSegmentNotExistsException,\n+                () -> container.registerSegment(segmentName, segmentLength, isSealed));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 179}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzI3NTE0Mg==", "bodyText": "Now returning it, which will be awaited for completion at the place of method call.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r473275142", "createdAt": "2020-08-19T19:41:11Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Get all segments in the container metadata for each debug segment container instance.\n+        Map<Integer, Set<String>> existingSegmentsMap = getExistingSegments(debugStreamSegmentContainers, executorService);\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            val currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            existingSegmentsMap.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        futures.clear();\n+        // Delete segments which only exist in the container metadata, not in storage.\n+        for (val existingSegmentsSetEntry : existingSegmentsMap.entrySet()) {\n+            for (String segmentName : existingSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in the storage.\", segmentName);\n+                futures.add(debugStreamSegmentContainers.get(existingSegmentsSetEntry.getKey()).deleteStreamSegment(segmentName, TIMEOUT));\n+            }\n+        }\n+        Futures.allOf(futures).join();\n+    }\n+\n+    /**\n+     * The method lists all segments present in the container metadata segments of the given {@link DebugStreamSegmentContainer}\n+     * instances, stores their names by container Id in a map and returns it.\n+     * @param containerMap              A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                  representing the containers to list the segments from.\n+     * @param executorService           A thread pool for execution.\n+     * @return                          A Map of Container Ids to segment names representing all segments present in the\n+     *                                  container metadata segment of a Container.\n+     * @throws InterruptedException     Required for Futures.get()\n+     * @throws ExecutionException       Required for Futures.get()\n+     * @throws TimeoutException         Required for Futures.get()\n+     */\n+    private static Map<Integer, Set<String>> getExistingSegments(Map<Integer, DebugStreamSegmentContainer> containerMap,\n+                                                                 ExecutorService executorService)\n+            throws InterruptedException, ExecutionException, TimeoutException {\n+        Map<Integer, Set<String>> metadataSegmentsMap = new HashMap<>();\n+        val args = IteratorArgs.builder().fetchTimeout(TIMEOUT).build();\n+\n+        // Get all segments for each container entry\n+        for (val containerEntry : containerMap.entrySet()) {\n+            Preconditions.checkNotNull(containerEntry.getValue());\n+            val tableExtension = containerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            val keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    containerEntry.getKey()), args).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+            // Store the segments in a set\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k ->\n+                    metadataSegments.addAll(k.getEntries().stream()\n+                            .map(entry -> entry.getKey().toString())\n+                            .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsMap.put(containerEntry.getKey(), metadataSegments);\n+        }\n+        return metadataSegmentsMap;\n+    }\n+\n+    /**\n+     * This method takes a {@link DebugStreamSegmentContainer} instance and a {@link SegmentProperties} object as arguments\n+     * and takes one of the following actions:\n+     * 1. If the segment is present in the container metadata and its length or sealed status or both doesn't match with the\n+     * given {@link SegmentProperties}, then it is deleted from there and registered using the properties from the given\n+     * {@link SegmentProperties} instance.\n+     * 2. If the segment is absent in the container metadata, then it is registered using the properties from the given\n+     * {@link SegmentProperties}.\n+     * @param container         A {@link DebugStreamSegmentContainer} instance for registering the given segment and checking\n+     *                          its existence in the container metadata.\n+     * @param storageSegment    A {@link SegmentProperties} instance which has properties of the segment to be registered.\n+     * @return                  CompletableFuture which when completed will have the segment registered on to the container\n+     *                          metadata.\n+     */\n+    private static CompletableFuture<Void> recoverSegment(DebugStreamSegmentContainer container, SegmentProperties storageSegment) {\n+        Preconditions.checkNotNull(container);\n+        Preconditions.checkNotNull(storageSegment);\n+        long segmentLength = storageSegment.getLength();\n+        boolean isSealed = storageSegment.isSealed();\n+        String segmentName = storageSegment.getName();\n+\n+        log.info(\"Registering: {}, {}, {}.\", segmentName, segmentLength, isSealed);\n+        return Futures.exceptionallyComposeExpecting(\n+                container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                                container.metadataStore.deleteSegment(segmentName, TIMEOUT)\n+                                        .thenAccept(x -> container.registerSegment(segmentName, segmentLength, isSealed));\n+                            }\n+                        }), ex -> Exceptions.unwrap(ex) instanceof StreamSegmentNotExistsException,\n+                () -> container.registerSegment(segmentName, segmentLength, isSealed));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg1MTE4Nw=="}, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 179}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0OTMzNDM4OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMDo0OToxNVrOHB_hHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxOTo0MToyMlrOHDWbYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg1MTI5Mg==", "bodyText": "no unwrapping?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r471851292", "createdAt": "2020-08-18T00:49:15Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Get all segments in the container metadata for each debug segment container instance.\n+        Map<Integer, Set<String>> existingSegmentsMap = getExistingSegments(debugStreamSegmentContainers, executorService);\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            val currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            existingSegmentsMap.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        futures.clear();\n+        // Delete segments which only exist in the container metadata, not in storage.\n+        for (val existingSegmentsSetEntry : existingSegmentsMap.entrySet()) {\n+            for (String segmentName : existingSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in the storage.\", segmentName);\n+                futures.add(debugStreamSegmentContainers.get(existingSegmentsSetEntry.getKey()).deleteStreamSegment(segmentName, TIMEOUT));\n+            }\n+        }\n+        Futures.allOf(futures).join();\n+    }\n+\n+    /**\n+     * The method lists all segments present in the container metadata segments of the given {@link DebugStreamSegmentContainer}\n+     * instances, stores their names by container Id in a map and returns it.\n+     * @param containerMap              A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                  representing the containers to list the segments from.\n+     * @param executorService           A thread pool for execution.\n+     * @return                          A Map of Container Ids to segment names representing all segments present in the\n+     *                                  container metadata segment of a Container.\n+     * @throws InterruptedException     Required for Futures.get()\n+     * @throws ExecutionException       Required for Futures.get()\n+     * @throws TimeoutException         Required for Futures.get()\n+     */\n+    private static Map<Integer, Set<String>> getExistingSegments(Map<Integer, DebugStreamSegmentContainer> containerMap,\n+                                                                 ExecutorService executorService)\n+            throws InterruptedException, ExecutionException, TimeoutException {\n+        Map<Integer, Set<String>> metadataSegmentsMap = new HashMap<>();\n+        val args = IteratorArgs.builder().fetchTimeout(TIMEOUT).build();\n+\n+        // Get all segments for each container entry\n+        for (val containerEntry : containerMap.entrySet()) {\n+            Preconditions.checkNotNull(containerEntry.getValue());\n+            val tableExtension = containerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            val keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    containerEntry.getKey()), args).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+            // Store the segments in a set\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k ->\n+                    metadataSegments.addAll(k.getEntries().stream()\n+                            .map(entry -> entry.getKey().toString())\n+                            .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsMap.put(containerEntry.getKey(), metadataSegments);\n+        }\n+        return metadataSegmentsMap;\n+    }\n+\n+    /**\n+     * This method takes a {@link DebugStreamSegmentContainer} instance and a {@link SegmentProperties} object as arguments\n+     * and takes one of the following actions:\n+     * 1. If the segment is present in the container metadata and its length or sealed status or both doesn't match with the\n+     * given {@link SegmentProperties}, then it is deleted from there and registered using the properties from the given\n+     * {@link SegmentProperties} instance.\n+     * 2. If the segment is absent in the container metadata, then it is registered using the properties from the given\n+     * {@link SegmentProperties}.\n+     * @param container         A {@link DebugStreamSegmentContainer} instance for registering the given segment and checking\n+     *                          its existence in the container metadata.\n+     * @param storageSegment    A {@link SegmentProperties} instance which has properties of the segment to be registered.\n+     * @return                  CompletableFuture which when completed will have the segment registered on to the container\n+     *                          metadata.\n+     */\n+    private static CompletableFuture<Void> recoverSegment(DebugStreamSegmentContainer container, SegmentProperties storageSegment) {\n+        Preconditions.checkNotNull(container);\n+        Preconditions.checkNotNull(storageSegment);\n+        long segmentLength = storageSegment.getLength();\n+        boolean isSealed = storageSegment.isSealed();\n+        String segmentName = storageSegment.getName();\n+\n+        log.info(\"Registering: {}, {}, {}.\", segmentName, segmentLength, isSealed);\n+        return Futures.exceptionallyComposeExpecting(\n+                container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                                container.metadataStore.deleteSegment(segmentName, TIMEOUT)\n+                                        .thenAccept(x -> container.registerSegment(segmentName, segmentLength, isSealed));\n+                            }\n+                        }), ex -> Exceptions.unwrap(ex) instanceof StreamSegmentNotExistsException,\n+                () -> container.registerSegment(segmentName, segmentLength, isSealed));\n+    }\n+\n+    /**\n+     * Deletes container metadata segment and its Attribute segment from the {@link Storage} for the given container Id.\n+     * @param storage       A {@link Storage} instance to delete the segments from.\n+     * @param containerId   Id of the container for which the segments has to be deleted.\n+     */\n+    public static CompletableFuture<Void> deleteMetadataAndAttributeSegments(Storage storage, int containerId) {\n+        Preconditions.checkNotNull(storage);\n+        String metadataSegmentName = NameUtils.getMetadataSegmentName(containerId);\n+        String attributeSegmentName = NameUtils.getAttributeSegmentName(metadataSegmentName);\n+        return deleteSegmentFromStorage(storage, metadataSegmentName)\n+                .thenAccept(x -> deleteSegmentFromStorage(storage, attributeSegmentName));\n+    }\n+\n+    /**\n+     * Deletes the segment with given name from the given {@link Storage} instance. If the segment doesn't exist, it does\n+     * nothing and returns.\n+     * @param storage       A {@link Storage} instance to delete the segments from.\n+     * @param segmentName   Name of the segment to be deleted.\n+     * @return              CompletableFuture which when completed will have the segment deleted. In case segment didn't\n+     *                      exist, a completed future will be returned.\n+     */\n+    private static CompletableFuture<Void> deleteSegmentFromStorage(Storage storage, String segmentName) {\n+        log.info(\"Deleting Segment '{}'\", segmentName);\n+        return Futures.exceptionallyComposeExpecting(\n+                storage.openWrite(segmentName).thenCompose(segmentHandle -> storage.delete(segmentHandle, TIMEOUT)),\n+                ex -> Exceptions.unwrap(ex) instanceof StreamSegmentNotExistsException,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 207}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzI3NTIzMw==", "bodyText": "Yes. removed.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r473275233", "createdAt": "2020-08-19T19:41:22Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Get all segments in the container metadata for each debug segment container instance.\n+        Map<Integer, Set<String>> existingSegmentsMap = getExistingSegments(debugStreamSegmentContainers, executorService);\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            val currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            existingSegmentsMap.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        futures.clear();\n+        // Delete segments which only exist in the container metadata, not in storage.\n+        for (val existingSegmentsSetEntry : existingSegmentsMap.entrySet()) {\n+            for (String segmentName : existingSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in the storage.\", segmentName);\n+                futures.add(debugStreamSegmentContainers.get(existingSegmentsSetEntry.getKey()).deleteStreamSegment(segmentName, TIMEOUT));\n+            }\n+        }\n+        Futures.allOf(futures).join();\n+    }\n+\n+    /**\n+     * The method lists all segments present in the container metadata segments of the given {@link DebugStreamSegmentContainer}\n+     * instances, stores their names by container Id in a map and returns it.\n+     * @param containerMap              A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                  representing the containers to list the segments from.\n+     * @param executorService           A thread pool for execution.\n+     * @return                          A Map of Container Ids to segment names representing all segments present in the\n+     *                                  container metadata segment of a Container.\n+     * @throws InterruptedException     Required for Futures.get()\n+     * @throws ExecutionException       Required for Futures.get()\n+     * @throws TimeoutException         Required for Futures.get()\n+     */\n+    private static Map<Integer, Set<String>> getExistingSegments(Map<Integer, DebugStreamSegmentContainer> containerMap,\n+                                                                 ExecutorService executorService)\n+            throws InterruptedException, ExecutionException, TimeoutException {\n+        Map<Integer, Set<String>> metadataSegmentsMap = new HashMap<>();\n+        val args = IteratorArgs.builder().fetchTimeout(TIMEOUT).build();\n+\n+        // Get all segments for each container entry\n+        for (val containerEntry : containerMap.entrySet()) {\n+            Preconditions.checkNotNull(containerEntry.getValue());\n+            val tableExtension = containerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            val keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    containerEntry.getKey()), args).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+            // Store the segments in a set\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k ->\n+                    metadataSegments.addAll(k.getEntries().stream()\n+                            .map(entry -> entry.getKey().toString())\n+                            .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsMap.put(containerEntry.getKey(), metadataSegments);\n+        }\n+        return metadataSegmentsMap;\n+    }\n+\n+    /**\n+     * This method takes a {@link DebugStreamSegmentContainer} instance and a {@link SegmentProperties} object as arguments\n+     * and takes one of the following actions:\n+     * 1. If the segment is present in the container metadata and its length or sealed status or both doesn't match with the\n+     * given {@link SegmentProperties}, then it is deleted from there and registered using the properties from the given\n+     * {@link SegmentProperties} instance.\n+     * 2. If the segment is absent in the container metadata, then it is registered using the properties from the given\n+     * {@link SegmentProperties}.\n+     * @param container         A {@link DebugStreamSegmentContainer} instance for registering the given segment and checking\n+     *                          its existence in the container metadata.\n+     * @param storageSegment    A {@link SegmentProperties} instance which has properties of the segment to be registered.\n+     * @return                  CompletableFuture which when completed will have the segment registered on to the container\n+     *                          metadata.\n+     */\n+    private static CompletableFuture<Void> recoverSegment(DebugStreamSegmentContainer container, SegmentProperties storageSegment) {\n+        Preconditions.checkNotNull(container);\n+        Preconditions.checkNotNull(storageSegment);\n+        long segmentLength = storageSegment.getLength();\n+        boolean isSealed = storageSegment.isSealed();\n+        String segmentName = storageSegment.getName();\n+\n+        log.info(\"Registering: {}, {}, {}.\", segmentName, segmentLength, isSealed);\n+        return Futures.exceptionallyComposeExpecting(\n+                container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                                container.metadataStore.deleteSegment(segmentName, TIMEOUT)\n+                                        .thenAccept(x -> container.registerSegment(segmentName, segmentLength, isSealed));\n+                            }\n+                        }), ex -> Exceptions.unwrap(ex) instanceof StreamSegmentNotExistsException,\n+                () -> container.registerSegment(segmentName, segmentLength, isSealed));\n+    }\n+\n+    /**\n+     * Deletes container metadata segment and its Attribute segment from the {@link Storage} for the given container Id.\n+     * @param storage       A {@link Storage} instance to delete the segments from.\n+     * @param containerId   Id of the container for which the segments has to be deleted.\n+     */\n+    public static CompletableFuture<Void> deleteMetadataAndAttributeSegments(Storage storage, int containerId) {\n+        Preconditions.checkNotNull(storage);\n+        String metadataSegmentName = NameUtils.getMetadataSegmentName(containerId);\n+        String attributeSegmentName = NameUtils.getAttributeSegmentName(metadataSegmentName);\n+        return deleteSegmentFromStorage(storage, metadataSegmentName)\n+                .thenAccept(x -> deleteSegmentFromStorage(storage, attributeSegmentName));\n+    }\n+\n+    /**\n+     * Deletes the segment with given name from the given {@link Storage} instance. If the segment doesn't exist, it does\n+     * nothing and returns.\n+     * @param storage       A {@link Storage} instance to delete the segments from.\n+     * @param segmentName   Name of the segment to be deleted.\n+     * @return              CompletableFuture which when completed will have the segment deleted. In case segment didn't\n+     *                      exist, a completed future will be returned.\n+     */\n+    private static CompletableFuture<Void> deleteSegmentFromStorage(Storage storage, String segmentName) {\n+        log.info(\"Deleting Segment '{}'\", segmentName);\n+        return Futures.exceptionallyComposeExpecting(\n+                storage.openWrite(segmentName).thenCompose(segmentHandle -> storage.delete(segmentHandle, TIMEOUT)),\n+                ex -> Exceptions.unwrap(ex) instanceof StreamSegmentNotExistsException,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg1MTI5Mg=="}, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 207}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0OTMzNjA1OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMDo0OTo1N1rOHB_iAA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOVQxOTo0MToyOVrOHDWbjw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg1MTUyMA==", "bodyText": "use exceptionallyExpecting and return null instead of CompletableFuture.completedFuture(null)", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r471851520", "createdAt": "2020-08-18T00:49:57Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Get all segments in the container metadata for each debug segment container instance.\n+        Map<Integer, Set<String>> existingSegmentsMap = getExistingSegments(debugStreamSegmentContainers, executorService);\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            val currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            existingSegmentsMap.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        futures.clear();\n+        // Delete segments which only exist in the container metadata, not in storage.\n+        for (val existingSegmentsSetEntry : existingSegmentsMap.entrySet()) {\n+            for (String segmentName : existingSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in the storage.\", segmentName);\n+                futures.add(debugStreamSegmentContainers.get(existingSegmentsSetEntry.getKey()).deleteStreamSegment(segmentName, TIMEOUT));\n+            }\n+        }\n+        Futures.allOf(futures).join();\n+    }\n+\n+    /**\n+     * The method lists all segments present in the container metadata segments of the given {@link DebugStreamSegmentContainer}\n+     * instances, stores their names by container Id in a map and returns it.\n+     * @param containerMap              A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                  representing the containers to list the segments from.\n+     * @param executorService           A thread pool for execution.\n+     * @return                          A Map of Container Ids to segment names representing all segments present in the\n+     *                                  container metadata segment of a Container.\n+     * @throws InterruptedException     Required for Futures.get()\n+     * @throws ExecutionException       Required for Futures.get()\n+     * @throws TimeoutException         Required for Futures.get()\n+     */\n+    private static Map<Integer, Set<String>> getExistingSegments(Map<Integer, DebugStreamSegmentContainer> containerMap,\n+                                                                 ExecutorService executorService)\n+            throws InterruptedException, ExecutionException, TimeoutException {\n+        Map<Integer, Set<String>> metadataSegmentsMap = new HashMap<>();\n+        val args = IteratorArgs.builder().fetchTimeout(TIMEOUT).build();\n+\n+        // Get all segments for each container entry\n+        for (val containerEntry : containerMap.entrySet()) {\n+            Preconditions.checkNotNull(containerEntry.getValue());\n+            val tableExtension = containerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            val keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    containerEntry.getKey()), args).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+            // Store the segments in a set\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k ->\n+                    metadataSegments.addAll(k.getEntries().stream()\n+                            .map(entry -> entry.getKey().toString())\n+                            .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsMap.put(containerEntry.getKey(), metadataSegments);\n+        }\n+        return metadataSegmentsMap;\n+    }\n+\n+    /**\n+     * This method takes a {@link DebugStreamSegmentContainer} instance and a {@link SegmentProperties} object as arguments\n+     * and takes one of the following actions:\n+     * 1. If the segment is present in the container metadata and its length or sealed status or both doesn't match with the\n+     * given {@link SegmentProperties}, then it is deleted from there and registered using the properties from the given\n+     * {@link SegmentProperties} instance.\n+     * 2. If the segment is absent in the container metadata, then it is registered using the properties from the given\n+     * {@link SegmentProperties}.\n+     * @param container         A {@link DebugStreamSegmentContainer} instance for registering the given segment and checking\n+     *                          its existence in the container metadata.\n+     * @param storageSegment    A {@link SegmentProperties} instance which has properties of the segment to be registered.\n+     * @return                  CompletableFuture which when completed will have the segment registered on to the container\n+     *                          metadata.\n+     */\n+    private static CompletableFuture<Void> recoverSegment(DebugStreamSegmentContainer container, SegmentProperties storageSegment) {\n+        Preconditions.checkNotNull(container);\n+        Preconditions.checkNotNull(storageSegment);\n+        long segmentLength = storageSegment.getLength();\n+        boolean isSealed = storageSegment.isSealed();\n+        String segmentName = storageSegment.getName();\n+\n+        log.info(\"Registering: {}, {}, {}.\", segmentName, segmentLength, isSealed);\n+        return Futures.exceptionallyComposeExpecting(\n+                container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                                container.metadataStore.deleteSegment(segmentName, TIMEOUT)\n+                                        .thenAccept(x -> container.registerSegment(segmentName, segmentLength, isSealed));\n+                            }\n+                        }), ex -> Exceptions.unwrap(ex) instanceof StreamSegmentNotExistsException,\n+                () -> container.registerSegment(segmentName, segmentLength, isSealed));\n+    }\n+\n+    /**\n+     * Deletes container metadata segment and its Attribute segment from the {@link Storage} for the given container Id.\n+     * @param storage       A {@link Storage} instance to delete the segments from.\n+     * @param containerId   Id of the container for which the segments has to be deleted.\n+     */\n+    public static CompletableFuture<Void> deleteMetadataAndAttributeSegments(Storage storage, int containerId) {\n+        Preconditions.checkNotNull(storage);\n+        String metadataSegmentName = NameUtils.getMetadataSegmentName(containerId);\n+        String attributeSegmentName = NameUtils.getAttributeSegmentName(metadataSegmentName);\n+        return deleteSegmentFromStorage(storage, metadataSegmentName)\n+                .thenAccept(x -> deleteSegmentFromStorage(storage, attributeSegmentName));\n+    }\n+\n+    /**\n+     * Deletes the segment with given name from the given {@link Storage} instance. If the segment doesn't exist, it does\n+     * nothing and returns.\n+     * @param storage       A {@link Storage} instance to delete the segments from.\n+     * @param segmentName   Name of the segment to be deleted.\n+     * @return              CompletableFuture which when completed will have the segment deleted. In case segment didn't\n+     *                      exist, a completed future will be returned.\n+     */\n+    private static CompletableFuture<Void> deleteSegmentFromStorage(Storage storage, String segmentName) {\n+        log.info(\"Deleting Segment '{}'\", segmentName);\n+        return Futures.exceptionallyComposeExpecting(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 205}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MzI3NTI3OQ==", "bodyText": "Ok.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r473275279", "createdAt": "2020-08-19T19:41:29Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ContainerRecoveryUtils.java", "diffHunk": "@@ -0,0 +1,210 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutionException;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for container recovery.\n+ */\n+@Slf4j\n+public class ContainerRecoveryUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * This method lists the segments from the given storage instance. It then registers all segments except Attribute\n+     * segments to the container metadata segment(s).\n+     * {@link DebugStreamSegmentContainer} instance(s) are provided to this method which can have some segments already present\n+     * in their respective container metadata segment(s). After the method successfully completes, only the segments which\n+     * existed in the {@link Storage} will remain in the container metadata. All segments which only existed in the container\n+     * metadata or which existed in both container metadata and the storage but with different lengths and/or sealed status,\n+     * will be deleted from the container metadata. If the method fails while execution, appropriate exception is thrown.\n+     * All segments from the storage are listed one by one, then mapped to their corresponding {@link DebugStreamSegmentContainer}\n+     * instances for registering them to container metadata segment.\n+     * @param storage                           A {@link Storage} instance that will be used to list segments from.\n+     * @param debugStreamSegmentContainers      A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                          representing the containers that will be recovered.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws InterruptedException             Required for Futures.get()\n+     * @throws ExecutionException               Required for Futures.get()\n+     * @throws TimeoutException                 Required for Futures.get()\n+     * @throws IOException                      Requited for Storage.listSegments()\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws InterruptedException, ExecutionException,\n+            TimeoutException, IOException {\n+        Preconditions.checkNotNull(storage);\n+        Preconditions.checkNotNull(executorService);\n+        Preconditions.checkNotNull(debugStreamSegmentContainers);\n+        Preconditions.checkArgument(debugStreamSegmentContainers.size() > 0, \"There should be at least one \" +\n+                \"debug segment container instance.\");\n+\n+        log.info(\"Recovery started for all containers...\");\n+        // Get all segments in the container metadata for each debug segment container instance.\n+        Map<Integer, Set<String>> existingSegmentsMap = getExistingSegments(debugStreamSegmentContainers, executorService);\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        Preconditions.checkNotNull(segmentIterator);\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            val currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            existingSegmentsMap.get(containerId).remove(currentSegment.getName());\n+            futures.add(recoverSegment(debugStreamSegmentContainers.get(containerId), currentSegment));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        futures.clear();\n+        // Delete segments which only exist in the container metadata, not in storage.\n+        for (val existingSegmentsSetEntry : existingSegmentsMap.entrySet()) {\n+            for (String segmentName : existingSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in the storage.\", segmentName);\n+                futures.add(debugStreamSegmentContainers.get(existingSegmentsSetEntry.getKey()).deleteStreamSegment(segmentName, TIMEOUT));\n+            }\n+        }\n+        Futures.allOf(futures).join();\n+    }\n+\n+    /**\n+     * The method lists all segments present in the container metadata segments of the given {@link DebugStreamSegmentContainer}\n+     * instances, stores their names by container Id in a map and returns it.\n+     * @param containerMap              A Map of Container Ids to {@link DebugStreamSegmentContainer} instances\n+     *                                  representing the containers to list the segments from.\n+     * @param executorService           A thread pool for execution.\n+     * @return                          A Map of Container Ids to segment names representing all segments present in the\n+     *                                  container metadata segment of a Container.\n+     * @throws InterruptedException     Required for Futures.get()\n+     * @throws ExecutionException       Required for Futures.get()\n+     * @throws TimeoutException         Required for Futures.get()\n+     */\n+    private static Map<Integer, Set<String>> getExistingSegments(Map<Integer, DebugStreamSegmentContainer> containerMap,\n+                                                                 ExecutorService executorService)\n+            throws InterruptedException, ExecutionException, TimeoutException {\n+        Map<Integer, Set<String>> metadataSegmentsMap = new HashMap<>();\n+        val args = IteratorArgs.builder().fetchTimeout(TIMEOUT).build();\n+\n+        // Get all segments for each container entry\n+        for (val containerEntry : containerMap.entrySet()) {\n+            Preconditions.checkNotNull(containerEntry.getValue());\n+            val tableExtension = containerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            val keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    containerEntry.getKey()), args).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+            // Store the segments in a set\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k ->\n+                    metadataSegments.addAll(k.getEntries().stream()\n+                            .map(entry -> entry.getKey().toString())\n+                            .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsMap.put(containerEntry.getKey(), metadataSegments);\n+        }\n+        return metadataSegmentsMap;\n+    }\n+\n+    /**\n+     * This method takes a {@link DebugStreamSegmentContainer} instance and a {@link SegmentProperties} object as arguments\n+     * and takes one of the following actions:\n+     * 1. If the segment is present in the container metadata and its length or sealed status or both doesn't match with the\n+     * given {@link SegmentProperties}, then it is deleted from there and registered using the properties from the given\n+     * {@link SegmentProperties} instance.\n+     * 2. If the segment is absent in the container metadata, then it is registered using the properties from the given\n+     * {@link SegmentProperties}.\n+     * @param container         A {@link DebugStreamSegmentContainer} instance for registering the given segment and checking\n+     *                          its existence in the container metadata.\n+     * @param storageSegment    A {@link SegmentProperties} instance which has properties of the segment to be registered.\n+     * @return                  CompletableFuture which when completed will have the segment registered on to the container\n+     *                          metadata.\n+     */\n+    private static CompletableFuture<Void> recoverSegment(DebugStreamSegmentContainer container, SegmentProperties storageSegment) {\n+        Preconditions.checkNotNull(container);\n+        Preconditions.checkNotNull(storageSegment);\n+        long segmentLength = storageSegment.getLength();\n+        boolean isSealed = storageSegment.isSealed();\n+        String segmentName = storageSegment.getName();\n+\n+        log.info(\"Registering: {}, {}, {}.\", segmentName, segmentLength, isSealed);\n+        return Futures.exceptionallyComposeExpecting(\n+                container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                                container.metadataStore.deleteSegment(segmentName, TIMEOUT)\n+                                        .thenAccept(x -> container.registerSegment(segmentName, segmentLength, isSealed));\n+                            }\n+                        }), ex -> Exceptions.unwrap(ex) instanceof StreamSegmentNotExistsException,\n+                () -> container.registerSegment(segmentName, segmentLength, isSealed));\n+    }\n+\n+    /**\n+     * Deletes container metadata segment and its Attribute segment from the {@link Storage} for the given container Id.\n+     * @param storage       A {@link Storage} instance to delete the segments from.\n+     * @param containerId   Id of the container for which the segments has to be deleted.\n+     */\n+    public static CompletableFuture<Void> deleteMetadataAndAttributeSegments(Storage storage, int containerId) {\n+        Preconditions.checkNotNull(storage);\n+        String metadataSegmentName = NameUtils.getMetadataSegmentName(containerId);\n+        String attributeSegmentName = NameUtils.getAttributeSegmentName(metadataSegmentName);\n+        return deleteSegmentFromStorage(storage, metadataSegmentName)\n+                .thenAccept(x -> deleteSegmentFromStorage(storage, attributeSegmentName));\n+    }\n+\n+    /**\n+     * Deletes the segment with given name from the given {@link Storage} instance. If the segment doesn't exist, it does\n+     * nothing and returns.\n+     * @param storage       A {@link Storage} instance to delete the segments from.\n+     * @param segmentName   Name of the segment to be deleted.\n+     * @return              CompletableFuture which when completed will have the segment deleted. In case segment didn't\n+     *                      exist, a completed future will be returned.\n+     */\n+    private static CompletableFuture<Void> deleteSegmentFromStorage(Storage storage, String segmentName) {\n+        log.info(\"Deleting Segment '{}'\", segmentName);\n+        return Futures.exceptionallyComposeExpecting(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg1MTUyMA=="}, "originalCommit": {"oid": "f4bf8fdcd2c1b88e135e60c4f131db873fc1f652"}, "originalPosition": 205}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODE4MDI3OnYy", "diffSide": "RIGHT", "path": "segmentstore/contracts/src/main/java/io/pravega/segmentstore/contracts/StreamSegmentStoreWrapper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxNzo1MTo0NlrOGzb9bA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjowNjowOVrOG2dMpg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4ODY1Mg==", "bodyText": "This class shouldn't be in contracts. Move it to wherever you have your DebugSegmentContainer.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456588652", "createdAt": "2020-07-17T17:51:46Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/contracts/src/main/java/io/pravega/segmentstore/contracts/StreamSegmentStoreWrapper.java", "diffHunk": "@@ -0,0 +1,97 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.contracts;\n+\n+import io.pravega.common.util.BufferView;\n+import lombok.AccessLevel;\n+import lombok.Getter;\n+\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+\n+/**\n+ * A wrapper class to StreamSegmentStore to track the segments being created or deleted.\n+ */\n+public class StreamSegmentStoreWrapper implements StreamSegmentStore {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NDY2Mg==", "bodyText": "Combined with TableStore to implement a single wrapper and moved it to segmenstore/server/tests.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459754662", "createdAt": "2020-07-23T22:06:09Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/contracts/src/main/java/io/pravega/segmentstore/contracts/StreamSegmentStoreWrapper.java", "diffHunk": "@@ -0,0 +1,97 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.contracts;\n+\n+import io.pravega.common.util.BufferView;\n+import lombok.AccessLevel;\n+import lombok.Getter;\n+\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+\n+/**\n+ * A wrapper class to StreamSegmentStore to track the segments being created or deleted.\n+ */\n+public class StreamSegmentStoreWrapper implements StreamSegmentStore {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4ODY1Mg=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODE4MTc3OnYy", "diffSide": "RIGHT", "path": "segmentstore/contracts/src/main/java/io/pravega/segmentstore/contracts/StreamSegmentStoreWrapper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxNzo1MjoxNlrOGzb-WQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjowNjoxOFrOG2dM6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4ODg4OQ==", "bodyText": "Make this final\nMake it a ConcurrentHashMap", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456588889", "createdAt": "2020-07-17T17:52:16Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/contracts/src/main/java/io/pravega/segmentstore/contracts/StreamSegmentStoreWrapper.java", "diffHunk": "@@ -0,0 +1,97 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.contracts;\n+\n+import io.pravega.common.util.BufferView;\n+import lombok.AccessLevel;\n+import lombok.Getter;\n+\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+\n+/**\n+ * A wrapper class to StreamSegmentStore to track the segments being created or deleted.\n+ */\n+public class StreamSegmentStoreWrapper implements StreamSegmentStore {\n+\n+    private final StreamSegmentStore streamSegmentStore;\n+\n+    @Getter(AccessLevel.PUBLIC)\n+    private HashSet<String> segments;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NDczMA==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459754730", "createdAt": "2020-07-23T22:06:18Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/contracts/src/main/java/io/pravega/segmentstore/contracts/StreamSegmentStoreWrapper.java", "diffHunk": "@@ -0,0 +1,97 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.contracts;\n+\n+import io.pravega.common.util.BufferView;\n+import lombok.AccessLevel;\n+import lombok.Getter;\n+\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+\n+/**\n+ * A wrapper class to StreamSegmentStore to track the segments being created or deleted.\n+ */\n+public class StreamSegmentStoreWrapper implements StreamSegmentStore {\n+\n+    private final StreamSegmentStore streamSegmentStore;\n+\n+    @Getter(AccessLevel.PUBLIC)\n+    private HashSet<String> segments;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4ODg4OQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODE4NDc1OnYy", "diffSide": "RIGHT", "path": "segmentstore/contracts/src/main/java/io/pravega/segmentstore/contracts/StreamSegmentStoreWrapper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxNzo1MzowNFrOGzcAGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjowNjozMFrOG2dNMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4OTMzNw==", "bodyText": "You risk adding this even if the below call failed. Add this as a thenRun callback to the createStreamSegment Future below.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456589337", "createdAt": "2020-07-17T17:53:04Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/contracts/src/main/java/io/pravega/segmentstore/contracts/StreamSegmentStoreWrapper.java", "diffHunk": "@@ -0,0 +1,97 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.contracts;\n+\n+import io.pravega.common.util.BufferView;\n+import lombok.AccessLevel;\n+import lombok.Getter;\n+\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+\n+/**\n+ * A wrapper class to StreamSegmentStore to track the segments being created or deleted.\n+ */\n+public class StreamSegmentStoreWrapper implements StreamSegmentStore {\n+\n+    private final StreamSegmentStore streamSegmentStore;\n+\n+    @Getter(AccessLevel.PUBLIC)\n+    private HashSet<String> segments;\n+\n+    public StreamSegmentStoreWrapper(StreamSegmentStore streamSegmentStore) {\n+        this.streamSegmentStore = streamSegmentStore;\n+        this.segments = new HashSet<>();\n+    }\n+\n+    @Override\n+    public CompletableFuture<Long> append(String streamSegmentName, BufferView data, Collection<AttributeUpdate> attributeUpdates, Duration timeout) {\n+        return this.streamSegmentStore.append(streamSegmentName, data, attributeUpdates, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Long> append(String streamSegmentName, long offset, BufferView data, Collection<AttributeUpdate> attributeUpdates,\n+                                          Duration timeout) {\n+        return this.streamSegmentStore.append(streamSegmentName, offset, data, attributeUpdates, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> updateAttributes(String streamSegmentName, Collection<AttributeUpdate> attributeUpdates, Duration timeout) {\n+        return this.streamSegmentStore.updateAttributes(streamSegmentName, attributeUpdates, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Map<UUID, Long>> getAttributes(String streamSegmentName, Collection<UUID> attributeIds, boolean cache, Duration timeout) {\n+        return this.streamSegmentStore.getAttributes(streamSegmentName, attributeIds, cache, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<ReadResult> read(String streamSegmentName, long offset, int maxLength, Duration timeout) {\n+        return this.streamSegmentStore.read(streamSegmentName, offset, maxLength, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentProperties> getStreamSegmentInfo(String streamSegmentName, Duration timeout) {\n+        return this.streamSegmentStore.getStreamSegmentInfo(streamSegmentName, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> createStreamSegment(String streamSegmentName, Collection<AttributeUpdate> attributes, Duration timeout) {\n+        segments.add(streamSegmentName); // Add the segmentName to the set", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NDgwMA==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459754800", "createdAt": "2020-07-23T22:06:30Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/contracts/src/main/java/io/pravega/segmentstore/contracts/StreamSegmentStoreWrapper.java", "diffHunk": "@@ -0,0 +1,97 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.contracts;\n+\n+import io.pravega.common.util.BufferView;\n+import lombok.AccessLevel;\n+import lombok.Getter;\n+\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+\n+/**\n+ * A wrapper class to StreamSegmentStore to track the segments being created or deleted.\n+ */\n+public class StreamSegmentStoreWrapper implements StreamSegmentStore {\n+\n+    private final StreamSegmentStore streamSegmentStore;\n+\n+    @Getter(AccessLevel.PUBLIC)\n+    private HashSet<String> segments;\n+\n+    public StreamSegmentStoreWrapper(StreamSegmentStore streamSegmentStore) {\n+        this.streamSegmentStore = streamSegmentStore;\n+        this.segments = new HashSet<>();\n+    }\n+\n+    @Override\n+    public CompletableFuture<Long> append(String streamSegmentName, BufferView data, Collection<AttributeUpdate> attributeUpdates, Duration timeout) {\n+        return this.streamSegmentStore.append(streamSegmentName, data, attributeUpdates, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Long> append(String streamSegmentName, long offset, BufferView data, Collection<AttributeUpdate> attributeUpdates,\n+                                          Duration timeout) {\n+        return this.streamSegmentStore.append(streamSegmentName, offset, data, attributeUpdates, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> updateAttributes(String streamSegmentName, Collection<AttributeUpdate> attributeUpdates, Duration timeout) {\n+        return this.streamSegmentStore.updateAttributes(streamSegmentName, attributeUpdates, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Map<UUID, Long>> getAttributes(String streamSegmentName, Collection<UUID> attributeIds, boolean cache, Duration timeout) {\n+        return this.streamSegmentStore.getAttributes(streamSegmentName, attributeIds, cache, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<ReadResult> read(String streamSegmentName, long offset, int maxLength, Duration timeout) {\n+        return this.streamSegmentStore.read(streamSegmentName, offset, maxLength, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentProperties> getStreamSegmentInfo(String streamSegmentName, Duration timeout) {\n+        return this.streamSegmentStore.getStreamSegmentInfo(streamSegmentName, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> createStreamSegment(String streamSegmentName, Collection<AttributeUpdate> attributes, Duration timeout) {\n+        segments.add(streamSegmentName); // Add the segmentName to the set", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4OTMzNw=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODE4Njg4OnYy", "diffSide": "RIGHT", "path": "segmentstore/contracts/src/main/java/io/pravega/segmentstore/contracts/StreamSegmentStoreWrapper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxNzo1MzozNlrOGzcBYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjowNjo0OFrOG2dNxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4OTY2Ng==", "bodyText": "no need to check if it exists. Remove won't do anything if it doesn't.\nSame comment as above, in createStreamSegment", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456589666", "createdAt": "2020-07-17T17:53:36Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/contracts/src/main/java/io/pravega/segmentstore/contracts/StreamSegmentStoreWrapper.java", "diffHunk": "@@ -0,0 +1,97 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.contracts;\n+\n+import io.pravega.common.util.BufferView;\n+import lombok.AccessLevel;\n+import lombok.Getter;\n+\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+\n+/**\n+ * A wrapper class to StreamSegmentStore to track the segments being created or deleted.\n+ */\n+public class StreamSegmentStoreWrapper implements StreamSegmentStore {\n+\n+    private final StreamSegmentStore streamSegmentStore;\n+\n+    @Getter(AccessLevel.PUBLIC)\n+    private HashSet<String> segments;\n+\n+    public StreamSegmentStoreWrapper(StreamSegmentStore streamSegmentStore) {\n+        this.streamSegmentStore = streamSegmentStore;\n+        this.segments = new HashSet<>();\n+    }\n+\n+    @Override\n+    public CompletableFuture<Long> append(String streamSegmentName, BufferView data, Collection<AttributeUpdate> attributeUpdates, Duration timeout) {\n+        return this.streamSegmentStore.append(streamSegmentName, data, attributeUpdates, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Long> append(String streamSegmentName, long offset, BufferView data, Collection<AttributeUpdate> attributeUpdates,\n+                                          Duration timeout) {\n+        return this.streamSegmentStore.append(streamSegmentName, offset, data, attributeUpdates, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> updateAttributes(String streamSegmentName, Collection<AttributeUpdate> attributeUpdates, Duration timeout) {\n+        return this.streamSegmentStore.updateAttributes(streamSegmentName, attributeUpdates, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Map<UUID, Long>> getAttributes(String streamSegmentName, Collection<UUID> attributeIds, boolean cache, Duration timeout) {\n+        return this.streamSegmentStore.getAttributes(streamSegmentName, attributeIds, cache, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<ReadResult> read(String streamSegmentName, long offset, int maxLength, Duration timeout) {\n+        return this.streamSegmentStore.read(streamSegmentName, offset, maxLength, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentProperties> getStreamSegmentInfo(String streamSegmentName, Duration timeout) {\n+        return this.streamSegmentStore.getStreamSegmentInfo(streamSegmentName, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> createStreamSegment(String streamSegmentName, Collection<AttributeUpdate> attributes, Duration timeout) {\n+        segments.add(streamSegmentName); // Add the segmentName to the set\n+        return this.streamSegmentStore.createStreamSegment(streamSegmentName, attributes, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<MergeStreamSegmentResult> mergeStreamSegment(String targetStreamSegment, String sourceStreamSegment, Duration timeout) {\n+        return this.streamSegmentStore.mergeStreamSegment(targetStreamSegment, sourceStreamSegment, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Long> sealStreamSegment(String streamSegmentName, Duration timeout) {\n+        return this.streamSegmentStore.sealStreamSegment(streamSegmentName, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> deleteStreamSegment(String streamSegmentName, Duration timeout) {\n+        if (segments.contains(streamSegmentName)) { // Remove the segmentName from the set", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NDk0OQ==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459754949", "createdAt": "2020-07-23T22:06:48Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/contracts/src/main/java/io/pravega/segmentstore/contracts/StreamSegmentStoreWrapper.java", "diffHunk": "@@ -0,0 +1,97 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.contracts;\n+\n+import io.pravega.common.util.BufferView;\n+import lombok.AccessLevel;\n+import lombok.Getter;\n+\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+\n+/**\n+ * A wrapper class to StreamSegmentStore to track the segments being created or deleted.\n+ */\n+public class StreamSegmentStoreWrapper implements StreamSegmentStore {\n+\n+    private final StreamSegmentStore streamSegmentStore;\n+\n+    @Getter(AccessLevel.PUBLIC)\n+    private HashSet<String> segments;\n+\n+    public StreamSegmentStoreWrapper(StreamSegmentStore streamSegmentStore) {\n+        this.streamSegmentStore = streamSegmentStore;\n+        this.segments = new HashSet<>();\n+    }\n+\n+    @Override\n+    public CompletableFuture<Long> append(String streamSegmentName, BufferView data, Collection<AttributeUpdate> attributeUpdates, Duration timeout) {\n+        return this.streamSegmentStore.append(streamSegmentName, data, attributeUpdates, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Long> append(String streamSegmentName, long offset, BufferView data, Collection<AttributeUpdate> attributeUpdates,\n+                                          Duration timeout) {\n+        return this.streamSegmentStore.append(streamSegmentName, offset, data, attributeUpdates, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> updateAttributes(String streamSegmentName, Collection<AttributeUpdate> attributeUpdates, Duration timeout) {\n+        return this.streamSegmentStore.updateAttributes(streamSegmentName, attributeUpdates, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Map<UUID, Long>> getAttributes(String streamSegmentName, Collection<UUID> attributeIds, boolean cache, Duration timeout) {\n+        return this.streamSegmentStore.getAttributes(streamSegmentName, attributeIds, cache, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<ReadResult> read(String streamSegmentName, long offset, int maxLength, Duration timeout) {\n+        return this.streamSegmentStore.read(streamSegmentName, offset, maxLength, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<SegmentProperties> getStreamSegmentInfo(String streamSegmentName, Duration timeout) {\n+        return this.streamSegmentStore.getStreamSegmentInfo(streamSegmentName, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> createStreamSegment(String streamSegmentName, Collection<AttributeUpdate> attributes, Duration timeout) {\n+        segments.add(streamSegmentName); // Add the segmentName to the set\n+        return this.streamSegmentStore.createStreamSegment(streamSegmentName, attributes, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<MergeStreamSegmentResult> mergeStreamSegment(String targetStreamSegment, String sourceStreamSegment, Duration timeout) {\n+        return this.streamSegmentStore.mergeStreamSegment(targetStreamSegment, sourceStreamSegment, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Long> sealStreamSegment(String streamSegmentName, Duration timeout) {\n+        return this.streamSegmentStore.sealStreamSegment(streamSegmentName, timeout);\n+    }\n+\n+    @Override\n+    public CompletableFuture<Void> deleteStreamSegment(String streamSegmentName, Duration timeout) {\n+        if (segments.contains(streamSegmentName)) { // Remove the segmentName from the set", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU4OTY2Ng=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODE4OTc3OnYy", "diffSide": "RIGHT", "path": "segmentstore/contracts/src/main/java/io/pravega/segmentstore/contracts/tables/TableStoreWrapper.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxNzo1NDozMVrOGzcDJQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjowNzowNVrOG2dOHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5MDExNw==", "bodyText": "Same comments as in the StreamSegmentStoreWrapper.\nQuestion: can we merge these classes into a single one? They do the same thing, but implement different interfaces.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456590117", "createdAt": "2020-07-17T17:54:31Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/contracts/src/main/java/io/pravega/segmentstore/contracts/tables/TableStoreWrapper.java", "diffHunk": "@@ -0,0 +1,91 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.contracts.tables;\n+\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.BufferView;\n+import lombok.AccessLevel;\n+import lombok.Getter;\n+\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+\n+/**\n+ * A wrapper class to TableStore to track the segments being created or deleted.\n+ */\n+public class TableStoreWrapper implements TableStore {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NTAzNg==", "bodyText": "Combined with TableStore to implement a single wrapper and moved it to segmenstore/server/tests", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459755036", "createdAt": "2020-07-23T22:07:05Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/contracts/src/main/java/io/pravega/segmentstore/contracts/tables/TableStoreWrapper.java", "diffHunk": "@@ -0,0 +1,91 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.contracts.tables;\n+\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.BufferView;\n+import lombok.AccessLevel;\n+import lombok.Getter;\n+\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.concurrent.CompletableFuture;\n+\n+/**\n+ * A wrapper class to TableStore to track the segments being created or deleted.\n+ */\n+public class TableStoreWrapper implements TableStore {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5MDExNw=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODE5MzMzOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/host/src/test/java/io/pravega/segmentstore/server/host/ExtendedS3IntegrationTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxNzo1NToxNlrOGzcFBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjowNzoxNFrOG2dOVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5MDU5OQ==", "bodyText": "Why is this \"out of concern\"? If it's broken, please add a link to a GitHub issue that tracks its implementation.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456590599", "createdAt": "2020-07-17T17:55:16Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/host/src/test/java/io/pravega/segmentstore/server/host/ExtendedS3IntegrationTest.java", "diffHunk": "@@ -74,6 +74,13 @@ protected ServiceBuilder createBuilder(ServiceBuilderConfig.Builder configBuilde\n                         getBookkeeper().getZkClient(), setup.getCoreExecutor()));\n     }\n \n+    /**\n+     * This method intentionally left blank as it's out of concern for ExtendedS3 Storage.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NTA5Mg==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459755092", "createdAt": "2020-07-23T22:07:14Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/host/src/test/java/io/pravega/segmentstore/server/host/ExtendedS3IntegrationTest.java", "diffHunk": "@@ -74,6 +74,13 @@ protected ServiceBuilder createBuilder(ServiceBuilderConfig.Builder configBuilde\n                         getBookkeeper().getZkClient(), setup.getCoreExecutor()));\n     }\n \n+    /**\n+     * This method intentionally left blank as it's out of concern for ExtendedS3 Storage.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5MDU5OQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODE5Njc2OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/DebugSegmentContainer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxNzo1NjoxNVrOGzcHGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjowNzoyNFrOG2dOkQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5MTEzMQ==", "bodyText": "Javadoc.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456591131", "createdAt": "2020-07-17T17:56:15Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/DebugSegmentContainer.java", "diffHunk": "@@ -0,0 +1,15 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+import java.util.concurrent.CompletableFuture;\n+\n+public interface DebugSegmentContainer extends SegmentContainer {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NTE1Mw==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459755153", "createdAt": "2020-07-23T22:07:24Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/DebugSegmentContainer.java", "diffHunk": "@@ -0,0 +1,15 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+import java.util.concurrent.CompletableFuture;\n+\n+public interface DebugSegmentContainer extends SegmentContainer {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5MTEzMQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODIwMTAwOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxNzo1NzoxOFrOGzcJnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjowNzozMFrOG2dOrA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5MTc3Mg==", "bodyText": "DebugStreamSegmentContainer class", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456591772", "createdAt": "2020-07-17T17:57:18Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainer.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.server.DebugSegmentContainer;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.time.Duration;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+@Slf4j\n+public class DebugStreamSegmentContainer extends StreamSegmentContainer implements DebugSegmentContainer {\n+    private static final Duration TIMEOUT = Duration.ofMinutes(1);\n+    private final ContainerConfig config;\n+\n+    /**\n+     * Creates a new instance of the StreamSegmentContainer class.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NTE4MA==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459755180", "createdAt": "2020-07-23T22:07:30Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainer.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.server.DebugSegmentContainer;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.time.Duration;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+@Slf4j\n+public class DebugStreamSegmentContainer extends StreamSegmentContainer implements DebugSegmentContainer {\n+    private static final Duration TIMEOUT = Duration.ofMinutes(1);\n+    private final ContainerConfig config;\n+\n+    /**\n+     * Creates a new instance of the StreamSegmentContainer class.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5MTc3Mg=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODIwMTQ0OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxNzo1NzoyNVrOGzcJ4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjowNzozN1rOG2dO0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5MTg0MA==", "bodyText": "and here", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456591840", "createdAt": "2020-07-17T17:57:25Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainer.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.server.DebugSegmentContainer;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.time.Duration;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+@Slf4j\n+public class DebugStreamSegmentContainer extends StreamSegmentContainer implements DebugSegmentContainer {\n+    private static final Duration TIMEOUT = Duration.ofMinutes(1);\n+    private final ContainerConfig config;\n+\n+    /**\n+     * Creates a new instance of the StreamSegmentContainer class.\n+     *\n+     * @param streamSegmentContainerId The Id of the StreamSegmentContainer.\n+     * @param config                   The ContainerConfig to use for this StreamSegmentContainer.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 38}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NTIxOQ==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459755219", "createdAt": "2020-07-23T22:07:37Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainer.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.server.DebugSegmentContainer;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.time.Duration;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+@Slf4j\n+public class DebugStreamSegmentContainer extends StreamSegmentContainer implements DebugSegmentContainer {\n+    private static final Duration TIMEOUT = Duration.ofMinutes(1);\n+    private final ContainerConfig config;\n+\n+    /**\n+     * Creates a new instance of the StreamSegmentContainer class.\n+     *\n+     * @param streamSegmentContainerId The Id of the StreamSegmentContainer.\n+     * @param config                   The ContainerConfig to use for this StreamSegmentContainer.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5MTg0MA=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODIwNjk1OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/DebugSegmentContainer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxNzo1ODo1OFrOGzcNMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjowNzo0NVrOG2dPAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5MjY5MA==", "bodyText": "Let's rename this to \"registerExistingSegment` to emphasize that the Segment already exists and that we are not actually \"creating\" it.\nPlease explain in the Javadoc what this method is doing and what its outcome is.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456592690", "createdAt": "2020-07-17T17:58:58Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/DebugSegmentContainer.java", "diffHunk": "@@ -0,0 +1,15 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+import java.util.concurrent.CompletableFuture;\n+\n+public interface DebugSegmentContainer extends SegmentContainer {\n+    CompletableFuture<Void> createStreamSegment(String streamSegmentName, long length, boolean isSealed);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NTI2Ng==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459755266", "createdAt": "2020-07-23T22:07:45Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/DebugSegmentContainer.java", "diffHunk": "@@ -0,0 +1,15 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+import java.util.concurrent.CompletableFuture;\n+\n+public interface DebugSegmentContainer extends SegmentContainer {\n+    CompletableFuture<Void> createStreamSegment(String streamSegmentName, long length, boolean isSealed);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5MjY5MA=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODIwODk2OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxNzo1OToyNlrOGzcOYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjowNzo1OFrOG2dPYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5Mjk5NA==", "bodyText": "Move this to the interface Javadoc (and explain more what it is doing).", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456592994", "createdAt": "2020-07-17T17:59:26Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainer.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.server.DebugSegmentContainer;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.time.Duration;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+@Slf4j\n+public class DebugStreamSegmentContainer extends StreamSegmentContainer implements DebugSegmentContainer {\n+    private static final Duration TIMEOUT = Duration.ofMinutes(1);\n+    private final ContainerConfig config;\n+\n+    /**\n+     * Creates a new instance of the StreamSegmentContainer class.\n+     *\n+     * @param streamSegmentContainerId The Id of the StreamSegmentContainer.\n+     * @param config                   The ContainerConfig to use for this StreamSegmentContainer.\n+     * @param durableLogFactory        The DurableLogFactory to use to create DurableLogs.\n+     * @param readIndexFactory         The ReadIndexFactory to use to create Read Indices.\n+     * @param attributeIndexFactory    The AttributeIndexFactory to use to create Attribute Indices.\n+     * @param writerFactory            The WriterFactory to use to create Writers.\n+     * @param storageFactory           The StorageFactory to use to create Storage Adapters.\n+     * @param createExtensions         A Function that, given an instance of this class, will create the set of\n+     *                                 {@link SegmentContainerExtension}s to be associated with that instance.\n+     * @param executor                 An Executor that can be used to run async tasks.\n+     */\n+    DebugStreamSegmentContainer(int streamSegmentContainerId, ContainerConfig config, OperationLogFactory durableLogFactory,\n+                                ReadIndexFactory readIndexFactory, AttributeIndexFactory attributeIndexFactory,\n+                                WriterFactory writerFactory, StorageFactory storageFactory,\n+                                SegmentContainerFactory.CreateExtensions createExtensions, ScheduledExecutorService executor) {\n+        super(streamSegmentContainerId, config, durableLogFactory, readIndexFactory, attributeIndexFactory, writerFactory,\n+                storageFactory, createExtensions, executor);\n+        this.config = config;\n+    }\n+\n+    /**\n+     * Creates a segment with given properties.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NTM2Mg==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459755362", "createdAt": "2020-07-23T22:07:58Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainer.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.server.DebugSegmentContainer;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.time.Duration;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+@Slf4j\n+public class DebugStreamSegmentContainer extends StreamSegmentContainer implements DebugSegmentContainer {\n+    private static final Duration TIMEOUT = Duration.ofMinutes(1);\n+    private final ContainerConfig config;\n+\n+    /**\n+     * Creates a new instance of the StreamSegmentContainer class.\n+     *\n+     * @param streamSegmentContainerId The Id of the StreamSegmentContainer.\n+     * @param config                   The ContainerConfig to use for this StreamSegmentContainer.\n+     * @param durableLogFactory        The DurableLogFactory to use to create DurableLogs.\n+     * @param readIndexFactory         The ReadIndexFactory to use to create Read Indices.\n+     * @param attributeIndexFactory    The AttributeIndexFactory to use to create Attribute Indices.\n+     * @param writerFactory            The WriterFactory to use to create Writers.\n+     * @param storageFactory           The StorageFactory to use to create Storage Adapters.\n+     * @param createExtensions         A Function that, given an instance of this class, will create the set of\n+     *                                 {@link SegmentContainerExtension}s to be associated with that instance.\n+     * @param executor                 An Executor that can be used to run async tasks.\n+     */\n+    DebugStreamSegmentContainer(int streamSegmentContainerId, ContainerConfig config, OperationLogFactory durableLogFactory,\n+                                ReadIndexFactory readIndexFactory, AttributeIndexFactory attributeIndexFactory,\n+                                WriterFactory writerFactory, StorageFactory storageFactory,\n+                                SegmentContainerFactory.CreateExtensions createExtensions, ScheduledExecutorService executor) {\n+        super(streamSegmentContainerId, config, durableLogFactory, readIndexFactory, attributeIndexFactory, writerFactory,\n+                storageFactory, createExtensions, executor);\n+        this.config = config;\n+    }\n+\n+    /**\n+     * Creates a segment with given properties.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5Mjk5NA=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODIxNDI0OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainer.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODowMDo1MVrOGzcRdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjowODowN1rOG2dPpg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5Mzc4MQ==", "bodyText": "Create a new method in MetadataStore.SegmentInfo and name it recoveredSegment where you hide away all these details. Then revert back the visibility of both MetadataStore.SegmentInfo and MetadataStore.SegmentInfo.serialize (They are in the same package as this so you don't need the extra public visibility).", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456593781", "createdAt": "2020-07-17T18:00:51Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainer.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.server.DebugSegmentContainer;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.time.Duration;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+@Slf4j\n+public class DebugStreamSegmentContainer extends StreamSegmentContainer implements DebugSegmentContainer {\n+    private static final Duration TIMEOUT = Duration.ofMinutes(1);\n+    private final ContainerConfig config;\n+\n+    /**\n+     * Creates a new instance of the StreamSegmentContainer class.\n+     *\n+     * @param streamSegmentContainerId The Id of the StreamSegmentContainer.\n+     * @param config                   The ContainerConfig to use for this StreamSegmentContainer.\n+     * @param durableLogFactory        The DurableLogFactory to use to create DurableLogs.\n+     * @param readIndexFactory         The ReadIndexFactory to use to create Read Indices.\n+     * @param attributeIndexFactory    The AttributeIndexFactory to use to create Attribute Indices.\n+     * @param writerFactory            The WriterFactory to use to create Writers.\n+     * @param storageFactory           The StorageFactory to use to create Storage Adapters.\n+     * @param createExtensions         A Function that, given an instance of this class, will create the set of\n+     *                                 {@link SegmentContainerExtension}s to be associated with that instance.\n+     * @param executor                 An Executor that can be used to run async tasks.\n+     */\n+    DebugStreamSegmentContainer(int streamSegmentContainerId, ContainerConfig config, OperationLogFactory durableLogFactory,\n+                                ReadIndexFactory readIndexFactory, AttributeIndexFactory attributeIndexFactory,\n+                                WriterFactory writerFactory, StorageFactory storageFactory,\n+                                SegmentContainerFactory.CreateExtensions createExtensions, ScheduledExecutorService executor) {\n+        super(streamSegmentContainerId, config, durableLogFactory, readIndexFactory, attributeIndexFactory, writerFactory,\n+                storageFactory, createExtensions, executor);\n+        this.config = config;\n+    }\n+\n+    /**\n+     * Creates a segment with given properties.\n+     * @param streamSegmentName         Name of the segment to be created.\n+     * @param length                    Length of the segment to be created.\n+     * @param isSealed                  Sealed status of the segment to be created.\n+     * @return                          A newly created segment.\n+     */\n+    @Override\n+    public CompletableFuture<Void> createStreamSegment(String streamSegmentName, long length, boolean isSealed) {\n+        StreamSegmentInformation segmentProp = StreamSegmentInformation.builder()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NTQzMA==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459755430", "createdAt": "2020-07-23T22:08:07Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainer.java", "diffHunk": "@@ -0,0 +1,74 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.server.DebugSegmentContainer;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.time.Duration;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+@Slf4j\n+public class DebugStreamSegmentContainer extends StreamSegmentContainer implements DebugSegmentContainer {\n+    private static final Duration TIMEOUT = Duration.ofMinutes(1);\n+    private final ContainerConfig config;\n+\n+    /**\n+     * Creates a new instance of the StreamSegmentContainer class.\n+     *\n+     * @param streamSegmentContainerId The Id of the StreamSegmentContainer.\n+     * @param config                   The ContainerConfig to use for this StreamSegmentContainer.\n+     * @param durableLogFactory        The DurableLogFactory to use to create DurableLogs.\n+     * @param readIndexFactory         The ReadIndexFactory to use to create Read Indices.\n+     * @param attributeIndexFactory    The AttributeIndexFactory to use to create Attribute Indices.\n+     * @param writerFactory            The WriterFactory to use to create Writers.\n+     * @param storageFactory           The StorageFactory to use to create Storage Adapters.\n+     * @param createExtensions         A Function that, given an instance of this class, will create the set of\n+     *                                 {@link SegmentContainerExtension}s to be associated with that instance.\n+     * @param executor                 An Executor that can be used to run async tasks.\n+     */\n+    DebugStreamSegmentContainer(int streamSegmentContainerId, ContainerConfig config, OperationLogFactory durableLogFactory,\n+                                ReadIndexFactory readIndexFactory, AttributeIndexFactory attributeIndexFactory,\n+                                WriterFactory writerFactory, StorageFactory storageFactory,\n+                                SegmentContainerFactory.CreateExtensions createExtensions, ScheduledExecutorService executor) {\n+        super(streamSegmentContainerId, config, durableLogFactory, readIndexFactory, attributeIndexFactory, writerFactory,\n+                storageFactory, createExtensions, executor);\n+        this.config = config;\n+    }\n+\n+    /**\n+     * Creates a segment with given properties.\n+     * @param streamSegmentName         Name of the segment to be created.\n+     * @param length                    Length of the segment to be created.\n+     * @param isSealed                  Sealed status of the segment to be created.\n+     * @return                          A newly created segment.\n+     */\n+    @Override\n+    public CompletableFuture<Void> createStreamSegment(String streamSegmentName, long length, boolean isSealed) {\n+        StreamSegmentInformation segmentProp = StreamSegmentInformation.builder()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5Mzc4MQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODIxNjc2OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/MetadataStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODowMTo0OFrOGzcTIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjowODoxM1rOG2dPzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5NDIwOQ==", "bodyText": "revert", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456594209", "createdAt": "2020-07-17T18:01:48Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/MetadataStore.java", "diffHunk": "@@ -685,7 +685,7 @@ void completeExceptionally(Throwable ex) {\n \n     @Data\n     @Builder\n-    protected static class SegmentInfo {\n+    public static class SegmentInfo {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NTQ2OQ==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459755469", "createdAt": "2020-07-23T22:08:13Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/MetadataStore.java", "diffHunk": "@@ -685,7 +685,7 @@ void completeExceptionally(Throwable ex) {\n \n     @Data\n     @Builder\n-    protected static class SegmentInfo {\n+    public static class SegmentInfo {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5NDIwOQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODIxODYyOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/MetadataStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODowMjoyOVrOGzcUUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjowODoyMFrOG2dP-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5NDUxMg==", "bodyText": "Use this in your DebugSegmentContainer above.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456594512", "createdAt": "2020-07-17T18:02:29Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/MetadataStore.java", "diffHunk": "@@ -704,8 +704,16 @@ static SegmentInfo newSegment(String name, Collection<AttributeUpdate> attribute\n                     .build();\n         }\n \n+        // createSegment in Metadata uses this method to get SegmentInfo\n+        static SegmentInfo recoveredSegment(SegmentProperties segmentProperties) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NTUxMw==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459755513", "createdAt": "2020-07-23T22:08:20Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/MetadataStore.java", "diffHunk": "@@ -704,8 +704,16 @@ static SegmentInfo newSegment(String name, Collection<AttributeUpdate> attribute\n                     .build();\n         }\n \n+        // createSegment in Metadata uses this method to get SegmentInfo\n+        static SegmentInfo recoveredSegment(SegmentProperties segmentProperties) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5NDUxMg=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODIyMDU0OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/MetadataStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODowMzowMlrOGzcVeA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjowODozMlrOG2dQUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5NDgwOA==", "bodyText": "This is incorrect. Please make sure you use the code you have in DebugStreamSegmentContainer instead of setting this here.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456594808", "createdAt": "2020-07-17T18:03:02Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/MetadataStore.java", "diffHunk": "@@ -704,8 +704,16 @@ static SegmentInfo newSegment(String name, Collection<AttributeUpdate> attribute\n                     .build();\n         }\n \n+        // createSegment in Metadata uses this method to get SegmentInfo\n+        static SegmentInfo recoveredSegment(SegmentProperties segmentProperties) {\n+            return builder()\n+                    .segmentId(ContainerMetadata.NO_STREAM_SEGMENT_ID)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NTYwMg==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459755602", "createdAt": "2020-07-23T22:08:32Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/MetadataStore.java", "diffHunk": "@@ -704,8 +704,16 @@ static SegmentInfo newSegment(String name, Collection<AttributeUpdate> attribute\n                     .build();\n         }\n \n+        // createSegment in Metadata uses this method to get SegmentInfo\n+        static SegmentInfo recoveredSegment(SegmentProperties segmentProperties) {\n+            return builder()\n+                    .segmentId(ContainerMetadata.NO_STREAM_SEGMENT_ID)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5NDgwOA=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODIyMDc4OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/MetadataStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODowMzowOFrOGzcVmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjowODozOVrOG2dQfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5NDg0MA==", "bodyText": "revert", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456594840", "createdAt": "2020-07-17T18:03:08Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/MetadataStore.java", "diffHunk": "@@ -704,8 +704,16 @@ static SegmentInfo newSegment(String name, Collection<AttributeUpdate> attribute\n                     .build();\n         }\n \n+        // createSegment in Metadata uses this method to get SegmentInfo\n+        static SegmentInfo recoveredSegment(SegmentProperties segmentProperties) {\n+            return builder()\n+                    .segmentId(ContainerMetadata.NO_STREAM_SEGMENT_ID)\n+                    .properties(segmentProperties)\n+                    .build();\n+        }\n+\n         @SneakyThrows(IOException.class)\n-        static ArrayView serialize(SegmentInfo state) {\n+        public static ArrayView serialize(SegmentInfo state) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NTY0NA==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459755644", "createdAt": "2020-07-23T22:08:39Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/MetadataStore.java", "diffHunk": "@@ -704,8 +704,16 @@ static SegmentInfo newSegment(String name, Collection<AttributeUpdate> attribute\n                     .build();\n         }\n \n+        // createSegment in Metadata uses this method to get SegmentInfo\n+        static SegmentInfo recoveredSegment(SegmentProperties segmentProperties) {\n+            return builder()\n+                    .segmentId(ContainerMetadata.NO_STREAM_SEGMENT_ID)\n+                    .properties(segmentProperties)\n+                    .build();\n+        }\n+\n         @SneakyThrows(IOException.class)\n-        static ArrayView serialize(SegmentInfo state) {\n+        public static ArrayView serialize(SegmentInfo state) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5NDg0MA=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODIyMzQ4OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ReadOnlySegmentContainerFactory.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODowNDowNFrOGzcXWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjowODo0M1rOG2dQmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5NTI4OA==", "bodyText": "throw new UnsupportedOperationException(\"DebugSegmentContainer not supported in ReadOnly mode.\")", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456595288", "createdAt": "2020-07-17T18:04:04Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ReadOnlySegmentContainerFactory.java", "diffHunk": "@@ -41,4 +42,9 @@ public SegmentContainer createStreamSegmentContainer(int containerId) {\n                 \"ReadOnly Containers can only have Id %s.\", READONLY_CONTAINER_ID);\n         return new ReadOnlySegmentContainer(this.storageFactory, this.executor);\n     }\n+\n+    @Override\n+    public DebugSegmentContainer createDebugStreamSegmentContainer(int containerId) {\n+        return null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NTY3NQ==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459755675", "createdAt": "2020-07-23T22:08:43Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/ReadOnlySegmentContainerFactory.java", "diffHunk": "@@ -41,4 +42,9 @@ public SegmentContainer createStreamSegmentContainer(int containerId) {\n                 \"ReadOnly Containers can only have Id %s.\", READONLY_CONTAINER_ID);\n         return new ReadOnlySegmentContainer(this.storageFactory, this.executor);\n     }\n+\n+    @Override\n+    public DebugSegmentContainer createDebugStreamSegmentContainer(int containerId) {\n+        return null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5NTI4OA=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODIzMTA4OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODowNjo0NlrOGzccRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjowODo1NFrOG2dQ3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5NjU1MA==", "bodyText": "You should never create a thread pool and keep it as a static variable. This pool will never shut down.\nI suggest removing this altogether from here and pass an Executor via your method calls whenever you need one.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456596550", "createdAt": "2020-07-17T18:06:46Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NTc0Mg==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459755742", "createdAt": "2020-07-23T22:08:54Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5NjU1MA=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODIzMTU2OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODowNjo1OVrOGzccsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjowOToyMFrOG2dRnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5NjY1OA==", "bodyText": "What's tier2?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456596658", "createdAt": "2020-07-17T18:06:59Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NTkzMg==", "bodyText": "Changed to storage.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459755932", "createdAt": "2020-07-23T22:09:20Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5NjY1OA=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODIzMjg4OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODowNzozMFrOGzcdiw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjowOTozNVrOG2dSAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5Njg3NQ==", "bodyText": "I don't see the point of this log message.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456596875", "createdAt": "2020-07-17T18:07:30Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NjAzNQ==", "bodyText": "Removed.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459756035", "createdAt": "2020-07-23T22:09:35Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5Njg3NQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODIzNjYyOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODowODo1NlrOGzcf-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjowOTo0NVrOG2dSOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5NzQ5OA==", "bodyText": "You can combine this line and the second half of line 76 into a single line after this if-else block. You already have a pointer to segmentsList so you can add it afterwards.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456597498", "createdAt": "2020-07-17T18:08:56Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NjA5MA==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459756090", "createdAt": "2020-07-23T22:09:45Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5NzQ5OA=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODIzNzM4OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODowOToxMVrOGzcgew==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjoxMDowNlrOG2dS0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5NzYyNw==", "bodyText": "This is not needed. Looks copy pasted from ExecutorServiceHelpers", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456597627", "createdAt": "2020-07-17T18:09:11Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 82}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NjI0Mw==", "bodyText": "Removed.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459756243", "createdAt": "2020-07-23T22:10:06Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5NzYyNw=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 82}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI0MDYzOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoxMDoyMlrOGzciiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMzo1NjoxNVrOG71ZHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5ODE1Mg==", "bodyText": "What is this worker and why is it in your test package? Isn't this doing the bulk of your recovery work?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456598152", "createdAt": "2020-07-17T18:10:22Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NjY2NA==", "bodyText": "It is only used by tests in integration and StreamSegmentTestBase. Though, I changed it's name.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459756664", "createdAt": "2020-07-23T22:11:05Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5ODE1Mg=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM5Mzk1MA==", "bodyText": "Should I move it to main? I think I will need to reuse it in CLI tool.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r465393950", "createdAt": "2020-08-04T23:56:15Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5ODE1Mg=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI0MTU5OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoxMDo0MVrOGzcjHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjoxMToxOFrOG2dUuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5ODMwMA==", "bodyText": "Instead of doing this, check for nulls in the constructor and throw an exception there.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456598300", "createdAt": "2020-07-17T18:10:41Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1NjcyOQ==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459756729", "createdAt": "2020-07-23T22:11:18Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5ODMwMA=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI0NDI5OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoxMTozNFrOGzcksA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjoxNDo1NVrOG2dZ5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5ODcwNA==", "bodyText": "join will not time out. You have a timeout defined somewhere in this class. Use get(timeoutMillis, TimeUnit.MILLISECONDS)", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456598704", "createdAt": "2020-07-17T18:11:34Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5ODc1Nw==", "bodyText": "And below", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456598757", "createdAt": "2020-07-17T18:11:40Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5ODcwNA=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1ODA1NQ==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459758055", "createdAt": "2020-07-23T22:14:55Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5ODcwNA=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 111}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI0ODkyOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoxMzowM1rOGzcnfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjoxNjoyMVrOG2db6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5OTQyMw==", "bodyText": "for better clarity, rename this to metadataSegments and make it a Set of String. There's no point in this being a TableKey here.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456599423", "createdAt": "2020-07-17T18:13:03Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<TableKey> segmentsInMD = new HashSet<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 114}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1ODU3MQ==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459758571", "createdAt": "2020-07-23T22:16:21Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<TableKey> segmentsInMD = new HashSet<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5OTQyMw=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 114}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI0OTkzOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoxMzoyNlrOGzcoJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjoxNjozMFrOG2dcOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5OTU5MQ==", "bodyText": "I suggest renaming this to storageSegments", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456599591", "createdAt": "2020-07-17T18:13:26Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1ODY1MA==", "bodyText": "Changed.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459758650", "createdAt": "2020-07-23T22:16:30Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjU5OTU5MQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI1Mjg0OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoxNDoyNlrOGzcp5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjoxODozNVrOG2dfDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMDAzOQ==", "bodyText": "Why do you attempt to create the segment if it already exists? getStreamSegmentInfo will return a value (not an exception) if it exists.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456600039", "createdAt": "2020-07-17T18:14:26Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<TableKey> segmentsInMD = new HashSet<>();\n+            it.forEachRemaining(k -> segmentsInMD.addAll(k.getEntries()), EXECUTOR_SERVICE).join();\n+\n+            for (SegmentProperties segment : segments) {\n+                long len = segment.getLength();\n+                boolean isSealed = segment.isSealed();\n+                String segmentName = segment.getName();\n+\n+                /*\n+                    1. segment exists in both metadata and storage, re-create it\n+                    2. segment only in metadata, delete\n+                    3. segment only in storage, re-create it\n+                 */\n+                segmentsInMD.remove(TableKey.unversioned(getTableKey(segmentName)));\n+                container.getStreamSegmentInfo(segment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            container.createStreamSegment(segmentName, len, isSealed)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1OTM3Mw==", "bodyText": "Now, deleting the segment to remove it from Tier1, and then recreating it using the length and sealed status from LTS.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459759373", "createdAt": "2020-07-23T22:18:35Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<TableKey> segmentsInMD = new HashSet<>();\n+            it.forEachRemaining(k -> segmentsInMD.addAll(k.getEntries()), EXECUTOR_SERVICE).join();\n+\n+            for (SegmentProperties segment : segments) {\n+                long len = segment.getLength();\n+                boolean isSealed = segment.isSealed();\n+                String segmentName = segment.getName();\n+\n+                /*\n+                    1. segment exists in both metadata and storage, re-create it\n+                    2. segment only in metadata, delete\n+                    3. segment only in storage, re-create it\n+                 */\n+                segmentsInMD.remove(TableKey.unversioned(getTableKey(segmentName)));\n+                container.getStreamSegmentInfo(segment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            container.createStreamSegment(segmentName, len, isSealed)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMDAzOQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 130}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI1NTA5OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoxNTowN1rOGzcrQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjoxODo1NFrOG2dffQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMDM4Nw==", "bodyText": "There's a method in Futures (exceptionallyComposeExpecting) that will simplify your code here.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456600387", "createdAt": "2020-07-17T18:15:07Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<TableKey> segmentsInMD = new HashSet<>();\n+            it.forEachRemaining(k -> segmentsInMD.addAll(k.getEntries()), EXECUTOR_SERVICE).join();\n+\n+            for (SegmentProperties segment : segments) {\n+                long len = segment.getLength();\n+                boolean isSealed = segment.isSealed();\n+                String segmentName = segment.getName();\n+\n+                /*\n+                    1. segment exists in both metadata and storage, re-create it\n+                    2. segment only in metadata, delete\n+                    3. segment only in storage, re-create it\n+                 */\n+                segmentsInMD.remove(TableKey.unversioned(getTableKey(segmentName)));\n+                container.getStreamSegmentInfo(segment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            container.createStreamSegment(segmentName, len, isSealed)\n+                                    .exceptionally(ex -> {\n+                                        log.error(\"Exception occurred while creating segment\", ex);\n+                                        return null;\n+                                    }).join();\n+                        })\n+                        .exceptionally(e -> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1OTQ4NQ==", "bodyText": "Using it now.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459759485", "createdAt": "2020-07-23T22:18:54Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<TableKey> segmentsInMD = new HashSet<>();\n+            it.forEachRemaining(k -> segmentsInMD.addAll(k.getEntries()), EXECUTOR_SERVICE).join();\n+\n+            for (SegmentProperties segment : segments) {\n+                long len = segment.getLength();\n+                boolean isSealed = segment.isSealed();\n+                String segmentName = segment.getName();\n+\n+                /*\n+                    1. segment exists in both metadata and storage, re-create it\n+                    2. segment only in metadata, delete\n+                    3. segment only in storage, re-create it\n+                 */\n+                segmentsInMD.remove(TableKey.unversioned(getTableKey(segmentName)));\n+                container.getStreamSegmentInfo(segment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            container.createStreamSegment(segmentName, len, isSealed)\n+                                    .exceptionally(ex -> {\n+                                        log.error(\"Exception occurred while creating segment\", ex);\n+                                        return null;\n+                                    }).join();\n+                        })\n+                        .exceptionally(e -> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMDM4Nw=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 136}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI1NTk5OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoxNToyNlrOGzcr1g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjoxOTowN1rOG2df1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMDUzNA==", "bodyText": "You should fail the whole process here instead of just logging a message.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456600534", "createdAt": "2020-07-17T18:15:26Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<TableKey> segmentsInMD = new HashSet<>();\n+            it.forEachRemaining(k -> segmentsInMD.addAll(k.getEntries()), EXECUTOR_SERVICE).join();\n+\n+            for (SegmentProperties segment : segments) {\n+                long len = segment.getLength();\n+                boolean isSealed = segment.isSealed();\n+                String segmentName = segment.getName();\n+\n+                /*\n+                    1. segment exists in both metadata and storage, re-create it\n+                    2. segment only in metadata, delete\n+                    3. segment only in storage, re-create it\n+                 */\n+                segmentsInMD.remove(TableKey.unversioned(getTableKey(segmentName)));\n+                container.getStreamSegmentInfo(segment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            container.createStreamSegment(segmentName, len, isSealed)\n+                                    .exceptionally(ex -> {\n+                                        log.error(\"Exception occurred while creating segment\", ex);\n+                                        return null;\n+                                    }).join();\n+                        })\n+                        .exceptionally(e -> {\n+                            log.error(\"Got an exception on getStreamSegmentInfo\", e);\n+                            if (Exceptions.unwrap(e) instanceof StreamSegmentNotExistsException) {\n+                                container.createStreamSegment(segmentName, len, isSealed)\n+                                        .exceptionally(ex -> {\n+                                            log.error(\"Exception occurred while creating segment\", ex);\n+                                            return null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 142}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1OTU3NQ==", "bodyText": "OK.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459759575", "createdAt": "2020-07-23T22:19:07Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<TableKey> segmentsInMD = new HashSet<>();\n+            it.forEachRemaining(k -> segmentsInMD.addAll(k.getEntries()), EXECUTOR_SERVICE).join();\n+\n+            for (SegmentProperties segment : segments) {\n+                long len = segment.getLength();\n+                boolean isSealed = segment.isSealed();\n+                String segmentName = segment.getName();\n+\n+                /*\n+                    1. segment exists in both metadata and storage, re-create it\n+                    2. segment only in metadata, delete\n+                    3. segment only in storage, re-create it\n+                 */\n+                segmentsInMD.remove(TableKey.unversioned(getTableKey(segmentName)));\n+                container.getStreamSegmentInfo(segment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            container.createStreamSegment(segmentName, len, isSealed)\n+                                    .exceptionally(ex -> {\n+                                        log.error(\"Exception occurred while creating segment\", ex);\n+                                        return null;\n+                                    }).join();\n+                        })\n+                        .exceptionally(e -> {\n+                            log.error(\"Got an exception on getStreamSegmentInfo\", e);\n+                            if (Exceptions.unwrap(e) instanceof StreamSegmentNotExistsException) {\n+                                container.createStreamSegment(segmentName, len, isSealed)\n+                                        .exceptionally(ex -> {\n+                                            log.error(\"Exception occurred while creating segment\", ex);\n+                                            return null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMDUzNA=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 142}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI1NjkzOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoxNTo0NlrOGzcsbg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjoxOToxNVrOG2dgDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMDY4Ng==", "bodyText": "Deleting segment '{}' ...", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456600686", "createdAt": "2020-07-17T18:15:46Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<TableKey> segmentsInMD = new HashSet<>();\n+            it.forEachRemaining(k -> segmentsInMD.addAll(k.getEntries()), EXECUTOR_SERVICE).join();\n+\n+            for (SegmentProperties segment : segments) {\n+                long len = segment.getLength();\n+                boolean isSealed = segment.isSealed();\n+                String segmentName = segment.getName();\n+\n+                /*\n+                    1. segment exists in both metadata and storage, re-create it\n+                    2. segment only in metadata, delete\n+                    3. segment only in storage, re-create it\n+                 */\n+                segmentsInMD.remove(TableKey.unversioned(getTableKey(segmentName)));\n+                container.getStreamSegmentInfo(segment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            container.createStreamSegment(segmentName, len, isSealed)\n+                                    .exceptionally(ex -> {\n+                                        log.error(\"Exception occurred while creating segment\", ex);\n+                                        return null;\n+                                    }).join();\n+                        })\n+                        .exceptionally(e -> {\n+                            log.error(\"Got an exception on getStreamSegmentInfo\", e);\n+                            if (Exceptions.unwrap(e) instanceof StreamSegmentNotExistsException) {\n+                                container.createStreamSegment(segmentName, len, isSealed)\n+                                        .exceptionally(ex -> {\n+                                            log.error(\"Exception occurred while creating segment\", ex);\n+                                            return null;\n+                                        }).join();\n+                            }\n+                            return null;\n+                        }).join();\n+            }\n+            for (TableKey k : segmentsInMD) {\n+                String segmentName = k.getKey().toString();\n+                log.info(\"Deleting segment : {} as it is not in storage\", segmentName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1OTYyOQ==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459759629", "createdAt": "2020-07-23T22:19:15Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<TableKey> segmentsInMD = new HashSet<>();\n+            it.forEachRemaining(k -> segmentsInMD.addAll(k.getEntries()), EXECUTOR_SERVICE).join();\n+\n+            for (SegmentProperties segment : segments) {\n+                long len = segment.getLength();\n+                boolean isSealed = segment.isSealed();\n+                String segmentName = segment.getName();\n+\n+                /*\n+                    1. segment exists in both metadata and storage, re-create it\n+                    2. segment only in metadata, delete\n+                    3. segment only in storage, re-create it\n+                 */\n+                segmentsInMD.remove(TableKey.unversioned(getTableKey(segmentName)));\n+                container.getStreamSegmentInfo(segment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            container.createStreamSegment(segmentName, len, isSealed)\n+                                    .exceptionally(ex -> {\n+                                        log.error(\"Exception occurred while creating segment\", ex);\n+                                        return null;\n+                                    }).join();\n+                        })\n+                        .exceptionally(e -> {\n+                            log.error(\"Got an exception on getStreamSegmentInfo\", e);\n+                            if (Exceptions.unwrap(e) instanceof StreamSegmentNotExistsException) {\n+                                container.createStreamSegment(segmentName, len, isSealed)\n+                                        .exceptionally(ex -> {\n+                                            log.error(\"Exception occurred while creating segment\", ex);\n+                                            return null;\n+                                        }).join();\n+                            }\n+                            return null;\n+                        }).join();\n+            }\n+            for (TableKey k : segmentsInMD) {\n+                String segmentName = k.getKey().toString();\n+                log.info(\"Deleting segment : {} as it is not in storage\", segmentName);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMDY4Ng=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 150}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI1NzU3OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoxNjowMFrOGzcsxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjoxOTo1OFrOG2dhMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMDc3NQ==", "bodyText": "Fail the whole process.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456600775", "createdAt": "2020-07-17T18:16:00Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<TableKey> segmentsInMD = new HashSet<>();\n+            it.forEachRemaining(k -> segmentsInMD.addAll(k.getEntries()), EXECUTOR_SERVICE).join();\n+\n+            for (SegmentProperties segment : segments) {\n+                long len = segment.getLength();\n+                boolean isSealed = segment.isSealed();\n+                String segmentName = segment.getName();\n+\n+                /*\n+                    1. segment exists in both metadata and storage, re-create it\n+                    2. segment only in metadata, delete\n+                    3. segment only in storage, re-create it\n+                 */\n+                segmentsInMD.remove(TableKey.unversioned(getTableKey(segmentName)));\n+                container.getStreamSegmentInfo(segment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            container.createStreamSegment(segmentName, len, isSealed)\n+                                    .exceptionally(ex -> {\n+                                        log.error(\"Exception occurred while creating segment\", ex);\n+                                        return null;\n+                                    }).join();\n+                        })\n+                        .exceptionally(e -> {\n+                            log.error(\"Got an exception on getStreamSegmentInfo\", e);\n+                            if (Exceptions.unwrap(e) instanceof StreamSegmentNotExistsException) {\n+                                container.createStreamSegment(segmentName, len, isSealed)\n+                                        .exceptionally(ex -> {\n+                                            log.error(\"Exception occurred while creating segment\", ex);\n+                                            return null;\n+                                        }).join();\n+                            }\n+                            return null;\n+                        }).join();\n+            }\n+            for (TableKey k : segmentsInMD) {\n+                String segmentName = k.getKey().toString();\n+                log.info(\"Deleting segment : {} as it is not in storage\", segmentName);\n+                try {\n+                    container.deleteStreamSegment(segmentName, TIMEOUT).join();\n+                } catch (Throwable e) {\n+                    log.error(\"Error while deleting the segment = {}\", segmentName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc1OTkyMA==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459759920", "createdAt": "2020-07-23T22:19:58Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<TableKey> segmentsInMD = new HashSet<>();\n+            it.forEachRemaining(k -> segmentsInMD.addAll(k.getEntries()), EXECUTOR_SERVICE).join();\n+\n+            for (SegmentProperties segment : segments) {\n+                long len = segment.getLength();\n+                boolean isSealed = segment.isSealed();\n+                String segmentName = segment.getName();\n+\n+                /*\n+                    1. segment exists in both metadata and storage, re-create it\n+                    2. segment only in metadata, delete\n+                    3. segment only in storage, re-create it\n+                 */\n+                segmentsInMD.remove(TableKey.unversioned(getTableKey(segmentName)));\n+                container.getStreamSegmentInfo(segment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            container.createStreamSegment(segmentName, len, isSealed)\n+                                    .exceptionally(ex -> {\n+                                        log.error(\"Exception occurred while creating segment\", ex);\n+                                        return null;\n+                                    }).join();\n+                        })\n+                        .exceptionally(e -> {\n+                            log.error(\"Got an exception on getStreamSegmentInfo\", e);\n+                            if (Exceptions.unwrap(e) instanceof StreamSegmentNotExistsException) {\n+                                container.createStreamSegment(segmentName, len, isSealed)\n+                                        .exceptionally(ex -> {\n+                                            log.error(\"Exception occurred while creating segment\", ex);\n+                                            return null;\n+                                        }).join();\n+                            }\n+                            return null;\n+                        }).join();\n+            }\n+            for (TableKey k : segmentsInMD) {\n+                String segmentName = k.getKey().toString();\n+                log.info(\"Deleting segment : {} as it is not in storage\", segmentName);\n+                try {\n+                    container.deleteStreamSegment(segmentName, TIMEOUT).join();\n+                } catch (Throwable e) {\n+                    log.error(\"Error while deleting the segment = {}\", segmentName);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMDc3NQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 154}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI1OTMzOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoxNjozNlrOGzctzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjoyMTowNFrOG2di3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMTAzOQ==", "bodyText": "You won't need this anymore if you implement my suggestion above.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456601039", "createdAt": "2020-07-17T18:16:36Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<TableKey> segmentsInMD = new HashSet<>();\n+            it.forEachRemaining(k -> segmentsInMD.addAll(k.getEntries()), EXECUTOR_SERVICE).join();\n+\n+            for (SegmentProperties segment : segments) {\n+                long len = segment.getLength();\n+                boolean isSealed = segment.isSealed();\n+                String segmentName = segment.getName();\n+\n+                /*\n+                    1. segment exists in both metadata and storage, re-create it\n+                    2. segment only in metadata, delete\n+                    3. segment only in storage, re-create it\n+                 */\n+                segmentsInMD.remove(TableKey.unversioned(getTableKey(segmentName)));\n+                container.getStreamSegmentInfo(segment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            container.createStreamSegment(segmentName, len, isSealed)\n+                                    .exceptionally(ex -> {\n+                                        log.error(\"Exception occurred while creating segment\", ex);\n+                                        return null;\n+                                    }).join();\n+                        })\n+                        .exceptionally(e -> {\n+                            log.error(\"Got an exception on getStreamSegmentInfo\", e);\n+                            if (Exceptions.unwrap(e) instanceof StreamSegmentNotExistsException) {\n+                                container.createStreamSegment(segmentName, len, isSealed)\n+                                        .exceptionally(ex -> {\n+                                            log.error(\"Exception occurred while creating segment\", ex);\n+                                            return null;\n+                                        }).join();\n+                            }\n+                            return null;\n+                        }).join();\n+            }\n+            for (TableKey k : segmentsInMD) {\n+                String segmentName = k.getKey().toString();\n+                log.info(\"Deleting segment : {} as it is not in storage\", segmentName);\n+                try {\n+                    container.deleteStreamSegment(segmentName, TIMEOUT).join();\n+                } catch (Throwable e) {\n+                    log.error(\"Error while deleting the segment = {}\", segmentName);\n+                }\n+            }\n+            log.info(\"Recovery done for container = {}\", containerId);\n+        }\n+    }\n+\n+    public static ArrayView getTableKey(String segmentName) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2MDM0OA==", "bodyText": "Yes. Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459760348", "createdAt": "2020-07-23T22:21:04Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<TableKey> segmentsInMD = new HashSet<>();\n+            it.forEachRemaining(k -> segmentsInMD.addAll(k.getEntries()), EXECUTOR_SERVICE).join();\n+\n+            for (SegmentProperties segment : segments) {\n+                long len = segment.getLength();\n+                boolean isSealed = segment.isSealed();\n+                String segmentName = segment.getName();\n+\n+                /*\n+                    1. segment exists in both metadata and storage, re-create it\n+                    2. segment only in metadata, delete\n+                    3. segment only in storage, re-create it\n+                 */\n+                segmentsInMD.remove(TableKey.unversioned(getTableKey(segmentName)));\n+                container.getStreamSegmentInfo(segment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            container.createStreamSegment(segmentName, len, isSealed)\n+                                    .exceptionally(ex -> {\n+                                        log.error(\"Exception occurred while creating segment\", ex);\n+                                        return null;\n+                                    }).join();\n+                        })\n+                        .exceptionally(e -> {\n+                            log.error(\"Got an exception on getStreamSegmentInfo\", e);\n+                            if (Exceptions.unwrap(e) instanceof StreamSegmentNotExistsException) {\n+                                container.createStreamSegment(segmentName, len, isSealed)\n+                                        .exceptionally(ex -> {\n+                                            log.error(\"Exception occurred while creating segment\", ex);\n+                                            return null;\n+                                        }).join();\n+                            }\n+                            return null;\n+                        }).join();\n+            }\n+            for (TableKey k : segmentsInMD) {\n+                String segmentName = k.getKey().toString();\n+                log.info(\"Deleting segment : {} as it is not in storage\", segmentName);\n+                try {\n+                    container.deleteStreamSegment(segmentName, TIMEOUT).join();\n+                } catch (Throwable e) {\n+                    log.error(\"Error while deleting the segment = {}\", segmentName);\n+                }\n+            }\n+            log.info(\"Recovery done for container = {}\", containerId);\n+        }\n+    }\n+\n+    public static ArrayView getTableKey(String segmentName) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMTAzOQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 161}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI2MTc2OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoxNzoyM1rOGzcvZA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjoyNToxMVrOG2do1w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMTQ0NA==", "bodyText": "Why is this static and public?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456601444", "createdAt": "2020-07-17T18:17:23Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<TableKey> segmentsInMD = new HashSet<>();\n+            it.forEachRemaining(k -> segmentsInMD.addAll(k.getEntries()), EXECUTOR_SERVICE).join();\n+\n+            for (SegmentProperties segment : segments) {\n+                long len = segment.getLength();\n+                boolean isSealed = segment.isSealed();\n+                String segmentName = segment.getName();\n+\n+                /*\n+                    1. segment exists in both metadata and storage, re-create it\n+                    2. segment only in metadata, delete\n+                    3. segment only in storage, re-create it\n+                 */\n+                segmentsInMD.remove(TableKey.unversioned(getTableKey(segmentName)));\n+                container.getStreamSegmentInfo(segment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            container.createStreamSegment(segmentName, len, isSealed)\n+                                    .exceptionally(ex -> {\n+                                        log.error(\"Exception occurred while creating segment\", ex);\n+                                        return null;\n+                                    }).join();\n+                        })\n+                        .exceptionally(e -> {\n+                            log.error(\"Got an exception on getStreamSegmentInfo\", e);\n+                            if (Exceptions.unwrap(e) instanceof StreamSegmentNotExistsException) {\n+                                container.createStreamSegment(segmentName, len, isSealed)\n+                                        .exceptionally(ex -> {\n+                                            log.error(\"Exception occurred while creating segment\", ex);\n+                                            return null;\n+                                        }).join();\n+                            }\n+                            return null;\n+                        }).join();\n+            }\n+            for (TableKey k : segmentsInMD) {\n+                String segmentName = k.getKey().toString();\n+                log.info(\"Deleting segment : {} as it is not in storage\", segmentName);\n+                try {\n+                    container.deleteStreamSegment(segmentName, TIMEOUT).join();\n+                } catch (Throwable e) {\n+                    log.error(\"Error while deleting the segment = {}\", segmentName);\n+                }\n+            }\n+            log.info(\"Recovery done for container = {}\", containerId);\n+        }\n+    }\n+\n+    public static ArrayView getTableKey(String segmentName) {\n+        return new ByteArraySegment(segmentName.getBytes(Charsets.UTF_8));\n+    }\n+\n+    /**\n+     * Deletes container-metadata segment and attribute index segment for the given container Id.\n+     * @param tier2         Long term storage to delete the segments from.\n+     * @param containerId   Id of the container for which the segments has to be deleted.\n+     */\n+    public static void deleteContainerMetadataSegments(Storage tier2, int containerId) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMjA0Mg==", "bodyText": "And what's tier2?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456602042", "createdAt": "2020-07-17T18:18:36Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<TableKey> segmentsInMD = new HashSet<>();\n+            it.forEachRemaining(k -> segmentsInMD.addAll(k.getEntries()), EXECUTOR_SERVICE).join();\n+\n+            for (SegmentProperties segment : segments) {\n+                long len = segment.getLength();\n+                boolean isSealed = segment.isSealed();\n+                String segmentName = segment.getName();\n+\n+                /*\n+                    1. segment exists in both metadata and storage, re-create it\n+                    2. segment only in metadata, delete\n+                    3. segment only in storage, re-create it\n+                 */\n+                segmentsInMD.remove(TableKey.unversioned(getTableKey(segmentName)));\n+                container.getStreamSegmentInfo(segment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            container.createStreamSegment(segmentName, len, isSealed)\n+                                    .exceptionally(ex -> {\n+                                        log.error(\"Exception occurred while creating segment\", ex);\n+                                        return null;\n+                                    }).join();\n+                        })\n+                        .exceptionally(e -> {\n+                            log.error(\"Got an exception on getStreamSegmentInfo\", e);\n+                            if (Exceptions.unwrap(e) instanceof StreamSegmentNotExistsException) {\n+                                container.createStreamSegment(segmentName, len, isSealed)\n+                                        .exceptionally(ex -> {\n+                                            log.error(\"Exception occurred while creating segment\", ex);\n+                                            return null;\n+                                        }).join();\n+                            }\n+                            return null;\n+                        }).join();\n+            }\n+            for (TableKey k : segmentsInMD) {\n+                String segmentName = k.getKey().toString();\n+                log.info(\"Deleting segment : {} as it is not in storage\", segmentName);\n+                try {\n+                    container.deleteStreamSegment(segmentName, TIMEOUT).join();\n+                } catch (Throwable e) {\n+                    log.error(\"Error while deleting the segment = {}\", segmentName);\n+                }\n+            }\n+            log.info(\"Recovery done for container = {}\", containerId);\n+        }\n+    }\n+\n+    public static ArrayView getTableKey(String segmentName) {\n+        return new ByteArraySegment(segmentName.getBytes(Charsets.UTF_8));\n+    }\n+\n+    /**\n+     * Deletes container-metadata segment and attribute index segment for the given container Id.\n+     * @param tier2         Long term storage to delete the segments from.\n+     * @param containerId   Id of the container for which the segments has to be deleted.\n+     */\n+    public static void deleteContainerMetadataSegments(Storage tier2, int containerId) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMTQ0NA=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2MTg3OQ==", "bodyText": "Made it private. But static, because static method recoverAllSegments uses it.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459761879", "createdAt": "2020-07-23T22:25:11Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<TableKey> segmentsInMD = new HashSet<>();\n+            it.forEachRemaining(k -> segmentsInMD.addAll(k.getEntries()), EXECUTOR_SERVICE).join();\n+\n+            for (SegmentProperties segment : segments) {\n+                long len = segment.getLength();\n+                boolean isSealed = segment.isSealed();\n+                String segmentName = segment.getName();\n+\n+                /*\n+                    1. segment exists in both metadata and storage, re-create it\n+                    2. segment only in metadata, delete\n+                    3. segment only in storage, re-create it\n+                 */\n+                segmentsInMD.remove(TableKey.unversioned(getTableKey(segmentName)));\n+                container.getStreamSegmentInfo(segment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            container.createStreamSegment(segmentName, len, isSealed)\n+                                    .exceptionally(ex -> {\n+                                        log.error(\"Exception occurred while creating segment\", ex);\n+                                        return null;\n+                                    }).join();\n+                        })\n+                        .exceptionally(e -> {\n+                            log.error(\"Got an exception on getStreamSegmentInfo\", e);\n+                            if (Exceptions.unwrap(e) instanceof StreamSegmentNotExistsException) {\n+                                container.createStreamSegment(segmentName, len, isSealed)\n+                                        .exceptionally(ex -> {\n+                                            log.error(\"Exception occurred while creating segment\", ex);\n+                                            return null;\n+                                        }).join();\n+                            }\n+                            return null;\n+                        }).join();\n+            }\n+            for (TableKey k : segmentsInMD) {\n+                String segmentName = k.getKey().toString();\n+                log.info(\"Deleting segment : {} as it is not in storage\", segmentName);\n+                try {\n+                    container.deleteStreamSegment(segmentName, TIMEOUT).join();\n+                } catch (Throwable e) {\n+                    log.error(\"Error while deleting the segment = {}\", segmentName);\n+                }\n+            }\n+            log.info(\"Recovery done for container = {}\", containerId);\n+        }\n+    }\n+\n+    public static ArrayView getTableKey(String segmentName) {\n+        return new ByteArraySegment(segmentName.getBytes(Charsets.UTF_8));\n+    }\n+\n+    /**\n+     * Deletes container-metadata segment and attribute index segment for the given container Id.\n+     * @param tier2         Long term storage to delete the segments from.\n+     * @param containerId   Id of the container for which the segments has to be deleted.\n+     */\n+    public static void deleteContainerMetadataSegments(Storage tier2, int containerId) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMTQ0NA=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 170}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI2NDE0OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoxODoxMFrOGzcw3w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjoyNToyN1rOG2dpLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMTgyMw==", "bodyText": "Do not hardcode this format here. Look in TableMetadataStore on how this is obtained.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456601823", "createdAt": "2020-07-17T18:18:10Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<TableKey> segmentsInMD = new HashSet<>();\n+            it.forEachRemaining(k -> segmentsInMD.addAll(k.getEntries()), EXECUTOR_SERVICE).join();\n+\n+            for (SegmentProperties segment : segments) {\n+                long len = segment.getLength();\n+                boolean isSealed = segment.isSealed();\n+                String segmentName = segment.getName();\n+\n+                /*\n+                    1. segment exists in both metadata and storage, re-create it\n+                    2. segment only in metadata, delete\n+                    3. segment only in storage, re-create it\n+                 */\n+                segmentsInMD.remove(TableKey.unversioned(getTableKey(segmentName)));\n+                container.getStreamSegmentInfo(segment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            container.createStreamSegment(segmentName, len, isSealed)\n+                                    .exceptionally(ex -> {\n+                                        log.error(\"Exception occurred while creating segment\", ex);\n+                                        return null;\n+                                    }).join();\n+                        })\n+                        .exceptionally(e -> {\n+                            log.error(\"Got an exception on getStreamSegmentInfo\", e);\n+                            if (Exceptions.unwrap(e) instanceof StreamSegmentNotExistsException) {\n+                                container.createStreamSegment(segmentName, len, isSealed)\n+                                        .exceptionally(ex -> {\n+                                            log.error(\"Exception occurred while creating segment\", ex);\n+                                            return null;\n+                                        }).join();\n+                            }\n+                            return null;\n+                        }).join();\n+            }\n+            for (TableKey k : segmentsInMD) {\n+                String segmentName = k.getKey().toString();\n+                log.info(\"Deleting segment : {} as it is not in storage\", segmentName);\n+                try {\n+                    container.deleteStreamSegment(segmentName, TIMEOUT).join();\n+                } catch (Throwable e) {\n+                    log.error(\"Error while deleting the segment = {}\", segmentName);\n+                }\n+            }\n+            log.info(\"Recovery done for container = {}\", containerId);\n+        }\n+    }\n+\n+    public static ArrayView getTableKey(String segmentName) {\n+        return new ByteArraySegment(segmentName.getBytes(Charsets.UTF_8));\n+    }\n+\n+    /**\n+     * Deletes container-metadata segment and attribute index segment for the given container Id.\n+     * @param tier2         Long term storage to delete the segments from.\n+     * @param containerId   Id of the container for which the segments has to be deleted.\n+     */\n+    public static void deleteContainerMetadataSegments(Storage tier2, int containerId) {\n+        deleteSegment(tier2, \"_system/containers/metadata_\" + containerId);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2MTk2Nw==", "bodyText": "Yes, done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459761967", "createdAt": "2020-07-23T22:25:27Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<TableKey> segmentsInMD = new HashSet<>();\n+            it.forEachRemaining(k -> segmentsInMD.addAll(k.getEntries()), EXECUTOR_SERVICE).join();\n+\n+            for (SegmentProperties segment : segments) {\n+                long len = segment.getLength();\n+                boolean isSealed = segment.isSealed();\n+                String segmentName = segment.getName();\n+\n+                /*\n+                    1. segment exists in both metadata and storage, re-create it\n+                    2. segment only in metadata, delete\n+                    3. segment only in storage, re-create it\n+                 */\n+                segmentsInMD.remove(TableKey.unversioned(getTableKey(segmentName)));\n+                container.getStreamSegmentInfo(segment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            container.createStreamSegment(segmentName, len, isSealed)\n+                                    .exceptionally(ex -> {\n+                                        log.error(\"Exception occurred while creating segment\", ex);\n+                                        return null;\n+                                    }).join();\n+                        })\n+                        .exceptionally(e -> {\n+                            log.error(\"Got an exception on getStreamSegmentInfo\", e);\n+                            if (Exceptions.unwrap(e) instanceof StreamSegmentNotExistsException) {\n+                                container.createStreamSegment(segmentName, len, isSealed)\n+                                        .exceptionally(ex -> {\n+                                            log.error(\"Exception occurred while creating segment\", ex);\n+                                            return null;\n+                                        }).join();\n+                            }\n+                            return null;\n+                        }).join();\n+            }\n+            for (TableKey k : segmentsInMD) {\n+                String segmentName = k.getKey().toString();\n+                log.info(\"Deleting segment : {} as it is not in storage\", segmentName);\n+                try {\n+                    container.deleteStreamSegment(segmentName, TIMEOUT).join();\n+                } catch (Throwable e) {\n+                    log.error(\"Error while deleting the segment = {}\", segmentName);\n+                }\n+            }\n+            log.info(\"Recovery done for container = {}\", containerId);\n+        }\n+    }\n+\n+    public static ArrayView getTableKey(String segmentName) {\n+        return new ByteArraySegment(segmentName.getBytes(Charsets.UTF_8));\n+    }\n+\n+    /**\n+     * Deletes container-metadata segment and attribute index segment for the given container Id.\n+     * @param tier2         Long term storage to delete the segments from.\n+     * @param containerId   Id of the container for which the segments has to be deleted.\n+     */\n+    public static void deleteContainerMetadataSegments(Storage tier2, int containerId) {\n+        deleteSegment(tier2, \"_system/containers/metadata_\" + containerId);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMTgyMw=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 171}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI2NTIzOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoxODoyOFrOGzcxfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjoyNjoxNVrOG2dqag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMTk4MQ==", "bodyText": "Same here. Why public and static?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456601981", "createdAt": "2020-07-17T18:18:28Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<TableKey> segmentsInMD = new HashSet<>();\n+            it.forEachRemaining(k -> segmentsInMD.addAll(k.getEntries()), EXECUTOR_SERVICE).join();\n+\n+            for (SegmentProperties segment : segments) {\n+                long len = segment.getLength();\n+                boolean isSealed = segment.isSealed();\n+                String segmentName = segment.getName();\n+\n+                /*\n+                    1. segment exists in both metadata and storage, re-create it\n+                    2. segment only in metadata, delete\n+                    3. segment only in storage, re-create it\n+                 */\n+                segmentsInMD.remove(TableKey.unversioned(getTableKey(segmentName)));\n+                container.getStreamSegmentInfo(segment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            container.createStreamSegment(segmentName, len, isSealed)\n+                                    .exceptionally(ex -> {\n+                                        log.error(\"Exception occurred while creating segment\", ex);\n+                                        return null;\n+                                    }).join();\n+                        })\n+                        .exceptionally(e -> {\n+                            log.error(\"Got an exception on getStreamSegmentInfo\", e);\n+                            if (Exceptions.unwrap(e) instanceof StreamSegmentNotExistsException) {\n+                                container.createStreamSegment(segmentName, len, isSealed)\n+                                        .exceptionally(ex -> {\n+                                            log.error(\"Exception occurred while creating segment\", ex);\n+                                            return null;\n+                                        }).join();\n+                            }\n+                            return null;\n+                        }).join();\n+            }\n+            for (TableKey k : segmentsInMD) {\n+                String segmentName = k.getKey().toString();\n+                log.info(\"Deleting segment : {} as it is not in storage\", segmentName);\n+                try {\n+                    container.deleteStreamSegment(segmentName, TIMEOUT).join();\n+                } catch (Throwable e) {\n+                    log.error(\"Error while deleting the segment = {}\", segmentName);\n+                }\n+            }\n+            log.info(\"Recovery done for container = {}\", containerId);\n+        }\n+    }\n+\n+    public static ArrayView getTableKey(String segmentName) {\n+        return new ByteArraySegment(segmentName.getBytes(Charsets.UTF_8));\n+    }\n+\n+    /**\n+     * Deletes container-metadata segment and attribute index segment for the given container Id.\n+     * @param tier2         Long term storage to delete the segments from.\n+     * @param containerId   Id of the container for which the segments has to be deleted.\n+     */\n+    public static void deleteContainerMetadataSegments(Storage tier2, int containerId) {\n+        deleteSegment(tier2, \"_system/containers/metadata_\" + containerId);\n+        deleteSegment(tier2, \"_system/containers/metadata_\" + containerId + \"$attributes.index\");\n+    }\n+\n+    /**\n+     * Deletes the segment with given segment name from the given long term storage.\n+     * @param tier2         Long term storage to delete the segment from.\n+     * @param segmentName   Name of the segment to be deleted.\n+     */\n+    public static void deleteSegment(Storage tier2, String segmentName) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 180}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2MjI4Mg==", "bodyText": "private now, but still static as it used by another static method.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459762282", "createdAt": "2020-07-23T22:26:15Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<TableKey> segmentsInMD = new HashSet<>();\n+            it.forEachRemaining(k -> segmentsInMD.addAll(k.getEntries()), EXECUTOR_SERVICE).join();\n+\n+            for (SegmentProperties segment : segments) {\n+                long len = segment.getLength();\n+                boolean isSealed = segment.isSealed();\n+                String segmentName = segment.getName();\n+\n+                /*\n+                    1. segment exists in both metadata and storage, re-create it\n+                    2. segment only in metadata, delete\n+                    3. segment only in storage, re-create it\n+                 */\n+                segmentsInMD.remove(TableKey.unversioned(getTableKey(segmentName)));\n+                container.getStreamSegmentInfo(segment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            container.createStreamSegment(segmentName, len, isSealed)\n+                                    .exceptionally(ex -> {\n+                                        log.error(\"Exception occurred while creating segment\", ex);\n+                                        return null;\n+                                    }).join();\n+                        })\n+                        .exceptionally(e -> {\n+                            log.error(\"Got an exception on getStreamSegmentInfo\", e);\n+                            if (Exceptions.unwrap(e) instanceof StreamSegmentNotExistsException) {\n+                                container.createStreamSegment(segmentName, len, isSealed)\n+                                        .exceptionally(ex -> {\n+                                            log.error(\"Exception occurred while creating segment\", ex);\n+                                            return null;\n+                                        }).join();\n+                            }\n+                            return null;\n+                        }).join();\n+            }\n+            for (TableKey k : segmentsInMD) {\n+                String segmentName = k.getKey().toString();\n+                log.info(\"Deleting segment : {} as it is not in storage\", segmentName);\n+                try {\n+                    container.deleteStreamSegment(segmentName, TIMEOUT).join();\n+                } catch (Throwable e) {\n+                    log.error(\"Error while deleting the segment = {}\", segmentName);\n+                }\n+            }\n+            log.info(\"Recovery done for container = {}\", containerId);\n+        }\n+    }\n+\n+    public static ArrayView getTableKey(String segmentName) {\n+        return new ByteArraySegment(segmentName.getBytes(Charsets.UTF_8));\n+    }\n+\n+    /**\n+     * Deletes container-metadata segment and attribute index segment for the given container Id.\n+     * @param tier2         Long term storage to delete the segments from.\n+     * @param containerId   Id of the container for which the segments has to be deleted.\n+     */\n+    public static void deleteContainerMetadataSegments(Storage tier2, int containerId) {\n+        deleteSegment(tier2, \"_system/containers/metadata_\" + containerId);\n+        deleteSegment(tier2, \"_system/containers/metadata_\" + containerId + \"$attributes.index\");\n+    }\n+\n+    /**\n+     * Deletes the segment with given segment name from the given long term storage.\n+     * @param tier2         Long term storage to delete the segment from.\n+     * @param segmentName   Name of the segment to be deleted.\n+     */\n+    public static void deleteSegment(Storage tier2, String segmentName) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMTk4MQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 180}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI2NjIyOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoxODo0NlrOGzcyIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QwNjo0MDo1OFrOG3VeRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMjE0Ng==", "bodyText": "Let the exception bubble up", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456602146", "createdAt": "2020-07-17T18:18:46Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<TableKey> segmentsInMD = new HashSet<>();\n+            it.forEachRemaining(k -> segmentsInMD.addAll(k.getEntries()), EXECUTOR_SERVICE).join();\n+\n+            for (SegmentProperties segment : segments) {\n+                long len = segment.getLength();\n+                boolean isSealed = segment.isSealed();\n+                String segmentName = segment.getName();\n+\n+                /*\n+                    1. segment exists in both metadata and storage, re-create it\n+                    2. segment only in metadata, delete\n+                    3. segment only in storage, re-create it\n+                 */\n+                segmentsInMD.remove(TableKey.unversioned(getTableKey(segmentName)));\n+                container.getStreamSegmentInfo(segment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            container.createStreamSegment(segmentName, len, isSealed)\n+                                    .exceptionally(ex -> {\n+                                        log.error(\"Exception occurred while creating segment\", ex);\n+                                        return null;\n+                                    }).join();\n+                        })\n+                        .exceptionally(e -> {\n+                            log.error(\"Got an exception on getStreamSegmentInfo\", e);\n+                            if (Exceptions.unwrap(e) instanceof StreamSegmentNotExistsException) {\n+                                container.createStreamSegment(segmentName, len, isSealed)\n+                                        .exceptionally(ex -> {\n+                                            log.error(\"Exception occurred while creating segment\", ex);\n+                                            return null;\n+                                        }).join();\n+                            }\n+                            return null;\n+                        }).join();\n+            }\n+            for (TableKey k : segmentsInMD) {\n+                String segmentName = k.getKey().toString();\n+                log.info(\"Deleting segment : {} as it is not in storage\", segmentName);\n+                try {\n+                    container.deleteStreamSegment(segmentName, TIMEOUT).join();\n+                } catch (Throwable e) {\n+                    log.error(\"Error while deleting the segment = {}\", segmentName);\n+                }\n+            }\n+            log.info(\"Recovery done for container = {}\", containerId);\n+        }\n+    }\n+\n+    public static ArrayView getTableKey(String segmentName) {\n+        return new ByteArraySegment(segmentName.getBytes(Charsets.UTF_8));\n+    }\n+\n+    /**\n+     * Deletes container-metadata segment and attribute index segment for the given container Id.\n+     * @param tier2         Long term storage to delete the segments from.\n+     * @param containerId   Id of the container for which the segments has to be deleted.\n+     */\n+    public static void deleteContainerMetadataSegments(Storage tier2, int containerId) {\n+        deleteSegment(tier2, \"_system/containers/metadata_\" + containerId);\n+        deleteSegment(tier2, \"_system/containers/metadata_\" + containerId + \"$attributes.index\");\n+    }\n+\n+    /**\n+     * Deletes the segment with given segment name from the given long term storage.\n+     * @param tier2         Long term storage to delete the segment from.\n+     * @param segmentName   Name of the segment to be deleted.\n+     */\n+    public static void deleteSegment(Storage tier2, String segmentName) {\n+        try {\n+            SegmentHandle segmentHandle = tier2.openWrite(segmentName).join();\n+            tier2.delete(segmentHandle, TIMEOUT).join();\n+        } catch (Throwable e) {\n+            log.info(\"Error while deleting segment: {}\", segmentName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 185}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2Mjg5NQ==", "bodyText": "It makes the test fail all the times, if I let it bubble up. The reason is most of the time, container metadata segments for some containers don't exist while I am trying to delete them.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459762895", "createdAt": "2020-07-23T22:27:56Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<TableKey> segmentsInMD = new HashSet<>();\n+            it.forEachRemaining(k -> segmentsInMD.addAll(k.getEntries()), EXECUTOR_SERVICE).join();\n+\n+            for (SegmentProperties segment : segments) {\n+                long len = segment.getLength();\n+                boolean isSealed = segment.isSealed();\n+                String segmentName = segment.getName();\n+\n+                /*\n+                    1. segment exists in both metadata and storage, re-create it\n+                    2. segment only in metadata, delete\n+                    3. segment only in storage, re-create it\n+                 */\n+                segmentsInMD.remove(TableKey.unversioned(getTableKey(segmentName)));\n+                container.getStreamSegmentInfo(segment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            container.createStreamSegment(segmentName, len, isSealed)\n+                                    .exceptionally(ex -> {\n+                                        log.error(\"Exception occurred while creating segment\", ex);\n+                                        return null;\n+                                    }).join();\n+                        })\n+                        .exceptionally(e -> {\n+                            log.error(\"Got an exception on getStreamSegmentInfo\", e);\n+                            if (Exceptions.unwrap(e) instanceof StreamSegmentNotExistsException) {\n+                                container.createStreamSegment(segmentName, len, isSealed)\n+                                        .exceptionally(ex -> {\n+                                            log.error(\"Exception occurred while creating segment\", ex);\n+                                            return null;\n+                                        }).join();\n+                            }\n+                            return null;\n+                        }).join();\n+            }\n+            for (TableKey k : segmentsInMD) {\n+                String segmentName = k.getKey().toString();\n+                log.info(\"Deleting segment : {} as it is not in storage\", segmentName);\n+                try {\n+                    container.deleteStreamSegment(segmentName, TIMEOUT).join();\n+                } catch (Throwable e) {\n+                    log.error(\"Error while deleting the segment = {}\", segmentName);\n+                }\n+            }\n+            log.info(\"Recovery done for container = {}\", containerId);\n+        }\n+    }\n+\n+    public static ArrayView getTableKey(String segmentName) {\n+        return new ByteArraySegment(segmentName.getBytes(Charsets.UTF_8));\n+    }\n+\n+    /**\n+     * Deletes container-metadata segment and attribute index segment for the given container Id.\n+     * @param tier2         Long term storage to delete the segments from.\n+     * @param containerId   Id of the container for which the segments has to be deleted.\n+     */\n+    public static void deleteContainerMetadataSegments(Storage tier2, int containerId) {\n+        deleteSegment(tier2, \"_system/containers/metadata_\" + containerId);\n+        deleteSegment(tier2, \"_system/containers/metadata_\" + containerId + \"$attributes.index\");\n+    }\n+\n+    /**\n+     * Deletes the segment with given segment name from the given long term storage.\n+     * @param tier2         Long term storage to delete the segment from.\n+     * @param segmentName   Name of the segment to be deleted.\n+     */\n+    public static void deleteSegment(Storage tier2, String segmentName) {\n+        try {\n+            SegmentHandle segmentHandle = tier2.openWrite(segmentName).join();\n+            tier2.delete(segmentHandle, TIMEOUT).join();\n+        } catch (Throwable e) {\n+            log.info(\"Error while deleting segment: {}\", segmentName);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMjE0Ng=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 185}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDY3NjY3OQ==", "bodyText": "It's the only place where I am not letting the exception bubble up.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r460676679", "createdAt": "2020-07-27T06:40:58Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,188 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Charsets;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.ScheduledThreadPoolExecutor;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+    private static final ScheduledExecutorService EXECUTOR_SERVICE = createExecutorService(10);\n+\n+    /**\n+     * Lists all segments from a given long term storage.\n+     * @param tier2             Long term storage.\n+     * @param containerCount    Total number of segment containers.\n+     * @return                  A map of lists containing segments by container Ids.\n+     * @throws                  IOException in case of exception during the execution.\n+     */\n+    public static Map<Integer, List<SegmentProperties>> listAllSegments(Storage tier2, int containerCount) throws IOException {\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        Map<Integer, List<SegmentProperties>> segmentToContainers = new HashMap<Integer, List<SegmentProperties>>();\n+        log.info(\"Generating container files with the segments they own...\");\n+        Iterator<SegmentProperties> it = tier2.listSegments();\n+        if (it == null) {\n+            return segmentToContainers;\n+        }\n+        // Iterate through all segments. Put each one of them in its respective list.\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            List<SegmentProperties> segmentsList = segmentToContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(curr);\n+                segmentToContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentToContainers.get(containerId).add(curr);\n+            }\n+        }\n+        return segmentToContainers;\n+    }\n+\n+    public static ScheduledExecutorService createExecutorService(int threadPoolSize) {\n+        ScheduledThreadPoolExecutor es = new ScheduledThreadPoolExecutor(threadPoolSize);\n+        es.setContinueExistingPeriodicTasksAfterShutdownPolicy(false);\n+        es.setExecuteExistingDelayedTasksAfterShutdownPolicy(false);\n+        es.setRemoveOnCancelPolicy(true);\n+        return es;\n+    }\n+\n+     /**\n+     * Creates all segments given in the list with the given DebugStreamSegmentContainer.\n+     */\n+     public static class Worker implements Runnable {\n+        private final int containerId;\n+        private final DebugStreamSegmentContainer container;\n+        private final List<SegmentProperties> segments;\n+        public Worker(DebugStreamSegmentContainer container, List<SegmentProperties> segments) {\n+            this.container = container;\n+            this.containerId = container.getId();\n+            this.segments = segments;\n+        }\n+\n+        @Override\n+        public void run() {\n+            if (segments == null) {\n+                return;\n+            }\n+            log.info(\"Recovery started for container = {}\", containerId);\n+            ContainerTableExtension ext = container.getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(containerId),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).join();\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<TableKey> segmentsInMD = new HashSet<>();\n+            it.forEachRemaining(k -> segmentsInMD.addAll(k.getEntries()), EXECUTOR_SERVICE).join();\n+\n+            for (SegmentProperties segment : segments) {\n+                long len = segment.getLength();\n+                boolean isSealed = segment.isSealed();\n+                String segmentName = segment.getName();\n+\n+                /*\n+                    1. segment exists in both metadata and storage, re-create it\n+                    2. segment only in metadata, delete\n+                    3. segment only in storage, re-create it\n+                 */\n+                segmentsInMD.remove(TableKey.unversioned(getTableKey(segmentName)));\n+                container.getStreamSegmentInfo(segment.getName(), TIMEOUT)\n+                        .thenAccept(e -> {\n+                            container.createStreamSegment(segmentName, len, isSealed)\n+                                    .exceptionally(ex -> {\n+                                        log.error(\"Exception occurred while creating segment\", ex);\n+                                        return null;\n+                                    }).join();\n+                        })\n+                        .exceptionally(e -> {\n+                            log.error(\"Got an exception on getStreamSegmentInfo\", e);\n+                            if (Exceptions.unwrap(e) instanceof StreamSegmentNotExistsException) {\n+                                container.createStreamSegment(segmentName, len, isSealed)\n+                                        .exceptionally(ex -> {\n+                                            log.error(\"Exception occurred while creating segment\", ex);\n+                                            return null;\n+                                        }).join();\n+                            }\n+                            return null;\n+                        }).join();\n+            }\n+            for (TableKey k : segmentsInMD) {\n+                String segmentName = k.getKey().toString();\n+                log.info(\"Deleting segment : {} as it is not in storage\", segmentName);\n+                try {\n+                    container.deleteStreamSegment(segmentName, TIMEOUT).join();\n+                } catch (Throwable e) {\n+                    log.error(\"Error while deleting the segment = {}\", segmentName);\n+                }\n+            }\n+            log.info(\"Recovery done for container = {}\", containerId);\n+        }\n+    }\n+\n+    public static ArrayView getTableKey(String segmentName) {\n+        return new ByteArraySegment(segmentName.getBytes(Charsets.UTF_8));\n+    }\n+\n+    /**\n+     * Deletes container-metadata segment and attribute index segment for the given container Id.\n+     * @param tier2         Long term storage to delete the segments from.\n+     * @param containerId   Id of the container for which the segments has to be deleted.\n+     */\n+    public static void deleteContainerMetadataSegments(Storage tier2, int containerId) {\n+        deleteSegment(tier2, \"_system/containers/metadata_\" + containerId);\n+        deleteSegment(tier2, \"_system/containers/metadata_\" + containerId + \"$attributes.index\");\n+    }\n+\n+    /**\n+     * Deletes the segment with given segment name from the given long term storage.\n+     * @param tier2         Long term storage to delete the segment from.\n+     * @param segmentName   Name of the segment to be deleted.\n+     */\n+    public static void deleteSegment(Storage tier2, String segmentName) {\n+        try {\n+            SegmentHandle segmentHandle = tier2.openWrite(segmentName).join();\n+            tier2.delete(segmentHandle, TIMEOUT).join();\n+        } catch (Throwable e) {\n+            log.info(\"Error while deleting segment: {}\", segmentName);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMjE0Ng=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 185}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI2NjkyOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentContainerRegistryTests.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoxOTowMlrOGzcylw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjoyODowOFrOG2dtGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMjI2Mw==", "bodyText": "throw unsupported", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456602263", "createdAt": "2020-07-17T18:19:02Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentContainerRegistryTests.java", "diffHunk": "@@ -249,6 +250,11 @@ public void testStartAlreadyRunning() throws Exception {\n             this.startReleaseSignal = startReleaseSignal;\n         }\n \n+        @Override\n+        public DebugSegmentContainer createDebugStreamSegmentContainer(int containerId) {\n+            return null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2Mjk2OA==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459762968", "createdAt": "2020-07-23T22:28:08Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentContainerRegistryTests.java", "diffHunk": "@@ -249,6 +250,11 @@ public void testStartAlreadyRunning() throws Exception {\n             this.startReleaseSignal = startReleaseSignal;\n         }\n \n+        @Override\n+        public DebugSegmentContainer createDebugStreamSegmentContainer(int containerId) {\n+            return null;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMjI2Mw=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI3MjEzOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoyMDo0OVrOGzc1tA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QwNjoxNjoyMFrOG3U8MA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMzA2MA==", "bodyText": "If you rename the parent method, remember to rename this test too.\nAlso, it's a good habit to describe in the test's Javadoc what it's doing.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456603060", "createdAt": "2020-07-17T18:20:49Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "diffHunk": "@@ -0,0 +1,348 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.SyncStorage;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.mocks.InMemoryDurableDataLogFactory;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorage;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorageFactory;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import lombok.Cleanup;\n+import lombok.val;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+/**\n+ * Tests for DebugStreamSegmentContainer class.\n+ */\n+public class DebugStreamSegmentContainerTests extends ThreadPooledTestSuite {\n+    private static final int MIN_SEGMENT_LENGTH = 0; // Used in randomly generating the length for a segment\n+    private static final int MAX_SEGMENT_LENGTH = 10100; // Used in randomly generating the length for a segment\n+    private static final int CONTAINER_ID = 1234567;\n+    private static final int EXPECTED_PINNED_SEGMENT_COUNT = 1;\n+    private static final int MAX_DATA_LOG_APPEND_SIZE = 100 * 1024;\n+    private static final int TEST_TIMEOUT_MILLIS = 10 * 1000;\n+    private static final Duration TIMEOUT = Duration.ofMillis(TEST_TIMEOUT_MILLIS);\n+    private static final Random RANDOM = new Random(1234);\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, 10 * 60)\n+            .build();\n+\n+    private static final DurableLogConfig DEFAULT_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10 * 1024 * 1024L)\n+            .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+            .build();\n+\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig DEFAULT_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+            .build();\n+    private static final ContainerConfig CONTAINER_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+            .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 200 + EXPECTED_PINNED_SEGMENT_COUNT)\n+            .build();\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+\n+    @Rule\n+    public Timeout globalTimeout = Timeout.millis(TEST_TIMEOUT_MILLIS);\n+\n+    /**\n+     * Tests the ability to create Segments.\n+     */\n+    @Test\n+    public void testCreateStreamSegment() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDY2Nzk1Mg==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r460667952", "createdAt": "2020-07-27T06:16:20Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "diffHunk": "@@ -0,0 +1,348 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.SyncStorage;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.mocks.InMemoryDurableDataLogFactory;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorage;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorageFactory;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import lombok.Cleanup;\n+import lombok.val;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+/**\n+ * Tests for DebugStreamSegmentContainer class.\n+ */\n+public class DebugStreamSegmentContainerTests extends ThreadPooledTestSuite {\n+    private static final int MIN_SEGMENT_LENGTH = 0; // Used in randomly generating the length for a segment\n+    private static final int MAX_SEGMENT_LENGTH = 10100; // Used in randomly generating the length for a segment\n+    private static final int CONTAINER_ID = 1234567;\n+    private static final int EXPECTED_PINNED_SEGMENT_COUNT = 1;\n+    private static final int MAX_DATA_LOG_APPEND_SIZE = 100 * 1024;\n+    private static final int TEST_TIMEOUT_MILLIS = 10 * 1000;\n+    private static final Duration TIMEOUT = Duration.ofMillis(TEST_TIMEOUT_MILLIS);\n+    private static final Random RANDOM = new Random(1234);\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, 10 * 60)\n+            .build();\n+\n+    private static final DurableLogConfig DEFAULT_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10 * 1024 * 1024L)\n+            .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+            .build();\n+\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig DEFAULT_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+            .build();\n+    private static final ContainerConfig CONTAINER_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+            .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 200 + EXPECTED_PINNED_SEGMENT_COUNT)\n+            .build();\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+\n+    @Rule\n+    public Timeout globalTimeout = Timeout.millis(TEST_TIMEOUT_MILLIS);\n+\n+    /**\n+     * Tests the ability to create Segments.\n+     */\n+    @Test\n+    public void testCreateStreamSegment() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMzA2MA=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 128}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI3NzQxOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoyMjozOVrOGzc46w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjoyODo0M1rOG2dt5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMzg4Mw==", "bodyText": "If you do this, the original exception will never show up in your test result. Remove this try-catch and let the exception bubble up.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456603883", "createdAt": "2020-07-17T18:22:39Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "diffHunk": "@@ -0,0 +1,348 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.SyncStorage;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.mocks.InMemoryDurableDataLogFactory;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorage;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorageFactory;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import lombok.Cleanup;\n+import lombok.val;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+/**\n+ * Tests for DebugStreamSegmentContainer class.\n+ */\n+public class DebugStreamSegmentContainerTests extends ThreadPooledTestSuite {\n+    private static final int MIN_SEGMENT_LENGTH = 0; // Used in randomly generating the length for a segment\n+    private static final int MAX_SEGMENT_LENGTH = 10100; // Used in randomly generating the length for a segment\n+    private static final int CONTAINER_ID = 1234567;\n+    private static final int EXPECTED_PINNED_SEGMENT_COUNT = 1;\n+    private static final int MAX_DATA_LOG_APPEND_SIZE = 100 * 1024;\n+    private static final int TEST_TIMEOUT_MILLIS = 10 * 1000;\n+    private static final Duration TIMEOUT = Duration.ofMillis(TEST_TIMEOUT_MILLIS);\n+    private static final Random RANDOM = new Random(1234);\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, 10 * 60)\n+            .build();\n+\n+    private static final DurableLogConfig DEFAULT_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10 * 1024 * 1024L)\n+            .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+            .build();\n+\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig DEFAULT_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+            .build();\n+    private static final ContainerConfig CONTAINER_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+            .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 200 + EXPECTED_PINNED_SEGMENT_COUNT)\n+            .build();\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+\n+    @Rule\n+    public Timeout globalTimeout = Timeout.millis(TEST_TIMEOUT_MILLIS);\n+\n+    /**\n+     * Tests the ability to create Segments.\n+     */\n+    @Test\n+    public void testCreateStreamSegment() {\n+        int maxSegmentCount = 100;\n+        final int createdSegmentCount = maxSegmentCount * 2;\n+\n+        // Sets up dataLogFactory, readIndexFactory, attributeIndexFactory etc for the DebugSegmentContainer.\n+        @Cleanup\n+        TestContext context = createContext();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DEFAULT_DURABLE_LOG_CONFIG, context.dataLogFactory, executorService());\n+        // Starts a DebugSegmentContainer.\n+        @Cleanup\n+        MetadataCleanupContainer localContainer = new MetadataCleanupContainer(CONTAINER_ID, CONTAINER_CONFIG, localDurableLogFactory,\n+                context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, context.storageFactory,\n+                context.getDefaultExtensions(), executorService());\n+        localContainer.startAsync().awaitRunning();\n+\n+        // Record details(name, length & sealed status) of each segment to be created.\n+        ArrayList<String> segments = new ArrayList<>();\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        long[] segmentLengths = new long[createdSegmentCount];\n+        boolean[] segmentSealedStatus = new boolean[createdSegmentCount];\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            segmentLengths[i] = MIN_SEGMENT_LENGTH + RANDOM.nextInt(MAX_SEGMENT_LENGTH - MIN_SEGMENT_LENGTH);\n+            segmentSealedStatus[i] = RANDOM.nextBoolean();\n+            String name = \"Segment_\" + i;\n+            segments.add(name);\n+            futures.add(localContainer.createStreamSegment(name, segmentLengths[i], segmentSealedStatus[i]));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        // Verify the Segments are still there with their length & sealed status.\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            SegmentProperties props = localContainer.getStreamSegmentInfo(segments.get(i), TIMEOUT).join();\n+            Assert.assertEquals(\"Segment length mismatch \", segmentLengths[i], props.getLength());\n+            Assert.assertEquals(\"Segment sealed status mismatch\", segmentSealedStatus[i], props.isSealed());\n+        }\n+        localContainer.stopAsync().awaitTerminated();\n+    }\n+\n+    /**\n+     * Use a storage instance to create segments. List the segments from the storage and recreate them.\n+     */\n+    @Test\n+    public void testEndToEnd() {\n+        // Segments are mapped to four different containers.\n+        // DebugSegmentContainer for each container Id is created and segments belonging to that container are recovered.\n+        int containerCount = 4;\n+        int segmentsToCreateCount = 50;\n+\n+        // Create a storage.\n+        @Cleanup\n+        val baseStorage = new InMemoryStorage();\n+        @Cleanup\n+        val s = new RollingStorage(baseStorage, new SegmentRollingPolicy(1));\n+        s.initialize(1);\n+\n+        // Record details(name, container Id & sealed status) of each segment to be created.\n+        Set<String> sealedSegments = new HashSet<>();\n+        byte[] data = \"data\".getBytes();\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        int[] segmentsCountByContainer = new int[containerCount];\n+        Map<Integer, ArrayList<String>> segmentByContainers = new HashMap<>();\n+\n+        // Create segments and get their container Ids, sealed status and names to verify.\n+        for (int i = 0; i < segmentsToCreateCount; i++) {\n+            String segmentName = \"segment-\" + RANDOM.nextInt();\n+\n+            // Count segments by each container Id.\n+            segmentsCountByContainer[segToConMapper.getContainerId(segmentName)]++;\n+\n+            // Use segmentName to map to different containers.\n+            int containerId = segToConMapper.getContainerId(segmentName);\n+            ArrayList<String> segmentsList = segmentByContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(segmentName);\n+                segmentByContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentByContainers.get(containerId).add(segmentName);\n+            }\n+\n+            // Create segments, write data and randomly seal some of them.\n+            try {\n+                val wh1 = s.create(segmentName);\n+                // Write data.\n+                s.write(wh1, 0, new ByteArrayInputStream(data), data.length);\n+                if (RANDOM.nextInt(2) == 1) {\n+                    s.seal(wh1);\n+                    sealedSegments.add(segmentName);\n+                }\n+            } catch (StreamSegmentException e) {\n+                Assert.fail(\"Exception occurred while test execution.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 218}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2MzE3Mg==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459763172", "createdAt": "2020-07-23T22:28:43Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "diffHunk": "@@ -0,0 +1,348 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.SyncStorage;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.mocks.InMemoryDurableDataLogFactory;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorage;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorageFactory;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import lombok.Cleanup;\n+import lombok.val;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+/**\n+ * Tests for DebugStreamSegmentContainer class.\n+ */\n+public class DebugStreamSegmentContainerTests extends ThreadPooledTestSuite {\n+    private static final int MIN_SEGMENT_LENGTH = 0; // Used in randomly generating the length for a segment\n+    private static final int MAX_SEGMENT_LENGTH = 10100; // Used in randomly generating the length for a segment\n+    private static final int CONTAINER_ID = 1234567;\n+    private static final int EXPECTED_PINNED_SEGMENT_COUNT = 1;\n+    private static final int MAX_DATA_LOG_APPEND_SIZE = 100 * 1024;\n+    private static final int TEST_TIMEOUT_MILLIS = 10 * 1000;\n+    private static final Duration TIMEOUT = Duration.ofMillis(TEST_TIMEOUT_MILLIS);\n+    private static final Random RANDOM = new Random(1234);\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, 10 * 60)\n+            .build();\n+\n+    private static final DurableLogConfig DEFAULT_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10 * 1024 * 1024L)\n+            .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+            .build();\n+\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig DEFAULT_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+            .build();\n+    private static final ContainerConfig CONTAINER_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+            .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 200 + EXPECTED_PINNED_SEGMENT_COUNT)\n+            .build();\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+\n+    @Rule\n+    public Timeout globalTimeout = Timeout.millis(TEST_TIMEOUT_MILLIS);\n+\n+    /**\n+     * Tests the ability to create Segments.\n+     */\n+    @Test\n+    public void testCreateStreamSegment() {\n+        int maxSegmentCount = 100;\n+        final int createdSegmentCount = maxSegmentCount * 2;\n+\n+        // Sets up dataLogFactory, readIndexFactory, attributeIndexFactory etc for the DebugSegmentContainer.\n+        @Cleanup\n+        TestContext context = createContext();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DEFAULT_DURABLE_LOG_CONFIG, context.dataLogFactory, executorService());\n+        // Starts a DebugSegmentContainer.\n+        @Cleanup\n+        MetadataCleanupContainer localContainer = new MetadataCleanupContainer(CONTAINER_ID, CONTAINER_CONFIG, localDurableLogFactory,\n+                context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, context.storageFactory,\n+                context.getDefaultExtensions(), executorService());\n+        localContainer.startAsync().awaitRunning();\n+\n+        // Record details(name, length & sealed status) of each segment to be created.\n+        ArrayList<String> segments = new ArrayList<>();\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        long[] segmentLengths = new long[createdSegmentCount];\n+        boolean[] segmentSealedStatus = new boolean[createdSegmentCount];\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            segmentLengths[i] = MIN_SEGMENT_LENGTH + RANDOM.nextInt(MAX_SEGMENT_LENGTH - MIN_SEGMENT_LENGTH);\n+            segmentSealedStatus[i] = RANDOM.nextBoolean();\n+            String name = \"Segment_\" + i;\n+            segments.add(name);\n+            futures.add(localContainer.createStreamSegment(name, segmentLengths[i], segmentSealedStatus[i]));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        // Verify the Segments are still there with their length & sealed status.\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            SegmentProperties props = localContainer.getStreamSegmentInfo(segments.get(i), TIMEOUT).join();\n+            Assert.assertEquals(\"Segment length mismatch \", segmentLengths[i], props.getLength());\n+            Assert.assertEquals(\"Segment sealed status mismatch\", segmentSealedStatus[i], props.isSealed());\n+        }\n+        localContainer.stopAsync().awaitTerminated();\n+    }\n+\n+    /**\n+     * Use a storage instance to create segments. List the segments from the storage and recreate them.\n+     */\n+    @Test\n+    public void testEndToEnd() {\n+        // Segments are mapped to four different containers.\n+        // DebugSegmentContainer for each container Id is created and segments belonging to that container are recovered.\n+        int containerCount = 4;\n+        int segmentsToCreateCount = 50;\n+\n+        // Create a storage.\n+        @Cleanup\n+        val baseStorage = new InMemoryStorage();\n+        @Cleanup\n+        val s = new RollingStorage(baseStorage, new SegmentRollingPolicy(1));\n+        s.initialize(1);\n+\n+        // Record details(name, container Id & sealed status) of each segment to be created.\n+        Set<String> sealedSegments = new HashSet<>();\n+        byte[] data = \"data\".getBytes();\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        int[] segmentsCountByContainer = new int[containerCount];\n+        Map<Integer, ArrayList<String>> segmentByContainers = new HashMap<>();\n+\n+        // Create segments and get their container Ids, sealed status and names to verify.\n+        for (int i = 0; i < segmentsToCreateCount; i++) {\n+            String segmentName = \"segment-\" + RANDOM.nextInt();\n+\n+            // Count segments by each container Id.\n+            segmentsCountByContainer[segToConMapper.getContainerId(segmentName)]++;\n+\n+            // Use segmentName to map to different containers.\n+            int containerId = segToConMapper.getContainerId(segmentName);\n+            ArrayList<String> segmentsList = segmentByContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(segmentName);\n+                segmentByContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentByContainers.get(containerId).add(segmentName);\n+            }\n+\n+            // Create segments, write data and randomly seal some of them.\n+            try {\n+                val wh1 = s.create(segmentName);\n+                // Write data.\n+                s.write(wh1, 0, new ByteArrayInputStream(data), data.length);\n+                if (RANDOM.nextInt(2) == 1) {\n+                    s.seal(wh1);\n+                    sealedSegments.add(segmentName);\n+                }\n+            } catch (StreamSegmentException e) {\n+                Assert.fail(\"Exception occurred while test execution.\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwMzg4Mw=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 218}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI3OTk1OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoyMzozOFrOGzc6nA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QwNjoxNTowOFrOG3U60A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNDMxNg==", "bodyText": "and here", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456604316", "createdAt": "2020-07-17T18:23:38Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "diffHunk": "@@ -0,0 +1,348 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.SyncStorage;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.mocks.InMemoryDurableDataLogFactory;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorage;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorageFactory;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import lombok.Cleanup;\n+import lombok.val;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+/**\n+ * Tests for DebugStreamSegmentContainer class.\n+ */\n+public class DebugStreamSegmentContainerTests extends ThreadPooledTestSuite {\n+    private static final int MIN_SEGMENT_LENGTH = 0; // Used in randomly generating the length for a segment\n+    private static final int MAX_SEGMENT_LENGTH = 10100; // Used in randomly generating the length for a segment\n+    private static final int CONTAINER_ID = 1234567;\n+    private static final int EXPECTED_PINNED_SEGMENT_COUNT = 1;\n+    private static final int MAX_DATA_LOG_APPEND_SIZE = 100 * 1024;\n+    private static final int TEST_TIMEOUT_MILLIS = 10 * 1000;\n+    private static final Duration TIMEOUT = Duration.ofMillis(TEST_TIMEOUT_MILLIS);\n+    private static final Random RANDOM = new Random(1234);\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, 10 * 60)\n+            .build();\n+\n+    private static final DurableLogConfig DEFAULT_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10 * 1024 * 1024L)\n+            .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+            .build();\n+\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig DEFAULT_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+            .build();\n+    private static final ContainerConfig CONTAINER_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+            .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 200 + EXPECTED_PINNED_SEGMENT_COUNT)\n+            .build();\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+\n+    @Rule\n+    public Timeout globalTimeout = Timeout.millis(TEST_TIMEOUT_MILLIS);\n+\n+    /**\n+     * Tests the ability to create Segments.\n+     */\n+    @Test\n+    public void testCreateStreamSegment() {\n+        int maxSegmentCount = 100;\n+        final int createdSegmentCount = maxSegmentCount * 2;\n+\n+        // Sets up dataLogFactory, readIndexFactory, attributeIndexFactory etc for the DebugSegmentContainer.\n+        @Cleanup\n+        TestContext context = createContext();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DEFAULT_DURABLE_LOG_CONFIG, context.dataLogFactory, executorService());\n+        // Starts a DebugSegmentContainer.\n+        @Cleanup\n+        MetadataCleanupContainer localContainer = new MetadataCleanupContainer(CONTAINER_ID, CONTAINER_CONFIG, localDurableLogFactory,\n+                context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, context.storageFactory,\n+                context.getDefaultExtensions(), executorService());\n+        localContainer.startAsync().awaitRunning();\n+\n+        // Record details(name, length & sealed status) of each segment to be created.\n+        ArrayList<String> segments = new ArrayList<>();\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        long[] segmentLengths = new long[createdSegmentCount];\n+        boolean[] segmentSealedStatus = new boolean[createdSegmentCount];\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            segmentLengths[i] = MIN_SEGMENT_LENGTH + RANDOM.nextInt(MAX_SEGMENT_LENGTH - MIN_SEGMENT_LENGTH);\n+            segmentSealedStatus[i] = RANDOM.nextBoolean();\n+            String name = \"Segment_\" + i;\n+            segments.add(name);\n+            futures.add(localContainer.createStreamSegment(name, segmentLengths[i], segmentSealedStatus[i]));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        // Verify the Segments are still there with their length & sealed status.\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            SegmentProperties props = localContainer.getStreamSegmentInfo(segments.get(i), TIMEOUT).join();\n+            Assert.assertEquals(\"Segment length mismatch \", segmentLengths[i], props.getLength());\n+            Assert.assertEquals(\"Segment sealed status mismatch\", segmentSealedStatus[i], props.isSealed());\n+        }\n+        localContainer.stopAsync().awaitTerminated();\n+    }\n+\n+    /**\n+     * Use a storage instance to create segments. List the segments from the storage and recreate them.\n+     */\n+    @Test\n+    public void testEndToEnd() {\n+        // Segments are mapped to four different containers.\n+        // DebugSegmentContainer for each container Id is created and segments belonging to that container are recovered.\n+        int containerCount = 4;\n+        int segmentsToCreateCount = 50;\n+\n+        // Create a storage.\n+        @Cleanup\n+        val baseStorage = new InMemoryStorage();\n+        @Cleanup\n+        val s = new RollingStorage(baseStorage, new SegmentRollingPolicy(1));\n+        s.initialize(1);\n+\n+        // Record details(name, container Id & sealed status) of each segment to be created.\n+        Set<String> sealedSegments = new HashSet<>();\n+        byte[] data = \"data\".getBytes();\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        int[] segmentsCountByContainer = new int[containerCount];\n+        Map<Integer, ArrayList<String>> segmentByContainers = new HashMap<>();\n+\n+        // Create segments and get their container Ids, sealed status and names to verify.\n+        for (int i = 0; i < segmentsToCreateCount; i++) {\n+            String segmentName = \"segment-\" + RANDOM.nextInt();\n+\n+            // Count segments by each container Id.\n+            segmentsCountByContainer[segToConMapper.getContainerId(segmentName)]++;\n+\n+            // Use segmentName to map to different containers.\n+            int containerId = segToConMapper.getContainerId(segmentName);\n+            ArrayList<String> segmentsList = segmentByContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(segmentName);\n+                segmentByContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentByContainers.get(containerId).add(segmentName);\n+            }\n+\n+            // Create segments, write data and randomly seal some of them.\n+            try {\n+                val wh1 = s.create(segmentName);\n+                // Write data.\n+                s.write(wh1, 0, new ByteArrayInputStream(data), data.length);\n+                if (RANDOM.nextInt(2) == 1) {\n+                    s.seal(wh1);\n+                    sealedSegments.add(segmentName);\n+                }\n+            } catch (StreamSegmentException e) {\n+                Assert.fail(\"Exception occurred while test execution.\");\n+            }\n+        }\n+\n+        // Keeps count of segments recovered in all container Ids.\n+        int segmentsRecoveredCount = 0;\n+\n+        // List all segments\n+        Map<Integer, List<SegmentProperties>> segments = null;\n+        try {\n+            segments = DataRecoveryTestUtils.listAllSegments(new AsyncStorageWrapper(s,\n+                    DataRecoveryTestUtils.createExecutorService(10)), containerCount);\n+        } catch (IOException e) {\n+            Assert.fail(\"Exception occurred while listing segments.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 231}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDY2NzYwMA==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r460667600", "createdAt": "2020-07-27T06:15:08Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "diffHunk": "@@ -0,0 +1,348 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.SyncStorage;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.mocks.InMemoryDurableDataLogFactory;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorage;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorageFactory;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import lombok.Cleanup;\n+import lombok.val;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+/**\n+ * Tests for DebugStreamSegmentContainer class.\n+ */\n+public class DebugStreamSegmentContainerTests extends ThreadPooledTestSuite {\n+    private static final int MIN_SEGMENT_LENGTH = 0; // Used in randomly generating the length for a segment\n+    private static final int MAX_SEGMENT_LENGTH = 10100; // Used in randomly generating the length for a segment\n+    private static final int CONTAINER_ID = 1234567;\n+    private static final int EXPECTED_PINNED_SEGMENT_COUNT = 1;\n+    private static final int MAX_DATA_LOG_APPEND_SIZE = 100 * 1024;\n+    private static final int TEST_TIMEOUT_MILLIS = 10 * 1000;\n+    private static final Duration TIMEOUT = Duration.ofMillis(TEST_TIMEOUT_MILLIS);\n+    private static final Random RANDOM = new Random(1234);\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, 10 * 60)\n+            .build();\n+\n+    private static final DurableLogConfig DEFAULT_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10 * 1024 * 1024L)\n+            .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+            .build();\n+\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig DEFAULT_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+            .build();\n+    private static final ContainerConfig CONTAINER_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+            .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 200 + EXPECTED_PINNED_SEGMENT_COUNT)\n+            .build();\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+\n+    @Rule\n+    public Timeout globalTimeout = Timeout.millis(TEST_TIMEOUT_MILLIS);\n+\n+    /**\n+     * Tests the ability to create Segments.\n+     */\n+    @Test\n+    public void testCreateStreamSegment() {\n+        int maxSegmentCount = 100;\n+        final int createdSegmentCount = maxSegmentCount * 2;\n+\n+        // Sets up dataLogFactory, readIndexFactory, attributeIndexFactory etc for the DebugSegmentContainer.\n+        @Cleanup\n+        TestContext context = createContext();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DEFAULT_DURABLE_LOG_CONFIG, context.dataLogFactory, executorService());\n+        // Starts a DebugSegmentContainer.\n+        @Cleanup\n+        MetadataCleanupContainer localContainer = new MetadataCleanupContainer(CONTAINER_ID, CONTAINER_CONFIG, localDurableLogFactory,\n+                context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, context.storageFactory,\n+                context.getDefaultExtensions(), executorService());\n+        localContainer.startAsync().awaitRunning();\n+\n+        // Record details(name, length & sealed status) of each segment to be created.\n+        ArrayList<String> segments = new ArrayList<>();\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        long[] segmentLengths = new long[createdSegmentCount];\n+        boolean[] segmentSealedStatus = new boolean[createdSegmentCount];\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            segmentLengths[i] = MIN_SEGMENT_LENGTH + RANDOM.nextInt(MAX_SEGMENT_LENGTH - MIN_SEGMENT_LENGTH);\n+            segmentSealedStatus[i] = RANDOM.nextBoolean();\n+            String name = \"Segment_\" + i;\n+            segments.add(name);\n+            futures.add(localContainer.createStreamSegment(name, segmentLengths[i], segmentSealedStatus[i]));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        // Verify the Segments are still there with their length & sealed status.\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            SegmentProperties props = localContainer.getStreamSegmentInfo(segments.get(i), TIMEOUT).join();\n+            Assert.assertEquals(\"Segment length mismatch \", segmentLengths[i], props.getLength());\n+            Assert.assertEquals(\"Segment sealed status mismatch\", segmentSealedStatus[i], props.isSealed());\n+        }\n+        localContainer.stopAsync().awaitTerminated();\n+    }\n+\n+    /**\n+     * Use a storage instance to create segments. List the segments from the storage and recreate them.\n+     */\n+    @Test\n+    public void testEndToEnd() {\n+        // Segments are mapped to four different containers.\n+        // DebugSegmentContainer for each container Id is created and segments belonging to that container are recovered.\n+        int containerCount = 4;\n+        int segmentsToCreateCount = 50;\n+\n+        // Create a storage.\n+        @Cleanup\n+        val baseStorage = new InMemoryStorage();\n+        @Cleanup\n+        val s = new RollingStorage(baseStorage, new SegmentRollingPolicy(1));\n+        s.initialize(1);\n+\n+        // Record details(name, container Id & sealed status) of each segment to be created.\n+        Set<String> sealedSegments = new HashSet<>();\n+        byte[] data = \"data\".getBytes();\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        int[] segmentsCountByContainer = new int[containerCount];\n+        Map<Integer, ArrayList<String>> segmentByContainers = new HashMap<>();\n+\n+        // Create segments and get their container Ids, sealed status and names to verify.\n+        for (int i = 0; i < segmentsToCreateCount; i++) {\n+            String segmentName = \"segment-\" + RANDOM.nextInt();\n+\n+            // Count segments by each container Id.\n+            segmentsCountByContainer[segToConMapper.getContainerId(segmentName)]++;\n+\n+            // Use segmentName to map to different containers.\n+            int containerId = segToConMapper.getContainerId(segmentName);\n+            ArrayList<String> segmentsList = segmentByContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(segmentName);\n+                segmentByContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentByContainers.get(containerId).add(segmentName);\n+            }\n+\n+            // Create segments, write data and randomly seal some of them.\n+            try {\n+                val wh1 = s.create(segmentName);\n+                // Write data.\n+                s.write(wh1, 0, new ByteArrayInputStream(data), data.length);\n+                if (RANDOM.nextInt(2) == 1) {\n+                    s.seal(wh1);\n+                    sealedSegments.add(segmentName);\n+                }\n+            } catch (StreamSegmentException e) {\n+                Assert.fail(\"Exception occurred while test execution.\");\n+            }\n+        }\n+\n+        // Keeps count of segments recovered in all container Ids.\n+        int segmentsRecoveredCount = 0;\n+\n+        // List all segments\n+        Map<Integer, List<SegmentProperties>> segments = null;\n+        try {\n+            segments = DataRecoveryTestUtils.listAllSegments(new AsyncStorageWrapper(s,\n+                    DataRecoveryTestUtils.createExecutorService(10)), containerCount);\n+        } catch (IOException e) {\n+            Assert.fail(\"Exception occurred while listing segments.\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNDMxNg=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 231}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI4MDU3OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoyMzo1MlrOGzc6_w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjoyODo1MlrOG2duFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNDQxNQ==", "bodyText": "Why do you create a random executor here?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456604415", "createdAt": "2020-07-17T18:23:52Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "diffHunk": "@@ -0,0 +1,348 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.SyncStorage;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.mocks.InMemoryDurableDataLogFactory;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorage;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorageFactory;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import lombok.Cleanup;\n+import lombok.val;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+/**\n+ * Tests for DebugStreamSegmentContainer class.\n+ */\n+public class DebugStreamSegmentContainerTests extends ThreadPooledTestSuite {\n+    private static final int MIN_SEGMENT_LENGTH = 0; // Used in randomly generating the length for a segment\n+    private static final int MAX_SEGMENT_LENGTH = 10100; // Used in randomly generating the length for a segment\n+    private static final int CONTAINER_ID = 1234567;\n+    private static final int EXPECTED_PINNED_SEGMENT_COUNT = 1;\n+    private static final int MAX_DATA_LOG_APPEND_SIZE = 100 * 1024;\n+    private static final int TEST_TIMEOUT_MILLIS = 10 * 1000;\n+    private static final Duration TIMEOUT = Duration.ofMillis(TEST_TIMEOUT_MILLIS);\n+    private static final Random RANDOM = new Random(1234);\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, 10 * 60)\n+            .build();\n+\n+    private static final DurableLogConfig DEFAULT_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10 * 1024 * 1024L)\n+            .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+            .build();\n+\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig DEFAULT_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+            .build();\n+    private static final ContainerConfig CONTAINER_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+            .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 200 + EXPECTED_PINNED_SEGMENT_COUNT)\n+            .build();\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+\n+    @Rule\n+    public Timeout globalTimeout = Timeout.millis(TEST_TIMEOUT_MILLIS);\n+\n+    /**\n+     * Tests the ability to create Segments.\n+     */\n+    @Test\n+    public void testCreateStreamSegment() {\n+        int maxSegmentCount = 100;\n+        final int createdSegmentCount = maxSegmentCount * 2;\n+\n+        // Sets up dataLogFactory, readIndexFactory, attributeIndexFactory etc for the DebugSegmentContainer.\n+        @Cleanup\n+        TestContext context = createContext();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DEFAULT_DURABLE_LOG_CONFIG, context.dataLogFactory, executorService());\n+        // Starts a DebugSegmentContainer.\n+        @Cleanup\n+        MetadataCleanupContainer localContainer = new MetadataCleanupContainer(CONTAINER_ID, CONTAINER_CONFIG, localDurableLogFactory,\n+                context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, context.storageFactory,\n+                context.getDefaultExtensions(), executorService());\n+        localContainer.startAsync().awaitRunning();\n+\n+        // Record details(name, length & sealed status) of each segment to be created.\n+        ArrayList<String> segments = new ArrayList<>();\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        long[] segmentLengths = new long[createdSegmentCount];\n+        boolean[] segmentSealedStatus = new boolean[createdSegmentCount];\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            segmentLengths[i] = MIN_SEGMENT_LENGTH + RANDOM.nextInt(MAX_SEGMENT_LENGTH - MIN_SEGMENT_LENGTH);\n+            segmentSealedStatus[i] = RANDOM.nextBoolean();\n+            String name = \"Segment_\" + i;\n+            segments.add(name);\n+            futures.add(localContainer.createStreamSegment(name, segmentLengths[i], segmentSealedStatus[i]));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        // Verify the Segments are still there with their length & sealed status.\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            SegmentProperties props = localContainer.getStreamSegmentInfo(segments.get(i), TIMEOUT).join();\n+            Assert.assertEquals(\"Segment length mismatch \", segmentLengths[i], props.getLength());\n+            Assert.assertEquals(\"Segment sealed status mismatch\", segmentSealedStatus[i], props.isSealed());\n+        }\n+        localContainer.stopAsync().awaitTerminated();\n+    }\n+\n+    /**\n+     * Use a storage instance to create segments. List the segments from the storage and recreate them.\n+     */\n+    @Test\n+    public void testEndToEnd() {\n+        // Segments are mapped to four different containers.\n+        // DebugSegmentContainer for each container Id is created and segments belonging to that container are recovered.\n+        int containerCount = 4;\n+        int segmentsToCreateCount = 50;\n+\n+        // Create a storage.\n+        @Cleanup\n+        val baseStorage = new InMemoryStorage();\n+        @Cleanup\n+        val s = new RollingStorage(baseStorage, new SegmentRollingPolicy(1));\n+        s.initialize(1);\n+\n+        // Record details(name, container Id & sealed status) of each segment to be created.\n+        Set<String> sealedSegments = new HashSet<>();\n+        byte[] data = \"data\".getBytes();\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        int[] segmentsCountByContainer = new int[containerCount];\n+        Map<Integer, ArrayList<String>> segmentByContainers = new HashMap<>();\n+\n+        // Create segments and get their container Ids, sealed status and names to verify.\n+        for (int i = 0; i < segmentsToCreateCount; i++) {\n+            String segmentName = \"segment-\" + RANDOM.nextInt();\n+\n+            // Count segments by each container Id.\n+            segmentsCountByContainer[segToConMapper.getContainerId(segmentName)]++;\n+\n+            // Use segmentName to map to different containers.\n+            int containerId = segToConMapper.getContainerId(segmentName);\n+            ArrayList<String> segmentsList = segmentByContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(segmentName);\n+                segmentByContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentByContainers.get(containerId).add(segmentName);\n+            }\n+\n+            // Create segments, write data and randomly seal some of them.\n+            try {\n+                val wh1 = s.create(segmentName);\n+                // Write data.\n+                s.write(wh1, 0, new ByteArrayInputStream(data), data.length);\n+                if (RANDOM.nextInt(2) == 1) {\n+                    s.seal(wh1);\n+                    sealedSegments.add(segmentName);\n+                }\n+            } catch (StreamSegmentException e) {\n+                Assert.fail(\"Exception occurred while test execution.\");\n+            }\n+        }\n+\n+        // Keeps count of segments recovered in all container Ids.\n+        int segmentsRecoveredCount = 0;\n+\n+        // List all segments\n+        Map<Integer, List<SegmentProperties>> segments = null;\n+        try {\n+            segments = DataRecoveryTestUtils.listAllSegments(new AsyncStorageWrapper(s,\n+                    DataRecoveryTestUtils.createExecutorService(10)), containerCount);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 229}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2MzIyMA==", "bodyText": "removed.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459763220", "createdAt": "2020-07-23T22:28:52Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "diffHunk": "@@ -0,0 +1,348 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.SyncStorage;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.mocks.InMemoryDurableDataLogFactory;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorage;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorageFactory;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import lombok.Cleanup;\n+import lombok.val;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+/**\n+ * Tests for DebugStreamSegmentContainer class.\n+ */\n+public class DebugStreamSegmentContainerTests extends ThreadPooledTestSuite {\n+    private static final int MIN_SEGMENT_LENGTH = 0; // Used in randomly generating the length for a segment\n+    private static final int MAX_SEGMENT_LENGTH = 10100; // Used in randomly generating the length for a segment\n+    private static final int CONTAINER_ID = 1234567;\n+    private static final int EXPECTED_PINNED_SEGMENT_COUNT = 1;\n+    private static final int MAX_DATA_LOG_APPEND_SIZE = 100 * 1024;\n+    private static final int TEST_TIMEOUT_MILLIS = 10 * 1000;\n+    private static final Duration TIMEOUT = Duration.ofMillis(TEST_TIMEOUT_MILLIS);\n+    private static final Random RANDOM = new Random(1234);\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, 10 * 60)\n+            .build();\n+\n+    private static final DurableLogConfig DEFAULT_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10 * 1024 * 1024L)\n+            .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+            .build();\n+\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig DEFAULT_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+            .build();\n+    private static final ContainerConfig CONTAINER_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+            .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 200 + EXPECTED_PINNED_SEGMENT_COUNT)\n+            .build();\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+\n+    @Rule\n+    public Timeout globalTimeout = Timeout.millis(TEST_TIMEOUT_MILLIS);\n+\n+    /**\n+     * Tests the ability to create Segments.\n+     */\n+    @Test\n+    public void testCreateStreamSegment() {\n+        int maxSegmentCount = 100;\n+        final int createdSegmentCount = maxSegmentCount * 2;\n+\n+        // Sets up dataLogFactory, readIndexFactory, attributeIndexFactory etc for the DebugSegmentContainer.\n+        @Cleanup\n+        TestContext context = createContext();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DEFAULT_DURABLE_LOG_CONFIG, context.dataLogFactory, executorService());\n+        // Starts a DebugSegmentContainer.\n+        @Cleanup\n+        MetadataCleanupContainer localContainer = new MetadataCleanupContainer(CONTAINER_ID, CONTAINER_CONFIG, localDurableLogFactory,\n+                context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, context.storageFactory,\n+                context.getDefaultExtensions(), executorService());\n+        localContainer.startAsync().awaitRunning();\n+\n+        // Record details(name, length & sealed status) of each segment to be created.\n+        ArrayList<String> segments = new ArrayList<>();\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        long[] segmentLengths = new long[createdSegmentCount];\n+        boolean[] segmentSealedStatus = new boolean[createdSegmentCount];\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            segmentLengths[i] = MIN_SEGMENT_LENGTH + RANDOM.nextInt(MAX_SEGMENT_LENGTH - MIN_SEGMENT_LENGTH);\n+            segmentSealedStatus[i] = RANDOM.nextBoolean();\n+            String name = \"Segment_\" + i;\n+            segments.add(name);\n+            futures.add(localContainer.createStreamSegment(name, segmentLengths[i], segmentSealedStatus[i]));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        // Verify the Segments are still there with their length & sealed status.\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            SegmentProperties props = localContainer.getStreamSegmentInfo(segments.get(i), TIMEOUT).join();\n+            Assert.assertEquals(\"Segment length mismatch \", segmentLengths[i], props.getLength());\n+            Assert.assertEquals(\"Segment sealed status mismatch\", segmentSealedStatus[i], props.isSealed());\n+        }\n+        localContainer.stopAsync().awaitTerminated();\n+    }\n+\n+    /**\n+     * Use a storage instance to create segments. List the segments from the storage and recreate them.\n+     */\n+    @Test\n+    public void testEndToEnd() {\n+        // Segments are mapped to four different containers.\n+        // DebugSegmentContainer for each container Id is created and segments belonging to that container are recovered.\n+        int containerCount = 4;\n+        int segmentsToCreateCount = 50;\n+\n+        // Create a storage.\n+        @Cleanup\n+        val baseStorage = new InMemoryStorage();\n+        @Cleanup\n+        val s = new RollingStorage(baseStorage, new SegmentRollingPolicy(1));\n+        s.initialize(1);\n+\n+        // Record details(name, container Id & sealed status) of each segment to be created.\n+        Set<String> sealedSegments = new HashSet<>();\n+        byte[] data = \"data\".getBytes();\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        int[] segmentsCountByContainer = new int[containerCount];\n+        Map<Integer, ArrayList<String>> segmentByContainers = new HashMap<>();\n+\n+        // Create segments and get their container Ids, sealed status and names to verify.\n+        for (int i = 0; i < segmentsToCreateCount; i++) {\n+            String segmentName = \"segment-\" + RANDOM.nextInt();\n+\n+            // Count segments by each container Id.\n+            segmentsCountByContainer[segToConMapper.getContainerId(segmentName)]++;\n+\n+            // Use segmentName to map to different containers.\n+            int containerId = segToConMapper.getContainerId(segmentName);\n+            ArrayList<String> segmentsList = segmentByContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(segmentName);\n+                segmentByContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentByContainers.get(containerId).add(segmentName);\n+            }\n+\n+            // Create segments, write data and randomly seal some of them.\n+            try {\n+                val wh1 = s.create(segmentName);\n+                // Write data.\n+                s.write(wh1, 0, new ByteArrayInputStream(data), data.length);\n+                if (RANDOM.nextInt(2) == 1) {\n+                    s.seal(wh1);\n+                    sealedSegments.add(segmentName);\n+                }\n+            } catch (StreamSegmentException e) {\n+                Assert.fail(\"Exception occurred while test execution.\");\n+            }\n+        }\n+\n+        // Keeps count of segments recovered in all container Ids.\n+        int segmentsRecoveredCount = 0;\n+\n+        // List all segments\n+        Map<Integer, List<SegmentProperties>> segments = null;\n+        try {\n+            segments = DataRecoveryTestUtils.listAllSegments(new AsyncStorageWrapper(s,\n+                    DataRecoveryTestUtils.createExecutorService(10)), containerCount);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNDQxNQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 229}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI4MTI0OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoyNDowN1rOGzc7dg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoyNDowN1rOGzc7dg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNDUzNA==", "bodyText": "and here", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456604534", "createdAt": "2020-07-17T18:24:07Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "diffHunk": "@@ -0,0 +1,348 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.SyncStorage;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.mocks.InMemoryDurableDataLogFactory;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorage;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorageFactory;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import lombok.Cleanup;\n+import lombok.val;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+/**\n+ * Tests for DebugStreamSegmentContainer class.\n+ */\n+public class DebugStreamSegmentContainerTests extends ThreadPooledTestSuite {\n+    private static final int MIN_SEGMENT_LENGTH = 0; // Used in randomly generating the length for a segment\n+    private static final int MAX_SEGMENT_LENGTH = 10100; // Used in randomly generating the length for a segment\n+    private static final int CONTAINER_ID = 1234567;\n+    private static final int EXPECTED_PINNED_SEGMENT_COUNT = 1;\n+    private static final int MAX_DATA_LOG_APPEND_SIZE = 100 * 1024;\n+    private static final int TEST_TIMEOUT_MILLIS = 10 * 1000;\n+    private static final Duration TIMEOUT = Duration.ofMillis(TEST_TIMEOUT_MILLIS);\n+    private static final Random RANDOM = new Random(1234);\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, 10 * 60)\n+            .build();\n+\n+    private static final DurableLogConfig DEFAULT_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10 * 1024 * 1024L)\n+            .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+            .build();\n+\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig DEFAULT_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+            .build();\n+    private static final ContainerConfig CONTAINER_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+            .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 200 + EXPECTED_PINNED_SEGMENT_COUNT)\n+            .build();\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+\n+    @Rule\n+    public Timeout globalTimeout = Timeout.millis(TEST_TIMEOUT_MILLIS);\n+\n+    /**\n+     * Tests the ability to create Segments.\n+     */\n+    @Test\n+    public void testCreateStreamSegment() {\n+        int maxSegmentCount = 100;\n+        final int createdSegmentCount = maxSegmentCount * 2;\n+\n+        // Sets up dataLogFactory, readIndexFactory, attributeIndexFactory etc for the DebugSegmentContainer.\n+        @Cleanup\n+        TestContext context = createContext();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DEFAULT_DURABLE_LOG_CONFIG, context.dataLogFactory, executorService());\n+        // Starts a DebugSegmentContainer.\n+        @Cleanup\n+        MetadataCleanupContainer localContainer = new MetadataCleanupContainer(CONTAINER_ID, CONTAINER_CONFIG, localDurableLogFactory,\n+                context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, context.storageFactory,\n+                context.getDefaultExtensions(), executorService());\n+        localContainer.startAsync().awaitRunning();\n+\n+        // Record details(name, length & sealed status) of each segment to be created.\n+        ArrayList<String> segments = new ArrayList<>();\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        long[] segmentLengths = new long[createdSegmentCount];\n+        boolean[] segmentSealedStatus = new boolean[createdSegmentCount];\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            segmentLengths[i] = MIN_SEGMENT_LENGTH + RANDOM.nextInt(MAX_SEGMENT_LENGTH - MIN_SEGMENT_LENGTH);\n+            segmentSealedStatus[i] = RANDOM.nextBoolean();\n+            String name = \"Segment_\" + i;\n+            segments.add(name);\n+            futures.add(localContainer.createStreamSegment(name, segmentLengths[i], segmentSealedStatus[i]));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        // Verify the Segments are still there with their length & sealed status.\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            SegmentProperties props = localContainer.getStreamSegmentInfo(segments.get(i), TIMEOUT).join();\n+            Assert.assertEquals(\"Segment length mismatch \", segmentLengths[i], props.getLength());\n+            Assert.assertEquals(\"Segment sealed status mismatch\", segmentSealedStatus[i], props.isSealed());\n+        }\n+        localContainer.stopAsync().awaitTerminated();\n+    }\n+\n+    /**\n+     * Use a storage instance to create segments. List the segments from the storage and recreate them.\n+     */\n+    @Test\n+    public void testEndToEnd() {\n+        // Segments are mapped to four different containers.\n+        // DebugSegmentContainer for each container Id is created and segments belonging to that container are recovered.\n+        int containerCount = 4;\n+        int segmentsToCreateCount = 50;\n+\n+        // Create a storage.\n+        @Cleanup\n+        val baseStorage = new InMemoryStorage();\n+        @Cleanup\n+        val s = new RollingStorage(baseStorage, new SegmentRollingPolicy(1));\n+        s.initialize(1);\n+\n+        // Record details(name, container Id & sealed status) of each segment to be created.\n+        Set<String> sealedSegments = new HashSet<>();\n+        byte[] data = \"data\".getBytes();\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        int[] segmentsCountByContainer = new int[containerCount];\n+        Map<Integer, ArrayList<String>> segmentByContainers = new HashMap<>();\n+\n+        // Create segments and get their container Ids, sealed status and names to verify.\n+        for (int i = 0; i < segmentsToCreateCount; i++) {\n+            String segmentName = \"segment-\" + RANDOM.nextInt();\n+\n+            // Count segments by each container Id.\n+            segmentsCountByContainer[segToConMapper.getContainerId(segmentName)]++;\n+\n+            // Use segmentName to map to different containers.\n+            int containerId = segToConMapper.getContainerId(segmentName);\n+            ArrayList<String> segmentsList = segmentByContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(segmentName);\n+                segmentByContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentByContainers.get(containerId).add(segmentName);\n+            }\n+\n+            // Create segments, write data and randomly seal some of them.\n+            try {\n+                val wh1 = s.create(segmentName);\n+                // Write data.\n+                s.write(wh1, 0, new ByteArrayInputStream(data), data.length);\n+                if (RANDOM.nextInt(2) == 1) {\n+                    s.seal(wh1);\n+                    sealedSegments.add(segmentName);\n+                }\n+            } catch (StreamSegmentException e) {\n+                Assert.fail(\"Exception occurred while test execution.\");\n+            }\n+        }\n+\n+        // Keeps count of segments recovered in all container Ids.\n+        int segmentsRecoveredCount = 0;\n+\n+        // List all segments\n+        Map<Integer, List<SegmentProperties>> segments = null;\n+        try {\n+            segments = DataRecoveryTestUtils.listAllSegments(new AsyncStorageWrapper(s,\n+                    DataRecoveryTestUtils.createExecutorService(10)), containerCount);\n+        } catch (IOException e) {\n+            Assert.fail(\"Exception occurred while listing segments.\");\n+        }\n+\n+        // Verify count of segments listed.\n+        for (int i = 0; i < segments.size(); i++) {\n+            segmentsRecoveredCount += segments.get(i).size();\n+            Assert.assertTrue(\"Number of segments listed is less than the number of segments created using this container.\",\n+                    segments.get(i).size() >= segmentsCountByContainer[i]);\n+        }\n+        Assert.assertTrue(\"Total number of segments created is less than the number of segments created.\",\n+                segmentsRecoveredCount >= segmentsToCreateCount);\n+\n+        @Cleanup\n+        TestContext context = createContext();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DEFAULT_DURABLE_LOG_CONFIG, context.dataLogFactory,\n+                DataRecoveryTestUtils.createExecutorService(10));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 246}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI4MTc3OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoyNDoxN1rOGzc7xg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjoyOTowMFrOG2duPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNDYxNA==", "bodyText": "and here", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456604614", "createdAt": "2020-07-17T18:24:17Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "diffHunk": "@@ -0,0 +1,348 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.SyncStorage;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.mocks.InMemoryDurableDataLogFactory;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorage;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorageFactory;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import lombok.Cleanup;\n+import lombok.val;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+/**\n+ * Tests for DebugStreamSegmentContainer class.\n+ */\n+public class DebugStreamSegmentContainerTests extends ThreadPooledTestSuite {\n+    private static final int MIN_SEGMENT_LENGTH = 0; // Used in randomly generating the length for a segment\n+    private static final int MAX_SEGMENT_LENGTH = 10100; // Used in randomly generating the length for a segment\n+    private static final int CONTAINER_ID = 1234567;\n+    private static final int EXPECTED_PINNED_SEGMENT_COUNT = 1;\n+    private static final int MAX_DATA_LOG_APPEND_SIZE = 100 * 1024;\n+    private static final int TEST_TIMEOUT_MILLIS = 10 * 1000;\n+    private static final Duration TIMEOUT = Duration.ofMillis(TEST_TIMEOUT_MILLIS);\n+    private static final Random RANDOM = new Random(1234);\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, 10 * 60)\n+            .build();\n+\n+    private static final DurableLogConfig DEFAULT_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10 * 1024 * 1024L)\n+            .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+            .build();\n+\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig DEFAULT_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+            .build();\n+    private static final ContainerConfig CONTAINER_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+            .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 200 + EXPECTED_PINNED_SEGMENT_COUNT)\n+            .build();\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+\n+    @Rule\n+    public Timeout globalTimeout = Timeout.millis(TEST_TIMEOUT_MILLIS);\n+\n+    /**\n+     * Tests the ability to create Segments.\n+     */\n+    @Test\n+    public void testCreateStreamSegment() {\n+        int maxSegmentCount = 100;\n+        final int createdSegmentCount = maxSegmentCount * 2;\n+\n+        // Sets up dataLogFactory, readIndexFactory, attributeIndexFactory etc for the DebugSegmentContainer.\n+        @Cleanup\n+        TestContext context = createContext();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DEFAULT_DURABLE_LOG_CONFIG, context.dataLogFactory, executorService());\n+        // Starts a DebugSegmentContainer.\n+        @Cleanup\n+        MetadataCleanupContainer localContainer = new MetadataCleanupContainer(CONTAINER_ID, CONTAINER_CONFIG, localDurableLogFactory,\n+                context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, context.storageFactory,\n+                context.getDefaultExtensions(), executorService());\n+        localContainer.startAsync().awaitRunning();\n+\n+        // Record details(name, length & sealed status) of each segment to be created.\n+        ArrayList<String> segments = new ArrayList<>();\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        long[] segmentLengths = new long[createdSegmentCount];\n+        boolean[] segmentSealedStatus = new boolean[createdSegmentCount];\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            segmentLengths[i] = MIN_SEGMENT_LENGTH + RANDOM.nextInt(MAX_SEGMENT_LENGTH - MIN_SEGMENT_LENGTH);\n+            segmentSealedStatus[i] = RANDOM.nextBoolean();\n+            String name = \"Segment_\" + i;\n+            segments.add(name);\n+            futures.add(localContainer.createStreamSegment(name, segmentLengths[i], segmentSealedStatus[i]));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        // Verify the Segments are still there with their length & sealed status.\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            SegmentProperties props = localContainer.getStreamSegmentInfo(segments.get(i), TIMEOUT).join();\n+            Assert.assertEquals(\"Segment length mismatch \", segmentLengths[i], props.getLength());\n+            Assert.assertEquals(\"Segment sealed status mismatch\", segmentSealedStatus[i], props.isSealed());\n+        }\n+        localContainer.stopAsync().awaitTerminated();\n+    }\n+\n+    /**\n+     * Use a storage instance to create segments. List the segments from the storage and recreate them.\n+     */\n+    @Test\n+    public void testEndToEnd() {\n+        // Segments are mapped to four different containers.\n+        // DebugSegmentContainer for each container Id is created and segments belonging to that container are recovered.\n+        int containerCount = 4;\n+        int segmentsToCreateCount = 50;\n+\n+        // Create a storage.\n+        @Cleanup\n+        val baseStorage = new InMemoryStorage();\n+        @Cleanup\n+        val s = new RollingStorage(baseStorage, new SegmentRollingPolicy(1));\n+        s.initialize(1);\n+\n+        // Record details(name, container Id & sealed status) of each segment to be created.\n+        Set<String> sealedSegments = new HashSet<>();\n+        byte[] data = \"data\".getBytes();\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        int[] segmentsCountByContainer = new int[containerCount];\n+        Map<Integer, ArrayList<String>> segmentByContainers = new HashMap<>();\n+\n+        // Create segments and get their container Ids, sealed status and names to verify.\n+        for (int i = 0; i < segmentsToCreateCount; i++) {\n+            String segmentName = \"segment-\" + RANDOM.nextInt();\n+\n+            // Count segments by each container Id.\n+            segmentsCountByContainer[segToConMapper.getContainerId(segmentName)]++;\n+\n+            // Use segmentName to map to different containers.\n+            int containerId = segToConMapper.getContainerId(segmentName);\n+            ArrayList<String> segmentsList = segmentByContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(segmentName);\n+                segmentByContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentByContainers.get(containerId).add(segmentName);\n+            }\n+\n+            // Create segments, write data and randomly seal some of them.\n+            try {\n+                val wh1 = s.create(segmentName);\n+                // Write data.\n+                s.write(wh1, 0, new ByteArrayInputStream(data), data.length);\n+                if (RANDOM.nextInt(2) == 1) {\n+                    s.seal(wh1);\n+                    sealedSegments.add(segmentName);\n+                }\n+            } catch (StreamSegmentException e) {\n+                Assert.fail(\"Exception occurred while test execution.\");\n+            }\n+        }\n+\n+        // Keeps count of segments recovered in all container Ids.\n+        int segmentsRecoveredCount = 0;\n+\n+        // List all segments\n+        Map<Integer, List<SegmentProperties>> segments = null;\n+        try {\n+            segments = DataRecoveryTestUtils.listAllSegments(new AsyncStorageWrapper(s,\n+                    DataRecoveryTestUtils.createExecutorService(10)), containerCount);\n+        } catch (IOException e) {\n+            Assert.fail(\"Exception occurred while listing segments.\");\n+        }\n+\n+        // Verify count of segments listed.\n+        for (int i = 0; i < segments.size(); i++) {\n+            segmentsRecoveredCount += segments.get(i).size();\n+            Assert.assertTrue(\"Number of segments listed is less than the number of segments created using this container.\",\n+                    segments.get(i).size() >= segmentsCountByContainer[i]);\n+        }\n+        Assert.assertTrue(\"Total number of segments created is less than the number of segments created.\",\n+                segmentsRecoveredCount >= segmentsToCreateCount);\n+\n+        @Cleanup\n+        TestContext context = createContext();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DEFAULT_DURABLE_LOG_CONFIG, context.dataLogFactory,\n+                DataRecoveryTestUtils.createExecutorService(10));\n+\n+        // Recover all segments\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            @Cleanup\n+            MetadataCleanupContainer localContainer = new MetadataCleanupContainer(containerId, CONTAINER_CONFIG, localDurableLogFactory,\n+                    context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, context.storageFactory,\n+                    context.getDefaultExtensions(), DataRecoveryTestUtils.createExecutorService(10));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 253}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2MzI2MA==", "bodyText": "removed.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459763260", "createdAt": "2020-07-23T22:29:00Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "diffHunk": "@@ -0,0 +1,348 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.SyncStorage;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.mocks.InMemoryDurableDataLogFactory;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorage;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorageFactory;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import lombok.Cleanup;\n+import lombok.val;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+/**\n+ * Tests for DebugStreamSegmentContainer class.\n+ */\n+public class DebugStreamSegmentContainerTests extends ThreadPooledTestSuite {\n+    private static final int MIN_SEGMENT_LENGTH = 0; // Used in randomly generating the length for a segment\n+    private static final int MAX_SEGMENT_LENGTH = 10100; // Used in randomly generating the length for a segment\n+    private static final int CONTAINER_ID = 1234567;\n+    private static final int EXPECTED_PINNED_SEGMENT_COUNT = 1;\n+    private static final int MAX_DATA_LOG_APPEND_SIZE = 100 * 1024;\n+    private static final int TEST_TIMEOUT_MILLIS = 10 * 1000;\n+    private static final Duration TIMEOUT = Duration.ofMillis(TEST_TIMEOUT_MILLIS);\n+    private static final Random RANDOM = new Random(1234);\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, 10 * 60)\n+            .build();\n+\n+    private static final DurableLogConfig DEFAULT_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10 * 1024 * 1024L)\n+            .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+            .build();\n+\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig DEFAULT_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+            .build();\n+    private static final ContainerConfig CONTAINER_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+            .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 200 + EXPECTED_PINNED_SEGMENT_COUNT)\n+            .build();\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+\n+    @Rule\n+    public Timeout globalTimeout = Timeout.millis(TEST_TIMEOUT_MILLIS);\n+\n+    /**\n+     * Tests the ability to create Segments.\n+     */\n+    @Test\n+    public void testCreateStreamSegment() {\n+        int maxSegmentCount = 100;\n+        final int createdSegmentCount = maxSegmentCount * 2;\n+\n+        // Sets up dataLogFactory, readIndexFactory, attributeIndexFactory etc for the DebugSegmentContainer.\n+        @Cleanup\n+        TestContext context = createContext();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DEFAULT_DURABLE_LOG_CONFIG, context.dataLogFactory, executorService());\n+        // Starts a DebugSegmentContainer.\n+        @Cleanup\n+        MetadataCleanupContainer localContainer = new MetadataCleanupContainer(CONTAINER_ID, CONTAINER_CONFIG, localDurableLogFactory,\n+                context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, context.storageFactory,\n+                context.getDefaultExtensions(), executorService());\n+        localContainer.startAsync().awaitRunning();\n+\n+        // Record details(name, length & sealed status) of each segment to be created.\n+        ArrayList<String> segments = new ArrayList<>();\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        long[] segmentLengths = new long[createdSegmentCount];\n+        boolean[] segmentSealedStatus = new boolean[createdSegmentCount];\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            segmentLengths[i] = MIN_SEGMENT_LENGTH + RANDOM.nextInt(MAX_SEGMENT_LENGTH - MIN_SEGMENT_LENGTH);\n+            segmentSealedStatus[i] = RANDOM.nextBoolean();\n+            String name = \"Segment_\" + i;\n+            segments.add(name);\n+            futures.add(localContainer.createStreamSegment(name, segmentLengths[i], segmentSealedStatus[i]));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        // Verify the Segments are still there with their length & sealed status.\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            SegmentProperties props = localContainer.getStreamSegmentInfo(segments.get(i), TIMEOUT).join();\n+            Assert.assertEquals(\"Segment length mismatch \", segmentLengths[i], props.getLength());\n+            Assert.assertEquals(\"Segment sealed status mismatch\", segmentSealedStatus[i], props.isSealed());\n+        }\n+        localContainer.stopAsync().awaitTerminated();\n+    }\n+\n+    /**\n+     * Use a storage instance to create segments. List the segments from the storage and recreate them.\n+     */\n+    @Test\n+    public void testEndToEnd() {\n+        // Segments are mapped to four different containers.\n+        // DebugSegmentContainer for each container Id is created and segments belonging to that container are recovered.\n+        int containerCount = 4;\n+        int segmentsToCreateCount = 50;\n+\n+        // Create a storage.\n+        @Cleanup\n+        val baseStorage = new InMemoryStorage();\n+        @Cleanup\n+        val s = new RollingStorage(baseStorage, new SegmentRollingPolicy(1));\n+        s.initialize(1);\n+\n+        // Record details(name, container Id & sealed status) of each segment to be created.\n+        Set<String> sealedSegments = new HashSet<>();\n+        byte[] data = \"data\".getBytes();\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        int[] segmentsCountByContainer = new int[containerCount];\n+        Map<Integer, ArrayList<String>> segmentByContainers = new HashMap<>();\n+\n+        // Create segments and get their container Ids, sealed status and names to verify.\n+        for (int i = 0; i < segmentsToCreateCount; i++) {\n+            String segmentName = \"segment-\" + RANDOM.nextInt();\n+\n+            // Count segments by each container Id.\n+            segmentsCountByContainer[segToConMapper.getContainerId(segmentName)]++;\n+\n+            // Use segmentName to map to different containers.\n+            int containerId = segToConMapper.getContainerId(segmentName);\n+            ArrayList<String> segmentsList = segmentByContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(segmentName);\n+                segmentByContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentByContainers.get(containerId).add(segmentName);\n+            }\n+\n+            // Create segments, write data and randomly seal some of them.\n+            try {\n+                val wh1 = s.create(segmentName);\n+                // Write data.\n+                s.write(wh1, 0, new ByteArrayInputStream(data), data.length);\n+                if (RANDOM.nextInt(2) == 1) {\n+                    s.seal(wh1);\n+                    sealedSegments.add(segmentName);\n+                }\n+            } catch (StreamSegmentException e) {\n+                Assert.fail(\"Exception occurred while test execution.\");\n+            }\n+        }\n+\n+        // Keeps count of segments recovered in all container Ids.\n+        int segmentsRecoveredCount = 0;\n+\n+        // List all segments\n+        Map<Integer, List<SegmentProperties>> segments = null;\n+        try {\n+            segments = DataRecoveryTestUtils.listAllSegments(new AsyncStorageWrapper(s,\n+                    DataRecoveryTestUtils.createExecutorService(10)), containerCount);\n+        } catch (IOException e) {\n+            Assert.fail(\"Exception occurred while listing segments.\");\n+        }\n+\n+        // Verify count of segments listed.\n+        for (int i = 0; i < segments.size(); i++) {\n+            segmentsRecoveredCount += segments.get(i).size();\n+            Assert.assertTrue(\"Number of segments listed is less than the number of segments created using this container.\",\n+                    segments.get(i).size() >= segmentsCountByContainer[i]);\n+        }\n+        Assert.assertTrue(\"Total number of segments created is less than the number of segments created.\",\n+                segmentsRecoveredCount >= segmentsToCreateCount);\n+\n+        @Cleanup\n+        TestContext context = createContext();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DEFAULT_DURABLE_LOG_CONFIG, context.dataLogFactory,\n+                DataRecoveryTestUtils.createExecutorService(10));\n+\n+        // Recover all segments\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            @Cleanup\n+            MetadataCleanupContainer localContainer = new MetadataCleanupContainer(containerId, CONTAINER_CONFIG, localDurableLogFactory,\n+                    context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, context.storageFactory,\n+                    context.getDefaultExtensions(), DataRecoveryTestUtils.createExecutorService(10));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNDYxNA=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 253}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI4NDE4OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoyNDo1N1rOGzc9MA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjoyOToxMFrOG2ducg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNDk3Ng==", "bodyText": "Replace this whole if block with Assert.assertEquals.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456604976", "createdAt": "2020-07-17T18:24:57Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "diffHunk": "@@ -0,0 +1,348 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.SyncStorage;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.mocks.InMemoryDurableDataLogFactory;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorage;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorageFactory;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import lombok.Cleanup;\n+import lombok.val;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+/**\n+ * Tests for DebugStreamSegmentContainer class.\n+ */\n+public class DebugStreamSegmentContainerTests extends ThreadPooledTestSuite {\n+    private static final int MIN_SEGMENT_LENGTH = 0; // Used in randomly generating the length for a segment\n+    private static final int MAX_SEGMENT_LENGTH = 10100; // Used in randomly generating the length for a segment\n+    private static final int CONTAINER_ID = 1234567;\n+    private static final int EXPECTED_PINNED_SEGMENT_COUNT = 1;\n+    private static final int MAX_DATA_LOG_APPEND_SIZE = 100 * 1024;\n+    private static final int TEST_TIMEOUT_MILLIS = 10 * 1000;\n+    private static final Duration TIMEOUT = Duration.ofMillis(TEST_TIMEOUT_MILLIS);\n+    private static final Random RANDOM = new Random(1234);\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, 10 * 60)\n+            .build();\n+\n+    private static final DurableLogConfig DEFAULT_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10 * 1024 * 1024L)\n+            .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+            .build();\n+\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig DEFAULT_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+            .build();\n+    private static final ContainerConfig CONTAINER_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+            .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 200 + EXPECTED_PINNED_SEGMENT_COUNT)\n+            .build();\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+\n+    @Rule\n+    public Timeout globalTimeout = Timeout.millis(TEST_TIMEOUT_MILLIS);\n+\n+    /**\n+     * Tests the ability to create Segments.\n+     */\n+    @Test\n+    public void testCreateStreamSegment() {\n+        int maxSegmentCount = 100;\n+        final int createdSegmentCount = maxSegmentCount * 2;\n+\n+        // Sets up dataLogFactory, readIndexFactory, attributeIndexFactory etc for the DebugSegmentContainer.\n+        @Cleanup\n+        TestContext context = createContext();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DEFAULT_DURABLE_LOG_CONFIG, context.dataLogFactory, executorService());\n+        // Starts a DebugSegmentContainer.\n+        @Cleanup\n+        MetadataCleanupContainer localContainer = new MetadataCleanupContainer(CONTAINER_ID, CONTAINER_CONFIG, localDurableLogFactory,\n+                context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, context.storageFactory,\n+                context.getDefaultExtensions(), executorService());\n+        localContainer.startAsync().awaitRunning();\n+\n+        // Record details(name, length & sealed status) of each segment to be created.\n+        ArrayList<String> segments = new ArrayList<>();\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        long[] segmentLengths = new long[createdSegmentCount];\n+        boolean[] segmentSealedStatus = new boolean[createdSegmentCount];\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            segmentLengths[i] = MIN_SEGMENT_LENGTH + RANDOM.nextInt(MAX_SEGMENT_LENGTH - MIN_SEGMENT_LENGTH);\n+            segmentSealedStatus[i] = RANDOM.nextBoolean();\n+            String name = \"Segment_\" + i;\n+            segments.add(name);\n+            futures.add(localContainer.createStreamSegment(name, segmentLengths[i], segmentSealedStatus[i]));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        // Verify the Segments are still there with their length & sealed status.\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            SegmentProperties props = localContainer.getStreamSegmentInfo(segments.get(i), TIMEOUT).join();\n+            Assert.assertEquals(\"Segment length mismatch \", segmentLengths[i], props.getLength());\n+            Assert.assertEquals(\"Segment sealed status mismatch\", segmentSealedStatus[i], props.isSealed());\n+        }\n+        localContainer.stopAsync().awaitTerminated();\n+    }\n+\n+    /**\n+     * Use a storage instance to create segments. List the segments from the storage and recreate them.\n+     */\n+    @Test\n+    public void testEndToEnd() {\n+        // Segments are mapped to four different containers.\n+        // DebugSegmentContainer for each container Id is created and segments belonging to that container are recovered.\n+        int containerCount = 4;\n+        int segmentsToCreateCount = 50;\n+\n+        // Create a storage.\n+        @Cleanup\n+        val baseStorage = new InMemoryStorage();\n+        @Cleanup\n+        val s = new RollingStorage(baseStorage, new SegmentRollingPolicy(1));\n+        s.initialize(1);\n+\n+        // Record details(name, container Id & sealed status) of each segment to be created.\n+        Set<String> sealedSegments = new HashSet<>();\n+        byte[] data = \"data\".getBytes();\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        int[] segmentsCountByContainer = new int[containerCount];\n+        Map<Integer, ArrayList<String>> segmentByContainers = new HashMap<>();\n+\n+        // Create segments and get their container Ids, sealed status and names to verify.\n+        for (int i = 0; i < segmentsToCreateCount; i++) {\n+            String segmentName = \"segment-\" + RANDOM.nextInt();\n+\n+            // Count segments by each container Id.\n+            segmentsCountByContainer[segToConMapper.getContainerId(segmentName)]++;\n+\n+            // Use segmentName to map to different containers.\n+            int containerId = segToConMapper.getContainerId(segmentName);\n+            ArrayList<String> segmentsList = segmentByContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(segmentName);\n+                segmentByContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentByContainers.get(containerId).add(segmentName);\n+            }\n+\n+            // Create segments, write data and randomly seal some of them.\n+            try {\n+                val wh1 = s.create(segmentName);\n+                // Write data.\n+                s.write(wh1, 0, new ByteArrayInputStream(data), data.length);\n+                if (RANDOM.nextInt(2) == 1) {\n+                    s.seal(wh1);\n+                    sealedSegments.add(segmentName);\n+                }\n+            } catch (StreamSegmentException e) {\n+                Assert.fail(\"Exception occurred while test execution.\");\n+            }\n+        }\n+\n+        // Keeps count of segments recovered in all container Ids.\n+        int segmentsRecoveredCount = 0;\n+\n+        // List all segments\n+        Map<Integer, List<SegmentProperties>> segments = null;\n+        try {\n+            segments = DataRecoveryTestUtils.listAllSegments(new AsyncStorageWrapper(s,\n+                    DataRecoveryTestUtils.createExecutorService(10)), containerCount);\n+        } catch (IOException e) {\n+            Assert.fail(\"Exception occurred while listing segments.\");\n+        }\n+\n+        // Verify count of segments listed.\n+        for (int i = 0; i < segments.size(); i++) {\n+            segmentsRecoveredCount += segments.get(i).size();\n+            Assert.assertTrue(\"Number of segments listed is less than the number of segments created using this container.\",\n+                    segments.get(i).size() >= segmentsCountByContainer[i]);\n+        }\n+        Assert.assertTrue(\"Total number of segments created is less than the number of segments created.\",\n+                segmentsRecoveredCount >= segmentsToCreateCount);\n+\n+        @Cleanup\n+        TestContext context = createContext();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DEFAULT_DURABLE_LOG_CONFIG, context.dataLogFactory,\n+                DataRecoveryTestUtils.createExecutorService(10));\n+\n+        // Recover all segments\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            @Cleanup\n+            MetadataCleanupContainer localContainer = new MetadataCleanupContainer(containerId, CONTAINER_CONFIG, localDurableLogFactory,\n+                    context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, context.storageFactory,\n+                    context.getDefaultExtensions(), DataRecoveryTestUtils.createExecutorService(10));\n+\n+            Services.startAsync(localContainer, executorService)\n+                    .thenRun(new DataRecoveryTestUtils.Worker(localContainer, segments.get(containerId))).join();\n+\n+            for (String segmentName : segmentByContainers.get(containerId)) {\n+                SegmentProperties props = localContainer.getStreamSegmentInfo(segmentName, TIMEOUT).join();\n+                Assert.assertEquals(\"Segment length mismatch \", data.length, props.getLength());\n+                if (sealedSegments.contains(segmentName)) {\n+                    Assert.assertTrue(\"Segment should have been sealed\", props.isSealed());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 262}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2MzMxNA==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459763314", "createdAt": "2020-07-23T22:29:10Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "diffHunk": "@@ -0,0 +1,348 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.SyncStorage;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.mocks.InMemoryDurableDataLogFactory;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorage;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorageFactory;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import lombok.Cleanup;\n+import lombok.val;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+/**\n+ * Tests for DebugStreamSegmentContainer class.\n+ */\n+public class DebugStreamSegmentContainerTests extends ThreadPooledTestSuite {\n+    private static final int MIN_SEGMENT_LENGTH = 0; // Used in randomly generating the length for a segment\n+    private static final int MAX_SEGMENT_LENGTH = 10100; // Used in randomly generating the length for a segment\n+    private static final int CONTAINER_ID = 1234567;\n+    private static final int EXPECTED_PINNED_SEGMENT_COUNT = 1;\n+    private static final int MAX_DATA_LOG_APPEND_SIZE = 100 * 1024;\n+    private static final int TEST_TIMEOUT_MILLIS = 10 * 1000;\n+    private static final Duration TIMEOUT = Duration.ofMillis(TEST_TIMEOUT_MILLIS);\n+    private static final Random RANDOM = new Random(1234);\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, 10 * 60)\n+            .build();\n+\n+    private static final DurableLogConfig DEFAULT_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10 * 1024 * 1024L)\n+            .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+            .build();\n+\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig DEFAULT_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+            .build();\n+    private static final ContainerConfig CONTAINER_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+            .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 200 + EXPECTED_PINNED_SEGMENT_COUNT)\n+            .build();\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+\n+    @Rule\n+    public Timeout globalTimeout = Timeout.millis(TEST_TIMEOUT_MILLIS);\n+\n+    /**\n+     * Tests the ability to create Segments.\n+     */\n+    @Test\n+    public void testCreateStreamSegment() {\n+        int maxSegmentCount = 100;\n+        final int createdSegmentCount = maxSegmentCount * 2;\n+\n+        // Sets up dataLogFactory, readIndexFactory, attributeIndexFactory etc for the DebugSegmentContainer.\n+        @Cleanup\n+        TestContext context = createContext();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DEFAULT_DURABLE_LOG_CONFIG, context.dataLogFactory, executorService());\n+        // Starts a DebugSegmentContainer.\n+        @Cleanup\n+        MetadataCleanupContainer localContainer = new MetadataCleanupContainer(CONTAINER_ID, CONTAINER_CONFIG, localDurableLogFactory,\n+                context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, context.storageFactory,\n+                context.getDefaultExtensions(), executorService());\n+        localContainer.startAsync().awaitRunning();\n+\n+        // Record details(name, length & sealed status) of each segment to be created.\n+        ArrayList<String> segments = new ArrayList<>();\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        long[] segmentLengths = new long[createdSegmentCount];\n+        boolean[] segmentSealedStatus = new boolean[createdSegmentCount];\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            segmentLengths[i] = MIN_SEGMENT_LENGTH + RANDOM.nextInt(MAX_SEGMENT_LENGTH - MIN_SEGMENT_LENGTH);\n+            segmentSealedStatus[i] = RANDOM.nextBoolean();\n+            String name = \"Segment_\" + i;\n+            segments.add(name);\n+            futures.add(localContainer.createStreamSegment(name, segmentLengths[i], segmentSealedStatus[i]));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        // Verify the Segments are still there with their length & sealed status.\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            SegmentProperties props = localContainer.getStreamSegmentInfo(segments.get(i), TIMEOUT).join();\n+            Assert.assertEquals(\"Segment length mismatch \", segmentLengths[i], props.getLength());\n+            Assert.assertEquals(\"Segment sealed status mismatch\", segmentSealedStatus[i], props.isSealed());\n+        }\n+        localContainer.stopAsync().awaitTerminated();\n+    }\n+\n+    /**\n+     * Use a storage instance to create segments. List the segments from the storage and recreate them.\n+     */\n+    @Test\n+    public void testEndToEnd() {\n+        // Segments are mapped to four different containers.\n+        // DebugSegmentContainer for each container Id is created and segments belonging to that container are recovered.\n+        int containerCount = 4;\n+        int segmentsToCreateCount = 50;\n+\n+        // Create a storage.\n+        @Cleanup\n+        val baseStorage = new InMemoryStorage();\n+        @Cleanup\n+        val s = new RollingStorage(baseStorage, new SegmentRollingPolicy(1));\n+        s.initialize(1);\n+\n+        // Record details(name, container Id & sealed status) of each segment to be created.\n+        Set<String> sealedSegments = new HashSet<>();\n+        byte[] data = \"data\".getBytes();\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        int[] segmentsCountByContainer = new int[containerCount];\n+        Map<Integer, ArrayList<String>> segmentByContainers = new HashMap<>();\n+\n+        // Create segments and get their container Ids, sealed status and names to verify.\n+        for (int i = 0; i < segmentsToCreateCount; i++) {\n+            String segmentName = \"segment-\" + RANDOM.nextInt();\n+\n+            // Count segments by each container Id.\n+            segmentsCountByContainer[segToConMapper.getContainerId(segmentName)]++;\n+\n+            // Use segmentName to map to different containers.\n+            int containerId = segToConMapper.getContainerId(segmentName);\n+            ArrayList<String> segmentsList = segmentByContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(segmentName);\n+                segmentByContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentByContainers.get(containerId).add(segmentName);\n+            }\n+\n+            // Create segments, write data and randomly seal some of them.\n+            try {\n+                val wh1 = s.create(segmentName);\n+                // Write data.\n+                s.write(wh1, 0, new ByteArrayInputStream(data), data.length);\n+                if (RANDOM.nextInt(2) == 1) {\n+                    s.seal(wh1);\n+                    sealedSegments.add(segmentName);\n+                }\n+            } catch (StreamSegmentException e) {\n+                Assert.fail(\"Exception occurred while test execution.\");\n+            }\n+        }\n+\n+        // Keeps count of segments recovered in all container Ids.\n+        int segmentsRecoveredCount = 0;\n+\n+        // List all segments\n+        Map<Integer, List<SegmentProperties>> segments = null;\n+        try {\n+            segments = DataRecoveryTestUtils.listAllSegments(new AsyncStorageWrapper(s,\n+                    DataRecoveryTestUtils.createExecutorService(10)), containerCount);\n+        } catch (IOException e) {\n+            Assert.fail(\"Exception occurred while listing segments.\");\n+        }\n+\n+        // Verify count of segments listed.\n+        for (int i = 0; i < segments.size(); i++) {\n+            segmentsRecoveredCount += segments.get(i).size();\n+            Assert.assertTrue(\"Number of segments listed is less than the number of segments created using this container.\",\n+                    segments.get(i).size() >= segmentsCountByContainer[i]);\n+        }\n+        Assert.assertTrue(\"Total number of segments created is less than the number of segments created.\",\n+                segmentsRecoveredCount >= segmentsToCreateCount);\n+\n+        @Cleanup\n+        TestContext context = createContext();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DEFAULT_DURABLE_LOG_CONFIG, context.dataLogFactory,\n+                DataRecoveryTestUtils.createExecutorService(10));\n+\n+        // Recover all segments\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            @Cleanup\n+            MetadataCleanupContainer localContainer = new MetadataCleanupContainer(containerId, CONTAINER_CONFIG, localDurableLogFactory,\n+                    context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, context.storageFactory,\n+                    context.getDefaultExtensions(), DataRecoveryTestUtils.createExecutorService(10));\n+\n+            Services.startAsync(localContainer, executorService)\n+                    .thenRun(new DataRecoveryTestUtils.Worker(localContainer, segments.get(containerId))).join();\n+\n+            for (String segmentName : segmentByContainers.get(containerId)) {\n+                SegmentProperties props = localContainer.getStreamSegmentInfo(segmentName, TIMEOUT).join();\n+                Assert.assertEquals(\"Segment length mismatch \", data.length, props.getLength());\n+                if (sealedSegments.contains(segmentName)) {\n+                    Assert.assertTrue(\"Segment should have been sealed\", props.isSealed());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNDk3Ng=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 262}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI4NjQ3OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoyNTo0MlrOGzc-lw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjoyOTo1MlrOG2dveA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNTMzNQ==", "bodyText": "localContainer.close() will do the same", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456605335", "createdAt": "2020-07-17T18:25:42Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "diffHunk": "@@ -0,0 +1,348 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.SyncStorage;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.mocks.InMemoryDurableDataLogFactory;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorage;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorageFactory;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import lombok.Cleanup;\n+import lombok.val;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+/**\n+ * Tests for DebugStreamSegmentContainer class.\n+ */\n+public class DebugStreamSegmentContainerTests extends ThreadPooledTestSuite {\n+    private static final int MIN_SEGMENT_LENGTH = 0; // Used in randomly generating the length for a segment\n+    private static final int MAX_SEGMENT_LENGTH = 10100; // Used in randomly generating the length for a segment\n+    private static final int CONTAINER_ID = 1234567;\n+    private static final int EXPECTED_PINNED_SEGMENT_COUNT = 1;\n+    private static final int MAX_DATA_LOG_APPEND_SIZE = 100 * 1024;\n+    private static final int TEST_TIMEOUT_MILLIS = 10 * 1000;\n+    private static final Duration TIMEOUT = Duration.ofMillis(TEST_TIMEOUT_MILLIS);\n+    private static final Random RANDOM = new Random(1234);\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, 10 * 60)\n+            .build();\n+\n+    private static final DurableLogConfig DEFAULT_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10 * 1024 * 1024L)\n+            .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+            .build();\n+\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig DEFAULT_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+            .build();\n+    private static final ContainerConfig CONTAINER_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+            .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 200 + EXPECTED_PINNED_SEGMENT_COUNT)\n+            .build();\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+\n+    @Rule\n+    public Timeout globalTimeout = Timeout.millis(TEST_TIMEOUT_MILLIS);\n+\n+    /**\n+     * Tests the ability to create Segments.\n+     */\n+    @Test\n+    public void testCreateStreamSegment() {\n+        int maxSegmentCount = 100;\n+        final int createdSegmentCount = maxSegmentCount * 2;\n+\n+        // Sets up dataLogFactory, readIndexFactory, attributeIndexFactory etc for the DebugSegmentContainer.\n+        @Cleanup\n+        TestContext context = createContext();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DEFAULT_DURABLE_LOG_CONFIG, context.dataLogFactory, executorService());\n+        // Starts a DebugSegmentContainer.\n+        @Cleanup\n+        MetadataCleanupContainer localContainer = new MetadataCleanupContainer(CONTAINER_ID, CONTAINER_CONFIG, localDurableLogFactory,\n+                context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, context.storageFactory,\n+                context.getDefaultExtensions(), executorService());\n+        localContainer.startAsync().awaitRunning();\n+\n+        // Record details(name, length & sealed status) of each segment to be created.\n+        ArrayList<String> segments = new ArrayList<>();\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        long[] segmentLengths = new long[createdSegmentCount];\n+        boolean[] segmentSealedStatus = new boolean[createdSegmentCount];\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            segmentLengths[i] = MIN_SEGMENT_LENGTH + RANDOM.nextInt(MAX_SEGMENT_LENGTH - MIN_SEGMENT_LENGTH);\n+            segmentSealedStatus[i] = RANDOM.nextBoolean();\n+            String name = \"Segment_\" + i;\n+            segments.add(name);\n+            futures.add(localContainer.createStreamSegment(name, segmentLengths[i], segmentSealedStatus[i]));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        // Verify the Segments are still there with their length & sealed status.\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            SegmentProperties props = localContainer.getStreamSegmentInfo(segments.get(i), TIMEOUT).join();\n+            Assert.assertEquals(\"Segment length mismatch \", segmentLengths[i], props.getLength());\n+            Assert.assertEquals(\"Segment sealed status mismatch\", segmentSealedStatus[i], props.isSealed());\n+        }\n+        localContainer.stopAsync().awaitTerminated();\n+    }\n+\n+    /**\n+     * Use a storage instance to create segments. List the segments from the storage and recreate them.\n+     */\n+    @Test\n+    public void testEndToEnd() {\n+        // Segments are mapped to four different containers.\n+        // DebugSegmentContainer for each container Id is created and segments belonging to that container are recovered.\n+        int containerCount = 4;\n+        int segmentsToCreateCount = 50;\n+\n+        // Create a storage.\n+        @Cleanup\n+        val baseStorage = new InMemoryStorage();\n+        @Cleanup\n+        val s = new RollingStorage(baseStorage, new SegmentRollingPolicy(1));\n+        s.initialize(1);\n+\n+        // Record details(name, container Id & sealed status) of each segment to be created.\n+        Set<String> sealedSegments = new HashSet<>();\n+        byte[] data = \"data\".getBytes();\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        int[] segmentsCountByContainer = new int[containerCount];\n+        Map<Integer, ArrayList<String>> segmentByContainers = new HashMap<>();\n+\n+        // Create segments and get their container Ids, sealed status and names to verify.\n+        for (int i = 0; i < segmentsToCreateCount; i++) {\n+            String segmentName = \"segment-\" + RANDOM.nextInt();\n+\n+            // Count segments by each container Id.\n+            segmentsCountByContainer[segToConMapper.getContainerId(segmentName)]++;\n+\n+            // Use segmentName to map to different containers.\n+            int containerId = segToConMapper.getContainerId(segmentName);\n+            ArrayList<String> segmentsList = segmentByContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(segmentName);\n+                segmentByContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentByContainers.get(containerId).add(segmentName);\n+            }\n+\n+            // Create segments, write data and randomly seal some of them.\n+            try {\n+                val wh1 = s.create(segmentName);\n+                // Write data.\n+                s.write(wh1, 0, new ByteArrayInputStream(data), data.length);\n+                if (RANDOM.nextInt(2) == 1) {\n+                    s.seal(wh1);\n+                    sealedSegments.add(segmentName);\n+                }\n+            } catch (StreamSegmentException e) {\n+                Assert.fail(\"Exception occurred while test execution.\");\n+            }\n+        }\n+\n+        // Keeps count of segments recovered in all container Ids.\n+        int segmentsRecoveredCount = 0;\n+\n+        // List all segments\n+        Map<Integer, List<SegmentProperties>> segments = null;\n+        try {\n+            segments = DataRecoveryTestUtils.listAllSegments(new AsyncStorageWrapper(s,\n+                    DataRecoveryTestUtils.createExecutorService(10)), containerCount);\n+        } catch (IOException e) {\n+            Assert.fail(\"Exception occurred while listing segments.\");\n+        }\n+\n+        // Verify count of segments listed.\n+        for (int i = 0; i < segments.size(); i++) {\n+            segmentsRecoveredCount += segments.get(i).size();\n+            Assert.assertTrue(\"Number of segments listed is less than the number of segments created using this container.\",\n+                    segments.get(i).size() >= segmentsCountByContainer[i]);\n+        }\n+        Assert.assertTrue(\"Total number of segments created is less than the number of segments created.\",\n+                segmentsRecoveredCount >= segmentsToCreateCount);\n+\n+        @Cleanup\n+        TestContext context = createContext();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DEFAULT_DURABLE_LOG_CONFIG, context.dataLogFactory,\n+                DataRecoveryTestUtils.createExecutorService(10));\n+\n+        // Recover all segments\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            @Cleanup\n+            MetadataCleanupContainer localContainer = new MetadataCleanupContainer(containerId, CONTAINER_CONFIG, localDurableLogFactory,\n+                    context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, context.storageFactory,\n+                    context.getDefaultExtensions(), DataRecoveryTestUtils.createExecutorService(10));\n+\n+            Services.startAsync(localContainer, executorService)\n+                    .thenRun(new DataRecoveryTestUtils.Worker(localContainer, segments.get(containerId))).join();\n+\n+            for (String segmentName : segmentByContainers.get(containerId)) {\n+                SegmentProperties props = localContainer.getStreamSegmentInfo(segmentName, TIMEOUT).join();\n+                Assert.assertEquals(\"Segment length mismatch \", data.length, props.getLength());\n+                if (sealedSegments.contains(segmentName)) {\n+                    Assert.assertTrue(\"Segment should have been sealed\", props.isSealed());\n+                }\n+            }\n+            Services.stopAsync(localContainer, executorService).join();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 265}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2MzU3Ng==", "bodyText": "Using @cleanup now.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459763576", "createdAt": "2020-07-23T22:29:52Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "diffHunk": "@@ -0,0 +1,348 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.SyncStorage;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.mocks.InMemoryDurableDataLogFactory;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorage;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorageFactory;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import lombok.Cleanup;\n+import lombok.val;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+/**\n+ * Tests for DebugStreamSegmentContainer class.\n+ */\n+public class DebugStreamSegmentContainerTests extends ThreadPooledTestSuite {\n+    private static final int MIN_SEGMENT_LENGTH = 0; // Used in randomly generating the length for a segment\n+    private static final int MAX_SEGMENT_LENGTH = 10100; // Used in randomly generating the length for a segment\n+    private static final int CONTAINER_ID = 1234567;\n+    private static final int EXPECTED_PINNED_SEGMENT_COUNT = 1;\n+    private static final int MAX_DATA_LOG_APPEND_SIZE = 100 * 1024;\n+    private static final int TEST_TIMEOUT_MILLIS = 10 * 1000;\n+    private static final Duration TIMEOUT = Duration.ofMillis(TEST_TIMEOUT_MILLIS);\n+    private static final Random RANDOM = new Random(1234);\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, 10 * 60)\n+            .build();\n+\n+    private static final DurableLogConfig DEFAULT_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10 * 1024 * 1024L)\n+            .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+            .build();\n+\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig DEFAULT_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+            .build();\n+    private static final ContainerConfig CONTAINER_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+            .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 200 + EXPECTED_PINNED_SEGMENT_COUNT)\n+            .build();\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+\n+    @Rule\n+    public Timeout globalTimeout = Timeout.millis(TEST_TIMEOUT_MILLIS);\n+\n+    /**\n+     * Tests the ability to create Segments.\n+     */\n+    @Test\n+    public void testCreateStreamSegment() {\n+        int maxSegmentCount = 100;\n+        final int createdSegmentCount = maxSegmentCount * 2;\n+\n+        // Sets up dataLogFactory, readIndexFactory, attributeIndexFactory etc for the DebugSegmentContainer.\n+        @Cleanup\n+        TestContext context = createContext();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DEFAULT_DURABLE_LOG_CONFIG, context.dataLogFactory, executorService());\n+        // Starts a DebugSegmentContainer.\n+        @Cleanup\n+        MetadataCleanupContainer localContainer = new MetadataCleanupContainer(CONTAINER_ID, CONTAINER_CONFIG, localDurableLogFactory,\n+                context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, context.storageFactory,\n+                context.getDefaultExtensions(), executorService());\n+        localContainer.startAsync().awaitRunning();\n+\n+        // Record details(name, length & sealed status) of each segment to be created.\n+        ArrayList<String> segments = new ArrayList<>();\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        long[] segmentLengths = new long[createdSegmentCount];\n+        boolean[] segmentSealedStatus = new boolean[createdSegmentCount];\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            segmentLengths[i] = MIN_SEGMENT_LENGTH + RANDOM.nextInt(MAX_SEGMENT_LENGTH - MIN_SEGMENT_LENGTH);\n+            segmentSealedStatus[i] = RANDOM.nextBoolean();\n+            String name = \"Segment_\" + i;\n+            segments.add(name);\n+            futures.add(localContainer.createStreamSegment(name, segmentLengths[i], segmentSealedStatus[i]));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        // Verify the Segments are still there with their length & sealed status.\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            SegmentProperties props = localContainer.getStreamSegmentInfo(segments.get(i), TIMEOUT).join();\n+            Assert.assertEquals(\"Segment length mismatch \", segmentLengths[i], props.getLength());\n+            Assert.assertEquals(\"Segment sealed status mismatch\", segmentSealedStatus[i], props.isSealed());\n+        }\n+        localContainer.stopAsync().awaitTerminated();\n+    }\n+\n+    /**\n+     * Use a storage instance to create segments. List the segments from the storage and recreate them.\n+     */\n+    @Test\n+    public void testEndToEnd() {\n+        // Segments are mapped to four different containers.\n+        // DebugSegmentContainer for each container Id is created and segments belonging to that container are recovered.\n+        int containerCount = 4;\n+        int segmentsToCreateCount = 50;\n+\n+        // Create a storage.\n+        @Cleanup\n+        val baseStorage = new InMemoryStorage();\n+        @Cleanup\n+        val s = new RollingStorage(baseStorage, new SegmentRollingPolicy(1));\n+        s.initialize(1);\n+\n+        // Record details(name, container Id & sealed status) of each segment to be created.\n+        Set<String> sealedSegments = new HashSet<>();\n+        byte[] data = \"data\".getBytes();\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        int[] segmentsCountByContainer = new int[containerCount];\n+        Map<Integer, ArrayList<String>> segmentByContainers = new HashMap<>();\n+\n+        // Create segments and get their container Ids, sealed status and names to verify.\n+        for (int i = 0; i < segmentsToCreateCount; i++) {\n+            String segmentName = \"segment-\" + RANDOM.nextInt();\n+\n+            // Count segments by each container Id.\n+            segmentsCountByContainer[segToConMapper.getContainerId(segmentName)]++;\n+\n+            // Use segmentName to map to different containers.\n+            int containerId = segToConMapper.getContainerId(segmentName);\n+            ArrayList<String> segmentsList = segmentByContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(segmentName);\n+                segmentByContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentByContainers.get(containerId).add(segmentName);\n+            }\n+\n+            // Create segments, write data and randomly seal some of them.\n+            try {\n+                val wh1 = s.create(segmentName);\n+                // Write data.\n+                s.write(wh1, 0, new ByteArrayInputStream(data), data.length);\n+                if (RANDOM.nextInt(2) == 1) {\n+                    s.seal(wh1);\n+                    sealedSegments.add(segmentName);\n+                }\n+            } catch (StreamSegmentException e) {\n+                Assert.fail(\"Exception occurred while test execution.\");\n+            }\n+        }\n+\n+        // Keeps count of segments recovered in all container Ids.\n+        int segmentsRecoveredCount = 0;\n+\n+        // List all segments\n+        Map<Integer, List<SegmentProperties>> segments = null;\n+        try {\n+            segments = DataRecoveryTestUtils.listAllSegments(new AsyncStorageWrapper(s,\n+                    DataRecoveryTestUtils.createExecutorService(10)), containerCount);\n+        } catch (IOException e) {\n+            Assert.fail(\"Exception occurred while listing segments.\");\n+        }\n+\n+        // Verify count of segments listed.\n+        for (int i = 0; i < segments.size(); i++) {\n+            segmentsRecoveredCount += segments.get(i).size();\n+            Assert.assertTrue(\"Number of segments listed is less than the number of segments created using this container.\",\n+                    segments.get(i).size() >= segmentsCountByContainer[i]);\n+        }\n+        Assert.assertTrue(\"Total number of segments created is less than the number of segments created.\",\n+                segmentsRecoveredCount >= segmentsToCreateCount);\n+\n+        @Cleanup\n+        TestContext context = createContext();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DEFAULT_DURABLE_LOG_CONFIG, context.dataLogFactory,\n+                DataRecoveryTestUtils.createExecutorService(10));\n+\n+        // Recover all segments\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            @Cleanup\n+            MetadataCleanupContainer localContainer = new MetadataCleanupContainer(containerId, CONTAINER_CONFIG, localDurableLogFactory,\n+                    context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, context.storageFactory,\n+                    context.getDefaultExtensions(), DataRecoveryTestUtils.createExecutorService(10));\n+\n+            Services.startAsync(localContainer, executorService)\n+                    .thenRun(new DataRecoveryTestUtils.Worker(localContainer, segments.get(containerId))).join();\n+\n+            for (String segmentName : segmentByContainers.get(containerId)) {\n+                SegmentProperties props = localContainer.getStreamSegmentInfo(segmentName, TIMEOUT).join();\n+                Assert.assertEquals(\"Segment length mismatch \", data.length, props.getLength());\n+                if (sealedSegments.contains(segmentName)) {\n+                    Assert.assertTrue(\"Segment should have been sealed\", props.isSealed());\n+                }\n+            }\n+            Services.stopAsync(localContainer, executorService).join();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNTMzNQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 265}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI4Nzc1OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoyNjowN1rOGzc_Wg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjozMDo1N1rOG2dxFA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNTUzMA==", "bodyText": "Do you actually need this?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456605530", "createdAt": "2020-07-17T18:26:07Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "diffHunk": "@@ -0,0 +1,348 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.SyncStorage;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.mocks.InMemoryDurableDataLogFactory;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorage;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorageFactory;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import lombok.Cleanup;\n+import lombok.val;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+/**\n+ * Tests for DebugStreamSegmentContainer class.\n+ */\n+public class DebugStreamSegmentContainerTests extends ThreadPooledTestSuite {\n+    private static final int MIN_SEGMENT_LENGTH = 0; // Used in randomly generating the length for a segment\n+    private static final int MAX_SEGMENT_LENGTH = 10100; // Used in randomly generating the length for a segment\n+    private static final int CONTAINER_ID = 1234567;\n+    private static final int EXPECTED_PINNED_SEGMENT_COUNT = 1;\n+    private static final int MAX_DATA_LOG_APPEND_SIZE = 100 * 1024;\n+    private static final int TEST_TIMEOUT_MILLIS = 10 * 1000;\n+    private static final Duration TIMEOUT = Duration.ofMillis(TEST_TIMEOUT_MILLIS);\n+    private static final Random RANDOM = new Random(1234);\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, 10 * 60)\n+            .build();\n+\n+    private static final DurableLogConfig DEFAULT_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10 * 1024 * 1024L)\n+            .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+            .build();\n+\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig DEFAULT_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+            .build();\n+    private static final ContainerConfig CONTAINER_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+            .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 200 + EXPECTED_PINNED_SEGMENT_COUNT)\n+            .build();\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+\n+    @Rule\n+    public Timeout globalTimeout = Timeout.millis(TEST_TIMEOUT_MILLIS);\n+\n+    /**\n+     * Tests the ability to create Segments.\n+     */\n+    @Test\n+    public void testCreateStreamSegment() {\n+        int maxSegmentCount = 100;\n+        final int createdSegmentCount = maxSegmentCount * 2;\n+\n+        // Sets up dataLogFactory, readIndexFactory, attributeIndexFactory etc for the DebugSegmentContainer.\n+        @Cleanup\n+        TestContext context = createContext();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DEFAULT_DURABLE_LOG_CONFIG, context.dataLogFactory, executorService());\n+        // Starts a DebugSegmentContainer.\n+        @Cleanup\n+        MetadataCleanupContainer localContainer = new MetadataCleanupContainer(CONTAINER_ID, CONTAINER_CONFIG, localDurableLogFactory,\n+                context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, context.storageFactory,\n+                context.getDefaultExtensions(), executorService());\n+        localContainer.startAsync().awaitRunning();\n+\n+        // Record details(name, length & sealed status) of each segment to be created.\n+        ArrayList<String> segments = new ArrayList<>();\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        long[] segmentLengths = new long[createdSegmentCount];\n+        boolean[] segmentSealedStatus = new boolean[createdSegmentCount];\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            segmentLengths[i] = MIN_SEGMENT_LENGTH + RANDOM.nextInt(MAX_SEGMENT_LENGTH - MIN_SEGMENT_LENGTH);\n+            segmentSealedStatus[i] = RANDOM.nextBoolean();\n+            String name = \"Segment_\" + i;\n+            segments.add(name);\n+            futures.add(localContainer.createStreamSegment(name, segmentLengths[i], segmentSealedStatus[i]));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        // Verify the Segments are still there with their length & sealed status.\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            SegmentProperties props = localContainer.getStreamSegmentInfo(segments.get(i), TIMEOUT).join();\n+            Assert.assertEquals(\"Segment length mismatch \", segmentLengths[i], props.getLength());\n+            Assert.assertEquals(\"Segment sealed status mismatch\", segmentSealedStatus[i], props.isSealed());\n+        }\n+        localContainer.stopAsync().awaitTerminated();\n+    }\n+\n+    /**\n+     * Use a storage instance to create segments. List the segments from the storage and recreate them.\n+     */\n+    @Test\n+    public void testEndToEnd() {\n+        // Segments are mapped to four different containers.\n+        // DebugSegmentContainer for each container Id is created and segments belonging to that container are recovered.\n+        int containerCount = 4;\n+        int segmentsToCreateCount = 50;\n+\n+        // Create a storage.\n+        @Cleanup\n+        val baseStorage = new InMemoryStorage();\n+        @Cleanup\n+        val s = new RollingStorage(baseStorage, new SegmentRollingPolicy(1));\n+        s.initialize(1);\n+\n+        // Record details(name, container Id & sealed status) of each segment to be created.\n+        Set<String> sealedSegments = new HashSet<>();\n+        byte[] data = \"data\".getBytes();\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        int[] segmentsCountByContainer = new int[containerCount];\n+        Map<Integer, ArrayList<String>> segmentByContainers = new HashMap<>();\n+\n+        // Create segments and get their container Ids, sealed status and names to verify.\n+        for (int i = 0; i < segmentsToCreateCount; i++) {\n+            String segmentName = \"segment-\" + RANDOM.nextInt();\n+\n+            // Count segments by each container Id.\n+            segmentsCountByContainer[segToConMapper.getContainerId(segmentName)]++;\n+\n+            // Use segmentName to map to different containers.\n+            int containerId = segToConMapper.getContainerId(segmentName);\n+            ArrayList<String> segmentsList = segmentByContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(segmentName);\n+                segmentByContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentByContainers.get(containerId).add(segmentName);\n+            }\n+\n+            // Create segments, write data and randomly seal some of them.\n+            try {\n+                val wh1 = s.create(segmentName);\n+                // Write data.\n+                s.write(wh1, 0, new ByteArrayInputStream(data), data.length);\n+                if (RANDOM.nextInt(2) == 1) {\n+                    s.seal(wh1);\n+                    sealedSegments.add(segmentName);\n+                }\n+            } catch (StreamSegmentException e) {\n+                Assert.fail(\"Exception occurred while test execution.\");\n+            }\n+        }\n+\n+        // Keeps count of segments recovered in all container Ids.\n+        int segmentsRecoveredCount = 0;\n+\n+        // List all segments\n+        Map<Integer, List<SegmentProperties>> segments = null;\n+        try {\n+            segments = DataRecoveryTestUtils.listAllSegments(new AsyncStorageWrapper(s,\n+                    DataRecoveryTestUtils.createExecutorService(10)), containerCount);\n+        } catch (IOException e) {\n+            Assert.fail(\"Exception occurred while listing segments.\");\n+        }\n+\n+        // Verify count of segments listed.\n+        for (int i = 0; i < segments.size(); i++) {\n+            segmentsRecoveredCount += segments.get(i).size();\n+            Assert.assertTrue(\"Number of segments listed is less than the number of segments created using this container.\",\n+                    segments.get(i).size() >= segmentsCountByContainer[i]);\n+        }\n+        Assert.assertTrue(\"Total number of segments created is less than the number of segments created.\",\n+                segmentsRecoveredCount >= segmentsToCreateCount);\n+\n+        @Cleanup\n+        TestContext context = createContext();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DEFAULT_DURABLE_LOG_CONFIG, context.dataLogFactory,\n+                DataRecoveryTestUtils.createExecutorService(10));\n+\n+        // Recover all segments\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            @Cleanup\n+            MetadataCleanupContainer localContainer = new MetadataCleanupContainer(containerId, CONTAINER_CONFIG, localDurableLogFactory,\n+                    context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, context.storageFactory,\n+                    context.getDefaultExtensions(), DataRecoveryTestUtils.createExecutorService(10));\n+\n+            Services.startAsync(localContainer, executorService)\n+                    .thenRun(new DataRecoveryTestUtils.Worker(localContainer, segments.get(containerId))).join();\n+\n+            for (String segmentName : segmentByContainers.get(containerId)) {\n+                SegmentProperties props = localContainer.getStreamSegmentInfo(segmentName, TIMEOUT).join();\n+                Assert.assertEquals(\"Segment length mismatch \", data.length, props.getLength());\n+                if (sealedSegments.contains(segmentName)) {\n+                    Assert.assertTrue(\"Segment should have been sealed\", props.isSealed());\n+                }\n+            }\n+            Services.stopAsync(localContainer, executorService).join();\n+        }\n+    }\n+\n+    public static class MetadataCleanupContainer extends DebugStreamSegmentContainer {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 269}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2Mzk4OA==", "bodyText": "Yes.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459763988", "createdAt": "2020-07-23T22:30:57Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "diffHunk": "@@ -0,0 +1,348 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentException;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.SyncStorage;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.mocks.InMemoryDurableDataLogFactory;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorage;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorageFactory;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import lombok.Cleanup;\n+import lombok.val;\n+import org.junit.Assert;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.ByteArrayInputStream;\n+import java.io.IOException;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+/**\n+ * Tests for DebugStreamSegmentContainer class.\n+ */\n+public class DebugStreamSegmentContainerTests extends ThreadPooledTestSuite {\n+    private static final int MIN_SEGMENT_LENGTH = 0; // Used in randomly generating the length for a segment\n+    private static final int MAX_SEGMENT_LENGTH = 10100; // Used in randomly generating the length for a segment\n+    private static final int CONTAINER_ID = 1234567;\n+    private static final int EXPECTED_PINNED_SEGMENT_COUNT = 1;\n+    private static final int MAX_DATA_LOG_APPEND_SIZE = 100 * 1024;\n+    private static final int TEST_TIMEOUT_MILLIS = 10 * 1000;\n+    private static final Duration TIMEOUT = Duration.ofMillis(TEST_TIMEOUT_MILLIS);\n+    private static final Random RANDOM = new Random(1234);\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, 10 * 60)\n+            .build();\n+\n+    private static final DurableLogConfig DEFAULT_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10 * 1024 * 1024L)\n+            .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+            .build();\n+\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig DEFAULT_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+            .build();\n+    private static final ContainerConfig CONTAINER_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+            .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 200 + EXPECTED_PINNED_SEGMENT_COUNT)\n+            .build();\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+\n+    @Rule\n+    public Timeout globalTimeout = Timeout.millis(TEST_TIMEOUT_MILLIS);\n+\n+    /**\n+     * Tests the ability to create Segments.\n+     */\n+    @Test\n+    public void testCreateStreamSegment() {\n+        int maxSegmentCount = 100;\n+        final int createdSegmentCount = maxSegmentCount * 2;\n+\n+        // Sets up dataLogFactory, readIndexFactory, attributeIndexFactory etc for the DebugSegmentContainer.\n+        @Cleanup\n+        TestContext context = createContext();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DEFAULT_DURABLE_LOG_CONFIG, context.dataLogFactory, executorService());\n+        // Starts a DebugSegmentContainer.\n+        @Cleanup\n+        MetadataCleanupContainer localContainer = new MetadataCleanupContainer(CONTAINER_ID, CONTAINER_CONFIG, localDurableLogFactory,\n+                context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, context.storageFactory,\n+                context.getDefaultExtensions(), executorService());\n+        localContainer.startAsync().awaitRunning();\n+\n+        // Record details(name, length & sealed status) of each segment to be created.\n+        ArrayList<String> segments = new ArrayList<>();\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        long[] segmentLengths = new long[createdSegmentCount];\n+        boolean[] segmentSealedStatus = new boolean[createdSegmentCount];\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            segmentLengths[i] = MIN_SEGMENT_LENGTH + RANDOM.nextInt(MAX_SEGMENT_LENGTH - MIN_SEGMENT_LENGTH);\n+            segmentSealedStatus[i] = RANDOM.nextBoolean();\n+            String name = \"Segment_\" + i;\n+            segments.add(name);\n+            futures.add(localContainer.createStreamSegment(name, segmentLengths[i], segmentSealedStatus[i]));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        // Verify the Segments are still there with their length & sealed status.\n+        for (int i = 0; i < createdSegmentCount; i++) {\n+            SegmentProperties props = localContainer.getStreamSegmentInfo(segments.get(i), TIMEOUT).join();\n+            Assert.assertEquals(\"Segment length mismatch \", segmentLengths[i], props.getLength());\n+            Assert.assertEquals(\"Segment sealed status mismatch\", segmentSealedStatus[i], props.isSealed());\n+        }\n+        localContainer.stopAsync().awaitTerminated();\n+    }\n+\n+    /**\n+     * Use a storage instance to create segments. List the segments from the storage and recreate them.\n+     */\n+    @Test\n+    public void testEndToEnd() {\n+        // Segments are mapped to four different containers.\n+        // DebugSegmentContainer for each container Id is created and segments belonging to that container are recovered.\n+        int containerCount = 4;\n+        int segmentsToCreateCount = 50;\n+\n+        // Create a storage.\n+        @Cleanup\n+        val baseStorage = new InMemoryStorage();\n+        @Cleanup\n+        val s = new RollingStorage(baseStorage, new SegmentRollingPolicy(1));\n+        s.initialize(1);\n+\n+        // Record details(name, container Id & sealed status) of each segment to be created.\n+        Set<String> sealedSegments = new HashSet<>();\n+        byte[] data = \"data\".getBytes();\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(containerCount);\n+        int[] segmentsCountByContainer = new int[containerCount];\n+        Map<Integer, ArrayList<String>> segmentByContainers = new HashMap<>();\n+\n+        // Create segments and get their container Ids, sealed status and names to verify.\n+        for (int i = 0; i < segmentsToCreateCount; i++) {\n+            String segmentName = \"segment-\" + RANDOM.nextInt();\n+\n+            // Count segments by each container Id.\n+            segmentsCountByContainer[segToConMapper.getContainerId(segmentName)]++;\n+\n+            // Use segmentName to map to different containers.\n+            int containerId = segToConMapper.getContainerId(segmentName);\n+            ArrayList<String> segmentsList = segmentByContainers.get(containerId);\n+            if (segmentsList == null) {\n+                segmentsList = new ArrayList<>();\n+                segmentsList.add(segmentName);\n+                segmentByContainers.put(containerId, segmentsList);\n+            } else {\n+                segmentByContainers.get(containerId).add(segmentName);\n+            }\n+\n+            // Create segments, write data and randomly seal some of them.\n+            try {\n+                val wh1 = s.create(segmentName);\n+                // Write data.\n+                s.write(wh1, 0, new ByteArrayInputStream(data), data.length);\n+                if (RANDOM.nextInt(2) == 1) {\n+                    s.seal(wh1);\n+                    sealedSegments.add(segmentName);\n+                }\n+            } catch (StreamSegmentException e) {\n+                Assert.fail(\"Exception occurred while test execution.\");\n+            }\n+        }\n+\n+        // Keeps count of segments recovered in all container Ids.\n+        int segmentsRecoveredCount = 0;\n+\n+        // List all segments\n+        Map<Integer, List<SegmentProperties>> segments = null;\n+        try {\n+            segments = DataRecoveryTestUtils.listAllSegments(new AsyncStorageWrapper(s,\n+                    DataRecoveryTestUtils.createExecutorService(10)), containerCount);\n+        } catch (IOException e) {\n+            Assert.fail(\"Exception occurred while listing segments.\");\n+        }\n+\n+        // Verify count of segments listed.\n+        for (int i = 0; i < segments.size(); i++) {\n+            segmentsRecoveredCount += segments.get(i).size();\n+            Assert.assertTrue(\"Number of segments listed is less than the number of segments created using this container.\",\n+                    segments.get(i).size() >= segmentsCountByContainer[i]);\n+        }\n+        Assert.assertTrue(\"Total number of segments created is less than the number of segments created.\",\n+                segmentsRecoveredCount >= segmentsToCreateCount);\n+\n+        @Cleanup\n+        TestContext context = createContext();\n+        OperationLogFactory localDurableLogFactory = new DurableLogFactory(DEFAULT_DURABLE_LOG_CONFIG, context.dataLogFactory,\n+                DataRecoveryTestUtils.createExecutorService(10));\n+\n+        // Recover all segments\n+        for (int containerId = 0; containerId < containerCount; containerId++) {\n+            @Cleanup\n+            MetadataCleanupContainer localContainer = new MetadataCleanupContainer(containerId, CONTAINER_CONFIG, localDurableLogFactory,\n+                    context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, context.storageFactory,\n+                    context.getDefaultExtensions(), DataRecoveryTestUtils.createExecutorService(10));\n+\n+            Services.startAsync(localContainer, executorService)\n+                    .thenRun(new DataRecoveryTestUtils.Worker(localContainer, segments.get(containerId))).join();\n+\n+            for (String segmentName : segmentByContainers.get(containerId)) {\n+                SegmentProperties props = localContainer.getStreamSegmentInfo(segmentName, TIMEOUT).join();\n+                Assert.assertEquals(\"Segment length mismatch \", data.length, props.getLength());\n+                if (sealedSegments.contains(segmentName)) {\n+                    Assert.assertTrue(\"Segment should have been sealed\", props.isSealed());\n+                }\n+            }\n+            Services.stopAsync(localContainer, executorService).join();\n+        }\n+    }\n+\n+    public static class MetadataCleanupContainer extends DebugStreamSegmentContainer {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNTUzMA=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 269}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI5MjA5OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoyNzoyNlrOGzdCEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjozMTowNVrOG2dxMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNjIyNg==", "bodyText": "You already have this defined in your configBuilder object", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456606226", "createdAt": "2020-07-17T18:27:26Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "diffHunk": "@@ -146,7 +178,98 @@ protected boolean appendAfterMerging() {\n         return true;\n     }\n \n-    //endregion\n+    /**\n+     * Tests an end-to-end scenario for the DebugSegmentContainer. SegmentStore creates some segments and then only \n+     * persisted storage is used to re-create all segments.\n+     * @throws Exception If an exception occurred.\n+     */\n+    @Test\n+    public void testDataRecovery() throws Exception {\n+        endToEndDebugSegmentContainer();\n+    }\n+\n+    /**\n+     * End to end test to verify DebugSegmentContainer process.\n+     * @throws Exception If an exception occurred.\n+     */\n+    public void endToEndDebugSegmentContainer() throws Exception {\n+        int containerCount = 4;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2NDAxOA==", "bodyText": "OK.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459764018", "createdAt": "2020-07-23T22:31:05Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "diffHunk": "@@ -146,7 +178,98 @@ protected boolean appendAfterMerging() {\n         return true;\n     }\n \n-    //endregion\n+    /**\n+     * Tests an end-to-end scenario for the DebugSegmentContainer. SegmentStore creates some segments and then only \n+     * persisted storage is used to re-create all segments.\n+     * @throws Exception If an exception occurred.\n+     */\n+    @Test\n+    public void testDataRecovery() throws Exception {\n+        endToEndDebugSegmentContainer();\n+    }\n+\n+    /**\n+     * End to end test to verify DebugSegmentContainer process.\n+     * @throws Exception If an exception occurred.\n+     */\n+    public void endToEndDebugSegmentContainer() throws Exception {\n+        int containerCount = 4;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNjIyNg=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI5MzgyOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoyNzo1OFrOGzdDJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjozMToxNFrOG2dxcg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNjUwMg==", "bodyText": "tier2", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456606502", "createdAt": "2020-07-17T18:27:58Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "diffHunk": "@@ -146,7 +178,98 @@ protected boolean appendAfterMerging() {\n         return true;\n     }\n \n-    //endregion\n+    /**\n+     * Tests an end-to-end scenario for the DebugSegmentContainer. SegmentStore creates some segments and then only \n+     * persisted storage is used to re-create all segments.\n+     * @throws Exception If an exception occurred.\n+     */\n+    @Test\n+    public void testDataRecovery() throws Exception {\n+        endToEndDebugSegmentContainer();\n+    }\n+\n+    /**\n+     * End to end test to verify DebugSegmentContainer process.\n+     * @throws Exception If an exception occurred.\n+     */\n+    public void endToEndDebugSegmentContainer() throws Exception {\n+        int containerCount = 4;\n+        ArrayList<String> segmentNames;\n+        HashMap<String, ArrayList<String>> transactionsBySegment;\n+        HashMap<String, Long> lengths = new HashMap<>();\n+        ArrayList<ByteBuf> appendBuffers = new ArrayList<>();\n+        HashMap<String, ByteArrayOutputStream> segmentContents = new HashMap<>();\n+\n+        try (val builder = createBuilder(0);\n+             val readOnlyBuilder = createReadOnlyBuilder()) {\n+            val segmentStore = builder.createStreamSegmentService();\n+            val readOnlySegmentStore = readOnlyBuilder.createStreamSegmentService();\n+\n+            segmentNames = createSegments(segmentStore);\n+            log.info(\"Created Segments: {}.\", String.join(\", \", segmentNames));\n+            transactionsBySegment = createTransactions(segmentNames, segmentStore);\n+            log.info(\"Created Transactions: {}.\", transactionsBySegment.values().stream().flatMap(Collection::stream).collect(Collectors.joining(\", \")));\n+\n+            // Add some appends and seal segments\n+            ArrayList<String> segmentsAndTransactions = new ArrayList<>(segmentNames);\n+            transactionsBySegment.values().forEach(segmentsAndTransactions::addAll);\n+            appendData(segmentsAndTransactions, segmentContents, lengths, appendBuffers, segmentStore).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Finished appending data.\");\n+\n+            // Wait for flushing the segments to tier2\n+            waitForSegmentsInStorage(segmentNames, segmentStore, readOnlySegmentStore).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Finished waiting for segments in Storage.\");\n+\n+            // Get the persistent storage from readOnlySegmentStore.\n+            Storage tier2 = getReadOnlyStorageFactory().createStorageAdapter();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2NDA4Mg==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459764082", "createdAt": "2020-07-23T22:31:14Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "diffHunk": "@@ -146,7 +178,98 @@ protected boolean appendAfterMerging() {\n         return true;\n     }\n \n-    //endregion\n+    /**\n+     * Tests an end-to-end scenario for the DebugSegmentContainer. SegmentStore creates some segments and then only \n+     * persisted storage is used to re-create all segments.\n+     * @throws Exception If an exception occurred.\n+     */\n+    @Test\n+    public void testDataRecovery() throws Exception {\n+        endToEndDebugSegmentContainer();\n+    }\n+\n+    /**\n+     * End to end test to verify DebugSegmentContainer process.\n+     * @throws Exception If an exception occurred.\n+     */\n+    public void endToEndDebugSegmentContainer() throws Exception {\n+        int containerCount = 4;\n+        ArrayList<String> segmentNames;\n+        HashMap<String, ArrayList<String>> transactionsBySegment;\n+        HashMap<String, Long> lengths = new HashMap<>();\n+        ArrayList<ByteBuf> appendBuffers = new ArrayList<>();\n+        HashMap<String, ByteArrayOutputStream> segmentContents = new HashMap<>();\n+\n+        try (val builder = createBuilder(0);\n+             val readOnlyBuilder = createReadOnlyBuilder()) {\n+            val segmentStore = builder.createStreamSegmentService();\n+            val readOnlySegmentStore = readOnlyBuilder.createStreamSegmentService();\n+\n+            segmentNames = createSegments(segmentStore);\n+            log.info(\"Created Segments: {}.\", String.join(\", \", segmentNames));\n+            transactionsBySegment = createTransactions(segmentNames, segmentStore);\n+            log.info(\"Created Transactions: {}.\", transactionsBySegment.values().stream().flatMap(Collection::stream).collect(Collectors.joining(\", \")));\n+\n+            // Add some appends and seal segments\n+            ArrayList<String> segmentsAndTransactions = new ArrayList<>(segmentNames);\n+            transactionsBySegment.values().forEach(segmentsAndTransactions::addAll);\n+            appendData(segmentsAndTransactions, segmentContents, lengths, appendBuffers, segmentStore).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Finished appending data.\");\n+\n+            // Wait for flushing the segments to tier2\n+            waitForSegmentsInStorage(segmentNames, segmentStore, readOnlySegmentStore).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Finished waiting for segments in Storage.\");\n+\n+            // Get the persistent storage from readOnlySegmentStore.\n+            Storage tier2 = getReadOnlyStorageFactory().createStorageAdapter();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNjUwMg=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 121}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI5NzQyOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoyODo1OFrOGzdFMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjozMToyMlrOG2dxpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNzAyNg==", "bodyText": "I suggest you move this to private final fields on top of this class and name them appropriately", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456607026", "createdAt": "2020-07-17T18:28:58Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "diffHunk": "@@ -146,7 +178,98 @@ protected boolean appendAfterMerging() {\n         return true;\n     }\n \n-    //endregion\n+    /**\n+     * Tests an end-to-end scenario for the DebugSegmentContainer. SegmentStore creates some segments and then only \n+     * persisted storage is used to re-create all segments.\n+     * @throws Exception If an exception occurred.\n+     */\n+    @Test\n+    public void testDataRecovery() throws Exception {\n+        endToEndDebugSegmentContainer();\n+    }\n+\n+    /**\n+     * End to end test to verify DebugSegmentContainer process.\n+     * @throws Exception If an exception occurred.\n+     */\n+    public void endToEndDebugSegmentContainer() throws Exception {\n+        int containerCount = 4;\n+        ArrayList<String> segmentNames;\n+        HashMap<String, ArrayList<String>> transactionsBySegment;\n+        HashMap<String, Long> lengths = new HashMap<>();\n+        ArrayList<ByteBuf> appendBuffers = new ArrayList<>();\n+        HashMap<String, ByteArrayOutputStream> segmentContents = new HashMap<>();\n+\n+        try (val builder = createBuilder(0);\n+             val readOnlyBuilder = createReadOnlyBuilder()) {\n+            val segmentStore = builder.createStreamSegmentService();\n+            val readOnlySegmentStore = readOnlyBuilder.createStreamSegmentService();\n+\n+            segmentNames = createSegments(segmentStore);\n+            log.info(\"Created Segments: {}.\", String.join(\", \", segmentNames));\n+            transactionsBySegment = createTransactions(segmentNames, segmentStore);\n+            log.info(\"Created Transactions: {}.\", transactionsBySegment.values().stream().flatMap(Collection::stream).collect(Collectors.joining(\", \")));\n+\n+            // Add some appends and seal segments\n+            ArrayList<String> segmentsAndTransactions = new ArrayList<>(segmentNames);\n+            transactionsBySegment.values().forEach(segmentsAndTransactions::addAll);\n+            appendData(segmentsAndTransactions, segmentContents, lengths, appendBuffers, segmentStore).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Finished appending data.\");\n+\n+            // Wait for flushing the segments to tier2\n+            waitForSegmentsInStorage(segmentNames, segmentStore, readOnlySegmentStore).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Finished waiting for segments in Storage.\");\n+\n+            // Get the persistent storage from readOnlySegmentStore.\n+            Storage tier2 = getReadOnlyStorageFactory().createStorageAdapter();\n+\n+            // Delete container metadata segment and attribute index segment for each container Id from the persistent storage.\n+            for (int containerId = 0; containerId < containerCount; containerId++) {\n+                DataRecoveryTestUtils.deleteContainerMetadataSegments(tier2, containerId);\n+            }\n+\n+            // List all segments from the long term storage.\n+            Map<Integer, List<SegmentProperties>> segments = DataRecoveryTestUtils.listAllSegments(tier2, containerCount);\n+\n+            // Configurations for DebugSegmentContainer\n+            final ContainerConfig containerConfig = ContainerConfig", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2NDEzNQ==", "bodyText": "Ok.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459764135", "createdAt": "2020-07-23T22:31:22Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "diffHunk": "@@ -146,7 +178,98 @@ protected boolean appendAfterMerging() {\n         return true;\n     }\n \n-    //endregion\n+    /**\n+     * Tests an end-to-end scenario for the DebugSegmentContainer. SegmentStore creates some segments and then only \n+     * persisted storage is used to re-create all segments.\n+     * @throws Exception If an exception occurred.\n+     */\n+    @Test\n+    public void testDataRecovery() throws Exception {\n+        endToEndDebugSegmentContainer();\n+    }\n+\n+    /**\n+     * End to end test to verify DebugSegmentContainer process.\n+     * @throws Exception If an exception occurred.\n+     */\n+    public void endToEndDebugSegmentContainer() throws Exception {\n+        int containerCount = 4;\n+        ArrayList<String> segmentNames;\n+        HashMap<String, ArrayList<String>> transactionsBySegment;\n+        HashMap<String, Long> lengths = new HashMap<>();\n+        ArrayList<ByteBuf> appendBuffers = new ArrayList<>();\n+        HashMap<String, ByteArrayOutputStream> segmentContents = new HashMap<>();\n+\n+        try (val builder = createBuilder(0);\n+             val readOnlyBuilder = createReadOnlyBuilder()) {\n+            val segmentStore = builder.createStreamSegmentService();\n+            val readOnlySegmentStore = readOnlyBuilder.createStreamSegmentService();\n+\n+            segmentNames = createSegments(segmentStore);\n+            log.info(\"Created Segments: {}.\", String.join(\", \", segmentNames));\n+            transactionsBySegment = createTransactions(segmentNames, segmentStore);\n+            log.info(\"Created Transactions: {}.\", transactionsBySegment.values().stream().flatMap(Collection::stream).collect(Collectors.joining(\", \")));\n+\n+            // Add some appends and seal segments\n+            ArrayList<String> segmentsAndTransactions = new ArrayList<>(segmentNames);\n+            transactionsBySegment.values().forEach(segmentsAndTransactions::addAll);\n+            appendData(segmentsAndTransactions, segmentContents, lengths, appendBuffers, segmentStore).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Finished appending data.\");\n+\n+            // Wait for flushing the segments to tier2\n+            waitForSegmentsInStorage(segmentNames, segmentStore, readOnlySegmentStore).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Finished waiting for segments in Storage.\");\n+\n+            // Get the persistent storage from readOnlySegmentStore.\n+            Storage tier2 = getReadOnlyStorageFactory().createStorageAdapter();\n+\n+            // Delete container metadata segment and attribute index segment for each container Id from the persistent storage.\n+            for (int containerId = 0; containerId < containerCount; containerId++) {\n+                DataRecoveryTestUtils.deleteContainerMetadataSegments(tier2, containerId);\n+            }\n+\n+            // List all segments from the long term storage.\n+            Map<Integer, List<SegmentProperties>> segments = DataRecoveryTestUtils.listAllSegments(tier2, containerCount);\n+\n+            // Configurations for DebugSegmentContainer\n+            final ContainerConfig containerConfig = ContainerConfig", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNzAyNg=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 132}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI5ODEzOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoyOToxMFrOGzdFmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjozMjoxN1rOG2dzAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNzEzMA==", "bodyText": "why create a new executor?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456607130", "createdAt": "2020-07-17T18:29:10Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "diffHunk": "@@ -146,7 +178,98 @@ protected boolean appendAfterMerging() {\n         return true;\n     }\n \n-    //endregion\n+    /**\n+     * Tests an end-to-end scenario for the DebugSegmentContainer. SegmentStore creates some segments and then only \n+     * persisted storage is used to re-create all segments.\n+     * @throws Exception If an exception occurred.\n+     */\n+    @Test\n+    public void testDataRecovery() throws Exception {\n+        endToEndDebugSegmentContainer();\n+    }\n+\n+    /**\n+     * End to end test to verify DebugSegmentContainer process.\n+     * @throws Exception If an exception occurred.\n+     */\n+    public void endToEndDebugSegmentContainer() throws Exception {\n+        int containerCount = 4;\n+        ArrayList<String> segmentNames;\n+        HashMap<String, ArrayList<String>> transactionsBySegment;\n+        HashMap<String, Long> lengths = new HashMap<>();\n+        ArrayList<ByteBuf> appendBuffers = new ArrayList<>();\n+        HashMap<String, ByteArrayOutputStream> segmentContents = new HashMap<>();\n+\n+        try (val builder = createBuilder(0);\n+             val readOnlyBuilder = createReadOnlyBuilder()) {\n+            val segmentStore = builder.createStreamSegmentService();\n+            val readOnlySegmentStore = readOnlyBuilder.createStreamSegmentService();\n+\n+            segmentNames = createSegments(segmentStore);\n+            log.info(\"Created Segments: {}.\", String.join(\", \", segmentNames));\n+            transactionsBySegment = createTransactions(segmentNames, segmentStore);\n+            log.info(\"Created Transactions: {}.\", transactionsBySegment.values().stream().flatMap(Collection::stream).collect(Collectors.joining(\", \")));\n+\n+            // Add some appends and seal segments\n+            ArrayList<String> segmentsAndTransactions = new ArrayList<>(segmentNames);\n+            transactionsBySegment.values().forEach(segmentsAndTransactions::addAll);\n+            appendData(segmentsAndTransactions, segmentContents, lengths, appendBuffers, segmentStore).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Finished appending data.\");\n+\n+            // Wait for flushing the segments to tier2\n+            waitForSegmentsInStorage(segmentNames, segmentStore, readOnlySegmentStore).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Finished waiting for segments in Storage.\");\n+\n+            // Get the persistent storage from readOnlySegmentStore.\n+            Storage tier2 = getReadOnlyStorageFactory().createStorageAdapter();\n+\n+            // Delete container metadata segment and attribute index segment for each container Id from the persistent storage.\n+            for (int containerId = 0; containerId < containerCount; containerId++) {\n+                DataRecoveryTestUtils.deleteContainerMetadataSegments(tier2, containerId);\n+            }\n+\n+            // List all segments from the long term storage.\n+            Map<Integer, List<SegmentProperties>> segments = DataRecoveryTestUtils.listAllSegments(tier2, containerCount);\n+\n+            // Configurations for DebugSegmentContainer\n+            final ContainerConfig containerConfig = ContainerConfig\n+                    .builder()\n+                    .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+                    .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 100)\n+                    .build();\n+            final DurableLogConfig durableLogConfig = DurableLogConfig\n+                    .builder()\n+                    .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                    .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                    .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024)\n+                    .build();\n+\n+            // Create the environment for DebugSegmentContainer using the given storageFactory.\n+            @Cleanup TestContext context = createContext(getReadOnlyStorageFactory());\n+            OperationLogFactory localDurableLogFactory = new DurableLogFactory(durableLogConfig, context.dataLogFactory,\n+                    DataRecoveryTestUtils.createExecutorService(10));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 147}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2NDQ4MQ==", "bodyText": "Removed.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459764481", "createdAt": "2020-07-23T22:32:17Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "diffHunk": "@@ -146,7 +178,98 @@ protected boolean appendAfterMerging() {\n         return true;\n     }\n \n-    //endregion\n+    /**\n+     * Tests an end-to-end scenario for the DebugSegmentContainer. SegmentStore creates some segments and then only \n+     * persisted storage is used to re-create all segments.\n+     * @throws Exception If an exception occurred.\n+     */\n+    @Test\n+    public void testDataRecovery() throws Exception {\n+        endToEndDebugSegmentContainer();\n+    }\n+\n+    /**\n+     * End to end test to verify DebugSegmentContainer process.\n+     * @throws Exception If an exception occurred.\n+     */\n+    public void endToEndDebugSegmentContainer() throws Exception {\n+        int containerCount = 4;\n+        ArrayList<String> segmentNames;\n+        HashMap<String, ArrayList<String>> transactionsBySegment;\n+        HashMap<String, Long> lengths = new HashMap<>();\n+        ArrayList<ByteBuf> appendBuffers = new ArrayList<>();\n+        HashMap<String, ByteArrayOutputStream> segmentContents = new HashMap<>();\n+\n+        try (val builder = createBuilder(0);\n+             val readOnlyBuilder = createReadOnlyBuilder()) {\n+            val segmentStore = builder.createStreamSegmentService();\n+            val readOnlySegmentStore = readOnlyBuilder.createStreamSegmentService();\n+\n+            segmentNames = createSegments(segmentStore);\n+            log.info(\"Created Segments: {}.\", String.join(\", \", segmentNames));\n+            transactionsBySegment = createTransactions(segmentNames, segmentStore);\n+            log.info(\"Created Transactions: {}.\", transactionsBySegment.values().stream().flatMap(Collection::stream).collect(Collectors.joining(\", \")));\n+\n+            // Add some appends and seal segments\n+            ArrayList<String> segmentsAndTransactions = new ArrayList<>(segmentNames);\n+            transactionsBySegment.values().forEach(segmentsAndTransactions::addAll);\n+            appendData(segmentsAndTransactions, segmentContents, lengths, appendBuffers, segmentStore).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Finished appending data.\");\n+\n+            // Wait for flushing the segments to tier2\n+            waitForSegmentsInStorage(segmentNames, segmentStore, readOnlySegmentStore).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Finished waiting for segments in Storage.\");\n+\n+            // Get the persistent storage from readOnlySegmentStore.\n+            Storage tier2 = getReadOnlyStorageFactory().createStorageAdapter();\n+\n+            // Delete container metadata segment and attribute index segment for each container Id from the persistent storage.\n+            for (int containerId = 0; containerId < containerCount; containerId++) {\n+                DataRecoveryTestUtils.deleteContainerMetadataSegments(tier2, containerId);\n+            }\n+\n+            // List all segments from the long term storage.\n+            Map<Integer, List<SegmentProperties>> segments = DataRecoveryTestUtils.listAllSegments(tier2, containerCount);\n+\n+            // Configurations for DebugSegmentContainer\n+            final ContainerConfig containerConfig = ContainerConfig\n+                    .builder()\n+                    .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+                    .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 100)\n+                    .build();\n+            final DurableLogConfig durableLogConfig = DurableLogConfig\n+                    .builder()\n+                    .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                    .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                    .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024)\n+                    .build();\n+\n+            // Create the environment for DebugSegmentContainer using the given storageFactory.\n+            @Cleanup TestContext context = createContext(getReadOnlyStorageFactory());\n+            OperationLogFactory localDurableLogFactory = new DurableLogFactory(durableLogConfig, context.dataLogFactory,\n+                    DataRecoveryTestUtils.createExecutorService(10));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNzEzMA=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 147}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI5ODcxOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoyOToxOFrOGzdF7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjozMjozNlrOG2dzdw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNzIxNQ==", "bodyText": "and here", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456607215", "createdAt": "2020-07-17T18:29:18Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "diffHunk": "@@ -146,7 +178,98 @@ protected boolean appendAfterMerging() {\n         return true;\n     }\n \n-    //endregion\n+    /**\n+     * Tests an end-to-end scenario for the DebugSegmentContainer. SegmentStore creates some segments and then only \n+     * persisted storage is used to re-create all segments.\n+     * @throws Exception If an exception occurred.\n+     */\n+    @Test\n+    public void testDataRecovery() throws Exception {\n+        endToEndDebugSegmentContainer();\n+    }\n+\n+    /**\n+     * End to end test to verify DebugSegmentContainer process.\n+     * @throws Exception If an exception occurred.\n+     */\n+    public void endToEndDebugSegmentContainer() throws Exception {\n+        int containerCount = 4;\n+        ArrayList<String> segmentNames;\n+        HashMap<String, ArrayList<String>> transactionsBySegment;\n+        HashMap<String, Long> lengths = new HashMap<>();\n+        ArrayList<ByteBuf> appendBuffers = new ArrayList<>();\n+        HashMap<String, ByteArrayOutputStream> segmentContents = new HashMap<>();\n+\n+        try (val builder = createBuilder(0);\n+             val readOnlyBuilder = createReadOnlyBuilder()) {\n+            val segmentStore = builder.createStreamSegmentService();\n+            val readOnlySegmentStore = readOnlyBuilder.createStreamSegmentService();\n+\n+            segmentNames = createSegments(segmentStore);\n+            log.info(\"Created Segments: {}.\", String.join(\", \", segmentNames));\n+            transactionsBySegment = createTransactions(segmentNames, segmentStore);\n+            log.info(\"Created Transactions: {}.\", transactionsBySegment.values().stream().flatMap(Collection::stream).collect(Collectors.joining(\", \")));\n+\n+            // Add some appends and seal segments\n+            ArrayList<String> segmentsAndTransactions = new ArrayList<>(segmentNames);\n+            transactionsBySegment.values().forEach(segmentsAndTransactions::addAll);\n+            appendData(segmentsAndTransactions, segmentContents, lengths, appendBuffers, segmentStore).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Finished appending data.\");\n+\n+            // Wait for flushing the segments to tier2\n+            waitForSegmentsInStorage(segmentNames, segmentStore, readOnlySegmentStore).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Finished waiting for segments in Storage.\");\n+\n+            // Get the persistent storage from readOnlySegmentStore.\n+            Storage tier2 = getReadOnlyStorageFactory().createStorageAdapter();\n+\n+            // Delete container metadata segment and attribute index segment for each container Id from the persistent storage.\n+            for (int containerId = 0; containerId < containerCount; containerId++) {\n+                DataRecoveryTestUtils.deleteContainerMetadataSegments(tier2, containerId);\n+            }\n+\n+            // List all segments from the long term storage.\n+            Map<Integer, List<SegmentProperties>> segments = DataRecoveryTestUtils.listAllSegments(tier2, containerCount);\n+\n+            // Configurations for DebugSegmentContainer\n+            final ContainerConfig containerConfig = ContainerConfig\n+                    .builder()\n+                    .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+                    .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 100)\n+                    .build();\n+            final DurableLogConfig durableLogConfig = DurableLogConfig\n+                    .builder()\n+                    .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                    .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                    .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024)\n+                    .build();\n+\n+            // Create the environment for DebugSegmentContainer using the given storageFactory.\n+            @Cleanup TestContext context = createContext(getReadOnlyStorageFactory());\n+            OperationLogFactory localDurableLogFactory = new DurableLogFactory(durableLogConfig, context.dataLogFactory,\n+                    DataRecoveryTestUtils.createExecutorService(10));\n+\n+            for (int containerId = 0; containerId < containerCount; containerId++) {\n+                // start DebugSegmentContainer with given container Id.\n+                DebugStreamSegmentContainerTests.MetadataCleanupContainer localContainer = new\n+                        DebugStreamSegmentContainerTests.MetadataCleanupContainer(containerId, containerConfig, localDurableLogFactory,\n+                        context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, getReadOnlyStorageFactory(),\n+                        context.getDefaultExtensions(), DataRecoveryTestUtils.createExecutorService(10));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2NDU5OQ==", "bodyText": "Removed.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459764599", "createdAt": "2020-07-23T22:32:36Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "diffHunk": "@@ -146,7 +178,98 @@ protected boolean appendAfterMerging() {\n         return true;\n     }\n \n-    //endregion\n+    /**\n+     * Tests an end-to-end scenario for the DebugSegmentContainer. SegmentStore creates some segments and then only \n+     * persisted storage is used to re-create all segments.\n+     * @throws Exception If an exception occurred.\n+     */\n+    @Test\n+    public void testDataRecovery() throws Exception {\n+        endToEndDebugSegmentContainer();\n+    }\n+\n+    /**\n+     * End to end test to verify DebugSegmentContainer process.\n+     * @throws Exception If an exception occurred.\n+     */\n+    public void endToEndDebugSegmentContainer() throws Exception {\n+        int containerCount = 4;\n+        ArrayList<String> segmentNames;\n+        HashMap<String, ArrayList<String>> transactionsBySegment;\n+        HashMap<String, Long> lengths = new HashMap<>();\n+        ArrayList<ByteBuf> appendBuffers = new ArrayList<>();\n+        HashMap<String, ByteArrayOutputStream> segmentContents = new HashMap<>();\n+\n+        try (val builder = createBuilder(0);\n+             val readOnlyBuilder = createReadOnlyBuilder()) {\n+            val segmentStore = builder.createStreamSegmentService();\n+            val readOnlySegmentStore = readOnlyBuilder.createStreamSegmentService();\n+\n+            segmentNames = createSegments(segmentStore);\n+            log.info(\"Created Segments: {}.\", String.join(\", \", segmentNames));\n+            transactionsBySegment = createTransactions(segmentNames, segmentStore);\n+            log.info(\"Created Transactions: {}.\", transactionsBySegment.values().stream().flatMap(Collection::stream).collect(Collectors.joining(\", \")));\n+\n+            // Add some appends and seal segments\n+            ArrayList<String> segmentsAndTransactions = new ArrayList<>(segmentNames);\n+            transactionsBySegment.values().forEach(segmentsAndTransactions::addAll);\n+            appendData(segmentsAndTransactions, segmentContents, lengths, appendBuffers, segmentStore).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Finished appending data.\");\n+\n+            // Wait for flushing the segments to tier2\n+            waitForSegmentsInStorage(segmentNames, segmentStore, readOnlySegmentStore).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Finished waiting for segments in Storage.\");\n+\n+            // Get the persistent storage from readOnlySegmentStore.\n+            Storage tier2 = getReadOnlyStorageFactory().createStorageAdapter();\n+\n+            // Delete container metadata segment and attribute index segment for each container Id from the persistent storage.\n+            for (int containerId = 0; containerId < containerCount; containerId++) {\n+                DataRecoveryTestUtils.deleteContainerMetadataSegments(tier2, containerId);\n+            }\n+\n+            // List all segments from the long term storage.\n+            Map<Integer, List<SegmentProperties>> segments = DataRecoveryTestUtils.listAllSegments(tier2, containerCount);\n+\n+            // Configurations for DebugSegmentContainer\n+            final ContainerConfig containerConfig = ContainerConfig\n+                    .builder()\n+                    .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+                    .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 100)\n+                    .build();\n+            final DurableLogConfig durableLogConfig = DurableLogConfig\n+                    .builder()\n+                    .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                    .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                    .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024)\n+                    .build();\n+\n+            // Create the environment for DebugSegmentContainer using the given storageFactory.\n+            @Cleanup TestContext context = createContext(getReadOnlyStorageFactory());\n+            OperationLogFactory localDurableLogFactory = new DurableLogFactory(durableLogConfig, context.dataLogFactory,\n+                    DataRecoveryTestUtils.createExecutorService(10));\n+\n+            for (int containerId = 0; containerId < containerCount; containerId++) {\n+                // start DebugSegmentContainer with given container Id.\n+                DebugStreamSegmentContainerTests.MetadataCleanupContainer localContainer = new\n+                        DebugStreamSegmentContainerTests.MetadataCleanupContainer(containerId, containerConfig, localDurableLogFactory,\n+                        context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, getReadOnlyStorageFactory(),\n+                        context.getDefaultExtensions(), DataRecoveryTestUtils.createExecutorService(10));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNzIxNQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 154}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODI5OTY2OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoyOTozNlrOGzdGdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjozMjo0NVrOG2dzqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNzM0OA==", "bodyText": "This is superfluous. The next line does this.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456607348", "createdAt": "2020-07-17T18:29:36Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "diffHunk": "@@ -146,7 +178,98 @@ protected boolean appendAfterMerging() {\n         return true;\n     }\n \n-    //endregion\n+    /**\n+     * Tests an end-to-end scenario for the DebugSegmentContainer. SegmentStore creates some segments and then only \n+     * persisted storage is used to re-create all segments.\n+     * @throws Exception If an exception occurred.\n+     */\n+    @Test\n+    public void testDataRecovery() throws Exception {\n+        endToEndDebugSegmentContainer();\n+    }\n+\n+    /**\n+     * End to end test to verify DebugSegmentContainer process.\n+     * @throws Exception If an exception occurred.\n+     */\n+    public void endToEndDebugSegmentContainer() throws Exception {\n+        int containerCount = 4;\n+        ArrayList<String> segmentNames;\n+        HashMap<String, ArrayList<String>> transactionsBySegment;\n+        HashMap<String, Long> lengths = new HashMap<>();\n+        ArrayList<ByteBuf> appendBuffers = new ArrayList<>();\n+        HashMap<String, ByteArrayOutputStream> segmentContents = new HashMap<>();\n+\n+        try (val builder = createBuilder(0);\n+             val readOnlyBuilder = createReadOnlyBuilder()) {\n+            val segmentStore = builder.createStreamSegmentService();\n+            val readOnlySegmentStore = readOnlyBuilder.createStreamSegmentService();\n+\n+            segmentNames = createSegments(segmentStore);\n+            log.info(\"Created Segments: {}.\", String.join(\", \", segmentNames));\n+            transactionsBySegment = createTransactions(segmentNames, segmentStore);\n+            log.info(\"Created Transactions: {}.\", transactionsBySegment.values().stream().flatMap(Collection::stream).collect(Collectors.joining(\", \")));\n+\n+            // Add some appends and seal segments\n+            ArrayList<String> segmentsAndTransactions = new ArrayList<>(segmentNames);\n+            transactionsBySegment.values().forEach(segmentsAndTransactions::addAll);\n+            appendData(segmentsAndTransactions, segmentContents, lengths, appendBuffers, segmentStore).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Finished appending data.\");\n+\n+            // Wait for flushing the segments to tier2\n+            waitForSegmentsInStorage(segmentNames, segmentStore, readOnlySegmentStore).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Finished waiting for segments in Storage.\");\n+\n+            // Get the persistent storage from readOnlySegmentStore.\n+            Storage tier2 = getReadOnlyStorageFactory().createStorageAdapter();\n+\n+            // Delete container metadata segment and attribute index segment for each container Id from the persistent storage.\n+            for (int containerId = 0; containerId < containerCount; containerId++) {\n+                DataRecoveryTestUtils.deleteContainerMetadataSegments(tier2, containerId);\n+            }\n+\n+            // List all segments from the long term storage.\n+            Map<Integer, List<SegmentProperties>> segments = DataRecoveryTestUtils.listAllSegments(tier2, containerCount);\n+\n+            // Configurations for DebugSegmentContainer\n+            final ContainerConfig containerConfig = ContainerConfig\n+                    .builder()\n+                    .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+                    .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 100)\n+                    .build();\n+            final DurableLogConfig durableLogConfig = DurableLogConfig\n+                    .builder()\n+                    .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                    .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                    .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024)\n+                    .build();\n+\n+            // Create the environment for DebugSegmentContainer using the given storageFactory.\n+            @Cleanup TestContext context = createContext(getReadOnlyStorageFactory());\n+            OperationLogFactory localDurableLogFactory = new DurableLogFactory(durableLogConfig, context.dataLogFactory,\n+                    DataRecoveryTestUtils.createExecutorService(10));\n+\n+            for (int containerId = 0; containerId < containerCount; containerId++) {\n+                // start DebugSegmentContainer with given container Id.\n+                DebugStreamSegmentContainerTests.MetadataCleanupContainer localContainer = new\n+                        DebugStreamSegmentContainerTests.MetadataCleanupContainer(containerId, containerConfig, localDurableLogFactory,\n+                        context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, getReadOnlyStorageFactory(),\n+                        context.getDefaultExtensions(), DataRecoveryTestUtils.createExecutorService(10));\n+\n+                // Create all segments under the given container Id .\n+                Services.startAsync(localContainer, executorService)\n+                        .thenRun(new DataRecoveryTestUtils.Worker(localContainer, segments.get(containerId))).join();\n+\n+                // Verify if the segment details match.\n+                for (SegmentProperties segmentProperties : segments.get(containerId)) {\n+                    SegmentProperties props = localContainer.getStreamSegmentInfo(segmentProperties.getName(), TIMEOUT).join();\n+                    Assert.assertEquals(\"Segment length mismatch \", segmentProperties.getLength(), props.getLength());\n+                }\n+                Services.stopAsync(localContainer, executorService).join();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 165}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2NDY1MA==", "bodyText": "Ok.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459764650", "createdAt": "2020-07-23T22:32:45Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "diffHunk": "@@ -146,7 +178,98 @@ protected boolean appendAfterMerging() {\n         return true;\n     }\n \n-    //endregion\n+    /**\n+     * Tests an end-to-end scenario for the DebugSegmentContainer. SegmentStore creates some segments and then only \n+     * persisted storage is used to re-create all segments.\n+     * @throws Exception If an exception occurred.\n+     */\n+    @Test\n+    public void testDataRecovery() throws Exception {\n+        endToEndDebugSegmentContainer();\n+    }\n+\n+    /**\n+     * End to end test to verify DebugSegmentContainer process.\n+     * @throws Exception If an exception occurred.\n+     */\n+    public void endToEndDebugSegmentContainer() throws Exception {\n+        int containerCount = 4;\n+        ArrayList<String> segmentNames;\n+        HashMap<String, ArrayList<String>> transactionsBySegment;\n+        HashMap<String, Long> lengths = new HashMap<>();\n+        ArrayList<ByteBuf> appendBuffers = new ArrayList<>();\n+        HashMap<String, ByteArrayOutputStream> segmentContents = new HashMap<>();\n+\n+        try (val builder = createBuilder(0);\n+             val readOnlyBuilder = createReadOnlyBuilder()) {\n+            val segmentStore = builder.createStreamSegmentService();\n+            val readOnlySegmentStore = readOnlyBuilder.createStreamSegmentService();\n+\n+            segmentNames = createSegments(segmentStore);\n+            log.info(\"Created Segments: {}.\", String.join(\", \", segmentNames));\n+            transactionsBySegment = createTransactions(segmentNames, segmentStore);\n+            log.info(\"Created Transactions: {}.\", transactionsBySegment.values().stream().flatMap(Collection::stream).collect(Collectors.joining(\", \")));\n+\n+            // Add some appends and seal segments\n+            ArrayList<String> segmentsAndTransactions = new ArrayList<>(segmentNames);\n+            transactionsBySegment.values().forEach(segmentsAndTransactions::addAll);\n+            appendData(segmentsAndTransactions, segmentContents, lengths, appendBuffers, segmentStore).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Finished appending data.\");\n+\n+            // Wait for flushing the segments to tier2\n+            waitForSegmentsInStorage(segmentNames, segmentStore, readOnlySegmentStore).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            log.info(\"Finished waiting for segments in Storage.\");\n+\n+            // Get the persistent storage from readOnlySegmentStore.\n+            Storage tier2 = getReadOnlyStorageFactory().createStorageAdapter();\n+\n+            // Delete container metadata segment and attribute index segment for each container Id from the persistent storage.\n+            for (int containerId = 0; containerId < containerCount; containerId++) {\n+                DataRecoveryTestUtils.deleteContainerMetadataSegments(tier2, containerId);\n+            }\n+\n+            // List all segments from the long term storage.\n+            Map<Integer, List<SegmentProperties>> segments = DataRecoveryTestUtils.listAllSegments(tier2, containerCount);\n+\n+            // Configurations for DebugSegmentContainer\n+            final ContainerConfig containerConfig = ContainerConfig\n+                    .builder()\n+                    .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+                    .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 100)\n+                    .build();\n+            final DurableLogConfig durableLogConfig = DurableLogConfig\n+                    .builder()\n+                    .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                    .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                    .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024)\n+                    .build();\n+\n+            // Create the environment for DebugSegmentContainer using the given storageFactory.\n+            @Cleanup TestContext context = createContext(getReadOnlyStorageFactory());\n+            OperationLogFactory localDurableLogFactory = new DurableLogFactory(durableLogConfig, context.dataLogFactory,\n+                    DataRecoveryTestUtils.createExecutorService(10));\n+\n+            for (int containerId = 0; containerId < containerCount; containerId++) {\n+                // start DebugSegmentContainer with given container Id.\n+                DebugStreamSegmentContainerTests.MetadataCleanupContainer localContainer = new\n+                        DebugStreamSegmentContainerTests.MetadataCleanupContainer(containerId, containerConfig, localDurableLogFactory,\n+                        context.readIndexFactory, context.attributeIndexFactory, context.writerFactory, getReadOnlyStorageFactory(),\n+                        context.getDefaultExtensions(), DataRecoveryTestUtils.createExecutorService(10));\n+\n+                // Create all segments under the given container Id .\n+                Services.startAsync(localContainer, executorService)\n+                        .thenRun(new DataRecoveryTestUtils.Worker(localContainer, segments.get(containerId))).join();\n+\n+                // Verify if the segment details match.\n+                for (SegmentProperties segmentProperties : segments.get(containerId)) {\n+                    SegmentProperties props = localContainer.getStreamSegmentInfo(segmentProperties.getName(), TIMEOUT).join();\n+                    Assert.assertEquals(\"Segment length mismatch \", segmentProperties.getLength(), props.getLength());\n+                }\n+                Services.stopAsync(localContainer, executorService).join();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNzM0OA=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 165}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODMwMDM1OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODoyOTo1MVrOGzdG7Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjozMzozMFrOG2d0ug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNzQ2OQ==", "bodyText": "Why do you need this?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456607469", "createdAt": "2020-07-17T18:29:51Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "diffHunk": "@@ -369,6 +492,10 @@ public void endToEndProcessWithFencing(boolean verifySegmentContent) throws Exce\n \n     //region Helpers\n \n+    private StorageFactory getReadOnlyStorageFactory() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2NDkyMg==", "bodyText": "I need it got the storage used while the normal segment store is executing.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459764922", "createdAt": "2020-07-23T22:33:30Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "diffHunk": "@@ -369,6 +492,10 @@ public void endToEndProcessWithFencing(boolean verifySegmentContent) throws Exce\n \n     //region Helpers\n \n+    private StorageFactory getReadOnlyStorageFactory() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwNzQ2OQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 177}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODMwNDYxOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODozMToxMFrOGzdJig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNFQyMTo1NjozMVrOG7y45w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwODEzOA==", "bodyText": "This method has a side effect of setting readOnlyStorageFactory. This is not documented (the documentation says something opposite).\nI don't think you need this.readOnlyBuilder; please rework your code to fetch this value after creating it and then using it where you need it.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456608138", "createdAt": "2020-07-17T18:31:10Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "diffHunk": "@@ -405,6 +532,28 @@ private ServiceBuilder createReadOnlyBuilder(int instanceId) throws Exception {\n         return builder;\n     }\n \n+    /**\n+     * Creates a ServiceBuilder instance, but also gets the storage Factory used in creating.\n+     * @return              A newly created ServiceBuilder instance.\n+     * @throws Exception    In case of any exception occurred during execution.\n+     */\n+    private ServiceBuilder createReadOnlyBuilder() throws Exception {\n+        // Copy base config properties to a new object.\n+        val props = new Properties();\n+        this.configBuilder.build().forEach(props::put);\n+\n+        // Create a new config (so we don't alter the base one) and set the ReadOnlySegmentStore to true).\n+        val configBuilder = ServiceBuilderConfig.builder()\n+                .include(props)\n+                .include(ServiceConfig.builder()\n+                        .with(ServiceConfig.READONLY_SEGMENT_STORE, true));\n+\n+        val builder = createBuilder(configBuilder, 0);\n+        builder.initialize();\n+        this.readOnlyStorageFactory = builder.getStorageFactory();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 206}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM1MjkzNQ==", "bodyText": "Removed.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r465352935", "createdAt": "2020-08-04T21:56:31Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "diffHunk": "@@ -405,6 +532,28 @@ private ServiceBuilder createReadOnlyBuilder(int instanceId) throws Exception {\n         return builder;\n     }\n \n+    /**\n+     * Creates a ServiceBuilder instance, but also gets the storage Factory used in creating.\n+     * @return              A newly created ServiceBuilder instance.\n+     * @throws Exception    In case of any exception occurred during execution.\n+     */\n+    private ServiceBuilder createReadOnlyBuilder() throws Exception {\n+        // Copy base config properties to a new object.\n+        val props = new Properties();\n+        this.configBuilder.build().forEach(props::put);\n+\n+        // Create a new config (so we don't alter the base one) and set the ReadOnlySegmentStore to true).\n+        val configBuilder = ServiceBuilderConfig.builder()\n+                .include(props)\n+                .include(ServiceConfig.builder()\n+                        .with(ServiceConfig.READONLY_SEGMENT_STORE, true));\n+\n+        val builder = createBuilder(configBuilder, 0);\n+        builder.initialize();\n+        this.readOnlyStorageFactory = builder.getStorageFactory();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwODEzOA=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 206}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODMwNjU4OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODozMTo0OFrOGzdKvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjozNzoyOFrOG2d6Fg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwODQ0NQ==", "bodyText": "I've seen these configs somewhere else. Can you combine them into a single one?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456608445", "createdAt": "2020-07-17T18:31:48Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "diffHunk": "@@ -1009,4 +1158,60 @@ void createNewInstance() {\n     private interface StoreRequest {\n         CompletableFuture<Void> apply(StreamSegmentStore store);\n     }\n+\n+    public TestContext createContext(StorageFactory storageFactory) {\n+        return new TestContext(storageFactory);\n+    }\n+\n+    public class TestContext implements AutoCloseable {\n+        private static final int MAX_DATA_LOG_APPEND_SIZE = 100 * 1024;\n+        private final StorageFactory storageFactory;\n+        private final DurableDataLogFactory dataLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheStorage cacheStorage;\n+        private final CacheManager cacheManager;\n+        private final ReadIndexConfig defaultReadIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+        private final AttributeIndexConfig defaultAttributeIndexConfig = AttributeIndexConfig", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 233}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2NjI5NA==", "bodyText": "Ok.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459766294", "createdAt": "2020-07-23T22:37:28Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentStoreTestBase.java", "diffHunk": "@@ -1009,4 +1158,60 @@ void createNewInstance() {\n     private interface StoreRequest {\n         CompletableFuture<Void> apply(StreamSegmentStore store);\n     }\n+\n+    public TestContext createContext(StorageFactory storageFactory) {\n+        return new TestContext(storageFactory);\n+    }\n+\n+    public class TestContext implements AutoCloseable {\n+        private static final int MAX_DATA_LOG_APPEND_SIZE = 100 * 1024;\n+        private final StorageFactory storageFactory;\n+        private final DurableDataLogFactory dataLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheStorage cacheStorage;\n+        private final CacheManager cacheManager;\n+        private final ReadIndexConfig defaultReadIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+        private final AttributeIndexConfig defaultAttributeIndexConfig = AttributeIndexConfig", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwODQ0NQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 233}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODMwNzU3OnYy", "diffSide": "RIGHT", "path": "shared/protocol/src/main/java/io/pravega/shared/NameUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODozMjowOFrOGzdLUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjozNzozNVrOG2d6SQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwODU5NA==", "bodyText": "I don't see unit tests for this.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456608594", "createdAt": "2020-07-17T18:32:08Z", "author": {"login": "andreipaduroiu"}, "path": "shared/protocol/src/main/java/io/pravega/shared/NameUtils.java", "diffHunk": "@@ -197,6 +197,16 @@ public static String getHeaderSegmentName(String segmentName) {\n         return segmentName + HEADER_SUFFIX;\n     }\n \n+    /**\n+     * Checks whether given name is an Attribute Segment.\n+     *\n+     * @param segmentName   The name of the segment.\n+     * @return              true if the name is Attribute Segment. False otherwise\n+     */\n+    public static boolean isAttributeSegment(String segmentName) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2NjM0NQ==", "bodyText": "Removed.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459766345", "createdAt": "2020-07-23T22:37:35Z", "author": {"login": "ManishKumarKeshri"}, "path": "shared/protocol/src/main/java/io/pravega/shared/NameUtils.java", "diffHunk": "@@ -197,6 +197,16 @@ public static String getHeaderSegmentName(String segmentName) {\n         return segmentName + HEADER_SUFFIX;\n     }\n \n+    /**\n+     * Checks whether given name is an Attribute Segment.\n+     *\n+     * @param segmentName   The name of the segment.\n+     * @return              true if the name is Attribute Segment. False otherwise\n+     */\n+    public static boolean isAttributeSegment(String segmentName) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwODU5NA=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 10}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODMwODMxOnYy", "diffSide": "RIGHT", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODozMjoyMlrOGzdLww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjozNzo1MFrOG2d6rg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwODcwNw==", "bodyText": "Huge timeout", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456608707", "createdAt": "2020-07-17T18:32:22Z", "author": {"login": "andreipaduroiu"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2NjQ0Ng==", "bodyText": "Changed to 100 s.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459766446", "createdAt": "2020-07-23T22:37:50Z", "author": {"login": "ManishKumarKeshri"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwODcwNw=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 121}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODMwODk2OnYy", "diffSide": "RIGHT", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODozMjozMlrOGzdMJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjozNzo1OFrOG2d62g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwODgwNA==", "bodyText": "provide a seed", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456608804", "createdAt": "2020-07-17T18:32:32Z", "author": {"login": "andreipaduroiu"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 134}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2NjQ5MA==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459766490", "createdAt": "2020-07-23T22:37:58Z", "author": {"login": "ManishKumarKeshri"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwODgwNA=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 134}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODMxMTQzOnYy", "diffSide": "RIGHT", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODozMzoyMVrOGzdNqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjozODowN1rOG2d7Bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwOTE5NA==", "bodyText": "Remove this.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456609194", "createdAt": "2020-07-17T18:33:21Z", "author": {"login": "andreipaduroiu"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 147}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2NjUzNA==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459766534", "createdAt": "2020-07-23T22:38:07Z", "author": {"login": "ManishKumarKeshri"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwOTE5NA=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 147}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODMxMjA1OnYy", "diffSide": "RIGHT", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODozMzozM1rOGzdOEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjozOTozN1rOG2d8-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwOTI5OQ==", "bodyText": "private", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456609299", "createdAt": "2020-07-17T18:33:33Z", "author": {"login": "andreipaduroiu"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMDg5NQ==", "bodyText": "And what's the point of this if it's just invoking the constructor?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456610895", "createdAt": "2020-07-17T18:36:52Z", "author": {"login": "andreipaduroiu"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwOTI5OQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2NzAzNQ==", "bodyText": "Made private. It has a bunch of variables, and close method which is used to close BK/ZK.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459767035", "createdAt": "2020-07-23T22:39:37Z", "author": {"login": "ManishKumarKeshri"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwOTI5OQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 183}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODMxNTU2OnYy", "diffSide": "RIGHT", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODozNDo0NlrOGzdQQA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjo0MDowMFrOG2d9kg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwOTg1Ng==", "bodyText": "Please clean this class up. There are a number of things you hardcoded in here and then you're just proliferating them.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456609856", "createdAt": "2020-07-17T18:34:46Z", "author": {"login": "andreipaduroiu"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 203}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMDEyNg==", "bodyText": "For example, there is no need for security, Remove all that is related to that.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456610126", "createdAt": "2020-07-17T18:35:16Z", "author": {"login": "andreipaduroiu"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwOTg1Ng=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 203}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2NzE4Ng==", "bodyText": "Cleaned.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459767186", "createdAt": "2020-07-23T22:40:00Z", "author": {"login": "ManishKumarKeshri"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYwOTg1Ng=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 203}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODMxODE4OnYy", "diffSide": "RIGHT", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODozNTozNFrOGzdR3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo1OTozNVrOG_0UlQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMDI2OQ==", "bodyText": "You only have 1 bookie. Simplify your code.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456610269", "createdAt": "2020-07-17T18:35:34Z", "author": {"login": "andreipaduroiu"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 205}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2NzMzMQ==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459767331", "createdAt": "2020-07-23T22:40:22Z", "author": {"login": "ManishKumarKeshri"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMDI2OQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 205}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU3MDcwOQ==", "bodyText": "So then why do you still have a list of ports?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469570709", "createdAt": "2020-08-12T21:59:35Z", "author": {"login": "andreipaduroiu"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMDI2OQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 205}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODMyMDAyOnYy", "diffSide": "RIGHT", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODozNjoxNVrOGzdTEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjo0MTowNVrOG2d_AQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMDU3Ng==", "bodyText": "@Override", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456610576", "createdAt": "2020-07-17T18:36:15Z", "author": {"login": "andreipaduroiu"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 257}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2NzU1Mw==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459767553", "createdAt": "2020-07-23T22:41:05Z", "author": {"login": "ManishKumarKeshri"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMDU3Ng=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 257}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODMyMTE3OnYy", "diffSide": "RIGHT", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODozNjozNlrOGzdTyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjo0MzoxMFrOG2eBow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMDc2MQ==", "bodyText": "What's the point of this if it's just invoking the constructor?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456610761", "createdAt": "2020-07-17T18:36:36Z", "author": {"login": "andreipaduroiu"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 276}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2ODIyNw==", "bodyText": "Close method is used, and makes it easy to see through the variables used.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459768227", "createdAt": "2020-07-23T22:43:10Z", "author": {"login": "ManishKumarKeshri"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMDc2MQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 276}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODMyMjc1OnYy", "diffSide": "RIGHT", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODozNzowOFrOGzdUzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMDowODoyNVrOG71nDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMTAyMQ==", "bodyText": "See if you can consolidate all these", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456611021", "createdAt": "2020-07-17T18:37:08Z", "author": {"login": "andreipaduroiu"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 303}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2ODMzMQ==", "bodyText": "Couldn't do that.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459768331", "createdAt": "2020-07-23T22:43:26Z", "author": {"login": "ManishKumarKeshri"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMTAyMQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 303}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM5NzUxNw==", "bodyText": "Removed it.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r465397517", "createdAt": "2020-08-05T00:08:25Z", "author": {"login": "ManishKumarKeshri"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMTAyMQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 303}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODMyMzEzOnYy", "diffSide": "RIGHT", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODozNzoxOVrOGzdVDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjo0MzozNlrOG2eCNQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMTA4NQ==", "bodyText": "Check for nulls.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456611085", "createdAt": "2020-07-17T18:37:19Z", "author": {"login": "andreipaduroiu"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig\n+                .builder()\n+                .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+                .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+                .build();\n+        private final WriterConfig writerConfig = WriterConfig\n+                .builder()\n+                .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+                .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+                .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+                .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+                .build();\n+\n+        DebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+            this.dataLogFactory = dataLogFactory;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 317}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2ODM3Mw==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459768373", "createdAt": "2020-07-23T22:43:36Z", "author": {"login": "ManishKumarKeshri"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig\n+                .builder()\n+                .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+                .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+                .build();\n+        private final WriterConfig writerConfig = WriterConfig\n+                .builder()\n+                .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+                .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+                .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+                .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+                .build();\n+\n+        DebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+            this.dataLogFactory = dataLogFactory;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMTA4NQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 317}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODMyNTY2OnYy", "diffSide": "RIGHT", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODozODoxOVrOGzdWyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMDowNzowMVrOG71lhQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMTUyOQ==", "bodyText": "This code looks copied from ServiceBuilder. Can you find a way to reuse that code? If we modify the extensions in ServiceBuilder, this code will break. Please try to reuse as much as you can from that class.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456611529", "createdAt": "2020-07-17T18:38:19Z", "author": {"login": "andreipaduroiu"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig\n+                .builder()\n+                .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+                .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+                .build();\n+        private final WriterConfig writerConfig = WriterConfig\n+                .builder()\n+                .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+                .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+                .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+                .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+                .build();\n+\n+        DebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+            this.dataLogFactory = dataLogFactory;\n+            this.storageFactory = storageFactory;\n+            this.operationLogFactory = new DurableLogFactory(durableLogConfig, this.dataLogFactory, executorService);\n+\n+            this.cacheStorage = new DirectMemoryCache(Integer.MAX_VALUE);\n+            this.cacheManager = new CacheManager(CachePolicy.INFINITE, this.cacheStorage, executorService);\n+            this.readIndexFactory = new ContainerReadIndexFactory(readIndexConfig, this.cacheManager, executorService);\n+            this.attributeIndexFactory = new ContainerAttributeIndexFactoryImpl(attributeIndexConfig, this.cacheManager, executorService);\n+            this.writerFactory = new StorageWriterFactory(writerConfig, executorService);\n+\n+            ContainerConfig containerConfig = ServiceBuilderConfig.getDefaultConfig().getConfig(ContainerConfig::builder);\n+            this.containerFactory = new StreamSegmentContainerFactory(containerConfig, this.operationLogFactory,\n+                    this.readIndexFactory, this.attributeIndexFactory, this.writerFactory, this.storageFactory,\n+                    this::createContainerExtensions, executorService);\n+        }\n+\n+        private Map<Class<? extends SegmentContainerExtension>, SegmentContainerExtension> createContainerExtensions(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 333}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2ODU4OA==", "bodyText": "Couldn't reuse. I will try again.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459768588", "createdAt": "2020-07-23T22:44:15Z", "author": {"login": "ManishKumarKeshri"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig\n+                .builder()\n+                .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+                .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+                .build();\n+        private final WriterConfig writerConfig = WriterConfig\n+                .builder()\n+                .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+                .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+                .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+                .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+                .build();\n+\n+        DebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+            this.dataLogFactory = dataLogFactory;\n+            this.storageFactory = storageFactory;\n+            this.operationLogFactory = new DurableLogFactory(durableLogConfig, this.dataLogFactory, executorService);\n+\n+            this.cacheStorage = new DirectMemoryCache(Integer.MAX_VALUE);\n+            this.cacheManager = new CacheManager(CachePolicy.INFINITE, this.cacheStorage, executorService);\n+            this.readIndexFactory = new ContainerReadIndexFactory(readIndexConfig, this.cacheManager, executorService);\n+            this.attributeIndexFactory = new ContainerAttributeIndexFactoryImpl(attributeIndexConfig, this.cacheManager, executorService);\n+            this.writerFactory = new StorageWriterFactory(writerConfig, executorService);\n+\n+            ContainerConfig containerConfig = ServiceBuilderConfig.getDefaultConfig().getConfig(ContainerConfig::builder);\n+            this.containerFactory = new StreamSegmentContainerFactory(containerConfig, this.operationLogFactory,\n+                    this.readIndexFactory, this.attributeIndexFactory, this.writerFactory, this.storageFactory,\n+                    this::createContainerExtensions, executorService);\n+        }\n+\n+        private Map<Class<? extends SegmentContainerExtension>, SegmentContainerExtension> createContainerExtensions(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMTUyOQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 333}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDY1Njg0Mg==", "bodyText": "I am not able to reuse it.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r460656842", "createdAt": "2020-07-27T05:38:50Z", "author": {"login": "ManishKumarKeshri"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig\n+                .builder()\n+                .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+                .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+                .build();\n+        private final WriterConfig writerConfig = WriterConfig\n+                .builder()\n+                .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+                .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+                .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+                .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+                .build();\n+\n+        DebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+            this.dataLogFactory = dataLogFactory;\n+            this.storageFactory = storageFactory;\n+            this.operationLogFactory = new DurableLogFactory(durableLogConfig, this.dataLogFactory, executorService);\n+\n+            this.cacheStorage = new DirectMemoryCache(Integer.MAX_VALUE);\n+            this.cacheManager = new CacheManager(CachePolicy.INFINITE, this.cacheStorage, executorService);\n+            this.readIndexFactory = new ContainerReadIndexFactory(readIndexConfig, this.cacheManager, executorService);\n+            this.attributeIndexFactory = new ContainerAttributeIndexFactoryImpl(attributeIndexConfig, this.cacheManager, executorService);\n+            this.writerFactory = new StorageWriterFactory(writerConfig, executorService);\n+\n+            ContainerConfig containerConfig = ServiceBuilderConfig.getDefaultConfig().getConfig(ContainerConfig::builder);\n+            this.containerFactory = new StreamSegmentContainerFactory(containerConfig, this.operationLogFactory,\n+                    this.readIndexFactory, this.attributeIndexFactory, this.writerFactory, this.storageFactory,\n+                    this::createContainerExtensions, executorService);\n+        }\n+\n+        private Map<Class<? extends SegmentContainerExtension>, SegmentContainerExtension> createContainerExtensions(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMTUyOQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 333}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM5NzEyNQ==", "bodyText": "Removed it all together.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r465397125", "createdAt": "2020-08-05T00:07:01Z", "author": {"login": "ManishKumarKeshri"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig\n+                .builder()\n+                .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+                .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+                .build();\n+        private final WriterConfig writerConfig = WriterConfig\n+                .builder()\n+                .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+                .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+                .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+                .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+                .build();\n+\n+        DebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+            this.dataLogFactory = dataLogFactory;\n+            this.storageFactory = storageFactory;\n+            this.operationLogFactory = new DurableLogFactory(durableLogConfig, this.dataLogFactory, executorService);\n+\n+            this.cacheStorage = new DirectMemoryCache(Integer.MAX_VALUE);\n+            this.cacheManager = new CacheManager(CachePolicy.INFINITE, this.cacheStorage, executorService);\n+            this.readIndexFactory = new ContainerReadIndexFactory(readIndexConfig, this.cacheManager, executorService);\n+            this.attributeIndexFactory = new ContainerAttributeIndexFactoryImpl(attributeIndexConfig, this.cacheManager, executorService);\n+            this.writerFactory = new StorageWriterFactory(writerConfig, executorService);\n+\n+            ContainerConfig containerConfig = ServiceBuilderConfig.getDefaultConfig().getConfig(ContainerConfig::builder);\n+            this.containerFactory = new StreamSegmentContainerFactory(containerConfig, this.operationLogFactory,\n+                    this.readIndexFactory, this.attributeIndexFactory, this.writerFactory, this.storageFactory,\n+                    this::createContainerExtensions, executorService);\n+        }\n+\n+        private Map<Class<? extends SegmentContainerExtension>, SegmentContainerExtension> createContainerExtensions(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMTUyOQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 333}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODMyOTA4OnYy", "diffSide": "RIGHT", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODozOToyNlrOGzdY4Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQwMDowNjo0MFrOG71lGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMjA2NQ==", "bodyText": "You do not need a 2GB cache.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456612065", "createdAt": "2020-07-17T18:39:26Z", "author": {"login": "andreipaduroiu"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig\n+                .builder()\n+                .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+                .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+                .build();\n+        private final WriterConfig writerConfig = WriterConfig\n+                .builder()\n+                .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+                .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+                .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+                .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+                .build();\n+\n+        DebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+            this.dataLogFactory = dataLogFactory;\n+            this.storageFactory = storageFactory;\n+            this.operationLogFactory = new DurableLogFactory(durableLogConfig, this.dataLogFactory, executorService);\n+\n+            this.cacheStorage = new DirectMemoryCache(Integer.MAX_VALUE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 321}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2ODY5Nw==", "bodyText": "Made it 1/5th..", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459768697", "createdAt": "2020-07-23T22:44:36Z", "author": {"login": "ManishKumarKeshri"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig\n+                .builder()\n+                .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+                .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+                .build();\n+        private final WriterConfig writerConfig = WriterConfig\n+                .builder()\n+                .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+                .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+                .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+                .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+                .build();\n+\n+        DebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+            this.dataLogFactory = dataLogFactory;\n+            this.storageFactory = storageFactory;\n+            this.operationLogFactory = new DurableLogFactory(durableLogConfig, this.dataLogFactory, executorService);\n+\n+            this.cacheStorage = new DirectMemoryCache(Integer.MAX_VALUE);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMjA2NQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 321}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTM5NzAxNg==", "bodyText": "Is it fine?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r465397016", "createdAt": "2020-08-05T00:06:40Z", "author": {"login": "ManishKumarKeshri"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig\n+                .builder()\n+                .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+                .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+                .build();\n+        private final WriterConfig writerConfig = WriterConfig\n+                .builder()\n+                .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+                .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+                .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+                .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+                .build();\n+\n+        DebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+            this.dataLogFactory = dataLogFactory;\n+            this.storageFactory = storageFactory;\n+            this.operationLogFactory = new DurableLogFactory(durableLogConfig, this.dataLogFactory, executorService);\n+\n+            this.cacheStorage = new DirectMemoryCache(Integer.MAX_VALUE);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMjA2NQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 321}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODMzMDM1OnYy", "diffSide": "RIGHT", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODozOTo1MlrOGzdZrw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QwNTozNjoxMFrOG3UOFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMjI3MQ==", "bodyText": "Verify that you are shutting down everything that needs to shut down.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456612271", "createdAt": "2020-07-17T18:39:52Z", "author": {"login": "andreipaduroiu"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig\n+                .builder()\n+                .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+                .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+                .build();\n+        private final WriterConfig writerConfig = WriterConfig\n+                .builder()\n+                .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+                .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+                .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+                .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+                .build();\n+\n+        DebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+            this.dataLogFactory = dataLogFactory;\n+            this.storageFactory = storageFactory;\n+            this.operationLogFactory = new DurableLogFactory(durableLogConfig, this.dataLogFactory, executorService);\n+\n+            this.cacheStorage = new DirectMemoryCache(Integer.MAX_VALUE);\n+            this.cacheManager = new CacheManager(CachePolicy.INFINITE, this.cacheStorage, executorService);\n+            this.readIndexFactory = new ContainerReadIndexFactory(readIndexConfig, this.cacheManager, executorService);\n+            this.attributeIndexFactory = new ContainerAttributeIndexFactoryImpl(attributeIndexConfig, this.cacheManager, executorService);\n+            this.writerFactory = new StorageWriterFactory(writerConfig, executorService);\n+\n+            ContainerConfig containerConfig = ServiceBuilderConfig.getDefaultConfig().getConfig(ContainerConfig::builder);\n+            this.containerFactory = new StreamSegmentContainerFactory(containerConfig, this.operationLogFactory,\n+                    this.readIndexFactory, this.attributeIndexFactory, this.writerFactory, this.storageFactory,\n+                    this::createContainerExtensions, executorService);\n+        }\n+\n+        private Map<Class<? extends SegmentContainerExtension>, SegmentContainerExtension> createContainerExtensions(\n+                SegmentContainer container, ScheduledExecutorService executor) {\n+            return Collections.singletonMap(ContainerTableExtension.class, new ContainerTableExtensionImpl(container, this.cacheManager, executor));\n+        }\n+\n+        @Override\n+        public void close() {\n+            this.readIndexFactory.close();\n+            this.cacheManager.close();\n+            this.cacheStorage.close();\n+            this.dataLogFactory.close();\n+        }\n+    }\n+\n+    SegmentStoreStarter startSegmentStore(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+        return new SegmentStoreStarter(storageFactory, dataLogFactory);\n+    }\n+\n+    /**\n+     * Creates a segment store server.\n+     */\n+    private static class SegmentStoreStarter {\n+        private final int servicePort = TestUtils.getAvailableListenPort();\n+        private ServiceBuilder serviceBuilder;\n+        private StreamSegmentStoreWrapper streamSegmentStoreWrapper;\n+        private AutoScaleMonitor monitor;\n+        private TableStoreWrapper tableStoreWrapper;\n+        private PravegaConnectionListener server;\n+\n+        SegmentStoreStarter(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+            if (storageFactory != null) {\n+                if (dataLogFactory != null) {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory)\n+                            .withDataLogFactory(setup -> dataLogFactory);\n+                } else {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory);\n+                }\n+            } else {\n+                this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n+            }\n+            this.serviceBuilder.initialize();\n+            this.streamSegmentStoreWrapper = new StreamSegmentStoreWrapper(serviceBuilder.createStreamSegmentService());\n+            this.monitor = new AutoScaleMonitor(streamSegmentStoreWrapper, AutoScalerConfig.builder().build());\n+            this.tableStoreWrapper = new TableStoreWrapper(serviceBuilder.createTableStoreService());\n+            this.server = new PravegaConnectionListener(false, false, \"localhost\", servicePort, streamSegmentStoreWrapper,\n+                    tableStoreWrapper, monitor.getStatsRecorder(), monitor.getTableSegmentStatsRecorder(), new PassingTokenVerifier(),\n+                    null, null, true, serviceBuilder.getLowPriorityExecutor());\n+            this.server.startListening();\n+        }\n+\n+        public void close() {\n+            if (this.server != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 386}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDY1NjE1MA==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r460656150", "createdAt": "2020-07-27T05:36:10Z", "author": {"login": "ManishKumarKeshri"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig\n+                .builder()\n+                .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+                .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+                .build();\n+        private final WriterConfig writerConfig = WriterConfig\n+                .builder()\n+                .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+                .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+                .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+                .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+                .build();\n+\n+        DebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+            this.dataLogFactory = dataLogFactory;\n+            this.storageFactory = storageFactory;\n+            this.operationLogFactory = new DurableLogFactory(durableLogConfig, this.dataLogFactory, executorService);\n+\n+            this.cacheStorage = new DirectMemoryCache(Integer.MAX_VALUE);\n+            this.cacheManager = new CacheManager(CachePolicy.INFINITE, this.cacheStorage, executorService);\n+            this.readIndexFactory = new ContainerReadIndexFactory(readIndexConfig, this.cacheManager, executorService);\n+            this.attributeIndexFactory = new ContainerAttributeIndexFactoryImpl(attributeIndexConfig, this.cacheManager, executorService);\n+            this.writerFactory = new StorageWriterFactory(writerConfig, executorService);\n+\n+            ContainerConfig containerConfig = ServiceBuilderConfig.getDefaultConfig().getConfig(ContainerConfig::builder);\n+            this.containerFactory = new StreamSegmentContainerFactory(containerConfig, this.operationLogFactory,\n+                    this.readIndexFactory, this.attributeIndexFactory, this.writerFactory, this.storageFactory,\n+                    this::createContainerExtensions, executorService);\n+        }\n+\n+        private Map<Class<? extends SegmentContainerExtension>, SegmentContainerExtension> createContainerExtensions(\n+                SegmentContainer container, ScheduledExecutorService executor) {\n+            return Collections.singletonMap(ContainerTableExtension.class, new ContainerTableExtensionImpl(container, this.cacheManager, executor));\n+        }\n+\n+        @Override\n+        public void close() {\n+            this.readIndexFactory.close();\n+            this.cacheManager.close();\n+            this.cacheStorage.close();\n+            this.dataLogFactory.close();\n+        }\n+    }\n+\n+    SegmentStoreStarter startSegmentStore(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+        return new SegmentStoreStarter(storageFactory, dataLogFactory);\n+    }\n+\n+    /**\n+     * Creates a segment store server.\n+     */\n+    private static class SegmentStoreStarter {\n+        private final int servicePort = TestUtils.getAvailableListenPort();\n+        private ServiceBuilder serviceBuilder;\n+        private StreamSegmentStoreWrapper streamSegmentStoreWrapper;\n+        private AutoScaleMonitor monitor;\n+        private TableStoreWrapper tableStoreWrapper;\n+        private PravegaConnectionListener server;\n+\n+        SegmentStoreStarter(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+            if (storageFactory != null) {\n+                if (dataLogFactory != null) {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory)\n+                            .withDataLogFactory(setup -> dataLogFactory);\n+                } else {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory);\n+                }\n+            } else {\n+                this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n+            }\n+            this.serviceBuilder.initialize();\n+            this.streamSegmentStoreWrapper = new StreamSegmentStoreWrapper(serviceBuilder.createStreamSegmentService());\n+            this.monitor = new AutoScaleMonitor(streamSegmentStoreWrapper, AutoScalerConfig.builder().build());\n+            this.tableStoreWrapper = new TableStoreWrapper(serviceBuilder.createTableStoreService());\n+            this.server = new PravegaConnectionListener(false, false, \"localhost\", servicePort, streamSegmentStoreWrapper,\n+                    tableStoreWrapper, monitor.getStatsRecorder(), monitor.getTableSegmentStatsRecorder(), new PassingTokenVerifier(),\n+                    null, null, true, serviceBuilder.getLowPriorityExecutor());\n+            this.server.startListening();\n+        }\n+\n+        public void close() {\n+            if (this.server != null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMjI3MQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 386}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODMzMDQ5OnYy", "diffSide": "RIGHT", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODozOTo1NVrOGzdZxQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjo0NDo0M1rOG2eDpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMjI5Mw==", "bodyText": "Verify that you are shutting down everything that needs to shut down.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456612293", "createdAt": "2020-07-17T18:39:55Z", "author": {"login": "andreipaduroiu"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig\n+                .builder()\n+                .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+                .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+                .build();\n+        private final WriterConfig writerConfig = WriterConfig\n+                .builder()\n+                .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+                .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+                .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+                .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+                .build();\n+\n+        DebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+            this.dataLogFactory = dataLogFactory;\n+            this.storageFactory = storageFactory;\n+            this.operationLogFactory = new DurableLogFactory(durableLogConfig, this.dataLogFactory, executorService);\n+\n+            this.cacheStorage = new DirectMemoryCache(Integer.MAX_VALUE);\n+            this.cacheManager = new CacheManager(CachePolicy.INFINITE, this.cacheStorage, executorService);\n+            this.readIndexFactory = new ContainerReadIndexFactory(readIndexConfig, this.cacheManager, executorService);\n+            this.attributeIndexFactory = new ContainerAttributeIndexFactoryImpl(attributeIndexConfig, this.cacheManager, executorService);\n+            this.writerFactory = new StorageWriterFactory(writerConfig, executorService);\n+\n+            ContainerConfig containerConfig = ServiceBuilderConfig.getDefaultConfig().getConfig(ContainerConfig::builder);\n+            this.containerFactory = new StreamSegmentContainerFactory(containerConfig, this.operationLogFactory,\n+                    this.readIndexFactory, this.attributeIndexFactory, this.writerFactory, this.storageFactory,\n+                    this::createContainerExtensions, executorService);\n+        }\n+\n+        private Map<Class<? extends SegmentContainerExtension>, SegmentContainerExtension> createContainerExtensions(\n+                SegmentContainer container, ScheduledExecutorService executor) {\n+            return Collections.singletonMap(ContainerTableExtension.class, new ContainerTableExtensionImpl(container, this.cacheManager, executor));\n+        }\n+\n+        @Override\n+        public void close() {\n+            this.readIndexFactory.close();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 340}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2ODc0Mw==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459768743", "createdAt": "2020-07-23T22:44:43Z", "author": {"login": "ManishKumarKeshri"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig\n+                .builder()\n+                .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+                .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+                .build();\n+        private final WriterConfig writerConfig = WriterConfig\n+                .builder()\n+                .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+                .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+                .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+                .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+                .build();\n+\n+        DebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+            this.dataLogFactory = dataLogFactory;\n+            this.storageFactory = storageFactory;\n+            this.operationLogFactory = new DurableLogFactory(durableLogConfig, this.dataLogFactory, executorService);\n+\n+            this.cacheStorage = new DirectMemoryCache(Integer.MAX_VALUE);\n+            this.cacheManager = new CacheManager(CachePolicy.INFINITE, this.cacheStorage, executorService);\n+            this.readIndexFactory = new ContainerReadIndexFactory(readIndexConfig, this.cacheManager, executorService);\n+            this.attributeIndexFactory = new ContainerAttributeIndexFactoryImpl(attributeIndexConfig, this.cacheManager, executorService);\n+            this.writerFactory = new StorageWriterFactory(writerConfig, executorService);\n+\n+            ContainerConfig containerConfig = ServiceBuilderConfig.getDefaultConfig().getConfig(ContainerConfig::builder);\n+            this.containerFactory = new StreamSegmentContainerFactory(containerConfig, this.operationLogFactory,\n+                    this.readIndexFactory, this.attributeIndexFactory, this.writerFactory, this.storageFactory,\n+                    this::createContainerExtensions, executorService);\n+        }\n+\n+        private Map<Class<? extends SegmentContainerExtension>, SegmentContainerExtension> createContainerExtensions(\n+                SegmentContainer container, ScheduledExecutorService executor) {\n+            return Collections.singletonMap(ContainerTableExtension.class, new ContainerTableExtensionImpl(container, this.cacheManager, executor));\n+        }\n+\n+        @Override\n+        public void close() {\n+            this.readIndexFactory.close();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMjI5Mw=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 340}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODMzMDg3OnYy", "diffSide": "RIGHT", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODo0MDowMlrOGzdaBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjo0NDo0OVrOG2eDyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMjM1Ng==", "bodyText": "Verify that you are shutting down everything that needs to shut down.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456612356", "createdAt": "2020-07-17T18:40:02Z", "author": {"login": "andreipaduroiu"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig\n+                .builder()\n+                .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+                .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+                .build();\n+        private final WriterConfig writerConfig = WriterConfig\n+                .builder()\n+                .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+                .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+                .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+                .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+                .build();\n+\n+        DebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+            this.dataLogFactory = dataLogFactory;\n+            this.storageFactory = storageFactory;\n+            this.operationLogFactory = new DurableLogFactory(durableLogConfig, this.dataLogFactory, executorService);\n+\n+            this.cacheStorage = new DirectMemoryCache(Integer.MAX_VALUE);\n+            this.cacheManager = new CacheManager(CachePolicy.INFINITE, this.cacheStorage, executorService);\n+            this.readIndexFactory = new ContainerReadIndexFactory(readIndexConfig, this.cacheManager, executorService);\n+            this.attributeIndexFactory = new ContainerAttributeIndexFactoryImpl(attributeIndexConfig, this.cacheManager, executorService);\n+            this.writerFactory = new StorageWriterFactory(writerConfig, executorService);\n+\n+            ContainerConfig containerConfig = ServiceBuilderConfig.getDefaultConfig().getConfig(ContainerConfig::builder);\n+            this.containerFactory = new StreamSegmentContainerFactory(containerConfig, this.operationLogFactory,\n+                    this.readIndexFactory, this.attributeIndexFactory, this.writerFactory, this.storageFactory,\n+                    this::createContainerExtensions, executorService);\n+        }\n+\n+        private Map<Class<? extends SegmentContainerExtension>, SegmentContainerExtension> createContainerExtensions(\n+                SegmentContainer container, ScheduledExecutorService executor) {\n+            return Collections.singletonMap(ContainerTableExtension.class, new ContainerTableExtensionImpl(container, this.cacheManager, executor));\n+        }\n+\n+        @Override\n+        public void close() {\n+            this.readIndexFactory.close();\n+            this.cacheManager.close();\n+            this.cacheStorage.close();\n+            this.dataLogFactory.close();\n+        }\n+    }\n+\n+    SegmentStoreStarter startSegmentStore(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+        return new SegmentStoreStarter(storageFactory, dataLogFactory);\n+    }\n+\n+    /**\n+     * Creates a segment store server.\n+     */\n+    private static class SegmentStoreStarter {\n+        private final int servicePort = TestUtils.getAvailableListenPort();\n+        private ServiceBuilder serviceBuilder;\n+        private StreamSegmentStoreWrapper streamSegmentStoreWrapper;\n+        private AutoScaleMonitor monitor;\n+        private TableStoreWrapper tableStoreWrapper;\n+        private PravegaConnectionListener server;\n+\n+        SegmentStoreStarter(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+            if (storageFactory != null) {\n+                if (dataLogFactory != null) {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory)\n+                            .withDataLogFactory(setup -> dataLogFactory);\n+                } else {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory);\n+                }\n+            } else {\n+                this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n+            }\n+            this.serviceBuilder.initialize();\n+            this.streamSegmentStoreWrapper = new StreamSegmentStoreWrapper(serviceBuilder.createStreamSegmentService());\n+            this.monitor = new AutoScaleMonitor(streamSegmentStoreWrapper, AutoScalerConfig.builder().build());\n+            this.tableStoreWrapper = new TableStoreWrapper(serviceBuilder.createTableStoreService());\n+            this.server = new PravegaConnectionListener(false, false, \"localhost\", servicePort, streamSegmentStoreWrapper,\n+                    tableStoreWrapper, monitor.getStatsRecorder(), monitor.getTableSegmentStatsRecorder(), new PassingTokenVerifier(),\n+                    null, null, true, serviceBuilder.getLowPriorityExecutor());\n+            this.server.startListening();\n+        }\n+\n+        public void close() {\n+            if (this.server != null) {\n+                this.server.close();\n+                this.server = null;\n+            }\n+\n+            if (this.monitor != null) {\n+                this.monitor.close();\n+                this.monitor = null;\n+            }\n+\n+            if (this.serviceBuilder != null) {\n+                this.serviceBuilder.close();\n+                this.serviceBuilder = null;\n+            }\n+        }\n+    }\n+\n+    ControllerStarter startController(int bkPort, int servicePort) throws InterruptedException {\n+        return new ControllerStarter(bkPort, servicePort);\n+    }\n+\n+    /**\n+     * Creates a controller instance and runs it.\n+     */\n+    private static class ControllerStarter {\n+        private final int controllerPort = TestUtils.getAvailableListenPort();\n+        private final String serviceHost = \"localhost\";\n+        private ControllerWrapper controllerWrapper = null;\n+        private Controller controller = null;\n+\n+        ControllerStarter(int bkPort, int servicePort) throws InterruptedException {\n+            this.controllerWrapper = new ControllerWrapper(\"localhost:\" + bkPort, false,\n+                    controllerPort, serviceHost, servicePort, CONTAINER_COUNT);\n+            this.controllerWrapper.awaitRunning();\n+            this.controller = controllerWrapper.getController();\n+        }\n+\n+        public void close() throws Exception {\n+            if (this.controller != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 424}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2ODc3OA==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459768778", "createdAt": "2020-07-23T22:44:49Z", "author": {"login": "ManishKumarKeshri"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig\n+                .builder()\n+                .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+                .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+                .build();\n+        private final WriterConfig writerConfig = WriterConfig\n+                .builder()\n+                .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+                .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+                .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+                .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+                .build();\n+\n+        DebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+            this.dataLogFactory = dataLogFactory;\n+            this.storageFactory = storageFactory;\n+            this.operationLogFactory = new DurableLogFactory(durableLogConfig, this.dataLogFactory, executorService);\n+\n+            this.cacheStorage = new DirectMemoryCache(Integer.MAX_VALUE);\n+            this.cacheManager = new CacheManager(CachePolicy.INFINITE, this.cacheStorage, executorService);\n+            this.readIndexFactory = new ContainerReadIndexFactory(readIndexConfig, this.cacheManager, executorService);\n+            this.attributeIndexFactory = new ContainerAttributeIndexFactoryImpl(attributeIndexConfig, this.cacheManager, executorService);\n+            this.writerFactory = new StorageWriterFactory(writerConfig, executorService);\n+\n+            ContainerConfig containerConfig = ServiceBuilderConfig.getDefaultConfig().getConfig(ContainerConfig::builder);\n+            this.containerFactory = new StreamSegmentContainerFactory(containerConfig, this.operationLogFactory,\n+                    this.readIndexFactory, this.attributeIndexFactory, this.writerFactory, this.storageFactory,\n+                    this::createContainerExtensions, executorService);\n+        }\n+\n+        private Map<Class<? extends SegmentContainerExtension>, SegmentContainerExtension> createContainerExtensions(\n+                SegmentContainer container, ScheduledExecutorService executor) {\n+            return Collections.singletonMap(ContainerTableExtension.class, new ContainerTableExtensionImpl(container, this.cacheManager, executor));\n+        }\n+\n+        @Override\n+        public void close() {\n+            this.readIndexFactory.close();\n+            this.cacheManager.close();\n+            this.cacheStorage.close();\n+            this.dataLogFactory.close();\n+        }\n+    }\n+\n+    SegmentStoreStarter startSegmentStore(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+        return new SegmentStoreStarter(storageFactory, dataLogFactory);\n+    }\n+\n+    /**\n+     * Creates a segment store server.\n+     */\n+    private static class SegmentStoreStarter {\n+        private final int servicePort = TestUtils.getAvailableListenPort();\n+        private ServiceBuilder serviceBuilder;\n+        private StreamSegmentStoreWrapper streamSegmentStoreWrapper;\n+        private AutoScaleMonitor monitor;\n+        private TableStoreWrapper tableStoreWrapper;\n+        private PravegaConnectionListener server;\n+\n+        SegmentStoreStarter(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+            if (storageFactory != null) {\n+                if (dataLogFactory != null) {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory)\n+                            .withDataLogFactory(setup -> dataLogFactory);\n+                } else {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory);\n+                }\n+            } else {\n+                this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n+            }\n+            this.serviceBuilder.initialize();\n+            this.streamSegmentStoreWrapper = new StreamSegmentStoreWrapper(serviceBuilder.createStreamSegmentService());\n+            this.monitor = new AutoScaleMonitor(streamSegmentStoreWrapper, AutoScalerConfig.builder().build());\n+            this.tableStoreWrapper = new TableStoreWrapper(serviceBuilder.createTableStoreService());\n+            this.server = new PravegaConnectionListener(false, false, \"localhost\", servicePort, streamSegmentStoreWrapper,\n+                    tableStoreWrapper, monitor.getStatsRecorder(), monitor.getTableSegmentStatsRecorder(), new PassingTokenVerifier(),\n+                    null, null, true, serviceBuilder.getLowPriorityExecutor());\n+            this.server.startListening();\n+        }\n+\n+        public void close() {\n+            if (this.server != null) {\n+                this.server.close();\n+                this.server = null;\n+            }\n+\n+            if (this.monitor != null) {\n+                this.monitor.close();\n+                this.monitor = null;\n+            }\n+\n+            if (this.serviceBuilder != null) {\n+                this.serviceBuilder.close();\n+                this.serviceBuilder = null;\n+            }\n+        }\n+    }\n+\n+    ControllerStarter startController(int bkPort, int servicePort) throws InterruptedException {\n+        return new ControllerStarter(bkPort, servicePort);\n+    }\n+\n+    /**\n+     * Creates a controller instance and runs it.\n+     */\n+    private static class ControllerStarter {\n+        private final int controllerPort = TestUtils.getAvailableListenPort();\n+        private final String serviceHost = \"localhost\";\n+        private ControllerWrapper controllerWrapper = null;\n+        private Controller controller = null;\n+\n+        ControllerStarter(int bkPort, int servicePort) throws InterruptedException {\n+            this.controllerWrapper = new ControllerWrapper(\"localhost:\" + bkPort, false,\n+                    controllerPort, serviceHost, servicePort, CONTAINER_COUNT);\n+            this.controllerWrapper.awaitRunning();\n+            this.controller = controllerWrapper.getController();\n+        }\n+\n+        public void close() throws Exception {\n+            if (this.controller != null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMjM1Ng=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 424}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODMzNDA3OnYy", "diffSide": "RIGHT", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODo0MTowM1rOGzdb9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjo0NTowM1rOG2eEMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMjg1NA==", "bodyText": "Put @Cleanup on a different line. Below too", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456612854", "createdAt": "2020-07-17T18:41:03Z", "author": {"login": "andreipaduroiu"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig\n+                .builder()\n+                .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+                .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+                .build();\n+        private final WriterConfig writerConfig = WriterConfig\n+                .builder()\n+                .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+                .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+                .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+                .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+                .build();\n+\n+        DebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+            this.dataLogFactory = dataLogFactory;\n+            this.storageFactory = storageFactory;\n+            this.operationLogFactory = new DurableLogFactory(durableLogConfig, this.dataLogFactory, executorService);\n+\n+            this.cacheStorage = new DirectMemoryCache(Integer.MAX_VALUE);\n+            this.cacheManager = new CacheManager(CachePolicy.INFINITE, this.cacheStorage, executorService);\n+            this.readIndexFactory = new ContainerReadIndexFactory(readIndexConfig, this.cacheManager, executorService);\n+            this.attributeIndexFactory = new ContainerAttributeIndexFactoryImpl(attributeIndexConfig, this.cacheManager, executorService);\n+            this.writerFactory = new StorageWriterFactory(writerConfig, executorService);\n+\n+            ContainerConfig containerConfig = ServiceBuilderConfig.getDefaultConfig().getConfig(ContainerConfig::builder);\n+            this.containerFactory = new StreamSegmentContainerFactory(containerConfig, this.operationLogFactory,\n+                    this.readIndexFactory, this.attributeIndexFactory, this.writerFactory, this.storageFactory,\n+                    this::createContainerExtensions, executorService);\n+        }\n+\n+        private Map<Class<? extends SegmentContainerExtension>, SegmentContainerExtension> createContainerExtensions(\n+                SegmentContainer container, ScheduledExecutorService executor) {\n+            return Collections.singletonMap(ContainerTableExtension.class, new ContainerTableExtensionImpl(container, this.cacheManager, executor));\n+        }\n+\n+        @Override\n+        public void close() {\n+            this.readIndexFactory.close();\n+            this.cacheManager.close();\n+            this.cacheStorage.close();\n+            this.dataLogFactory.close();\n+        }\n+    }\n+\n+    SegmentStoreStarter startSegmentStore(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+        return new SegmentStoreStarter(storageFactory, dataLogFactory);\n+    }\n+\n+    /**\n+     * Creates a segment store server.\n+     */\n+    private static class SegmentStoreStarter {\n+        private final int servicePort = TestUtils.getAvailableListenPort();\n+        private ServiceBuilder serviceBuilder;\n+        private StreamSegmentStoreWrapper streamSegmentStoreWrapper;\n+        private AutoScaleMonitor monitor;\n+        private TableStoreWrapper tableStoreWrapper;\n+        private PravegaConnectionListener server;\n+\n+        SegmentStoreStarter(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+            if (storageFactory != null) {\n+                if (dataLogFactory != null) {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory)\n+                            .withDataLogFactory(setup -> dataLogFactory);\n+                } else {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory);\n+                }\n+            } else {\n+                this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n+            }\n+            this.serviceBuilder.initialize();\n+            this.streamSegmentStoreWrapper = new StreamSegmentStoreWrapper(serviceBuilder.createStreamSegmentService());\n+            this.monitor = new AutoScaleMonitor(streamSegmentStoreWrapper, AutoScalerConfig.builder().build());\n+            this.tableStoreWrapper = new TableStoreWrapper(serviceBuilder.createTableStoreService());\n+            this.server = new PravegaConnectionListener(false, false, \"localhost\", servicePort, streamSegmentStoreWrapper,\n+                    tableStoreWrapper, monitor.getStatsRecorder(), monitor.getTableSegmentStatsRecorder(), new PassingTokenVerifier(),\n+                    null, null, true, serviceBuilder.getLowPriorityExecutor());\n+            this.server.startListening();\n+        }\n+\n+        public void close() {\n+            if (this.server != null) {\n+                this.server.close();\n+                this.server = null;\n+            }\n+\n+            if (this.monitor != null) {\n+                this.monitor.close();\n+                this.monitor = null;\n+            }\n+\n+            if (this.serviceBuilder != null) {\n+                this.serviceBuilder.close();\n+                this.serviceBuilder = null;\n+            }\n+        }\n+    }\n+\n+    ControllerStarter startController(int bkPort, int servicePort) throws InterruptedException {\n+        return new ControllerStarter(bkPort, servicePort);\n+    }\n+\n+    /**\n+     * Creates a controller instance and runs it.\n+     */\n+    private static class ControllerStarter {\n+        private final int controllerPort = TestUtils.getAvailableListenPort();\n+        private final String serviceHost = \"localhost\";\n+        private ControllerWrapper controllerWrapper = null;\n+        private Controller controller = null;\n+\n+        ControllerStarter(int bkPort, int servicePort) throws InterruptedException {\n+            this.controllerWrapper = new ControllerWrapper(\"localhost:\" + bkPort, false,\n+                    controllerPort, serviceHost, servicePort, CONTAINER_COUNT);\n+            this.controllerWrapper.awaitRunning();\n+            this.controller = controllerWrapper.getController();\n+        }\n+\n+        public void close() throws Exception {\n+            if (this.controller != null) {\n+                this.controller.close();\n+                this.controller = null;\n+            }\n+\n+            if (this.controllerWrapper != null) {\n+                this.controllerWrapper.close();\n+                this.controllerWrapper = null;\n+            }\n+        }\n+    }\n+\n+    @Test(timeout = 240000)\n+    public void testDurableDataLogFail() throws Exception {\n+        int instanceId = 0;\n+\n+        // Creating tier 2 only once here.\n+        this.baseDir = Files.createTempDirectory(\"test_nfs\").toFile().getAbsoluteFile();\n+        FileSystemStorageConfig fsConfig = FileSystemStorageConfig\n+                .builder()\n+                .with(FileSystemStorageConfig.ROOT, this.baseDir.getAbsolutePath())\n+                .build();\n+        this.storageFactory = new FileSystemStorageFactory(fsConfig, executorService);\n+\n+        // Start a new BK & ZK, segment store and controller\n+        this.bkzk = setUpNewBK(instanceId++);\n+        this.segmentStoreStarter = startSegmentStore(this.storageFactory, null);\n+        @Cleanup ControllerStarter controllerStarter = startController(this.bkzk.bkPort, this.segmentStoreStarter.servicePort);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 451}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2ODg4Mw==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459768883", "createdAt": "2020-07-23T22:45:03Z", "author": {"login": "ManishKumarKeshri"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig\n+                .builder()\n+                .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+                .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+                .build();\n+        private final WriterConfig writerConfig = WriterConfig\n+                .builder()\n+                .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+                .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+                .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+                .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+                .build();\n+\n+        DebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+            this.dataLogFactory = dataLogFactory;\n+            this.storageFactory = storageFactory;\n+            this.operationLogFactory = new DurableLogFactory(durableLogConfig, this.dataLogFactory, executorService);\n+\n+            this.cacheStorage = new DirectMemoryCache(Integer.MAX_VALUE);\n+            this.cacheManager = new CacheManager(CachePolicy.INFINITE, this.cacheStorage, executorService);\n+            this.readIndexFactory = new ContainerReadIndexFactory(readIndexConfig, this.cacheManager, executorService);\n+            this.attributeIndexFactory = new ContainerAttributeIndexFactoryImpl(attributeIndexConfig, this.cacheManager, executorService);\n+            this.writerFactory = new StorageWriterFactory(writerConfig, executorService);\n+\n+            ContainerConfig containerConfig = ServiceBuilderConfig.getDefaultConfig().getConfig(ContainerConfig::builder);\n+            this.containerFactory = new StreamSegmentContainerFactory(containerConfig, this.operationLogFactory,\n+                    this.readIndexFactory, this.attributeIndexFactory, this.writerFactory, this.storageFactory,\n+                    this::createContainerExtensions, executorService);\n+        }\n+\n+        private Map<Class<? extends SegmentContainerExtension>, SegmentContainerExtension> createContainerExtensions(\n+                SegmentContainer container, ScheduledExecutorService executor) {\n+            return Collections.singletonMap(ContainerTableExtension.class, new ContainerTableExtensionImpl(container, this.cacheManager, executor));\n+        }\n+\n+        @Override\n+        public void close() {\n+            this.readIndexFactory.close();\n+            this.cacheManager.close();\n+            this.cacheStorage.close();\n+            this.dataLogFactory.close();\n+        }\n+    }\n+\n+    SegmentStoreStarter startSegmentStore(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+        return new SegmentStoreStarter(storageFactory, dataLogFactory);\n+    }\n+\n+    /**\n+     * Creates a segment store server.\n+     */\n+    private static class SegmentStoreStarter {\n+        private final int servicePort = TestUtils.getAvailableListenPort();\n+        private ServiceBuilder serviceBuilder;\n+        private StreamSegmentStoreWrapper streamSegmentStoreWrapper;\n+        private AutoScaleMonitor monitor;\n+        private TableStoreWrapper tableStoreWrapper;\n+        private PravegaConnectionListener server;\n+\n+        SegmentStoreStarter(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+            if (storageFactory != null) {\n+                if (dataLogFactory != null) {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory)\n+                            .withDataLogFactory(setup -> dataLogFactory);\n+                } else {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory);\n+                }\n+            } else {\n+                this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n+            }\n+            this.serviceBuilder.initialize();\n+            this.streamSegmentStoreWrapper = new StreamSegmentStoreWrapper(serviceBuilder.createStreamSegmentService());\n+            this.monitor = new AutoScaleMonitor(streamSegmentStoreWrapper, AutoScalerConfig.builder().build());\n+            this.tableStoreWrapper = new TableStoreWrapper(serviceBuilder.createTableStoreService());\n+            this.server = new PravegaConnectionListener(false, false, \"localhost\", servicePort, streamSegmentStoreWrapper,\n+                    tableStoreWrapper, monitor.getStatsRecorder(), monitor.getTableSegmentStatsRecorder(), new PassingTokenVerifier(),\n+                    null, null, true, serviceBuilder.getLowPriorityExecutor());\n+            this.server.startListening();\n+        }\n+\n+        public void close() {\n+            if (this.server != null) {\n+                this.server.close();\n+                this.server = null;\n+            }\n+\n+            if (this.monitor != null) {\n+                this.monitor.close();\n+                this.monitor = null;\n+            }\n+\n+            if (this.serviceBuilder != null) {\n+                this.serviceBuilder.close();\n+                this.serviceBuilder = null;\n+            }\n+        }\n+    }\n+\n+    ControllerStarter startController(int bkPort, int servicePort) throws InterruptedException {\n+        return new ControllerStarter(bkPort, servicePort);\n+    }\n+\n+    /**\n+     * Creates a controller instance and runs it.\n+     */\n+    private static class ControllerStarter {\n+        private final int controllerPort = TestUtils.getAvailableListenPort();\n+        private final String serviceHost = \"localhost\";\n+        private ControllerWrapper controllerWrapper = null;\n+        private Controller controller = null;\n+\n+        ControllerStarter(int bkPort, int servicePort) throws InterruptedException {\n+            this.controllerWrapper = new ControllerWrapper(\"localhost:\" + bkPort, false,\n+                    controllerPort, serviceHost, servicePort, CONTAINER_COUNT);\n+            this.controllerWrapper.awaitRunning();\n+            this.controller = controllerWrapper.getController();\n+        }\n+\n+        public void close() throws Exception {\n+            if (this.controller != null) {\n+                this.controller.close();\n+                this.controller = null;\n+            }\n+\n+            if (this.controllerWrapper != null) {\n+                this.controllerWrapper.close();\n+                this.controllerWrapper = null;\n+            }\n+        }\n+    }\n+\n+    @Test(timeout = 240000)\n+    public void testDurableDataLogFail() throws Exception {\n+        int instanceId = 0;\n+\n+        // Creating tier 2 only once here.\n+        this.baseDir = Files.createTempDirectory(\"test_nfs\").toFile().getAbsoluteFile();\n+        FileSystemStorageConfig fsConfig = FileSystemStorageConfig\n+                .builder()\n+                .with(FileSystemStorageConfig.ROOT, this.baseDir.getAbsolutePath())\n+                .build();\n+        this.storageFactory = new FileSystemStorageFactory(fsConfig, executorService);\n+\n+        // Start a new BK & ZK, segment store and controller\n+        this.bkzk = setUpNewBK(instanceId++);\n+        this.segmentStoreStarter = startSegmentStore(this.storageFactory, null);\n+        @Cleanup ControllerStarter controllerStarter = startController(this.bkzk.bkPort, this.segmentStoreStarter.servicePort);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMjg1NA=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 451}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODMzNjM0OnYy", "diffSide": "RIGHT", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODo0MTo0NVrOGzddRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QwNTozNToyNFrOG3UNRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMzE5MQ==", "bodyText": "You need to figure out how to make this work without sleeping.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456613191", "createdAt": "2020-07-17T18:41:45Z", "author": {"login": "andreipaduroiu"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig\n+                .builder()\n+                .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+                .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+                .build();\n+        private final WriterConfig writerConfig = WriterConfig\n+                .builder()\n+                .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+                .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+                .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+                .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+                .build();\n+\n+        DebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+            this.dataLogFactory = dataLogFactory;\n+            this.storageFactory = storageFactory;\n+            this.operationLogFactory = new DurableLogFactory(durableLogConfig, this.dataLogFactory, executorService);\n+\n+            this.cacheStorage = new DirectMemoryCache(Integer.MAX_VALUE);\n+            this.cacheManager = new CacheManager(CachePolicy.INFINITE, this.cacheStorage, executorService);\n+            this.readIndexFactory = new ContainerReadIndexFactory(readIndexConfig, this.cacheManager, executorService);\n+            this.attributeIndexFactory = new ContainerAttributeIndexFactoryImpl(attributeIndexConfig, this.cacheManager, executorService);\n+            this.writerFactory = new StorageWriterFactory(writerConfig, executorService);\n+\n+            ContainerConfig containerConfig = ServiceBuilderConfig.getDefaultConfig().getConfig(ContainerConfig::builder);\n+            this.containerFactory = new StreamSegmentContainerFactory(containerConfig, this.operationLogFactory,\n+                    this.readIndexFactory, this.attributeIndexFactory, this.writerFactory, this.storageFactory,\n+                    this::createContainerExtensions, executorService);\n+        }\n+\n+        private Map<Class<? extends SegmentContainerExtension>, SegmentContainerExtension> createContainerExtensions(\n+                SegmentContainer container, ScheduledExecutorService executor) {\n+            return Collections.singletonMap(ContainerTableExtension.class, new ContainerTableExtensionImpl(container, this.cacheManager, executor));\n+        }\n+\n+        @Override\n+        public void close() {\n+            this.readIndexFactory.close();\n+            this.cacheManager.close();\n+            this.cacheStorage.close();\n+            this.dataLogFactory.close();\n+        }\n+    }\n+\n+    SegmentStoreStarter startSegmentStore(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+        return new SegmentStoreStarter(storageFactory, dataLogFactory);\n+    }\n+\n+    /**\n+     * Creates a segment store server.\n+     */\n+    private static class SegmentStoreStarter {\n+        private final int servicePort = TestUtils.getAvailableListenPort();\n+        private ServiceBuilder serviceBuilder;\n+        private StreamSegmentStoreWrapper streamSegmentStoreWrapper;\n+        private AutoScaleMonitor monitor;\n+        private TableStoreWrapper tableStoreWrapper;\n+        private PravegaConnectionListener server;\n+\n+        SegmentStoreStarter(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+            if (storageFactory != null) {\n+                if (dataLogFactory != null) {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory)\n+                            .withDataLogFactory(setup -> dataLogFactory);\n+                } else {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory);\n+                }\n+            } else {\n+                this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n+            }\n+            this.serviceBuilder.initialize();\n+            this.streamSegmentStoreWrapper = new StreamSegmentStoreWrapper(serviceBuilder.createStreamSegmentService());\n+            this.monitor = new AutoScaleMonitor(streamSegmentStoreWrapper, AutoScalerConfig.builder().build());\n+            this.tableStoreWrapper = new TableStoreWrapper(serviceBuilder.createTableStoreService());\n+            this.server = new PravegaConnectionListener(false, false, \"localhost\", servicePort, streamSegmentStoreWrapper,\n+                    tableStoreWrapper, monitor.getStatsRecorder(), monitor.getTableSegmentStatsRecorder(), new PassingTokenVerifier(),\n+                    null, null, true, serviceBuilder.getLowPriorityExecutor());\n+            this.server.startListening();\n+        }\n+\n+        public void close() {\n+            if (this.server != null) {\n+                this.server.close();\n+                this.server = null;\n+            }\n+\n+            if (this.monitor != null) {\n+                this.monitor.close();\n+                this.monitor = null;\n+            }\n+\n+            if (this.serviceBuilder != null) {\n+                this.serviceBuilder.close();\n+                this.serviceBuilder = null;\n+            }\n+        }\n+    }\n+\n+    ControllerStarter startController(int bkPort, int servicePort) throws InterruptedException {\n+        return new ControllerStarter(bkPort, servicePort);\n+    }\n+\n+    /**\n+     * Creates a controller instance and runs it.\n+     */\n+    private static class ControllerStarter {\n+        private final int controllerPort = TestUtils.getAvailableListenPort();\n+        private final String serviceHost = \"localhost\";\n+        private ControllerWrapper controllerWrapper = null;\n+        private Controller controller = null;\n+\n+        ControllerStarter(int bkPort, int servicePort) throws InterruptedException {\n+            this.controllerWrapper = new ControllerWrapper(\"localhost:\" + bkPort, false,\n+                    controllerPort, serviceHost, servicePort, CONTAINER_COUNT);\n+            this.controllerWrapper.awaitRunning();\n+            this.controller = controllerWrapper.getController();\n+        }\n+\n+        public void close() throws Exception {\n+            if (this.controller != null) {\n+                this.controller.close();\n+                this.controller = null;\n+            }\n+\n+            if (this.controllerWrapper != null) {\n+                this.controllerWrapper.close();\n+                this.controllerWrapper = null;\n+            }\n+        }\n+    }\n+\n+    @Test(timeout = 240000)\n+    public void testDurableDataLogFail() throws Exception {\n+        int instanceId = 0;\n+\n+        // Creating tier 2 only once here.\n+        this.baseDir = Files.createTempDirectory(\"test_nfs\").toFile().getAbsoluteFile();\n+        FileSystemStorageConfig fsConfig = FileSystemStorageConfig\n+                .builder()\n+                .with(FileSystemStorageConfig.ROOT, this.baseDir.getAbsolutePath())\n+                .build();\n+        this.storageFactory = new FileSystemStorageFactory(fsConfig, executorService);\n+\n+        // Start a new BK & ZK, segment store and controller\n+        this.bkzk = setUpNewBK(instanceId++);\n+        this.segmentStoreStarter = startSegmentStore(this.storageFactory, null);\n+        @Cleanup ControllerStarter controllerStarter = startController(this.bkzk.bkPort, this.segmentStoreStarter.servicePort);\n+\n+        // Create two streams for writing data onto two different segments\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM1);\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM2);\n+\n+        @Cleanup ConnectionFactory connectionFactory = new ConnectionFactoryImpl(ClientConfig.builder().build());\n+        @Cleanup ClientFactoryImpl clientFactory = new ClientFactoryImpl(SCOPE, controllerStarter.controller, connectionFactory);\n+        @Cleanup ReaderGroupManager readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerStarter.controller, clientFactory, connectionFactory);\n+\n+        writeEvents(STREAM1, clientFactory); // write 300 events on one segment\n+        writeEvents(STREAM2, clientFactory); // write 300 events on other segment\n+\n+        // Verify events write by reading them.\n+        readAllEvents(STREAM1, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n+                \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n+        readAllEvents(STREAM2, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n+                \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n+\n+        readerGroupManager.close();\n+        clientFactory.close();\n+\n+        controllerStarter.close(); // Shut down the controller\n+\n+        // Get names of all the segments created.\n+        HashSet<String> allSegments = new HashSet<>(this.segmentStoreStarter.streamSegmentStoreWrapper.getSegments());\n+        allSegments.addAll(this.segmentStoreStarter.tableStoreWrapper.getSegments());\n+        log.info(\"No. of segments created = {}\", allSegments.size());\n+\n+        // Get the long term storage from the running pravega instance\n+        @Cleanup Storage tier2 = new AsyncStorageWrapper(new RollingStorage(this.storageFactory.createSyncStorage(),\n+                new SegmentRollingPolicy(DEFAULT_ROLLING_SIZE)), DataRecoveryTestUtils.createExecutorService(1));\n+\n+        // wait for all segments to be flushed to the long term storage.\n+        waitForSegmentsInStorage(allSegments, this.segmentStoreStarter.streamSegmentStoreWrapper, tier2)\n+                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        this.segmentStoreStarter.close(); // Shutdown SegmentStore\n+        this.segmentStoreStarter = null;\n+        log.info(\"Segment Store Shutdown\");\n+\n+        this.bkzk.close(); // Shutdown BookKeeper & ZooKeeper\n+        this.bkzk = null;\n+        log.info(\"BookKeeper & ZooKeeper shutdown\");\n+\n+        // start a new BookKeeper and ZooKeeper.\n+        this.bkzk = setUpNewBK(instanceId++);\n+        this.dataLogFactory = new BookKeeperLogFactory(this.bkzk.bkConfig.get(), this.bkzk.zkClient.get(),\n+                DataRecoveryTestUtils.createExecutorService(1));\n+        this.dataLogFactory.initialize();\n+\n+        // Delete container metadata segment and attributes index segment corresponding to the container Id from the long term storage\n+        DataRecoveryTestUtils.deleteContainerMetadataSegments(tier2, CONTAINER_ID);\n+\n+        // List all segments from the long term storage\n+        Map<Integer, List<SegmentProperties>> segmentsToCreate = DataRecoveryTestUtils.listAllSegments(tier2, CONTAINER_COUNT);\n+\n+        // Start debug segment container using dataLogFactory from new BK instance and old long term storageFactory.\n+        DebugTool debugTool = createDebugTool(this.dataLogFactory, this.storageFactory);\n+        DebugStreamSegmentContainer debugStreamSegmentContainer = (DebugStreamSegmentContainer)\n+                debugTool.containerFactory.createDebugStreamSegmentContainer(CONTAINER_ID);\n+\n+        // Re-create all segments which were listed.\n+        Services.startAsync(debugStreamSegmentContainer, executorService)\n+                .thenRun(new DataRecoveryTestUtils.Worker(debugStreamSegmentContainer, segmentsToCreate.get(CONTAINER_ID))).join();\n+        sleep(5000); // Without sleep the test fails sometimes complaining some segment offsets don't exist.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 516}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2OTA3Nw==", "bodyText": "Removed sleep. Doesn't fail on Travis, though fails locally sometimes. Will create an issue for it.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459769077", "createdAt": "2020-07-23T22:45:35Z", "author": {"login": "ManishKumarKeshri"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig\n+                .builder()\n+                .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+                .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+                .build();\n+        private final WriterConfig writerConfig = WriterConfig\n+                .builder()\n+                .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+                .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+                .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+                .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+                .build();\n+\n+        DebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+            this.dataLogFactory = dataLogFactory;\n+            this.storageFactory = storageFactory;\n+            this.operationLogFactory = new DurableLogFactory(durableLogConfig, this.dataLogFactory, executorService);\n+\n+            this.cacheStorage = new DirectMemoryCache(Integer.MAX_VALUE);\n+            this.cacheManager = new CacheManager(CachePolicy.INFINITE, this.cacheStorage, executorService);\n+            this.readIndexFactory = new ContainerReadIndexFactory(readIndexConfig, this.cacheManager, executorService);\n+            this.attributeIndexFactory = new ContainerAttributeIndexFactoryImpl(attributeIndexConfig, this.cacheManager, executorService);\n+            this.writerFactory = new StorageWriterFactory(writerConfig, executorService);\n+\n+            ContainerConfig containerConfig = ServiceBuilderConfig.getDefaultConfig().getConfig(ContainerConfig::builder);\n+            this.containerFactory = new StreamSegmentContainerFactory(containerConfig, this.operationLogFactory,\n+                    this.readIndexFactory, this.attributeIndexFactory, this.writerFactory, this.storageFactory,\n+                    this::createContainerExtensions, executorService);\n+        }\n+\n+        private Map<Class<? extends SegmentContainerExtension>, SegmentContainerExtension> createContainerExtensions(\n+                SegmentContainer container, ScheduledExecutorService executor) {\n+            return Collections.singletonMap(ContainerTableExtension.class, new ContainerTableExtensionImpl(container, this.cacheManager, executor));\n+        }\n+\n+        @Override\n+        public void close() {\n+            this.readIndexFactory.close();\n+            this.cacheManager.close();\n+            this.cacheStorage.close();\n+            this.dataLogFactory.close();\n+        }\n+    }\n+\n+    SegmentStoreStarter startSegmentStore(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+        return new SegmentStoreStarter(storageFactory, dataLogFactory);\n+    }\n+\n+    /**\n+     * Creates a segment store server.\n+     */\n+    private static class SegmentStoreStarter {\n+        private final int servicePort = TestUtils.getAvailableListenPort();\n+        private ServiceBuilder serviceBuilder;\n+        private StreamSegmentStoreWrapper streamSegmentStoreWrapper;\n+        private AutoScaleMonitor monitor;\n+        private TableStoreWrapper tableStoreWrapper;\n+        private PravegaConnectionListener server;\n+\n+        SegmentStoreStarter(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+            if (storageFactory != null) {\n+                if (dataLogFactory != null) {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory)\n+                            .withDataLogFactory(setup -> dataLogFactory);\n+                } else {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory);\n+                }\n+            } else {\n+                this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n+            }\n+            this.serviceBuilder.initialize();\n+            this.streamSegmentStoreWrapper = new StreamSegmentStoreWrapper(serviceBuilder.createStreamSegmentService());\n+            this.monitor = new AutoScaleMonitor(streamSegmentStoreWrapper, AutoScalerConfig.builder().build());\n+            this.tableStoreWrapper = new TableStoreWrapper(serviceBuilder.createTableStoreService());\n+            this.server = new PravegaConnectionListener(false, false, \"localhost\", servicePort, streamSegmentStoreWrapper,\n+                    tableStoreWrapper, monitor.getStatsRecorder(), monitor.getTableSegmentStatsRecorder(), new PassingTokenVerifier(),\n+                    null, null, true, serviceBuilder.getLowPriorityExecutor());\n+            this.server.startListening();\n+        }\n+\n+        public void close() {\n+            if (this.server != null) {\n+                this.server.close();\n+                this.server = null;\n+            }\n+\n+            if (this.monitor != null) {\n+                this.monitor.close();\n+                this.monitor = null;\n+            }\n+\n+            if (this.serviceBuilder != null) {\n+                this.serviceBuilder.close();\n+                this.serviceBuilder = null;\n+            }\n+        }\n+    }\n+\n+    ControllerStarter startController(int bkPort, int servicePort) throws InterruptedException {\n+        return new ControllerStarter(bkPort, servicePort);\n+    }\n+\n+    /**\n+     * Creates a controller instance and runs it.\n+     */\n+    private static class ControllerStarter {\n+        private final int controllerPort = TestUtils.getAvailableListenPort();\n+        private final String serviceHost = \"localhost\";\n+        private ControllerWrapper controllerWrapper = null;\n+        private Controller controller = null;\n+\n+        ControllerStarter(int bkPort, int servicePort) throws InterruptedException {\n+            this.controllerWrapper = new ControllerWrapper(\"localhost:\" + bkPort, false,\n+                    controllerPort, serviceHost, servicePort, CONTAINER_COUNT);\n+            this.controllerWrapper.awaitRunning();\n+            this.controller = controllerWrapper.getController();\n+        }\n+\n+        public void close() throws Exception {\n+            if (this.controller != null) {\n+                this.controller.close();\n+                this.controller = null;\n+            }\n+\n+            if (this.controllerWrapper != null) {\n+                this.controllerWrapper.close();\n+                this.controllerWrapper = null;\n+            }\n+        }\n+    }\n+\n+    @Test(timeout = 240000)\n+    public void testDurableDataLogFail() throws Exception {\n+        int instanceId = 0;\n+\n+        // Creating tier 2 only once here.\n+        this.baseDir = Files.createTempDirectory(\"test_nfs\").toFile().getAbsoluteFile();\n+        FileSystemStorageConfig fsConfig = FileSystemStorageConfig\n+                .builder()\n+                .with(FileSystemStorageConfig.ROOT, this.baseDir.getAbsolutePath())\n+                .build();\n+        this.storageFactory = new FileSystemStorageFactory(fsConfig, executorService);\n+\n+        // Start a new BK & ZK, segment store and controller\n+        this.bkzk = setUpNewBK(instanceId++);\n+        this.segmentStoreStarter = startSegmentStore(this.storageFactory, null);\n+        @Cleanup ControllerStarter controllerStarter = startController(this.bkzk.bkPort, this.segmentStoreStarter.servicePort);\n+\n+        // Create two streams for writing data onto two different segments\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM1);\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM2);\n+\n+        @Cleanup ConnectionFactory connectionFactory = new ConnectionFactoryImpl(ClientConfig.builder().build());\n+        @Cleanup ClientFactoryImpl clientFactory = new ClientFactoryImpl(SCOPE, controllerStarter.controller, connectionFactory);\n+        @Cleanup ReaderGroupManager readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerStarter.controller, clientFactory, connectionFactory);\n+\n+        writeEvents(STREAM1, clientFactory); // write 300 events on one segment\n+        writeEvents(STREAM2, clientFactory); // write 300 events on other segment\n+\n+        // Verify events write by reading them.\n+        readAllEvents(STREAM1, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n+                \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n+        readAllEvents(STREAM2, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n+                \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n+\n+        readerGroupManager.close();\n+        clientFactory.close();\n+\n+        controllerStarter.close(); // Shut down the controller\n+\n+        // Get names of all the segments created.\n+        HashSet<String> allSegments = new HashSet<>(this.segmentStoreStarter.streamSegmentStoreWrapper.getSegments());\n+        allSegments.addAll(this.segmentStoreStarter.tableStoreWrapper.getSegments());\n+        log.info(\"No. of segments created = {}\", allSegments.size());\n+\n+        // Get the long term storage from the running pravega instance\n+        @Cleanup Storage tier2 = new AsyncStorageWrapper(new RollingStorage(this.storageFactory.createSyncStorage(),\n+                new SegmentRollingPolicy(DEFAULT_ROLLING_SIZE)), DataRecoveryTestUtils.createExecutorService(1));\n+\n+        // wait for all segments to be flushed to the long term storage.\n+        waitForSegmentsInStorage(allSegments, this.segmentStoreStarter.streamSegmentStoreWrapper, tier2)\n+                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        this.segmentStoreStarter.close(); // Shutdown SegmentStore\n+        this.segmentStoreStarter = null;\n+        log.info(\"Segment Store Shutdown\");\n+\n+        this.bkzk.close(); // Shutdown BookKeeper & ZooKeeper\n+        this.bkzk = null;\n+        log.info(\"BookKeeper & ZooKeeper shutdown\");\n+\n+        // start a new BookKeeper and ZooKeeper.\n+        this.bkzk = setUpNewBK(instanceId++);\n+        this.dataLogFactory = new BookKeeperLogFactory(this.bkzk.bkConfig.get(), this.bkzk.zkClient.get(),\n+                DataRecoveryTestUtils.createExecutorService(1));\n+        this.dataLogFactory.initialize();\n+\n+        // Delete container metadata segment and attributes index segment corresponding to the container Id from the long term storage\n+        DataRecoveryTestUtils.deleteContainerMetadataSegments(tier2, CONTAINER_ID);\n+\n+        // List all segments from the long term storage\n+        Map<Integer, List<SegmentProperties>> segmentsToCreate = DataRecoveryTestUtils.listAllSegments(tier2, CONTAINER_COUNT);\n+\n+        // Start debug segment container using dataLogFactory from new BK instance and old long term storageFactory.\n+        DebugTool debugTool = createDebugTool(this.dataLogFactory, this.storageFactory);\n+        DebugStreamSegmentContainer debugStreamSegmentContainer = (DebugStreamSegmentContainer)\n+                debugTool.containerFactory.createDebugStreamSegmentContainer(CONTAINER_ID);\n+\n+        // Re-create all segments which were listed.\n+        Services.startAsync(debugStreamSegmentContainer, executorService)\n+                .thenRun(new DataRecoveryTestUtils.Worker(debugStreamSegmentContainer, segmentsToCreate.get(CONTAINER_ID))).join();\n+        sleep(5000); // Without sleep the test fails sometimes complaining some segment offsets don't exist.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMzE5MQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 516}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDY1NTk0Mg==", "bodyText": "It doesn't fail locally now even with multiple runs. Just added an extra line to wait for container metadata segment to be flushed to LTS, after debug segment container re-creates it in the Tier1.\nI saw that the test failed only when, debug segment container closed while storage writer has not completed some iterations.\nHere are snippets from two failure tests.\n2020-07-26 20:40:53,317 53008 [pool-2-thread-56] DEBUG i.p.s.server.writer.StorageWriter - StorageWriter[0]: Iteration[5].Finish (Elapsed 53ms).\n2020-07-26 20:40:53,317 53008 [pool-2-thread-23] ERROR i.p.s.s.writer.AttributeAggregator - AttributeAggregator[0-1]: Unable to persist root pointer RootPointer=2970, LastSeqNo=134.\njava.util.concurrent.CancellationException: OperationProcessor is shutting down.\n\tat io.pravega.segmentstore.server.logs.OperationProcessor.doStop(OperationProcessor.java:161)\n\tat com.google.common.util.concurrent.AbstractService.stopAsync(AbstractService.java:280)\n\tat io.pravega.common.concurrent.Services.stopAsync(Services.java:61)\n\tat io.pravega.segmentstore.server.logs.DurableLog.doStop(DurableLog.java:247)\n\tat com.google.common.util.concurrent.AbstractService.stopAsync(AbstractService.java:280)\n\tat io.pravega.common.concurrent.Services.stopAsync(Services.java:61)\n\tat io.pravega.segmentstore.server.containers.StreamSegmentContainer.doStop(StreamSegmentContainer.java:286)\n\tat io.pravega.segmentstore.server.containers.StreamSegmentContainer.doStop(StreamSegmentContainer.java:268)\n\tat com.google.common.util.concurrent.AbstractService.stopAsync(AbstractService.java:280)\n\tat io.pravega.common.concurrent.Services.stopAsync(Services.java:61)\n\tat io.pravega.test.integration.RestoreBackUpDataRecoveryTest.testDurableDataLogFail(RestoreBackUpDataRecoveryTest.java:474)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:50)\n\tat org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)\n\tat org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:47)\n\tat org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)\n\tat org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:298)\n\tat org.junit.internal.runners.statements.FailOnTimeout$CallableStatement.call(FailOnTimeout.java:292)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.lang.Thread.run(Thread.java:748)\n\nand another one:\n2020-07-26 12:47:51,375 96296 [pool-82-thread-70] DEBUG i.p.s.server.writer.StorageWriter - StorageWriter[0]: Iteration[6].InputRead (Count=38, Bytes=8769, LastReadSN=142).\n2020-07-26 12:47:51,376 96297 [pool-82-thread-70] ERROR i.p.s.server.writer.StorageWriter - StorageWriter[0]: Iteration[6].Error.\nio.pravega.segmentstore.server.IllegalContainerStateException: Container 0 is in an invalid state for this operation. Expected: RUNNING; Actual: STOPPING.\n\tat io.pravega.segmentstore.server.containers.StreamSegmentContainer.ensureRunning(StreamSegmentContainer.java:799)\n\tat io.pravega.segmentstore.server.containers.StreamSegmentContainer.forSegment(StreamSegmentContainer.java:553)\n\tat io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer.forSegment(DebugStreamSegmentContainer.java:28)\n\tat io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl$TableWriterConnectorImpl.getSegment(ContainerTableExtensionImpl.java:507)\n\tat io.pravega.segmentstore.server.tables.WriterTableProcessor.flush(WriterTableProcessor.java:162)\n\tat io.pravega.segmentstore.server.writer.StorageWriter$ProcessorCollection.lambda$flush$0(StorageWriter.java:646)\n\tat java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)\n\tat java.util.Spliterators$ArraySpliterator.forEachRemaining(Spliterators.java:948)\n\tat java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)\n\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)\n\tat java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)\n\tat java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\n\tat java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)\n\tat io.pravega.segmentstore.server.writer.StorageWriter$ProcessorCollection.flush(StorageWriter.java:646)\n\tat io.pravega.segmentstore.server.writer.StorageWriter.lambda$flush$8(StorageWriter.java:314)\n\tat java.util.stream.ReferencePipeline$3$1.accept(ReferencePipeline.java:193)\n\tat java.util.stream.ReferencePipeline$2$1.accept(ReferencePipeline.java:175)\n\tat java.util.HashMap$ValueSpliterator.forEachRemaining(HashMap.java:1628)\n\tat java.util.stream.AbstractPipeline.copyInto(AbstractPipeline.java:482)\n\tat java.util.stream.AbstractPipeline.wrapAndCopyInto(AbstractPipeline.java:472)\n\tat java.util.stream.ReduceOps$ReduceOp.evaluateSequential(ReduceOps.java:708)\n\tat java.util.stream.AbstractPipeline.evaluate(AbstractPipeline.java:234)\n\tat java.util.stream.ReferencePipeline.collect(ReferencePipeline.java:566)\n\tat io.pravega.segmentstore.server.writer.StorageWriter.flush(StorageWriter.java:315)\n\tat java.util.concurrent.CompletableFuture.uniCompose(CompletableFuture.java:966)\n\tat java.util.concurrent.CompletableFuture$UniCompose.tryFire(CompletableFuture.java:940)\n\tat java.util.concurrent.CompletableFuture$Completion.run(CompletableFuture.java:456)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\n2020-07-26 12:47:51,376 96297 [pool-82-thread-70] DEBUG i.p.s.server.writer.StorageW\n\nAfter these errors, test starts complaining about some segment chunks being deleted.\nMaybe waiting for container metadata segment to be flushed, provides some time for iterations to finish.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r460655942", "createdAt": "2020-07-27T05:35:24Z", "author": {"login": "ManishKumarKeshri"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig\n+                .builder()\n+                .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+                .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+                .build();\n+        private final WriterConfig writerConfig = WriterConfig\n+                .builder()\n+                .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+                .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+                .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+                .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+                .build();\n+\n+        DebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+            this.dataLogFactory = dataLogFactory;\n+            this.storageFactory = storageFactory;\n+            this.operationLogFactory = new DurableLogFactory(durableLogConfig, this.dataLogFactory, executorService);\n+\n+            this.cacheStorage = new DirectMemoryCache(Integer.MAX_VALUE);\n+            this.cacheManager = new CacheManager(CachePolicy.INFINITE, this.cacheStorage, executorService);\n+            this.readIndexFactory = new ContainerReadIndexFactory(readIndexConfig, this.cacheManager, executorService);\n+            this.attributeIndexFactory = new ContainerAttributeIndexFactoryImpl(attributeIndexConfig, this.cacheManager, executorService);\n+            this.writerFactory = new StorageWriterFactory(writerConfig, executorService);\n+\n+            ContainerConfig containerConfig = ServiceBuilderConfig.getDefaultConfig().getConfig(ContainerConfig::builder);\n+            this.containerFactory = new StreamSegmentContainerFactory(containerConfig, this.operationLogFactory,\n+                    this.readIndexFactory, this.attributeIndexFactory, this.writerFactory, this.storageFactory,\n+                    this::createContainerExtensions, executorService);\n+        }\n+\n+        private Map<Class<? extends SegmentContainerExtension>, SegmentContainerExtension> createContainerExtensions(\n+                SegmentContainer container, ScheduledExecutorService executor) {\n+            return Collections.singletonMap(ContainerTableExtension.class, new ContainerTableExtensionImpl(container, this.cacheManager, executor));\n+        }\n+\n+        @Override\n+        public void close() {\n+            this.readIndexFactory.close();\n+            this.cacheManager.close();\n+            this.cacheStorage.close();\n+            this.dataLogFactory.close();\n+        }\n+    }\n+\n+    SegmentStoreStarter startSegmentStore(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+        return new SegmentStoreStarter(storageFactory, dataLogFactory);\n+    }\n+\n+    /**\n+     * Creates a segment store server.\n+     */\n+    private static class SegmentStoreStarter {\n+        private final int servicePort = TestUtils.getAvailableListenPort();\n+        private ServiceBuilder serviceBuilder;\n+        private StreamSegmentStoreWrapper streamSegmentStoreWrapper;\n+        private AutoScaleMonitor monitor;\n+        private TableStoreWrapper tableStoreWrapper;\n+        private PravegaConnectionListener server;\n+\n+        SegmentStoreStarter(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+            if (storageFactory != null) {\n+                if (dataLogFactory != null) {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory)\n+                            .withDataLogFactory(setup -> dataLogFactory);\n+                } else {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory);\n+                }\n+            } else {\n+                this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n+            }\n+            this.serviceBuilder.initialize();\n+            this.streamSegmentStoreWrapper = new StreamSegmentStoreWrapper(serviceBuilder.createStreamSegmentService());\n+            this.monitor = new AutoScaleMonitor(streamSegmentStoreWrapper, AutoScalerConfig.builder().build());\n+            this.tableStoreWrapper = new TableStoreWrapper(serviceBuilder.createTableStoreService());\n+            this.server = new PravegaConnectionListener(false, false, \"localhost\", servicePort, streamSegmentStoreWrapper,\n+                    tableStoreWrapper, monitor.getStatsRecorder(), monitor.getTableSegmentStatsRecorder(), new PassingTokenVerifier(),\n+                    null, null, true, serviceBuilder.getLowPriorityExecutor());\n+            this.server.startListening();\n+        }\n+\n+        public void close() {\n+            if (this.server != null) {\n+                this.server.close();\n+                this.server = null;\n+            }\n+\n+            if (this.monitor != null) {\n+                this.monitor.close();\n+                this.monitor = null;\n+            }\n+\n+            if (this.serviceBuilder != null) {\n+                this.serviceBuilder.close();\n+                this.serviceBuilder = null;\n+            }\n+        }\n+    }\n+\n+    ControllerStarter startController(int bkPort, int servicePort) throws InterruptedException {\n+        return new ControllerStarter(bkPort, servicePort);\n+    }\n+\n+    /**\n+     * Creates a controller instance and runs it.\n+     */\n+    private static class ControllerStarter {\n+        private final int controllerPort = TestUtils.getAvailableListenPort();\n+        private final String serviceHost = \"localhost\";\n+        private ControllerWrapper controllerWrapper = null;\n+        private Controller controller = null;\n+\n+        ControllerStarter(int bkPort, int servicePort) throws InterruptedException {\n+            this.controllerWrapper = new ControllerWrapper(\"localhost:\" + bkPort, false,\n+                    controllerPort, serviceHost, servicePort, CONTAINER_COUNT);\n+            this.controllerWrapper.awaitRunning();\n+            this.controller = controllerWrapper.getController();\n+        }\n+\n+        public void close() throws Exception {\n+            if (this.controller != null) {\n+                this.controller.close();\n+                this.controller = null;\n+            }\n+\n+            if (this.controllerWrapper != null) {\n+                this.controllerWrapper.close();\n+                this.controllerWrapper = null;\n+            }\n+        }\n+    }\n+\n+    @Test(timeout = 240000)\n+    public void testDurableDataLogFail() throws Exception {\n+        int instanceId = 0;\n+\n+        // Creating tier 2 only once here.\n+        this.baseDir = Files.createTempDirectory(\"test_nfs\").toFile().getAbsoluteFile();\n+        FileSystemStorageConfig fsConfig = FileSystemStorageConfig\n+                .builder()\n+                .with(FileSystemStorageConfig.ROOT, this.baseDir.getAbsolutePath())\n+                .build();\n+        this.storageFactory = new FileSystemStorageFactory(fsConfig, executorService);\n+\n+        // Start a new BK & ZK, segment store and controller\n+        this.bkzk = setUpNewBK(instanceId++);\n+        this.segmentStoreStarter = startSegmentStore(this.storageFactory, null);\n+        @Cleanup ControllerStarter controllerStarter = startController(this.bkzk.bkPort, this.segmentStoreStarter.servicePort);\n+\n+        // Create two streams for writing data onto two different segments\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM1);\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM2);\n+\n+        @Cleanup ConnectionFactory connectionFactory = new ConnectionFactoryImpl(ClientConfig.builder().build());\n+        @Cleanup ClientFactoryImpl clientFactory = new ClientFactoryImpl(SCOPE, controllerStarter.controller, connectionFactory);\n+        @Cleanup ReaderGroupManager readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerStarter.controller, clientFactory, connectionFactory);\n+\n+        writeEvents(STREAM1, clientFactory); // write 300 events on one segment\n+        writeEvents(STREAM2, clientFactory); // write 300 events on other segment\n+\n+        // Verify events write by reading them.\n+        readAllEvents(STREAM1, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n+                \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n+        readAllEvents(STREAM2, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n+                \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n+\n+        readerGroupManager.close();\n+        clientFactory.close();\n+\n+        controllerStarter.close(); // Shut down the controller\n+\n+        // Get names of all the segments created.\n+        HashSet<String> allSegments = new HashSet<>(this.segmentStoreStarter.streamSegmentStoreWrapper.getSegments());\n+        allSegments.addAll(this.segmentStoreStarter.tableStoreWrapper.getSegments());\n+        log.info(\"No. of segments created = {}\", allSegments.size());\n+\n+        // Get the long term storage from the running pravega instance\n+        @Cleanup Storage tier2 = new AsyncStorageWrapper(new RollingStorage(this.storageFactory.createSyncStorage(),\n+                new SegmentRollingPolicy(DEFAULT_ROLLING_SIZE)), DataRecoveryTestUtils.createExecutorService(1));\n+\n+        // wait for all segments to be flushed to the long term storage.\n+        waitForSegmentsInStorage(allSegments, this.segmentStoreStarter.streamSegmentStoreWrapper, tier2)\n+                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        this.segmentStoreStarter.close(); // Shutdown SegmentStore\n+        this.segmentStoreStarter = null;\n+        log.info(\"Segment Store Shutdown\");\n+\n+        this.bkzk.close(); // Shutdown BookKeeper & ZooKeeper\n+        this.bkzk = null;\n+        log.info(\"BookKeeper & ZooKeeper shutdown\");\n+\n+        // start a new BookKeeper and ZooKeeper.\n+        this.bkzk = setUpNewBK(instanceId++);\n+        this.dataLogFactory = new BookKeeperLogFactory(this.bkzk.bkConfig.get(), this.bkzk.zkClient.get(),\n+                DataRecoveryTestUtils.createExecutorService(1));\n+        this.dataLogFactory.initialize();\n+\n+        // Delete container metadata segment and attributes index segment corresponding to the container Id from the long term storage\n+        DataRecoveryTestUtils.deleteContainerMetadataSegments(tier2, CONTAINER_ID);\n+\n+        // List all segments from the long term storage\n+        Map<Integer, List<SegmentProperties>> segmentsToCreate = DataRecoveryTestUtils.listAllSegments(tier2, CONTAINER_COUNT);\n+\n+        // Start debug segment container using dataLogFactory from new BK instance and old long term storageFactory.\n+        DebugTool debugTool = createDebugTool(this.dataLogFactory, this.storageFactory);\n+        DebugStreamSegmentContainer debugStreamSegmentContainer = (DebugStreamSegmentContainer)\n+                debugTool.containerFactory.createDebugStreamSegmentContainer(CONTAINER_ID);\n+\n+        // Re-create all segments which were listed.\n+        Services.startAsync(debugStreamSegmentContainer, executorService)\n+                .thenRun(new DataRecoveryTestUtils.Worker(debugStreamSegmentContainer, segmentsToCreate.get(CONTAINER_ID))).join();\n+        sleep(5000); // Without sleep the test fails sometimes complaining some segment offsets don't exist.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMzE5MQ=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 516}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODMzNzQ1OnYy", "diffSide": "RIGHT", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODo0MjoxMFrOGzdd_Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjo0NTo0NVrOG2eFKg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMzM3Mw==", "bodyText": "Why not bubble up?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456613373", "createdAt": "2020-07-17T18:42:10Z", "author": {"login": "andreipaduroiu"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig\n+                .builder()\n+                .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+                .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+                .build();\n+        private final WriterConfig writerConfig = WriterConfig\n+                .builder()\n+                .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+                .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+                .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+                .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+                .build();\n+\n+        DebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+            this.dataLogFactory = dataLogFactory;\n+            this.storageFactory = storageFactory;\n+            this.operationLogFactory = new DurableLogFactory(durableLogConfig, this.dataLogFactory, executorService);\n+\n+            this.cacheStorage = new DirectMemoryCache(Integer.MAX_VALUE);\n+            this.cacheManager = new CacheManager(CachePolicy.INFINITE, this.cacheStorage, executorService);\n+            this.readIndexFactory = new ContainerReadIndexFactory(readIndexConfig, this.cacheManager, executorService);\n+            this.attributeIndexFactory = new ContainerAttributeIndexFactoryImpl(attributeIndexConfig, this.cacheManager, executorService);\n+            this.writerFactory = new StorageWriterFactory(writerConfig, executorService);\n+\n+            ContainerConfig containerConfig = ServiceBuilderConfig.getDefaultConfig().getConfig(ContainerConfig::builder);\n+            this.containerFactory = new StreamSegmentContainerFactory(containerConfig, this.operationLogFactory,\n+                    this.readIndexFactory, this.attributeIndexFactory, this.writerFactory, this.storageFactory,\n+                    this::createContainerExtensions, executorService);\n+        }\n+\n+        private Map<Class<? extends SegmentContainerExtension>, SegmentContainerExtension> createContainerExtensions(\n+                SegmentContainer container, ScheduledExecutorService executor) {\n+            return Collections.singletonMap(ContainerTableExtension.class, new ContainerTableExtensionImpl(container, this.cacheManager, executor));\n+        }\n+\n+        @Override\n+        public void close() {\n+            this.readIndexFactory.close();\n+            this.cacheManager.close();\n+            this.cacheStorage.close();\n+            this.dataLogFactory.close();\n+        }\n+    }\n+\n+    SegmentStoreStarter startSegmentStore(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+        return new SegmentStoreStarter(storageFactory, dataLogFactory);\n+    }\n+\n+    /**\n+     * Creates a segment store server.\n+     */\n+    private static class SegmentStoreStarter {\n+        private final int servicePort = TestUtils.getAvailableListenPort();\n+        private ServiceBuilder serviceBuilder;\n+        private StreamSegmentStoreWrapper streamSegmentStoreWrapper;\n+        private AutoScaleMonitor monitor;\n+        private TableStoreWrapper tableStoreWrapper;\n+        private PravegaConnectionListener server;\n+\n+        SegmentStoreStarter(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+            if (storageFactory != null) {\n+                if (dataLogFactory != null) {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory)\n+                            .withDataLogFactory(setup -> dataLogFactory);\n+                } else {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory);\n+                }\n+            } else {\n+                this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n+            }\n+            this.serviceBuilder.initialize();\n+            this.streamSegmentStoreWrapper = new StreamSegmentStoreWrapper(serviceBuilder.createStreamSegmentService());\n+            this.monitor = new AutoScaleMonitor(streamSegmentStoreWrapper, AutoScalerConfig.builder().build());\n+            this.tableStoreWrapper = new TableStoreWrapper(serviceBuilder.createTableStoreService());\n+            this.server = new PravegaConnectionListener(false, false, \"localhost\", servicePort, streamSegmentStoreWrapper,\n+                    tableStoreWrapper, monitor.getStatsRecorder(), monitor.getTableSegmentStatsRecorder(), new PassingTokenVerifier(),\n+                    null, null, true, serviceBuilder.getLowPriorityExecutor());\n+            this.server.startListening();\n+        }\n+\n+        public void close() {\n+            if (this.server != null) {\n+                this.server.close();\n+                this.server = null;\n+            }\n+\n+            if (this.monitor != null) {\n+                this.monitor.close();\n+                this.monitor = null;\n+            }\n+\n+            if (this.serviceBuilder != null) {\n+                this.serviceBuilder.close();\n+                this.serviceBuilder = null;\n+            }\n+        }\n+    }\n+\n+    ControllerStarter startController(int bkPort, int servicePort) throws InterruptedException {\n+        return new ControllerStarter(bkPort, servicePort);\n+    }\n+\n+    /**\n+     * Creates a controller instance and runs it.\n+     */\n+    private static class ControllerStarter {\n+        private final int controllerPort = TestUtils.getAvailableListenPort();\n+        private final String serviceHost = \"localhost\";\n+        private ControllerWrapper controllerWrapper = null;\n+        private Controller controller = null;\n+\n+        ControllerStarter(int bkPort, int servicePort) throws InterruptedException {\n+            this.controllerWrapper = new ControllerWrapper(\"localhost:\" + bkPort, false,\n+                    controllerPort, serviceHost, servicePort, CONTAINER_COUNT);\n+            this.controllerWrapper.awaitRunning();\n+            this.controller = controllerWrapper.getController();\n+        }\n+\n+        public void close() throws Exception {\n+            if (this.controller != null) {\n+                this.controller.close();\n+                this.controller = null;\n+            }\n+\n+            if (this.controllerWrapper != null) {\n+                this.controllerWrapper.close();\n+                this.controllerWrapper = null;\n+            }\n+        }\n+    }\n+\n+    @Test(timeout = 240000)\n+    public void testDurableDataLogFail() throws Exception {\n+        int instanceId = 0;\n+\n+        // Creating tier 2 only once here.\n+        this.baseDir = Files.createTempDirectory(\"test_nfs\").toFile().getAbsoluteFile();\n+        FileSystemStorageConfig fsConfig = FileSystemStorageConfig\n+                .builder()\n+                .with(FileSystemStorageConfig.ROOT, this.baseDir.getAbsolutePath())\n+                .build();\n+        this.storageFactory = new FileSystemStorageFactory(fsConfig, executorService);\n+\n+        // Start a new BK & ZK, segment store and controller\n+        this.bkzk = setUpNewBK(instanceId++);\n+        this.segmentStoreStarter = startSegmentStore(this.storageFactory, null);\n+        @Cleanup ControllerStarter controllerStarter = startController(this.bkzk.bkPort, this.segmentStoreStarter.servicePort);\n+\n+        // Create two streams for writing data onto two different segments\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM1);\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM2);\n+\n+        @Cleanup ConnectionFactory connectionFactory = new ConnectionFactoryImpl(ClientConfig.builder().build());\n+        @Cleanup ClientFactoryImpl clientFactory = new ClientFactoryImpl(SCOPE, controllerStarter.controller, connectionFactory);\n+        @Cleanup ReaderGroupManager readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerStarter.controller, clientFactory, connectionFactory);\n+\n+        writeEvents(STREAM1, clientFactory); // write 300 events on one segment\n+        writeEvents(STREAM2, clientFactory); // write 300 events on other segment\n+\n+        // Verify events write by reading them.\n+        readAllEvents(STREAM1, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n+                \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n+        readAllEvents(STREAM2, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n+                \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n+\n+        readerGroupManager.close();\n+        clientFactory.close();\n+\n+        controllerStarter.close(); // Shut down the controller\n+\n+        // Get names of all the segments created.\n+        HashSet<String> allSegments = new HashSet<>(this.segmentStoreStarter.streamSegmentStoreWrapper.getSegments());\n+        allSegments.addAll(this.segmentStoreStarter.tableStoreWrapper.getSegments());\n+        log.info(\"No. of segments created = {}\", allSegments.size());\n+\n+        // Get the long term storage from the running pravega instance\n+        @Cleanup Storage tier2 = new AsyncStorageWrapper(new RollingStorage(this.storageFactory.createSyncStorage(),\n+                new SegmentRollingPolicy(DEFAULT_ROLLING_SIZE)), DataRecoveryTestUtils.createExecutorService(1));\n+\n+        // wait for all segments to be flushed to the long term storage.\n+        waitForSegmentsInStorage(allSegments, this.segmentStoreStarter.streamSegmentStoreWrapper, tier2)\n+                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        this.segmentStoreStarter.close(); // Shutdown SegmentStore\n+        this.segmentStoreStarter = null;\n+        log.info(\"Segment Store Shutdown\");\n+\n+        this.bkzk.close(); // Shutdown BookKeeper & ZooKeeper\n+        this.bkzk = null;\n+        log.info(\"BookKeeper & ZooKeeper shutdown\");\n+\n+        // start a new BookKeeper and ZooKeeper.\n+        this.bkzk = setUpNewBK(instanceId++);\n+        this.dataLogFactory = new BookKeeperLogFactory(this.bkzk.bkConfig.get(), this.bkzk.zkClient.get(),\n+                DataRecoveryTestUtils.createExecutorService(1));\n+        this.dataLogFactory.initialize();\n+\n+        // Delete container metadata segment and attributes index segment corresponding to the container Id from the long term storage\n+        DataRecoveryTestUtils.deleteContainerMetadataSegments(tier2, CONTAINER_ID);\n+\n+        // List all segments from the long term storage\n+        Map<Integer, List<SegmentProperties>> segmentsToCreate = DataRecoveryTestUtils.listAllSegments(tier2, CONTAINER_COUNT);\n+\n+        // Start debug segment container using dataLogFactory from new BK instance and old long term storageFactory.\n+        DebugTool debugTool = createDebugTool(this.dataLogFactory, this.storageFactory);\n+        DebugStreamSegmentContainer debugStreamSegmentContainer = (DebugStreamSegmentContainer)\n+                debugTool.containerFactory.createDebugStreamSegmentContainer(CONTAINER_ID);\n+\n+        // Re-create all segments which were listed.\n+        Services.startAsync(debugStreamSegmentContainer, executorService)\n+                .thenRun(new DataRecoveryTestUtils.Worker(debugStreamSegmentContainer, segmentsToCreate.get(CONTAINER_ID))).join();\n+        sleep(5000); // Without sleep the test fails sometimes complaining some segment offsets don't exist.\n+        Services.stopAsync(debugStreamSegmentContainer, executorService).join();\n+        debugStreamSegmentContainer.close();\n+        debugTool.close();\n+\n+        // Start a new segment store and controller\n+        this.segmentStoreStarter = startSegmentStore(this.storageFactory, this.dataLogFactory);\n+        controllerStarter = startController(this.bkzk.bkPort, this.segmentStoreStarter.servicePort);\n+\n+        connectionFactory = new ConnectionFactoryImpl(ClientConfig.builder().build());\n+        clientFactory = new ClientFactoryImpl(SCOPE, controllerStarter.controller, connectionFactory);\n+        readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerStarter.controller, clientFactory, connectionFactory);\n+\n+        // Try creating the same segments again with the new controller\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM1);\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM2);\n+\n+        // Try reading all events again\n+        readAllEvents(STREAM1, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n+                \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n+        readAllEvents(STREAM2, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n+                \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n+    }\n+\n+    public void createScopeStream(Controller controller, String scopeName, String streamName) {\n+        try (ConnectionFactory cf = new ConnectionFactoryImpl(ClientConfig.builder().build());\n+             StreamManager streamManager = new StreamManagerImpl(controller, cf)) {\n+            streamManager.createScope(scopeName);\n+            streamManager.createStream(scopeName, streamName, config);\n+        }\n+    }\n+\n+    private void writeEvents(String streamName, ClientFactoryImpl clientFactory) {\n+        EventStreamWriter<String> writer = clientFactory.createEventWriter(streamName,\n+                new UTF8StringSerializer(),\n+                EventWriterConfig.builder().build());\n+        for (int i = 0; i < TOTAL_NUM_EVENTS;) {\n+            try {\n+                writer.writeEvent(\"\", EVENT).join();\n+                i++;\n+            } catch (Throwable e) {\n+                Assert.fail(\"Error occurred while writing events.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 557}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2OTEzMA==", "bodyText": "Ok", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459769130", "createdAt": "2020-07-23T22:45:45Z", "author": {"login": "ManishKumarKeshri"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig\n+                .builder()\n+                .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+                .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+                .build();\n+        private final WriterConfig writerConfig = WriterConfig\n+                .builder()\n+                .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+                .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+                .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+                .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+                .build();\n+\n+        DebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+            this.dataLogFactory = dataLogFactory;\n+            this.storageFactory = storageFactory;\n+            this.operationLogFactory = new DurableLogFactory(durableLogConfig, this.dataLogFactory, executorService);\n+\n+            this.cacheStorage = new DirectMemoryCache(Integer.MAX_VALUE);\n+            this.cacheManager = new CacheManager(CachePolicy.INFINITE, this.cacheStorage, executorService);\n+            this.readIndexFactory = new ContainerReadIndexFactory(readIndexConfig, this.cacheManager, executorService);\n+            this.attributeIndexFactory = new ContainerAttributeIndexFactoryImpl(attributeIndexConfig, this.cacheManager, executorService);\n+            this.writerFactory = new StorageWriterFactory(writerConfig, executorService);\n+\n+            ContainerConfig containerConfig = ServiceBuilderConfig.getDefaultConfig().getConfig(ContainerConfig::builder);\n+            this.containerFactory = new StreamSegmentContainerFactory(containerConfig, this.operationLogFactory,\n+                    this.readIndexFactory, this.attributeIndexFactory, this.writerFactory, this.storageFactory,\n+                    this::createContainerExtensions, executorService);\n+        }\n+\n+        private Map<Class<? extends SegmentContainerExtension>, SegmentContainerExtension> createContainerExtensions(\n+                SegmentContainer container, ScheduledExecutorService executor) {\n+            return Collections.singletonMap(ContainerTableExtension.class, new ContainerTableExtensionImpl(container, this.cacheManager, executor));\n+        }\n+\n+        @Override\n+        public void close() {\n+            this.readIndexFactory.close();\n+            this.cacheManager.close();\n+            this.cacheStorage.close();\n+            this.dataLogFactory.close();\n+        }\n+    }\n+\n+    SegmentStoreStarter startSegmentStore(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+        return new SegmentStoreStarter(storageFactory, dataLogFactory);\n+    }\n+\n+    /**\n+     * Creates a segment store server.\n+     */\n+    private static class SegmentStoreStarter {\n+        private final int servicePort = TestUtils.getAvailableListenPort();\n+        private ServiceBuilder serviceBuilder;\n+        private StreamSegmentStoreWrapper streamSegmentStoreWrapper;\n+        private AutoScaleMonitor monitor;\n+        private TableStoreWrapper tableStoreWrapper;\n+        private PravegaConnectionListener server;\n+\n+        SegmentStoreStarter(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+            if (storageFactory != null) {\n+                if (dataLogFactory != null) {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory)\n+                            .withDataLogFactory(setup -> dataLogFactory);\n+                } else {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory);\n+                }\n+            } else {\n+                this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n+            }\n+            this.serviceBuilder.initialize();\n+            this.streamSegmentStoreWrapper = new StreamSegmentStoreWrapper(serviceBuilder.createStreamSegmentService());\n+            this.monitor = new AutoScaleMonitor(streamSegmentStoreWrapper, AutoScalerConfig.builder().build());\n+            this.tableStoreWrapper = new TableStoreWrapper(serviceBuilder.createTableStoreService());\n+            this.server = new PravegaConnectionListener(false, false, \"localhost\", servicePort, streamSegmentStoreWrapper,\n+                    tableStoreWrapper, monitor.getStatsRecorder(), monitor.getTableSegmentStatsRecorder(), new PassingTokenVerifier(),\n+                    null, null, true, serviceBuilder.getLowPriorityExecutor());\n+            this.server.startListening();\n+        }\n+\n+        public void close() {\n+            if (this.server != null) {\n+                this.server.close();\n+                this.server = null;\n+            }\n+\n+            if (this.monitor != null) {\n+                this.monitor.close();\n+                this.monitor = null;\n+            }\n+\n+            if (this.serviceBuilder != null) {\n+                this.serviceBuilder.close();\n+                this.serviceBuilder = null;\n+            }\n+        }\n+    }\n+\n+    ControllerStarter startController(int bkPort, int servicePort) throws InterruptedException {\n+        return new ControllerStarter(bkPort, servicePort);\n+    }\n+\n+    /**\n+     * Creates a controller instance and runs it.\n+     */\n+    private static class ControllerStarter {\n+        private final int controllerPort = TestUtils.getAvailableListenPort();\n+        private final String serviceHost = \"localhost\";\n+        private ControllerWrapper controllerWrapper = null;\n+        private Controller controller = null;\n+\n+        ControllerStarter(int bkPort, int servicePort) throws InterruptedException {\n+            this.controllerWrapper = new ControllerWrapper(\"localhost:\" + bkPort, false,\n+                    controllerPort, serviceHost, servicePort, CONTAINER_COUNT);\n+            this.controllerWrapper.awaitRunning();\n+            this.controller = controllerWrapper.getController();\n+        }\n+\n+        public void close() throws Exception {\n+            if (this.controller != null) {\n+                this.controller.close();\n+                this.controller = null;\n+            }\n+\n+            if (this.controllerWrapper != null) {\n+                this.controllerWrapper.close();\n+                this.controllerWrapper = null;\n+            }\n+        }\n+    }\n+\n+    @Test(timeout = 240000)\n+    public void testDurableDataLogFail() throws Exception {\n+        int instanceId = 0;\n+\n+        // Creating tier 2 only once here.\n+        this.baseDir = Files.createTempDirectory(\"test_nfs\").toFile().getAbsoluteFile();\n+        FileSystemStorageConfig fsConfig = FileSystemStorageConfig\n+                .builder()\n+                .with(FileSystemStorageConfig.ROOT, this.baseDir.getAbsolutePath())\n+                .build();\n+        this.storageFactory = new FileSystemStorageFactory(fsConfig, executorService);\n+\n+        // Start a new BK & ZK, segment store and controller\n+        this.bkzk = setUpNewBK(instanceId++);\n+        this.segmentStoreStarter = startSegmentStore(this.storageFactory, null);\n+        @Cleanup ControllerStarter controllerStarter = startController(this.bkzk.bkPort, this.segmentStoreStarter.servicePort);\n+\n+        // Create two streams for writing data onto two different segments\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM1);\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM2);\n+\n+        @Cleanup ConnectionFactory connectionFactory = new ConnectionFactoryImpl(ClientConfig.builder().build());\n+        @Cleanup ClientFactoryImpl clientFactory = new ClientFactoryImpl(SCOPE, controllerStarter.controller, connectionFactory);\n+        @Cleanup ReaderGroupManager readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerStarter.controller, clientFactory, connectionFactory);\n+\n+        writeEvents(STREAM1, clientFactory); // write 300 events on one segment\n+        writeEvents(STREAM2, clientFactory); // write 300 events on other segment\n+\n+        // Verify events write by reading them.\n+        readAllEvents(STREAM1, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n+                \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n+        readAllEvents(STREAM2, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n+                \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n+\n+        readerGroupManager.close();\n+        clientFactory.close();\n+\n+        controllerStarter.close(); // Shut down the controller\n+\n+        // Get names of all the segments created.\n+        HashSet<String> allSegments = new HashSet<>(this.segmentStoreStarter.streamSegmentStoreWrapper.getSegments());\n+        allSegments.addAll(this.segmentStoreStarter.tableStoreWrapper.getSegments());\n+        log.info(\"No. of segments created = {}\", allSegments.size());\n+\n+        // Get the long term storage from the running pravega instance\n+        @Cleanup Storage tier2 = new AsyncStorageWrapper(new RollingStorage(this.storageFactory.createSyncStorage(),\n+                new SegmentRollingPolicy(DEFAULT_ROLLING_SIZE)), DataRecoveryTestUtils.createExecutorService(1));\n+\n+        // wait for all segments to be flushed to the long term storage.\n+        waitForSegmentsInStorage(allSegments, this.segmentStoreStarter.streamSegmentStoreWrapper, tier2)\n+                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        this.segmentStoreStarter.close(); // Shutdown SegmentStore\n+        this.segmentStoreStarter = null;\n+        log.info(\"Segment Store Shutdown\");\n+\n+        this.bkzk.close(); // Shutdown BookKeeper & ZooKeeper\n+        this.bkzk = null;\n+        log.info(\"BookKeeper & ZooKeeper shutdown\");\n+\n+        // start a new BookKeeper and ZooKeeper.\n+        this.bkzk = setUpNewBK(instanceId++);\n+        this.dataLogFactory = new BookKeeperLogFactory(this.bkzk.bkConfig.get(), this.bkzk.zkClient.get(),\n+                DataRecoveryTestUtils.createExecutorService(1));\n+        this.dataLogFactory.initialize();\n+\n+        // Delete container metadata segment and attributes index segment corresponding to the container Id from the long term storage\n+        DataRecoveryTestUtils.deleteContainerMetadataSegments(tier2, CONTAINER_ID);\n+\n+        // List all segments from the long term storage\n+        Map<Integer, List<SegmentProperties>> segmentsToCreate = DataRecoveryTestUtils.listAllSegments(tier2, CONTAINER_COUNT);\n+\n+        // Start debug segment container using dataLogFactory from new BK instance and old long term storageFactory.\n+        DebugTool debugTool = createDebugTool(this.dataLogFactory, this.storageFactory);\n+        DebugStreamSegmentContainer debugStreamSegmentContainer = (DebugStreamSegmentContainer)\n+                debugTool.containerFactory.createDebugStreamSegmentContainer(CONTAINER_ID);\n+\n+        // Re-create all segments which were listed.\n+        Services.startAsync(debugStreamSegmentContainer, executorService)\n+                .thenRun(new DataRecoveryTestUtils.Worker(debugStreamSegmentContainer, segmentsToCreate.get(CONTAINER_ID))).join();\n+        sleep(5000); // Without sleep the test fails sometimes complaining some segment offsets don't exist.\n+        Services.stopAsync(debugStreamSegmentContainer, executorService).join();\n+        debugStreamSegmentContainer.close();\n+        debugTool.close();\n+\n+        // Start a new segment store and controller\n+        this.segmentStoreStarter = startSegmentStore(this.storageFactory, this.dataLogFactory);\n+        controllerStarter = startController(this.bkzk.bkPort, this.segmentStoreStarter.servicePort);\n+\n+        connectionFactory = new ConnectionFactoryImpl(ClientConfig.builder().build());\n+        clientFactory = new ClientFactoryImpl(SCOPE, controllerStarter.controller, connectionFactory);\n+        readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerStarter.controller, clientFactory, connectionFactory);\n+\n+        // Try creating the same segments again with the new controller\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM1);\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM2);\n+\n+        // Try reading all events again\n+        readAllEvents(STREAM1, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n+                \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n+        readAllEvents(STREAM2, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n+                \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n+    }\n+\n+    public void createScopeStream(Controller controller, String scopeName, String streamName) {\n+        try (ConnectionFactory cf = new ConnectionFactoryImpl(ClientConfig.builder().build());\n+             StreamManager streamManager = new StreamManagerImpl(controller, cf)) {\n+            streamManager.createScope(scopeName);\n+            streamManager.createStream(scopeName, streamName, config);\n+        }\n+    }\n+\n+    private void writeEvents(String streamName, ClientFactoryImpl clientFactory) {\n+        EventStreamWriter<String> writer = clientFactory.createEventWriter(streamName,\n+                new UTF8StringSerializer(),\n+                EventWriterConfig.builder().build());\n+        for (int i = 0; i < TOTAL_NUM_EVENTS;) {\n+            try {\n+                writer.writeEvent(\"\", EVENT).join();\n+                i++;\n+            } catch (Throwable e) {\n+                Assert.fail(\"Error occurred while writing events.\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMzM3Mw=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 557}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg0ODMzNzgxOnYy", "diffSide": "RIGHT", "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxODo0MjoyMVrOGzdeOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjo0NTo1MVrOG2eFUQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMzQzMw==", "bodyText": "here too", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r456613433", "createdAt": "2020-07-17T18:42:21Z", "author": {"login": "andreipaduroiu"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig\n+                .builder()\n+                .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+                .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+                .build();\n+        private final WriterConfig writerConfig = WriterConfig\n+                .builder()\n+                .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+                .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+                .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+                .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+                .build();\n+\n+        DebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+            this.dataLogFactory = dataLogFactory;\n+            this.storageFactory = storageFactory;\n+            this.operationLogFactory = new DurableLogFactory(durableLogConfig, this.dataLogFactory, executorService);\n+\n+            this.cacheStorage = new DirectMemoryCache(Integer.MAX_VALUE);\n+            this.cacheManager = new CacheManager(CachePolicy.INFINITE, this.cacheStorage, executorService);\n+            this.readIndexFactory = new ContainerReadIndexFactory(readIndexConfig, this.cacheManager, executorService);\n+            this.attributeIndexFactory = new ContainerAttributeIndexFactoryImpl(attributeIndexConfig, this.cacheManager, executorService);\n+            this.writerFactory = new StorageWriterFactory(writerConfig, executorService);\n+\n+            ContainerConfig containerConfig = ServiceBuilderConfig.getDefaultConfig().getConfig(ContainerConfig::builder);\n+            this.containerFactory = new StreamSegmentContainerFactory(containerConfig, this.operationLogFactory,\n+                    this.readIndexFactory, this.attributeIndexFactory, this.writerFactory, this.storageFactory,\n+                    this::createContainerExtensions, executorService);\n+        }\n+\n+        private Map<Class<? extends SegmentContainerExtension>, SegmentContainerExtension> createContainerExtensions(\n+                SegmentContainer container, ScheduledExecutorService executor) {\n+            return Collections.singletonMap(ContainerTableExtension.class, new ContainerTableExtensionImpl(container, this.cacheManager, executor));\n+        }\n+\n+        @Override\n+        public void close() {\n+            this.readIndexFactory.close();\n+            this.cacheManager.close();\n+            this.cacheStorage.close();\n+            this.dataLogFactory.close();\n+        }\n+    }\n+\n+    SegmentStoreStarter startSegmentStore(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+        return new SegmentStoreStarter(storageFactory, dataLogFactory);\n+    }\n+\n+    /**\n+     * Creates a segment store server.\n+     */\n+    private static class SegmentStoreStarter {\n+        private final int servicePort = TestUtils.getAvailableListenPort();\n+        private ServiceBuilder serviceBuilder;\n+        private StreamSegmentStoreWrapper streamSegmentStoreWrapper;\n+        private AutoScaleMonitor monitor;\n+        private TableStoreWrapper tableStoreWrapper;\n+        private PravegaConnectionListener server;\n+\n+        SegmentStoreStarter(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+            if (storageFactory != null) {\n+                if (dataLogFactory != null) {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory)\n+                            .withDataLogFactory(setup -> dataLogFactory);\n+                } else {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory);\n+                }\n+            } else {\n+                this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n+            }\n+            this.serviceBuilder.initialize();\n+            this.streamSegmentStoreWrapper = new StreamSegmentStoreWrapper(serviceBuilder.createStreamSegmentService());\n+            this.monitor = new AutoScaleMonitor(streamSegmentStoreWrapper, AutoScalerConfig.builder().build());\n+            this.tableStoreWrapper = new TableStoreWrapper(serviceBuilder.createTableStoreService());\n+            this.server = new PravegaConnectionListener(false, false, \"localhost\", servicePort, streamSegmentStoreWrapper,\n+                    tableStoreWrapper, monitor.getStatsRecorder(), monitor.getTableSegmentStatsRecorder(), new PassingTokenVerifier(),\n+                    null, null, true, serviceBuilder.getLowPriorityExecutor());\n+            this.server.startListening();\n+        }\n+\n+        public void close() {\n+            if (this.server != null) {\n+                this.server.close();\n+                this.server = null;\n+            }\n+\n+            if (this.monitor != null) {\n+                this.monitor.close();\n+                this.monitor = null;\n+            }\n+\n+            if (this.serviceBuilder != null) {\n+                this.serviceBuilder.close();\n+                this.serviceBuilder = null;\n+            }\n+        }\n+    }\n+\n+    ControllerStarter startController(int bkPort, int servicePort) throws InterruptedException {\n+        return new ControllerStarter(bkPort, servicePort);\n+    }\n+\n+    /**\n+     * Creates a controller instance and runs it.\n+     */\n+    private static class ControllerStarter {\n+        private final int controllerPort = TestUtils.getAvailableListenPort();\n+        private final String serviceHost = \"localhost\";\n+        private ControllerWrapper controllerWrapper = null;\n+        private Controller controller = null;\n+\n+        ControllerStarter(int bkPort, int servicePort) throws InterruptedException {\n+            this.controllerWrapper = new ControllerWrapper(\"localhost:\" + bkPort, false,\n+                    controllerPort, serviceHost, servicePort, CONTAINER_COUNT);\n+            this.controllerWrapper.awaitRunning();\n+            this.controller = controllerWrapper.getController();\n+        }\n+\n+        public void close() throws Exception {\n+            if (this.controller != null) {\n+                this.controller.close();\n+                this.controller = null;\n+            }\n+\n+            if (this.controllerWrapper != null) {\n+                this.controllerWrapper.close();\n+                this.controllerWrapper = null;\n+            }\n+        }\n+    }\n+\n+    @Test(timeout = 240000)\n+    public void testDurableDataLogFail() throws Exception {\n+        int instanceId = 0;\n+\n+        // Creating tier 2 only once here.\n+        this.baseDir = Files.createTempDirectory(\"test_nfs\").toFile().getAbsoluteFile();\n+        FileSystemStorageConfig fsConfig = FileSystemStorageConfig\n+                .builder()\n+                .with(FileSystemStorageConfig.ROOT, this.baseDir.getAbsolutePath())\n+                .build();\n+        this.storageFactory = new FileSystemStorageFactory(fsConfig, executorService);\n+\n+        // Start a new BK & ZK, segment store and controller\n+        this.bkzk = setUpNewBK(instanceId++);\n+        this.segmentStoreStarter = startSegmentStore(this.storageFactory, null);\n+        @Cleanup ControllerStarter controllerStarter = startController(this.bkzk.bkPort, this.segmentStoreStarter.servicePort);\n+\n+        // Create two streams for writing data onto two different segments\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM1);\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM2);\n+\n+        @Cleanup ConnectionFactory connectionFactory = new ConnectionFactoryImpl(ClientConfig.builder().build());\n+        @Cleanup ClientFactoryImpl clientFactory = new ClientFactoryImpl(SCOPE, controllerStarter.controller, connectionFactory);\n+        @Cleanup ReaderGroupManager readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerStarter.controller, clientFactory, connectionFactory);\n+\n+        writeEvents(STREAM1, clientFactory); // write 300 events on one segment\n+        writeEvents(STREAM2, clientFactory); // write 300 events on other segment\n+\n+        // Verify events write by reading them.\n+        readAllEvents(STREAM1, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n+                \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n+        readAllEvents(STREAM2, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n+                \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n+\n+        readerGroupManager.close();\n+        clientFactory.close();\n+\n+        controllerStarter.close(); // Shut down the controller\n+\n+        // Get names of all the segments created.\n+        HashSet<String> allSegments = new HashSet<>(this.segmentStoreStarter.streamSegmentStoreWrapper.getSegments());\n+        allSegments.addAll(this.segmentStoreStarter.tableStoreWrapper.getSegments());\n+        log.info(\"No. of segments created = {}\", allSegments.size());\n+\n+        // Get the long term storage from the running pravega instance\n+        @Cleanup Storage tier2 = new AsyncStorageWrapper(new RollingStorage(this.storageFactory.createSyncStorage(),\n+                new SegmentRollingPolicy(DEFAULT_ROLLING_SIZE)), DataRecoveryTestUtils.createExecutorService(1));\n+\n+        // wait for all segments to be flushed to the long term storage.\n+        waitForSegmentsInStorage(allSegments, this.segmentStoreStarter.streamSegmentStoreWrapper, tier2)\n+                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        this.segmentStoreStarter.close(); // Shutdown SegmentStore\n+        this.segmentStoreStarter = null;\n+        log.info(\"Segment Store Shutdown\");\n+\n+        this.bkzk.close(); // Shutdown BookKeeper & ZooKeeper\n+        this.bkzk = null;\n+        log.info(\"BookKeeper & ZooKeeper shutdown\");\n+\n+        // start a new BookKeeper and ZooKeeper.\n+        this.bkzk = setUpNewBK(instanceId++);\n+        this.dataLogFactory = new BookKeeperLogFactory(this.bkzk.bkConfig.get(), this.bkzk.zkClient.get(),\n+                DataRecoveryTestUtils.createExecutorService(1));\n+        this.dataLogFactory.initialize();\n+\n+        // Delete container metadata segment and attributes index segment corresponding to the container Id from the long term storage\n+        DataRecoveryTestUtils.deleteContainerMetadataSegments(tier2, CONTAINER_ID);\n+\n+        // List all segments from the long term storage\n+        Map<Integer, List<SegmentProperties>> segmentsToCreate = DataRecoveryTestUtils.listAllSegments(tier2, CONTAINER_COUNT);\n+\n+        // Start debug segment container using dataLogFactory from new BK instance and old long term storageFactory.\n+        DebugTool debugTool = createDebugTool(this.dataLogFactory, this.storageFactory);\n+        DebugStreamSegmentContainer debugStreamSegmentContainer = (DebugStreamSegmentContainer)\n+                debugTool.containerFactory.createDebugStreamSegmentContainer(CONTAINER_ID);\n+\n+        // Re-create all segments which were listed.\n+        Services.startAsync(debugStreamSegmentContainer, executorService)\n+                .thenRun(new DataRecoveryTestUtils.Worker(debugStreamSegmentContainer, segmentsToCreate.get(CONTAINER_ID))).join();\n+        sleep(5000); // Without sleep the test fails sometimes complaining some segment offsets don't exist.\n+        Services.stopAsync(debugStreamSegmentContainer, executorService).join();\n+        debugStreamSegmentContainer.close();\n+        debugTool.close();\n+\n+        // Start a new segment store and controller\n+        this.segmentStoreStarter = startSegmentStore(this.storageFactory, this.dataLogFactory);\n+        controllerStarter = startController(this.bkzk.bkPort, this.segmentStoreStarter.servicePort);\n+\n+        connectionFactory = new ConnectionFactoryImpl(ClientConfig.builder().build());\n+        clientFactory = new ClientFactoryImpl(SCOPE, controllerStarter.controller, connectionFactory);\n+        readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerStarter.controller, clientFactory, connectionFactory);\n+\n+        // Try creating the same segments again with the new controller\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM1);\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM2);\n+\n+        // Try reading all events again\n+        readAllEvents(STREAM1, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n+                \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n+        readAllEvents(STREAM2, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n+                \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n+    }\n+\n+    public void createScopeStream(Controller controller, String scopeName, String streamName) {\n+        try (ConnectionFactory cf = new ConnectionFactoryImpl(ClientConfig.builder().build());\n+             StreamManager streamManager = new StreamManagerImpl(controller, cf)) {\n+            streamManager.createScope(scopeName);\n+            streamManager.createStream(scopeName, streamName, config);\n+        }\n+    }\n+\n+    private void writeEvents(String streamName, ClientFactoryImpl clientFactory) {\n+        EventStreamWriter<String> writer = clientFactory.createEventWriter(streamName,\n+                new UTF8StringSerializer(),\n+                EventWriterConfig.builder().build());\n+        for (int i = 0; i < TOTAL_NUM_EVENTS;) {\n+            try {\n+                writer.writeEvent(\"\", EVENT).join();\n+                i++;\n+            } catch (Throwable e) {\n+                Assert.fail(\"Error occurred while writing events.\");\n+                break;\n+            }\n+        }\n+        writer.flush();\n+        writer.close();\n+    }\n+\n+    private void readAllEvents(String streamName, ClientFactoryImpl clientFactory, ReaderGroupManager readerGroupManager,\n+                               String readerGroupName, String readerName) {\n+        readerGroupManager.createReaderGroup(readerGroupName,\n+                ReaderGroupConfig\n+                        .builder()\n+                        .stream(Stream.of(SCOPE, streamName))\n+                        .automaticCheckpointIntervalMillis(2000)\n+                        .build());\n+\n+        EventStreamReader<String> reader = clientFactory.createReader(readerName,\n+                readerGroupName,\n+                new UTF8StringSerializer(),\n+                ReaderConfig.builder().build());\n+\n+        for (int q = 0; q < TOTAL_NUM_EVENTS;) {\n+            try {\n+                String eventRead = reader.readNextEvent(SECONDS.toMillis(500)).getEvent();\n+                Assert.assertEquals(\"Event written and read back don't match\", EVENT, eventRead);\n+                q++;\n+            } catch (Exception e) {\n+                Assert.fail(\"Error occurred while reading the events.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 585}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2OTE2OQ==", "bodyText": "Ok", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r459769169", "createdAt": "2020-07-23T22:45:51Z", "author": {"login": "ManishKumarKeshri"}, "path": "test/integration/src/test/java/io/pravega/test/integration/RestoreBackUpDataRecoveryTest.java", "diffHunk": "@@ -0,0 +1,648 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.test.integration;\n+\n+import io.pravega.client.ClientConfig;\n+import io.pravega.client.admin.ReaderGroupManager;\n+import io.pravega.client.admin.StreamManager;\n+import io.pravega.client.admin.impl.ReaderGroupManagerImpl;\n+import io.pravega.client.admin.impl.StreamManagerImpl;\n+import io.pravega.client.control.impl.Controller;\n+import io.pravega.client.netty.impl.ConnectionFactory;\n+import io.pravega.client.netty.impl.ConnectionFactoryImpl;\n+import io.pravega.client.stream.EventStreamReader;\n+import io.pravega.client.stream.EventStreamWriter;\n+import io.pravega.client.stream.EventWriterConfig;\n+import io.pravega.client.stream.ReaderConfig;\n+import io.pravega.client.stream.ReaderGroupConfig;\n+import io.pravega.client.stream.ScalingPolicy;\n+import io.pravega.client.stream.Stream;\n+import io.pravega.client.stream.StreamConfiguration;\n+import io.pravega.client.stream.impl.ClientFactoryImpl;\n+import io.pravega.client.stream.impl.UTF8StringSerializer;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.common.io.FileHelpers;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.StreamSegmentStoreWrapper;\n+import io.pravega.segmentstore.contracts.tables.TableStoreWrapper;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.DataRecoveryTestUtils;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerExtension;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.containers.ContainerConfig;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.containers.StreamSegmentContainerFactory;\n+import io.pravega.segmentstore.server.host.delegationtoken.PassingTokenVerifier;\n+import io.pravega.segmentstore.server.host.handler.PravegaConnectionListener;\n+import io.pravega.segmentstore.server.host.stat.AutoScaleMonitor;\n+import io.pravega.segmentstore.server.host.stat.AutoScalerConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.store.ServiceBuilder;\n+import io.pravega.segmentstore.server.store.ServiceBuilderConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogException;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperConfig;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperLogFactory;\n+import io.pravega.segmentstore.storage.impl.bookkeeper.BookKeeperServiceRunner;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.storage.filesystem.FileSystemStorageConfig;\n+import io.pravega.storage.filesystem.FileSystemStorageFactory;\n+import io.pravega.test.common.TestUtils;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import io.pravega.test.integration.demo.ControllerWrapper;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.apache.curator.framework.CuratorFramework;\n+import org.apache.curator.framework.CuratorFrameworkFactory;\n+import org.apache.curator.retry.ExponentialBackoffRetry;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+import java.io.File;\n+import java.nio.file.Files;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Collections;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.concurrent.TimeoutException;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicReference;\n+\n+import static java.lang.Thread.sleep;\n+import static java.util.concurrent.TimeUnit.SECONDS;\n+\n+/**\n+ * Integration test to verify data recovery.\n+ * Recovery scenario: when data written to Pravega is already flushed to the long term storage.\n+ */\n+@Slf4j\n+public class RestoreBackUpDataRecoveryTest extends ThreadPooledTestSuite {\n+    protected static final Duration TIMEOUT = Duration.ofMillis(60000 * 1000);\n+\n+    private static final int CONTAINER_COUNT = 1;\n+    private static final int CONTAINER_ID = 0;\n+\n+    /**\n+     * Write 300 events to different segments.\n+     */\n+    private static final long TOTAL_NUM_EVENTS = 300;\n+\n+    private static final String APPEND_FORMAT = \"Segment_%s_Append_%d\";\n+    private static final long DEFAULT_ROLLING_SIZE = (int) (APPEND_FORMAT.length() * 1.5);\n+\n+    private static final Random RANDOM = new Random();\n+\n+    /**\n+     * Scope and streams to read and write events.\n+     */\n+    private static final String SCOPE = \"testMetricsScope\";\n+    private static final String STREAM1 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String STREAM2 = \"testMetricsStream\" + RANDOM.nextInt(Integer.MAX_VALUE);\n+    private static final String EVENT = \"12345\";\n+\n+    private final ScalingPolicy scalingPolicy = ScalingPolicy.fixed(1);\n+    private final StreamConfiguration config = StreamConfiguration.builder().scalingPolicy(scalingPolicy).build();\n+\n+    private ScheduledExecutorService executorService = DataRecoveryTestUtils.createExecutorService(100);\n+    private File baseDir;\n+    private FileSystemStorageFactory storageFactory;\n+    private BookKeeperLogFactory dataLogFactory;\n+    private SegmentStoreStarter segmentStoreStarter;\n+    private BKZK bkzk = null;\n+\n+    @After\n+    public void tearDown() throws Exception {\n+        if (this.dataLogFactory != null) {\n+            this.dataLogFactory.close();\n+            this.dataLogFactory = null;\n+        }\n+\n+        if (this.segmentStoreStarter != null) {\n+            this.segmentStoreStarter.close();\n+            this.segmentStoreStarter = null;\n+        }\n+\n+        if (this.bkzk != null) {\n+            this.bkzk.close();\n+            this.bkzk = null;\n+        }\n+\n+        if (this.baseDir != null) {\n+            FileHelpers.deleteFileOrDirectory(this.baseDir);\n+            this.baseDir = null;\n+        }\n+        executorService.shutdown();\n+    }\n+\n+    @Override\n+    protected int getThreadPoolSize() {\n+        return 100;\n+    }\n+\n+    BKZK setUpNewBK(int instanceId) throws Exception {\n+        return new BKZK(instanceId);\n+    }\n+\n+    /**\n+     * Sets up a new BookKeeper & ZooKeeper.\n+     */\n+    private static class BKZK implements AutoCloseable {\n+        private final int writeCount = 500;\n+        private final int maxWriteAttempts = 3;\n+        private final int maxLedgerSize = 200 * Math.max(10, writeCount / 20);\n+        private final AtomicBoolean secureBk = new AtomicBoolean();\n+        private final int bookieCount = 1;\n+        private AtomicReference<BookKeeperConfig> bkConfig = new AtomicReference<>();\n+        private AtomicReference<CuratorFramework> zkClient = new AtomicReference<>();\n+        private BookKeeperServiceRunner bookKeeperServiceRunner;\n+        private AtomicReference<BookKeeperServiceRunner> bkService = new AtomicReference<>();\n+        private int bkPort;\n+\n+        BKZK(int instanceId) throws Exception {\n+            secureBk.set(false);\n+            bkPort = TestUtils.getAvailableListenPort();\n+            val bookiePorts = new ArrayList<Integer>();\n+            for (int i = 0; i < bookieCount; i++) {\n+                bookiePorts.add(TestUtils.getAvailableListenPort());\n+            }\n+\n+            this.bookKeeperServiceRunner = BookKeeperServiceRunner.builder()\n+                    .startZk(true)\n+                    .zkPort(bkPort)\n+                    .ledgersPath(\"/pravega/bookkeeper/ledgers\")\n+                    .secureBK(isSecure())\n+                    .secureZK(isSecure())\n+                    .tlsTrustStore(\"../segmentstore/config/bookie.truststore.jks\")\n+                    .tLSKeyStore(\"../segmentstore/config/bookie.keystore.jks\")\n+                    .tLSKeyStorePasswordPath(\"../segmentstore/config/bookie.keystore.jks.passwd\")\n+                    .bookiePorts(bookiePorts)\n+                    .build();\n+            this.bookKeeperServiceRunner.startAll();\n+            bkService.set(this.bookKeeperServiceRunner);\n+\n+            // Create a ZKClient with a unique namespace.\n+            String baseNamespace = \"pravega/\" + instanceId + \"_\" + Long.toHexString(System.nanoTime());\n+            this.zkClient.set(CuratorFrameworkFactory\n+                    .builder()\n+                    .connectString(\"localhost:\" + bkPort)\n+                    .namespace(baseNamespace)\n+                    .retryPolicy(new ExponentialBackoffRetry(1000, 5))\n+                    .connectionTimeoutMs(10000)\n+                    .sessionTimeoutMs(10000)\n+                    .build());\n+\n+            this.zkClient.get().start();\n+\n+            String logMetaNamespace = \"segmentstore/containers\" + instanceId;\n+            this.bkConfig.set(BookKeeperConfig\n+                    .builder()\n+                    .with(BookKeeperConfig.ZK_ADDRESS, \"localhost:\" + bkPort)\n+                    .with(BookKeeperConfig.MAX_WRITE_ATTEMPTS, maxWriteAttempts)\n+                    .with(BookKeeperConfig.BK_LEDGER_MAX_SIZE, maxLedgerSize)\n+                    .with(BookKeeperConfig.ZK_METADATA_PATH, logMetaNamespace)\n+                    .with(BookKeeperConfig.BK_LEDGER_PATH, \"/pravega/bookkeeper/ledgers\")\n+                    .with(BookKeeperConfig.BK_ENSEMBLE_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_WRITE_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_ACK_QUORUM_SIZE, bookieCount)\n+                    .with(BookKeeperConfig.BK_TLS_ENABLED, isSecure())\n+                    .with(BookKeeperConfig.BK_WRITE_TIMEOUT, 1000)\n+                    .build());\n+        }\n+\n+        public boolean isSecure() {\n+            return secureBk.get();\n+        }\n+\n+        public void close() throws Exception {\n+            val process = this.bkService.getAndSet(null);\n+            if (process != null) {\n+                process.close();\n+            }\n+\n+            val bk = this.bookKeeperServiceRunner;\n+            if (bk != null) {\n+                bk.close();\n+                this.bookKeeperServiceRunner = null;\n+            }\n+\n+            val zkClient = this.zkClient.getAndSet(null);\n+            if (zkClient != null) {\n+                zkClient.close();\n+            }\n+        }\n+    }\n+\n+    DebugTool createDebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+        return new DebugTool(dataLogFactory, storageFactory);\n+    }\n+\n+    /**\n+     * Sets up the environment for creating a DebugSegmentContainer.\n+     */\n+    private class DebugTool implements AutoCloseable {\n+        private final CacheStorage cacheStorage;\n+        private final OperationLogFactory operationLogFactory;\n+        private final ReadIndexFactory readIndexFactory;\n+        private final AttributeIndexFactory attributeIndexFactory;\n+        private final WriterFactory writerFactory;\n+        private final CacheManager cacheManager;\n+        private final StreamSegmentContainerFactory containerFactory;\n+        private final BookKeeperLogFactory dataLogFactory;\n+        private final StorageFactory storageFactory;\n+\n+        private final DurableLogConfig durableLogConfig = DurableLogConfig\n+                .builder()\n+                .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+                .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+                .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10L * 1024 * 1024L)\n+                .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+                .build();\n+\n+        private final ReadIndexConfig readIndexConfig = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+        private final AttributeIndexConfig attributeIndexConfig = AttributeIndexConfig\n+                .builder()\n+                .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+                .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+                .build();\n+        private final WriterConfig writerConfig = WriterConfig\n+                .builder()\n+                .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+                .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+                .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+                .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+                .build();\n+\n+        DebugTool(BookKeeperLogFactory dataLogFactory, StorageFactory storageFactory) {\n+            this.dataLogFactory = dataLogFactory;\n+            this.storageFactory = storageFactory;\n+            this.operationLogFactory = new DurableLogFactory(durableLogConfig, this.dataLogFactory, executorService);\n+\n+            this.cacheStorage = new DirectMemoryCache(Integer.MAX_VALUE);\n+            this.cacheManager = new CacheManager(CachePolicy.INFINITE, this.cacheStorage, executorService);\n+            this.readIndexFactory = new ContainerReadIndexFactory(readIndexConfig, this.cacheManager, executorService);\n+            this.attributeIndexFactory = new ContainerAttributeIndexFactoryImpl(attributeIndexConfig, this.cacheManager, executorService);\n+            this.writerFactory = new StorageWriterFactory(writerConfig, executorService);\n+\n+            ContainerConfig containerConfig = ServiceBuilderConfig.getDefaultConfig().getConfig(ContainerConfig::builder);\n+            this.containerFactory = new StreamSegmentContainerFactory(containerConfig, this.operationLogFactory,\n+                    this.readIndexFactory, this.attributeIndexFactory, this.writerFactory, this.storageFactory,\n+                    this::createContainerExtensions, executorService);\n+        }\n+\n+        private Map<Class<? extends SegmentContainerExtension>, SegmentContainerExtension> createContainerExtensions(\n+                SegmentContainer container, ScheduledExecutorService executor) {\n+            return Collections.singletonMap(ContainerTableExtension.class, new ContainerTableExtensionImpl(container, this.cacheManager, executor));\n+        }\n+\n+        @Override\n+        public void close() {\n+            this.readIndexFactory.close();\n+            this.cacheManager.close();\n+            this.cacheStorage.close();\n+            this.dataLogFactory.close();\n+        }\n+    }\n+\n+    SegmentStoreStarter startSegmentStore(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+        return new SegmentStoreStarter(storageFactory, dataLogFactory);\n+    }\n+\n+    /**\n+     * Creates a segment store server.\n+     */\n+    private static class SegmentStoreStarter {\n+        private final int servicePort = TestUtils.getAvailableListenPort();\n+        private ServiceBuilder serviceBuilder;\n+        private StreamSegmentStoreWrapper streamSegmentStoreWrapper;\n+        private AutoScaleMonitor monitor;\n+        private TableStoreWrapper tableStoreWrapper;\n+        private PravegaConnectionListener server;\n+\n+        SegmentStoreStarter(StorageFactory storageFactory, BookKeeperLogFactory dataLogFactory) throws DurableDataLogException {\n+            if (storageFactory != null) {\n+                if (dataLogFactory != null) {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory)\n+                            .withDataLogFactory(setup -> dataLogFactory);\n+                } else {\n+                    this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig())\n+                            .withStorageFactory(setup -> storageFactory);\n+                }\n+            } else {\n+                this.serviceBuilder = ServiceBuilder.newInMemoryBuilder(ServiceBuilderConfig.getDefaultConfig());\n+            }\n+            this.serviceBuilder.initialize();\n+            this.streamSegmentStoreWrapper = new StreamSegmentStoreWrapper(serviceBuilder.createStreamSegmentService());\n+            this.monitor = new AutoScaleMonitor(streamSegmentStoreWrapper, AutoScalerConfig.builder().build());\n+            this.tableStoreWrapper = new TableStoreWrapper(serviceBuilder.createTableStoreService());\n+            this.server = new PravegaConnectionListener(false, false, \"localhost\", servicePort, streamSegmentStoreWrapper,\n+                    tableStoreWrapper, monitor.getStatsRecorder(), monitor.getTableSegmentStatsRecorder(), new PassingTokenVerifier(),\n+                    null, null, true, serviceBuilder.getLowPriorityExecutor());\n+            this.server.startListening();\n+        }\n+\n+        public void close() {\n+            if (this.server != null) {\n+                this.server.close();\n+                this.server = null;\n+            }\n+\n+            if (this.monitor != null) {\n+                this.monitor.close();\n+                this.monitor = null;\n+            }\n+\n+            if (this.serviceBuilder != null) {\n+                this.serviceBuilder.close();\n+                this.serviceBuilder = null;\n+            }\n+        }\n+    }\n+\n+    ControllerStarter startController(int bkPort, int servicePort) throws InterruptedException {\n+        return new ControllerStarter(bkPort, servicePort);\n+    }\n+\n+    /**\n+     * Creates a controller instance and runs it.\n+     */\n+    private static class ControllerStarter {\n+        private final int controllerPort = TestUtils.getAvailableListenPort();\n+        private final String serviceHost = \"localhost\";\n+        private ControllerWrapper controllerWrapper = null;\n+        private Controller controller = null;\n+\n+        ControllerStarter(int bkPort, int servicePort) throws InterruptedException {\n+            this.controllerWrapper = new ControllerWrapper(\"localhost:\" + bkPort, false,\n+                    controllerPort, serviceHost, servicePort, CONTAINER_COUNT);\n+            this.controllerWrapper.awaitRunning();\n+            this.controller = controllerWrapper.getController();\n+        }\n+\n+        public void close() throws Exception {\n+            if (this.controller != null) {\n+                this.controller.close();\n+                this.controller = null;\n+            }\n+\n+            if (this.controllerWrapper != null) {\n+                this.controllerWrapper.close();\n+                this.controllerWrapper = null;\n+            }\n+        }\n+    }\n+\n+    @Test(timeout = 240000)\n+    public void testDurableDataLogFail() throws Exception {\n+        int instanceId = 0;\n+\n+        // Creating tier 2 only once here.\n+        this.baseDir = Files.createTempDirectory(\"test_nfs\").toFile().getAbsoluteFile();\n+        FileSystemStorageConfig fsConfig = FileSystemStorageConfig\n+                .builder()\n+                .with(FileSystemStorageConfig.ROOT, this.baseDir.getAbsolutePath())\n+                .build();\n+        this.storageFactory = new FileSystemStorageFactory(fsConfig, executorService);\n+\n+        // Start a new BK & ZK, segment store and controller\n+        this.bkzk = setUpNewBK(instanceId++);\n+        this.segmentStoreStarter = startSegmentStore(this.storageFactory, null);\n+        @Cleanup ControllerStarter controllerStarter = startController(this.bkzk.bkPort, this.segmentStoreStarter.servicePort);\n+\n+        // Create two streams for writing data onto two different segments\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM1);\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM2);\n+\n+        @Cleanup ConnectionFactory connectionFactory = new ConnectionFactoryImpl(ClientConfig.builder().build());\n+        @Cleanup ClientFactoryImpl clientFactory = new ClientFactoryImpl(SCOPE, controllerStarter.controller, connectionFactory);\n+        @Cleanup ReaderGroupManager readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerStarter.controller, clientFactory, connectionFactory);\n+\n+        writeEvents(STREAM1, clientFactory); // write 300 events on one segment\n+        writeEvents(STREAM2, clientFactory); // write 300 events on other segment\n+\n+        // Verify events write by reading them.\n+        readAllEvents(STREAM1, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n+                \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n+        readAllEvents(STREAM2, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n+                \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n+\n+        readerGroupManager.close();\n+        clientFactory.close();\n+\n+        controllerStarter.close(); // Shut down the controller\n+\n+        // Get names of all the segments created.\n+        HashSet<String> allSegments = new HashSet<>(this.segmentStoreStarter.streamSegmentStoreWrapper.getSegments());\n+        allSegments.addAll(this.segmentStoreStarter.tableStoreWrapper.getSegments());\n+        log.info(\"No. of segments created = {}\", allSegments.size());\n+\n+        // Get the long term storage from the running pravega instance\n+        @Cleanup Storage tier2 = new AsyncStorageWrapper(new RollingStorage(this.storageFactory.createSyncStorage(),\n+                new SegmentRollingPolicy(DEFAULT_ROLLING_SIZE)), DataRecoveryTestUtils.createExecutorService(1));\n+\n+        // wait for all segments to be flushed to the long term storage.\n+        waitForSegmentsInStorage(allSegments, this.segmentStoreStarter.streamSegmentStoreWrapper, tier2)\n+                .get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+        this.segmentStoreStarter.close(); // Shutdown SegmentStore\n+        this.segmentStoreStarter = null;\n+        log.info(\"Segment Store Shutdown\");\n+\n+        this.bkzk.close(); // Shutdown BookKeeper & ZooKeeper\n+        this.bkzk = null;\n+        log.info(\"BookKeeper & ZooKeeper shutdown\");\n+\n+        // start a new BookKeeper and ZooKeeper.\n+        this.bkzk = setUpNewBK(instanceId++);\n+        this.dataLogFactory = new BookKeeperLogFactory(this.bkzk.bkConfig.get(), this.bkzk.zkClient.get(),\n+                DataRecoveryTestUtils.createExecutorService(1));\n+        this.dataLogFactory.initialize();\n+\n+        // Delete container metadata segment and attributes index segment corresponding to the container Id from the long term storage\n+        DataRecoveryTestUtils.deleteContainerMetadataSegments(tier2, CONTAINER_ID);\n+\n+        // List all segments from the long term storage\n+        Map<Integer, List<SegmentProperties>> segmentsToCreate = DataRecoveryTestUtils.listAllSegments(tier2, CONTAINER_COUNT);\n+\n+        // Start debug segment container using dataLogFactory from new BK instance and old long term storageFactory.\n+        DebugTool debugTool = createDebugTool(this.dataLogFactory, this.storageFactory);\n+        DebugStreamSegmentContainer debugStreamSegmentContainer = (DebugStreamSegmentContainer)\n+                debugTool.containerFactory.createDebugStreamSegmentContainer(CONTAINER_ID);\n+\n+        // Re-create all segments which were listed.\n+        Services.startAsync(debugStreamSegmentContainer, executorService)\n+                .thenRun(new DataRecoveryTestUtils.Worker(debugStreamSegmentContainer, segmentsToCreate.get(CONTAINER_ID))).join();\n+        sleep(5000); // Without sleep the test fails sometimes complaining some segment offsets don't exist.\n+        Services.stopAsync(debugStreamSegmentContainer, executorService).join();\n+        debugStreamSegmentContainer.close();\n+        debugTool.close();\n+\n+        // Start a new segment store and controller\n+        this.segmentStoreStarter = startSegmentStore(this.storageFactory, this.dataLogFactory);\n+        controllerStarter = startController(this.bkzk.bkPort, this.segmentStoreStarter.servicePort);\n+\n+        connectionFactory = new ConnectionFactoryImpl(ClientConfig.builder().build());\n+        clientFactory = new ClientFactoryImpl(SCOPE, controllerStarter.controller, connectionFactory);\n+        readerGroupManager = new ReaderGroupManagerImpl(SCOPE, controllerStarter.controller, clientFactory, connectionFactory);\n+\n+        // Try creating the same segments again with the new controller\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM1);\n+        createScopeStream(controllerStarter.controller, SCOPE, STREAM2);\n+\n+        // Try reading all events again\n+        readAllEvents(STREAM1, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n+                \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n+        readAllEvents(STREAM2, clientFactory, readerGroupManager, \"RG\" + RANDOM.nextInt(Integer.MAX_VALUE),\n+                \"R\" + RANDOM.nextInt(Integer.MAX_VALUE));\n+    }\n+\n+    public void createScopeStream(Controller controller, String scopeName, String streamName) {\n+        try (ConnectionFactory cf = new ConnectionFactoryImpl(ClientConfig.builder().build());\n+             StreamManager streamManager = new StreamManagerImpl(controller, cf)) {\n+            streamManager.createScope(scopeName);\n+            streamManager.createStream(scopeName, streamName, config);\n+        }\n+    }\n+\n+    private void writeEvents(String streamName, ClientFactoryImpl clientFactory) {\n+        EventStreamWriter<String> writer = clientFactory.createEventWriter(streamName,\n+                new UTF8StringSerializer(),\n+                EventWriterConfig.builder().build());\n+        for (int i = 0; i < TOTAL_NUM_EVENTS;) {\n+            try {\n+                writer.writeEvent(\"\", EVENT).join();\n+                i++;\n+            } catch (Throwable e) {\n+                Assert.fail(\"Error occurred while writing events.\");\n+                break;\n+            }\n+        }\n+        writer.flush();\n+        writer.close();\n+    }\n+\n+    private void readAllEvents(String streamName, ClientFactoryImpl clientFactory, ReaderGroupManager readerGroupManager,\n+                               String readerGroupName, String readerName) {\n+        readerGroupManager.createReaderGroup(readerGroupName,\n+                ReaderGroupConfig\n+                        .builder()\n+                        .stream(Stream.of(SCOPE, streamName))\n+                        .automaticCheckpointIntervalMillis(2000)\n+                        .build());\n+\n+        EventStreamReader<String> reader = clientFactory.createReader(readerName,\n+                readerGroupName,\n+                new UTF8StringSerializer(),\n+                ReaderConfig.builder().build());\n+\n+        for (int q = 0; q < TOTAL_NUM_EVENTS;) {\n+            try {\n+                String eventRead = reader.readNextEvent(SECONDS.toMillis(500)).getEvent();\n+                Assert.assertEquals(\"Event written and read back don't match\", EVENT, eventRead);\n+                q++;\n+            } catch (Exception e) {\n+                Assert.fail(\"Error occurred while reading the events.\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYxMzQzMw=="}, "originalCommit": {"oid": "43fa63bbd1203beb32d05f30adcb0c9674aa88bc"}, "originalPosition": 585}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMDU1NDQyOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentServiceNoOpWriteReadTests.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMDo0OTo0NFrOG8aGXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwNzozMzoxMFrOG8mxnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk5NTM1OQ==", "bodyText": "Is it possible to define this method as empty block in the base class itself so that we don't have to keep overriding in derived classes?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r465995359", "createdAt": "2020-08-05T20:49:44Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentServiceNoOpWriteReadTests.java", "diffHunk": "@@ -65,4 +65,12 @@ protected ServiceBuilder createBuilder(ServiceBuilderConfig.Builder builderConfi\n                              .withStorageFactory(setup -> this.storageFactory)\n                              .withDataLogFactory(setup -> this.durableDataLogFactory);\n     }\n+\n+    /**\n+     * This method intentionally left blank as it's out of concern for No-Op Storage.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c05a63a0c5e5863c603b08af1cf6e3dcd4c88766"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIwMzAzNg==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r466203036", "createdAt": "2020-08-06T07:33:10Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/store/StreamSegmentServiceNoOpWriteReadTests.java", "diffHunk": "@@ -65,4 +65,12 @@ protected ServiceBuilder createBuilder(ServiceBuilderConfig.Builder builderConfi\n                              .withStorageFactory(setup -> this.storageFactory)\n                              .withDataLogFactory(setup -> this.durableDataLogFactory);\n     }\n+\n+    /**\n+     * This method intentionally left blank as it's out of concern for No-Op Storage.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk5NTM1OQ=="}, "originalCommit": {"oid": "c05a63a0c5e5863c603b08af1cf6e3dcd4c88766"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMDU2NDAyOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMDo1MjozNFrOG8aL_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwNzozNToxNVrOG8m1fQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk5Njc5OA==", "bodyText": "Is it possible for this method to end up deleting the actual data in LTS?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r465996798", "createdAt": "2020-08-05T20:52:34Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        for (Map.Entry<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainer : debugStreamSegmentContainers.entrySet()) {\n+\n+            ContainerTableExtension ext = debugStreamSegmentContainer.getValue().getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(debugStreamSegmentContainer.getKey()),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<String> metadataSegments = new HashSet<>();\n+            it.forEachRemaining(k -> metadataSegments.addAll(k.getEntries().stream().map(entry -> entry.getKey().toString())\n+                    .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainer.getValue(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> it = storage.listSegments();\n+        if (it == null) {\n+            log.info(\"No segments found in the long term storage.\");\n+            return;\n+        }\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            log.info(\"Segment to be recovered = {}\", curr.getName());\n+            metadataSegmentsByContainer.get(debugStreamSegmentContainers.get(containerId)).remove(curr.getName());\n+            futures.add(CompletableFuture.runAsync(new SegmentRecovery(debugStreamSegmentContainers.get(containerId), curr)));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        for (Map.Entry<DebugStreamSegmentContainer, Set<String>> metadataSegmentsSetEntry : metadataSegmentsByContainer.entrySet()) {\n+            for (String segmentName : metadataSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in storage\", segmentName);\n+                metadataSegmentsSetEntry.getKey().deleteStreamSegment(segmentName, TIMEOUT).join();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Creates the given segment with the given DebugStreamSegmentContainer instance.\n+     */\n+    public static class SegmentRecovery implements Runnable {\n+        private final DebugStreamSegmentContainer container;\n+        private final SegmentProperties storageSegment;\n+\n+        public SegmentRecovery(DebugStreamSegmentContainer container, SegmentProperties segment) {\n+            Preconditions.checkNotNull(container);\n+            Preconditions.checkNotNull(segment);\n+            this.container = container;\n+            this.storageSegment = segment;\n+        }\n+\n+        @Override\n+        public void run() {\n+            long segmentLength = storageSegment.getLength();\n+            boolean isSealed = storageSegment.isSealed();\n+            String segmentName = storageSegment.getName();\n+\n+            log.info(\"Recovering segment with name = {}, length = {}, sealed status = {}.\", segmentName, segmentLength, isSealed);\n+            /*\n+                1. segment exists in both metadata and storage, re-create it\n+                2. segment only in metadata, delete\n+                3. segment only in storage, re-create it\n+             */\n+            val streamSegmentInfo = container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                    .thenAccept(e -> {\n+                        if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                            container.deleteStreamSegment(segmentName, TIMEOUT).join();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c05a63a0c5e5863c603b08af1cf6e3dcd4c88766"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk5NzY3MQ==", "bodyText": "container.deleteStreamSegment(segmentName followed by container.registerExistingSegment(segmentName looks odd .\nMay be rename registerExistingSegment ?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r465997671", "createdAt": "2020-08-05T20:54:19Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        for (Map.Entry<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainer : debugStreamSegmentContainers.entrySet()) {\n+\n+            ContainerTableExtension ext = debugStreamSegmentContainer.getValue().getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(debugStreamSegmentContainer.getKey()),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<String> metadataSegments = new HashSet<>();\n+            it.forEachRemaining(k -> metadataSegments.addAll(k.getEntries().stream().map(entry -> entry.getKey().toString())\n+                    .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainer.getValue(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> it = storage.listSegments();\n+        if (it == null) {\n+            log.info(\"No segments found in the long term storage.\");\n+            return;\n+        }\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            log.info(\"Segment to be recovered = {}\", curr.getName());\n+            metadataSegmentsByContainer.get(debugStreamSegmentContainers.get(containerId)).remove(curr.getName());\n+            futures.add(CompletableFuture.runAsync(new SegmentRecovery(debugStreamSegmentContainers.get(containerId), curr)));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        for (Map.Entry<DebugStreamSegmentContainer, Set<String>> metadataSegmentsSetEntry : metadataSegmentsByContainer.entrySet()) {\n+            for (String segmentName : metadataSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in storage\", segmentName);\n+                metadataSegmentsSetEntry.getKey().deleteStreamSegment(segmentName, TIMEOUT).join();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Creates the given segment with the given DebugStreamSegmentContainer instance.\n+     */\n+    public static class SegmentRecovery implements Runnable {\n+        private final DebugStreamSegmentContainer container;\n+        private final SegmentProperties storageSegment;\n+\n+        public SegmentRecovery(DebugStreamSegmentContainer container, SegmentProperties segment) {\n+            Preconditions.checkNotNull(container);\n+            Preconditions.checkNotNull(segment);\n+            this.container = container;\n+            this.storageSegment = segment;\n+        }\n+\n+        @Override\n+        public void run() {\n+            long segmentLength = storageSegment.getLength();\n+            boolean isSealed = storageSegment.isSealed();\n+            String segmentName = storageSegment.getName();\n+\n+            log.info(\"Recovering segment with name = {}, length = {}, sealed status = {}.\", segmentName, segmentLength, isSealed);\n+            /*\n+                1. segment exists in both metadata and storage, re-create it\n+                2. segment only in metadata, delete\n+                3. segment only in storage, re-create it\n+             */\n+            val streamSegmentInfo = container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                    .thenAccept(e -> {\n+                        if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                            container.deleteStreamSegment(segmentName, TIMEOUT).join();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk5Njc5OA=="}, "originalCommit": {"oid": "c05a63a0c5e5863c603b08af1cf6e3dcd4c88766"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAxODc0Mg==", "bodyText": "To elaborate -\nWe want to just update the metadata and not touch the actual segment on LTS. (Except for metadata segment, all other segments should be left alone.)\nI fear that calling container.deleteStreamSegment like this will actually trigger request to delete the segment from LTS (it will enqueue operation to delete in BK ledger). Once the new recovered instance restarts then it will reprocess all the enqueued operations and may end up  actually deleting the data from LTS.\nWhat we need here is ability to just delete the metadata about the segment and re-create or update it.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r466018742", "createdAt": "2020-08-05T21:37:27Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        for (Map.Entry<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainer : debugStreamSegmentContainers.entrySet()) {\n+\n+            ContainerTableExtension ext = debugStreamSegmentContainer.getValue().getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(debugStreamSegmentContainer.getKey()),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<String> metadataSegments = new HashSet<>();\n+            it.forEachRemaining(k -> metadataSegments.addAll(k.getEntries().stream().map(entry -> entry.getKey().toString())\n+                    .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainer.getValue(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> it = storage.listSegments();\n+        if (it == null) {\n+            log.info(\"No segments found in the long term storage.\");\n+            return;\n+        }\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            log.info(\"Segment to be recovered = {}\", curr.getName());\n+            metadataSegmentsByContainer.get(debugStreamSegmentContainers.get(containerId)).remove(curr.getName());\n+            futures.add(CompletableFuture.runAsync(new SegmentRecovery(debugStreamSegmentContainers.get(containerId), curr)));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        for (Map.Entry<DebugStreamSegmentContainer, Set<String>> metadataSegmentsSetEntry : metadataSegmentsByContainer.entrySet()) {\n+            for (String segmentName : metadataSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in storage\", segmentName);\n+                metadataSegmentsSetEntry.getKey().deleteStreamSegment(segmentName, TIMEOUT).join();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Creates the given segment with the given DebugStreamSegmentContainer instance.\n+     */\n+    public static class SegmentRecovery implements Runnable {\n+        private final DebugStreamSegmentContainer container;\n+        private final SegmentProperties storageSegment;\n+\n+        public SegmentRecovery(DebugStreamSegmentContainer container, SegmentProperties segment) {\n+            Preconditions.checkNotNull(container);\n+            Preconditions.checkNotNull(segment);\n+            this.container = container;\n+            this.storageSegment = segment;\n+        }\n+\n+        @Override\n+        public void run() {\n+            long segmentLength = storageSegment.getLength();\n+            boolean isSealed = storageSegment.isSealed();\n+            String segmentName = storageSegment.getName();\n+\n+            log.info(\"Recovering segment with name = {}, length = {}, sealed status = {}.\", segmentName, segmentLength, isSealed);\n+            /*\n+                1. segment exists in both metadata and storage, re-create it\n+                2. segment only in metadata, delete\n+                3. segment only in storage, re-create it\n+             */\n+            val streamSegmentInfo = container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                    .thenAccept(e -> {\n+                        if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                            container.deleteStreamSegment(segmentName, TIMEOUT).join();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk5Njc5OA=="}, "originalCommit": {"oid": "c05a63a0c5e5863c603b08af1cf6e3dcd4c88766"}, "originalPosition": 132}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIwNDAyOQ==", "bodyText": "Using deleteSegment from MetadataStore now. From the javadoc, this method only deletes a Segment and any associated information from the Metadata Store.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r466204029", "createdAt": "2020-08-06T07:35:15Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        for (Map.Entry<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainer : debugStreamSegmentContainers.entrySet()) {\n+\n+            ContainerTableExtension ext = debugStreamSegmentContainer.getValue().getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> it = ext.keyIterator(getMetadataSegmentName(debugStreamSegmentContainer.getKey()),\n+                    IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+\n+            // Add all segments present in the container metadata in a set.\n+            Set<String> metadataSegments = new HashSet<>();\n+            it.forEachRemaining(k -> metadataSegments.addAll(k.getEntries().stream().map(entry -> entry.getKey().toString())\n+                    .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainer.getValue(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> it = storage.listSegments();\n+        if (it == null) {\n+            log.info(\"No segments found in the long term storage.\");\n+            return;\n+        }\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (it.hasNext()) {\n+            SegmentProperties curr = it.next();\n+            int containerId = segToConMapper.getContainerId(curr.getName());\n+            log.info(\"Segment to be recovered = {}\", curr.getName());\n+            metadataSegmentsByContainer.get(debugStreamSegmentContainers.get(containerId)).remove(curr.getName());\n+            futures.add(CompletableFuture.runAsync(new SegmentRecovery(debugStreamSegmentContainers.get(containerId), curr)));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        for (Map.Entry<DebugStreamSegmentContainer, Set<String>> metadataSegmentsSetEntry : metadataSegmentsByContainer.entrySet()) {\n+            for (String segmentName : metadataSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in storage\", segmentName);\n+                metadataSegmentsSetEntry.getKey().deleteStreamSegment(segmentName, TIMEOUT).join();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Creates the given segment with the given DebugStreamSegmentContainer instance.\n+     */\n+    public static class SegmentRecovery implements Runnable {\n+        private final DebugStreamSegmentContainer container;\n+        private final SegmentProperties storageSegment;\n+\n+        public SegmentRecovery(DebugStreamSegmentContainer container, SegmentProperties segment) {\n+            Preconditions.checkNotNull(container);\n+            Preconditions.checkNotNull(segment);\n+            this.container = container;\n+            this.storageSegment = segment;\n+        }\n+\n+        @Override\n+        public void run() {\n+            long segmentLength = storageSegment.getLength();\n+            boolean isSealed = storageSegment.isSealed();\n+            String segmentName = storageSegment.getName();\n+\n+            log.info(\"Recovering segment with name = {}, length = {}, sealed status = {}.\", segmentName, segmentLength, isSealed);\n+            /*\n+                1. segment exists in both metadata and storage, re-create it\n+                2. segment only in metadata, delete\n+                3. segment only in storage, re-create it\n+             */\n+            val streamSegmentInfo = container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                    .thenAccept(e -> {\n+                        if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                            container.deleteStreamSegment(segmentName, TIMEOUT).join();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NTk5Njc5OA=="}, "originalCommit": {"oid": "c05a63a0c5e5863c603b08af1cf6e3dcd4c88766"}, "originalPosition": 132}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMDY4MzQ3OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMTozMDowMlrOG8bUhg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQwNzo0MToxNlrOG8nCAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAxNTM2Ng==", "bodyText": "This functionality needs to live in product code and must be accessible to the admin tool. I suggest moving it in DebugStreamSegmentContainer.java", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r466015366", "createdAt": "2020-08-05T21:30:02Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c05a63a0c5e5863c603b08af1cf6e3dcd4c88766"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjIwNzIzNA==", "bodyText": "Moved it to io.pravega.segmentstore.server.containers with new name as SegmentsRecovery.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r466207234", "createdAt": "2020-08-06T07:41:16Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/DataRecoveryTestUtils.java", "diffHunk": "@@ -0,0 +1,171 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.containers.DebugStreamSegmentContainer;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery tests.\n+ */\n+@Slf4j\n+public class DataRecoveryTestUtils {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAxNTM2Ng=="}, "originalCommit": {"oid": "c05a63a0c5e5863c603b08af1cf6e3dcd4c88766"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMzk3NzY2OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/DebugSegmentContainer.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMToyODo0M1rOG_zg5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNTo0NzoxNVrOHAoT2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU1NzQ3Ng==", "bodyText": "The more I think about it, the more I do not like this name. This has little to nothing to do with \"Debugging\" so DebugSegmentContainer doesn't sound too descriptive.\nAt the same time I do not have any better ideas at this moment. But something to keep in mind as we make progress on this task.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469557476", "createdAt": "2020-08-12T21:28:43Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/DebugSegmentContainer.java", "diffHunk": "@@ -0,0 +1,28 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+import java.util.concurrent.CompletableFuture;\n+\n+/**\n+ * Defines debug segment container for stream segments.\n+ */\n+public interface DebugSegmentContainer extends SegmentContainer {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyMjQ4OQ==", "bodyText": "Ok. Is RecoverySegmentContainer good?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470422489", "createdAt": "2020-08-14T05:47:15Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/DebugSegmentContainer.java", "diffHunk": "@@ -0,0 +1,28 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+import java.util.concurrent.CompletableFuture;\n+\n+/**\n+ * Defines debug segment container for stream segments.\n+ */\n+public interface DebugSegmentContainer extends SegmentContainer {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU1NzQ3Ng=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMzk4MjI3OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/MetadataStore.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTozMDoxN1rOG_zjtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNTo1MTowM1rOHAoX4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU1ODE5Nw==", "bodyText": "Make this a proper Javadoc. Also, \"Used by XYZ\" is not really appropriate as a class should not be concerned with who is using it.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469558197", "createdAt": "2020-08-12T21:30:17Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/MetadataStore.java", "diffHunk": "@@ -704,6 +704,19 @@ static SegmentInfo newSegment(String name, Collection<AttributeUpdate> attribute\n                     .build();\n         }\n \n+        // Used by DebugStreamSegmentContainer to get SegmentInfo while registering a segment.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyMzUyMg==", "bodyText": "Even other methods in this class don't have Javadoc, so removed it here.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470423522", "createdAt": "2020-08-14T05:51:03Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/MetadataStore.java", "diffHunk": "@@ -704,6 +704,19 @@ static SegmentInfo newSegment(String name, Collection<AttributeUpdate> attribute\n                     .build();\n         }\n \n+        // Used by DebugStreamSegmentContainer to get SegmentInfo while registering a segment.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU1ODE5Nw=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMzk4NjI3OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTozMTozOFrOG_zmIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNTo1Mjo0MlrOHAoZeA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU1ODgxNg==", "bodyText": "Does this class need to be public?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469558816", "createdAt": "2020-08-12T21:31:38Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        for (Map.Entry<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            ContainerTableExtension tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).get(TIMEOUT.toMillis(),\n+                    TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k -> metadataSegments.addAll(k.getEntries().stream().map(entry -> entry.getKey().toString())\n+                    .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getValue(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        if (segmentIterator == null) {\n+            log.info(\"No segments found in the long term storage.\");\n+            return;\n+        }\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            log.info(\"Segment to be recovered = {}\", currentSegment.getName());\n+            metadataSegmentsByContainer.get(debugStreamSegmentContainers.get(containerId)).remove(currentSegment.getName());\n+            futures.add(CompletableFuture.runAsync(new SegmentRecovery(debugStreamSegmentContainers.get(containerId), currentSegment)));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        for (Map.Entry<DebugStreamSegmentContainer, Set<String>> metadataSegmentsSetEntry : metadataSegmentsByContainer.entrySet()) {\n+            for (String segmentName : metadataSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in storage\", segmentName);\n+                metadataSegmentsSetEntry.getKey().deleteStreamSegment(segmentName, TIMEOUT).join();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Creates the given segment with the given DebugStreamSegmentContainer instance.\n+     */\n+    public static class SegmentRecovery implements Runnable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyMzkyOA==", "bodyText": "Converted this class to a private method which returns a CompletableFuture and recovers one segment.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470423928", "createdAt": "2020-08-14T05:52:42Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        for (Map.Entry<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            ContainerTableExtension tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).get(TIMEOUT.toMillis(),\n+                    TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k -> metadataSegments.addAll(k.getEntries().stream().map(entry -> entry.getKey().toString())\n+                    .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getValue(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        if (segmentIterator == null) {\n+            log.info(\"No segments found in the long term storage.\");\n+            return;\n+        }\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            log.info(\"Segment to be recovered = {}\", currentSegment.getName());\n+            metadataSegmentsByContainer.get(debugStreamSegmentContainers.get(containerId)).remove(currentSegment.getName());\n+            futures.add(CompletableFuture.runAsync(new SegmentRecovery(debugStreamSegmentContainers.get(containerId), currentSegment)));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        for (Map.Entry<DebugStreamSegmentContainer, Set<String>> metadataSegmentsSetEntry : metadataSegmentsByContainer.entrySet()) {\n+            for (String segmentName : metadataSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in storage\", segmentName);\n+                metadataSegmentsSetEntry.getKey().deleteStreamSegment(segmentName, TIMEOUT).join();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Creates the given segment with the given DebugStreamSegmentContainer instance.\n+     */\n+    public static class SegmentRecovery implements Runnable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU1ODgxNg=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 111}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMzk4NzY2OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTozMjowOFrOG_zm7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNTo1Mjo1NFrOHAoZww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU1OTAyMw==", "bodyText": "Rename this to ContainerRecoveryUtils", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469559023", "createdAt": "2020-08-12T21:32:08Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyNDAwMw==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470424003", "createdAt": "2020-08-14T05:52:54Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU1OTAyMw=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMzk5MjA4OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTozMzo0NlrOG_zprA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNTo1MzoxMFrOHAoaGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU1OTcyNA==", "bodyText": "This description is misleading.\nYou need to begin with a sentence stating what this method does.\nThen you clearly state what happens when this method completes successfully and what happens if it fails.\nThen you explain the process you follow.\nYou need to clearly document all exceptions that can be thrown out of it.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469559724", "createdAt": "2020-08-12T21:33:46Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyNDA4OQ==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470424089", "createdAt": "2020-08-14T05:53:10Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU1OTcyNA=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMzk5NDgwOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTozNDo0NVrOG_zrcA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNTo1MzoxOVrOHAoaOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2MDE3Ng==", "bodyText": "A {@link Storage} instance that will be used to list segments from.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469560176", "createdAt": "2020-08-12T21:34:45Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyNDEyMQ==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470424121", "createdAt": "2020-08-14T05:53:19Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2MDE3Ng=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMzk5NjQyOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTozNToyMVrOG_zsdQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNTo1MzozNFrOHAoaiw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2MDQzNw==", "bodyText": "A Map of Container Ids to {@link DebugStreamSegmentContainer} instances representing the Containers that will be recovered.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469560437", "createdAt": "2020-08-12T21:35:21Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyNDIwMw==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470424203", "createdAt": "2020-08-14T05:53:34Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2MDQzNw=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzMzk5ODAzOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTozNTo1NFrOG_ztdA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNTo1NDowM1rOHAobEA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2MDY5Mg==", "bodyText": "Validate all arguments prior to beginning. Null checks. Are all containers present in your hash map?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469560692", "createdAt": "2020-08-12T21:35:54Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyNDMzNg==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470424336", "createdAt": "2020-08-14T05:54:03Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2MDY5Mg=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDAxNDQ4OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo0MTozNlrOG_z3Lw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNTo1NDoyM1rOHAobYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2MzE4Mw==", "bodyText": "Please reformat this code to be more readable.\n\nFeel free to use val when declaring types with long names (for (val debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()))\nMove IteratorArgs... into some variable outside the loop and reuse it.\nPlease do not chain all method calls in one big line or wrapped-around line. If you choose to do chaining (no problem there), do it on a new line and let your IDE auto-format the proper placement of the the code. Look for other cases in the codebase where this is done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469563183", "createdAt": "2020-08-12T21:41:36Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        for (Map.Entry<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyNDQxOA==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470424418", "createdAt": "2020-08-14T05:54:23Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        for (Map.Entry<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2MzE4Mw=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDAxNjE1OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo0MjowM1rOG_z4Eg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNTo1NDozOFrOHAobqA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2MzQxMA==", "bodyText": "Why do you key the Map on the DebugStreamSegmentContainer? Can't you just do it on the id of the container?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469563410", "createdAt": "2020-08-12T21:42:03Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyNDQ4OA==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470424488", "createdAt": "2020-08-14T05:54:38Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2MzQxMA=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDAxODM3OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo0Mjo1NFrOG_z5fA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNTo1Njo1OVrOHAoekQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2Mzc3Mg==", "bodyText": "This is not what a null segmentIterator means.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469563772", "createdAt": "2020-08-12T21:42:54Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        for (Map.Entry<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            ContainerTableExtension tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).get(TIMEOUT.toMillis(),\n+                    TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k -> metadataSegments.addAll(k.getEntries().stream().map(entry -> entry.getKey().toString())\n+                    .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getValue(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        if (segmentIterator == null) {\n+            log.info(\"No segments found in the long term storage.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2MzkyOA==", "bodyText": "And you should throw an exception in this case, not return (returning with no exception indicates a successful completion).", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469563928", "createdAt": "2020-08-12T21:43:19Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        for (Map.Entry<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            ContainerTableExtension tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).get(TIMEOUT.toMillis(),\n+                    TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k -> metadataSegments.addAll(k.getEntries().stream().map(entry -> entry.getKey().toString())\n+                    .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getValue(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        if (segmentIterator == null) {\n+            log.info(\"No segments found in the long term storage.\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2Mzc3Mg=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyNTIzMw==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470425233", "createdAt": "2020-08-14T05:56:59Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        for (Map.Entry<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            ContainerTableExtension tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).get(TIMEOUT.toMillis(),\n+                    TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k -> metadataSegments.addAll(k.getEntries().stream().map(entry -> entry.getKey().toString())\n+                    .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getValue(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        if (segmentIterator == null) {\n+            log.info(\"No segments found in the long term storage.\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2Mzc3Mg=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDAyMDk0OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo0Mzo1MlrOG_z7FQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNTo1NzoyMlrOHAoe9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2NDE4MQ==", "bodyText": "Registering Segment: {}", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469564181", "createdAt": "2020-08-12T21:43:52Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        for (Map.Entry<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            ContainerTableExtension tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).get(TIMEOUT.toMillis(),\n+                    TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k -> metadataSegments.addAll(k.getEntries().stream().map(entry -> entry.getKey().toString())\n+                    .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getValue(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        if (segmentIterator == null) {\n+            log.info(\"No segments found in the long term storage.\");\n+            return;\n+        }\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            log.info(\"Segment to be recovered = {}\", currentSegment.getName());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2NTI4Nw==", "bodyText": "Actually this is superfluous. The SegmentRecovery logs it too.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469565287", "createdAt": "2020-08-12T21:46:28Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        for (Map.Entry<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            ContainerTableExtension tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).get(TIMEOUT.toMillis(),\n+                    TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k -> metadataSegments.addAll(k.getEntries().stream().map(entry -> entry.getKey().toString())\n+                    .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getValue(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        if (segmentIterator == null) {\n+            log.info(\"No segments found in the long term storage.\");\n+            return;\n+        }\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            log.info(\"Segment to be recovered = {}\", currentSegment.getName());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2NDE4MQ=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyNTI5Mg==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470425292", "createdAt": "2020-08-14T05:57:11Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        for (Map.Entry<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            ContainerTableExtension tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).get(TIMEOUT.toMillis(),\n+                    TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k -> metadataSegments.addAll(k.getEntries().stream().map(entry -> entry.getKey().toString())\n+                    .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getValue(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        if (segmentIterator == null) {\n+            log.info(\"No segments found in the long term storage.\");\n+            return;\n+        }\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            log.info(\"Segment to be recovered = {}\", currentSegment.getName());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2NDE4MQ=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 94}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyNTMzMg==", "bodyText": "Removed.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470425332", "createdAt": "2020-08-14T05:57:22Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        for (Map.Entry<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            ContainerTableExtension tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).get(TIMEOUT.toMillis(),\n+                    TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k -> metadataSegments.addAll(k.getEntries().stream().map(entry -> entry.getKey().toString())\n+                    .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getValue(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        if (segmentIterator == null) {\n+            log.info(\"No segments found in the long term storage.\");\n+            return;\n+        }\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            log.info(\"Segment to be recovered = {}\", currentSegment.getName());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2NDE4MQ=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 94}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDAyNDkwOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo0NToxM1rOG_z9Uw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo0NToxM1rOG_z9Uw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2NDc1NQ==", "bodyText": "This is going to create A LOT of concurrent threads, possibly taking over your entire thread pool.\nWhy do you do each segment individually in a thread?", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469564755", "createdAt": "2020-08-12T21:45:13Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        for (Map.Entry<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            ContainerTableExtension tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).get(TIMEOUT.toMillis(),\n+                    TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k -> metadataSegments.addAll(k.getEntries().stream().map(entry -> entry.getKey().toString())\n+                    .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getValue(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        if (segmentIterator == null) {\n+            log.info(\"No segments found in the long term storage.\");\n+            return;\n+        }\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            log.info(\"Segment to be recovered = {}\", currentSegment.getName());\n+            metadataSegmentsByContainer.get(debugStreamSegmentContainers.get(containerId)).remove(currentSegment.getName());\n+            futures.add(CompletableFuture.runAsync(new SegmentRecovery(debugStreamSegmentContainers.get(containerId), currentSegment)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 96}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDAyNjE1OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo0NTozOVrOG_z-Eg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNTo1Nzo0MlrOHAofSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2NDk0Ng==", "bodyText": "Please be consistent about how you log it. Is it storage or long term storage. I prefer the former.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469564946", "createdAt": "2020-08-12T21:45:39Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        for (Map.Entry<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            ContainerTableExtension tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).get(TIMEOUT.toMillis(),\n+                    TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k -> metadataSegments.addAll(k.getEntries().stream().map(entry -> entry.getKey().toString())\n+                    .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getValue(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        if (segmentIterator == null) {\n+            log.info(\"No segments found in the long term storage.\");\n+            return;\n+        }\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            log.info(\"Segment to be recovered = {}\", currentSegment.getName());\n+            metadataSegmentsByContainer.get(debugStreamSegmentContainers.get(containerId)).remove(currentSegment.getName());\n+            futures.add(CompletableFuture.runAsync(new SegmentRecovery(debugStreamSegmentContainers.get(containerId), currentSegment)));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        for (Map.Entry<DebugStreamSegmentContainer, Set<String>> metadataSegmentsSetEntry : metadataSegmentsByContainer.entrySet()) {\n+            for (String segmentName : metadataSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in storage\", segmentName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyNTQxNg==", "bodyText": "Ok. Using storage", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470425416", "createdAt": "2020-08-14T05:57:42Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        for (Map.Entry<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            ContainerTableExtension tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).get(TIMEOUT.toMillis(),\n+                    TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k -> metadataSegments.addAll(k.getEntries().stream().map(entry -> entry.getKey().toString())\n+                    .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getValue(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        if (segmentIterator == null) {\n+            log.info(\"No segments found in the long term storage.\");\n+            return;\n+        }\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            log.info(\"Segment to be recovered = {}\", currentSegment.getName());\n+            metadataSegmentsByContainer.get(debugStreamSegmentContainers.get(containerId)).remove(currentSegment.getName());\n+            futures.add(CompletableFuture.runAsync(new SegmentRecovery(debugStreamSegmentContainers.get(containerId), currentSegment)));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        for (Map.Entry<DebugStreamSegmentContainer, Set<String>> metadataSegmentsSetEntry : metadataSegmentsByContainer.entrySet()) {\n+            for (String segmentName : metadataSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in storage\", segmentName);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2NDk0Ng=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 102}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDAyOTQ1OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo0Njo0OFrOG_0AHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNTo1ODozOFrOHAogOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2NTQ2OA==", "bodyText": "Registering: {}, {}, {}.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469565468", "createdAt": "2020-08-12T21:46:48Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        for (Map.Entry<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            ContainerTableExtension tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).get(TIMEOUT.toMillis(),\n+                    TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k -> metadataSegments.addAll(k.getEntries().stream().map(entry -> entry.getKey().toString())\n+                    .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getValue(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        if (segmentIterator == null) {\n+            log.info(\"No segments found in the long term storage.\");\n+            return;\n+        }\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            log.info(\"Segment to be recovered = {}\", currentSegment.getName());\n+            metadataSegmentsByContainer.get(debugStreamSegmentContainers.get(containerId)).remove(currentSegment.getName());\n+            futures.add(CompletableFuture.runAsync(new SegmentRecovery(debugStreamSegmentContainers.get(containerId), currentSegment)));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        for (Map.Entry<DebugStreamSegmentContainer, Set<String>> metadataSegmentsSetEntry : metadataSegmentsByContainer.entrySet()) {\n+            for (String segmentName : metadataSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in storage\", segmentName);\n+                metadataSegmentsSetEntry.getKey().deleteStreamSegment(segmentName, TIMEOUT).join();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Creates the given segment with the given DebugStreamSegmentContainer instance.\n+     */\n+    public static class SegmentRecovery implements Runnable {\n+        private final DebugStreamSegmentContainer container;\n+        private final SegmentProperties storageSegment;\n+\n+        public SegmentRecovery(DebugStreamSegmentContainer container, SegmentProperties segment) {\n+            Preconditions.checkNotNull(container);\n+            Preconditions.checkNotNull(segment);\n+            this.container = container;\n+            this.storageSegment = segment;\n+        }\n+\n+        @Override\n+        public void run() {\n+            long segmentLength = storageSegment.getLength();\n+            boolean isSealed = storageSegment.isSealed();\n+            String segmentName = storageSegment.getName();\n+\n+            log.info(\"Recovering segment with name = {}, length = {}, sealed status = {}.\", segmentName, segmentLength, isSealed);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyNTY1OQ==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470425659", "createdAt": "2020-08-14T05:58:38Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        for (Map.Entry<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            ContainerTableExtension tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).get(TIMEOUT.toMillis(),\n+                    TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k -> metadataSegments.addAll(k.getEntries().stream().map(entry -> entry.getKey().toString())\n+                    .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getValue(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        if (segmentIterator == null) {\n+            log.info(\"No segments found in the long term storage.\");\n+            return;\n+        }\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            log.info(\"Segment to be recovered = {}\", currentSegment.getName());\n+            metadataSegmentsByContainer.get(debugStreamSegmentContainers.get(containerId)).remove(currentSegment.getName());\n+            futures.add(CompletableFuture.runAsync(new SegmentRecovery(debugStreamSegmentContainers.get(containerId), currentSegment)));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        for (Map.Entry<DebugStreamSegmentContainer, Set<String>> metadataSegmentsSetEntry : metadataSegmentsByContainer.entrySet()) {\n+            for (String segmentName : metadataSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in storage\", segmentName);\n+                metadataSegmentsSetEntry.getKey().deleteStreamSegment(segmentName, TIMEOUT).join();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Creates the given segment with the given DebugStreamSegmentContainer instance.\n+     */\n+    public static class SegmentRecovery implements Runnable {\n+        private final DebugStreamSegmentContainer container;\n+        private final SegmentProperties storageSegment;\n+\n+        public SegmentRecovery(DebugStreamSegmentContainer container, SegmentProperties segment) {\n+            Preconditions.checkNotNull(container);\n+            Preconditions.checkNotNull(segment);\n+            this.container = container;\n+            this.storageSegment = segment;\n+        }\n+\n+        @Override\n+        public void run() {\n+            long segmentLength = storageSegment.getLength();\n+            boolean isSealed = storageSegment.isSealed();\n+            String segmentName = storageSegment.getName();\n+\n+            log.info(\"Recovering segment with name = {}, length = {}, sealed status = {}.\", segmentName, segmentLength, isSealed);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2NTQ2OA=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 128}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDAzNTc5OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo0OTowNFrOG_0D2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNjowMToxMlrOHAojLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2NjQyNg==", "bodyText": "You are effectively using 2 threads for this operation. One thread is the one that SegmentRecovery is running on and the other is whatever this call (and the one above) is using (this is what happens when you invoke join).\nPlease rework this entire class (the outer one) to not have a single join or get. This means you will be using CompletableFuture joins (thenCompose, thenAccept, etc). This also means your main method in this class will return a CompletableFuture.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469566426", "createdAt": "2020-08-12T21:49:04Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        for (Map.Entry<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            ContainerTableExtension tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).get(TIMEOUT.toMillis(),\n+                    TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k -> metadataSegments.addAll(k.getEntries().stream().map(entry -> entry.getKey().toString())\n+                    .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getValue(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        if (segmentIterator == null) {\n+            log.info(\"No segments found in the long term storage.\");\n+            return;\n+        }\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            log.info(\"Segment to be recovered = {}\", currentSegment.getName());\n+            metadataSegmentsByContainer.get(debugStreamSegmentContainers.get(containerId)).remove(currentSegment.getName());\n+            futures.add(CompletableFuture.runAsync(new SegmentRecovery(debugStreamSegmentContainers.get(containerId), currentSegment)));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        for (Map.Entry<DebugStreamSegmentContainer, Set<String>> metadataSegmentsSetEntry : metadataSegmentsByContainer.entrySet()) {\n+            for (String segmentName : metadataSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in storage\", segmentName);\n+                metadataSegmentsSetEntry.getKey().deleteStreamSegment(segmentName, TIMEOUT).join();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Creates the given segment with the given DebugStreamSegmentContainer instance.\n+     */\n+    public static class SegmentRecovery implements Runnable {\n+        private final DebugStreamSegmentContainer container;\n+        private final SegmentProperties storageSegment;\n+\n+        public SegmentRecovery(DebugStreamSegmentContainer container, SegmentProperties segment) {\n+            Preconditions.checkNotNull(container);\n+            Preconditions.checkNotNull(segment);\n+            this.container = container;\n+            this.storageSegment = segment;\n+        }\n+\n+        @Override\n+        public void run() {\n+            long segmentLength = storageSegment.getLength();\n+            boolean isSealed = storageSegment.isSealed();\n+            String segmentName = storageSegment.getName();\n+\n+            log.info(\"Recovering segment with name = {}, length = {}, sealed status = {}.\", segmentName, segmentLength, isSealed);\n+            /*\n+                1. segment exists in both metadata and storage, re-create it\n+                2. segment only in metadata, delete\n+                3. segment only in storage, re-create it\n+             */\n+            val streamSegmentInfo = container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                    .thenAccept(e -> {\n+                        if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                            container.metadataStore.deleteSegment(segmentName, TIMEOUT).join();\n+                            container.registerSegment(segmentName, segmentLength, isSealed).join();\n+                        }\n+                    });\n+\n+            Futures.exceptionallyComposeExpecting(streamSegmentInfo, ex -> Exceptions.unwrap(ex) instanceof StreamSegmentNotExistsException,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 142}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyNjQxNQ==", "bodyText": "Converted this class to a private method which returns a CompletableFuture and recovers one segment. Removed all join or get.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470426415", "createdAt": "2020-08-14T06:01:12Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        for (Map.Entry<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            ContainerTableExtension tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).get(TIMEOUT.toMillis(),\n+                    TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k -> metadataSegments.addAll(k.getEntries().stream().map(entry -> entry.getKey().toString())\n+                    .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getValue(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        if (segmentIterator == null) {\n+            log.info(\"No segments found in the long term storage.\");\n+            return;\n+        }\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            log.info(\"Segment to be recovered = {}\", currentSegment.getName());\n+            metadataSegmentsByContainer.get(debugStreamSegmentContainers.get(containerId)).remove(currentSegment.getName());\n+            futures.add(CompletableFuture.runAsync(new SegmentRecovery(debugStreamSegmentContainers.get(containerId), currentSegment)));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        for (Map.Entry<DebugStreamSegmentContainer, Set<String>> metadataSegmentsSetEntry : metadataSegmentsByContainer.entrySet()) {\n+            for (String segmentName : metadataSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in storage\", segmentName);\n+                metadataSegmentsSetEntry.getKey().deleteStreamSegment(segmentName, TIMEOUT).join();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Creates the given segment with the given DebugStreamSegmentContainer instance.\n+     */\n+    public static class SegmentRecovery implements Runnable {\n+        private final DebugStreamSegmentContainer container;\n+        private final SegmentProperties storageSegment;\n+\n+        public SegmentRecovery(DebugStreamSegmentContainer container, SegmentProperties segment) {\n+            Preconditions.checkNotNull(container);\n+            Preconditions.checkNotNull(segment);\n+            this.container = container;\n+            this.storageSegment = segment;\n+        }\n+\n+        @Override\n+        public void run() {\n+            long segmentLength = storageSegment.getLength();\n+            boolean isSealed = storageSegment.isSealed();\n+            String segmentName = storageSegment.getName();\n+\n+            log.info(\"Recovering segment with name = {}, length = {}, sealed status = {}.\", segmentName, segmentLength, isSealed);\n+            /*\n+                1. segment exists in both metadata and storage, re-create it\n+                2. segment only in metadata, delete\n+                3. segment only in storage, re-create it\n+             */\n+            val streamSegmentInfo = container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                    .thenAccept(e -> {\n+                        if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                            container.metadataStore.deleteSegment(segmentName, TIMEOUT).join();\n+                            container.registerSegment(segmentName, segmentLength, isSealed).join();\n+                        }\n+                    });\n+\n+            Futures.exceptionallyComposeExpecting(streamSegmentInfo, ex -> Exceptions.unwrap(ex) instanceof StreamSegmentNotExistsException,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2NjQyNg=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 142}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDAzNzQ1OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo0OTo0M1rOG_0E0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNjowMzo1OFrOHAol7A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2NjY3Mw==", "bodyText": "Do not use join", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469566673", "createdAt": "2020-08-12T21:49:43Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        for (Map.Entry<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            ContainerTableExtension tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).get(TIMEOUT.toMillis(),\n+                    TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k -> metadataSegments.addAll(k.getEntries().stream().map(entry -> entry.getKey().toString())\n+                    .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getValue(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        if (segmentIterator == null) {\n+            log.info(\"No segments found in the long term storage.\");\n+            return;\n+        }\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            log.info(\"Segment to be recovered = {}\", currentSegment.getName());\n+            metadataSegmentsByContainer.get(debugStreamSegmentContainers.get(containerId)).remove(currentSegment.getName());\n+            futures.add(CompletableFuture.runAsync(new SegmentRecovery(debugStreamSegmentContainers.get(containerId), currentSegment)));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        for (Map.Entry<DebugStreamSegmentContainer, Set<String>> metadataSegmentsSetEntry : metadataSegmentsByContainer.entrySet()) {\n+            for (String segmentName : metadataSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in storage\", segmentName);\n+                metadataSegmentsSetEntry.getKey().deleteStreamSegment(segmentName, TIMEOUT).join();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Creates the given segment with the given DebugStreamSegmentContainer instance.\n+     */\n+    public static class SegmentRecovery implements Runnable {\n+        private final DebugStreamSegmentContainer container;\n+        private final SegmentProperties storageSegment;\n+\n+        public SegmentRecovery(DebugStreamSegmentContainer container, SegmentProperties segment) {\n+            Preconditions.checkNotNull(container);\n+            Preconditions.checkNotNull(segment);\n+            this.container = container;\n+            this.storageSegment = segment;\n+        }\n+\n+        @Override\n+        public void run() {\n+            long segmentLength = storageSegment.getLength();\n+            boolean isSealed = storageSegment.isSealed();\n+            String segmentName = storageSegment.getName();\n+\n+            log.info(\"Recovering segment with name = {}, length = {}, sealed status = {}.\", segmentName, segmentLength, isSealed);\n+            /*\n+                1. segment exists in both metadata and storage, re-create it\n+                2. segment only in metadata, delete\n+                3. segment only in storage, re-create it\n+             */\n+            val streamSegmentInfo = container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                    .thenAccept(e -> {\n+                        if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                            container.metadataStore.deleteSegment(segmentName, TIMEOUT).join();\n+                            container.registerSegment(segmentName, segmentLength, isSealed).join();\n+                        }\n+                    });\n+\n+            Futures.exceptionallyComposeExpecting(streamSegmentInfo, ex -> Exceptions.unwrap(ex) instanceof StreamSegmentNotExistsException,\n+                    () -> container.registerSegment(segmentName, segmentLength, isSealed)).join();\n+        }\n+    }\n+\n+    /**\n+     * Deletes container-metadata segment and attribute segment of the container with given container Id.\n+     * @param storage       Long term storage to delete the segments from.\n+     * @param containerId   Id of the container for which the segments has to be deleted.\n+     */\n+    public static void deleteContainerMetadataSegments(Storage storage, int containerId) {\n+        String metadataSegmentName = NameUtils.getMetadataSegmentName(containerId);\n+        deleteSegment(storage, metadataSegmentName);\n+        String attributeSegmentName = NameUtils.getAttributeSegmentName(metadataSegmentName);\n+        deleteSegment(storage, attributeSegmentName);\n+    }\n+\n+    /**\n+     * Deletes the segment with given segment name from the given long term storage.\n+     * @param storage       Long term storage to delete the segment from.\n+     * @param segmentName   Name of the segment to be deleted.\n+     */\n+    private static void deleteSegment(Storage storage, String segmentName) {\n+        try {\n+            SegmentHandle segmentHandle = storage.openWrite(segmentName).join();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 166}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyNzExNg==", "bodyText": "I have just kept it here to make sure that method call deletes the segment. It is called from deleteContainerMetadataAndAttributeSegments which deletes both the segments.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470427116", "createdAt": "2020-08-14T06:03:58Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/SegmentsRecovery.java", "diffHunk": "@@ -0,0 +1,176 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.Exceptions;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentNotExistsException;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.storage.SegmentHandle;\n+import io.pravega.segmentstore.storage.Storage;\n+import io.pravega.shared.NameUtils;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ExecutorService;\n+import java.util.concurrent.TimeUnit;\n+import java.util.stream.Collectors;\n+\n+import static io.pravega.shared.NameUtils.getMetadataSegmentName;\n+\n+/**\n+ * Utility methods for data recovery.\n+ */\n+@Slf4j\n+public class SegmentsRecovery {\n+    private static final Duration TIMEOUT = Duration.ofSeconds(30);\n+\n+    /**\n+     * Lists all segments from a given long term storage and then re-creates them using their corresponding debug segment\n+     * container.\n+     * @param storage                           Long term storage.\n+     * @param debugStreamSegmentContainers      A hashmap which has debug segment container instances to create segments.\n+     * @param executorService                   A thread pool for execution.\n+     * @throws                                  Exception in case of exception during the execution.\n+     */\n+    public static void recoverAllSegments(Storage storage, Map<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainers,\n+                                          ExecutorService executorService) throws Exception {\n+        log.info(\"Recovery started for all containers...\");\n+\n+        // Add all segments in the container metadata in a set for each debug segment container instance.\n+        Map<DebugStreamSegmentContainer, Set<String>> metadataSegmentsByContainer = new HashMap<>();\n+        for (Map.Entry<Integer, DebugStreamSegmentContainer> debugStreamSegmentContainerEntry : debugStreamSegmentContainers.entrySet()) {\n+            ContainerTableExtension tableExtension = debugStreamSegmentContainerEntry.getValue().getExtension(ContainerTableExtension.class);\n+            AsyncIterator<IteratorItem<TableKey>> keyIterator = tableExtension.keyIterator(getMetadataSegmentName(\n+                    debugStreamSegmentContainerEntry.getKey()), IteratorArgs.builder().fetchTimeout(TIMEOUT).build()).get(TIMEOUT.toMillis(),\n+                    TimeUnit.MILLISECONDS);\n+            Set<String> metadataSegments = new HashSet<>();\n+            keyIterator.forEachRemaining(k -> metadataSegments.addAll(k.getEntries().stream().map(entry -> entry.getKey().toString())\n+                    .collect(Collectors.toSet())), executorService).get(TIMEOUT.toMillis(), TimeUnit.MILLISECONDS);\n+            metadataSegmentsByContainer.put(debugStreamSegmentContainerEntry.getValue(), metadataSegments);\n+        }\n+\n+        SegmentToContainerMapper segToConMapper = new SegmentToContainerMapper(debugStreamSegmentContainers.size());\n+\n+        Iterator<SegmentProperties> segmentIterator = storage.listSegments();\n+        if (segmentIterator == null) {\n+            log.info(\"No segments found in the long term storage.\");\n+            return;\n+        }\n+\n+        // Iterate through all segments. Create each one of their using their respective debugSegmentContainer instance.\n+        ArrayList<CompletableFuture<Void>> futures = new ArrayList<>();\n+        while (segmentIterator.hasNext()) {\n+            SegmentProperties currentSegment = segmentIterator.next();\n+\n+            // skip recovery if the segment is an attribute segment.\n+            if (NameUtils.isAttributeSegment(currentSegment.getName())) {\n+                continue;\n+            }\n+\n+            int containerId = segToConMapper.getContainerId(currentSegment.getName());\n+            log.info(\"Segment to be recovered = {}\", currentSegment.getName());\n+            metadataSegmentsByContainer.get(debugStreamSegmentContainers.get(containerId)).remove(currentSegment.getName());\n+            futures.add(CompletableFuture.runAsync(new SegmentRecovery(debugStreamSegmentContainers.get(containerId), currentSegment)));\n+        }\n+        Futures.allOf(futures).join();\n+\n+        for (Map.Entry<DebugStreamSegmentContainer, Set<String>> metadataSegmentsSetEntry : metadataSegmentsByContainer.entrySet()) {\n+            for (String segmentName : metadataSegmentsSetEntry.getValue()) {\n+                log.info(\"Deleting segment '{}' as it is not in storage\", segmentName);\n+                metadataSegmentsSetEntry.getKey().deleteStreamSegment(segmentName, TIMEOUT).join();\n+            }\n+        }\n+    }\n+\n+    /**\n+     * Creates the given segment with the given DebugStreamSegmentContainer instance.\n+     */\n+    public static class SegmentRecovery implements Runnable {\n+        private final DebugStreamSegmentContainer container;\n+        private final SegmentProperties storageSegment;\n+\n+        public SegmentRecovery(DebugStreamSegmentContainer container, SegmentProperties segment) {\n+            Preconditions.checkNotNull(container);\n+            Preconditions.checkNotNull(segment);\n+            this.container = container;\n+            this.storageSegment = segment;\n+        }\n+\n+        @Override\n+        public void run() {\n+            long segmentLength = storageSegment.getLength();\n+            boolean isSealed = storageSegment.isSealed();\n+            String segmentName = storageSegment.getName();\n+\n+            log.info(\"Recovering segment with name = {}, length = {}, sealed status = {}.\", segmentName, segmentLength, isSealed);\n+            /*\n+                1. segment exists in both metadata and storage, re-create it\n+                2. segment only in metadata, delete\n+                3. segment only in storage, re-create it\n+             */\n+            val streamSegmentInfo = container.getStreamSegmentInfo(storageSegment.getName(), TIMEOUT)\n+                    .thenAccept(e -> {\n+                        if (segmentLength != e.getLength() || isSealed != e.isSealed()) {\n+                            container.metadataStore.deleteSegment(segmentName, TIMEOUT).join();\n+                            container.registerSegment(segmentName, segmentLength, isSealed).join();\n+                        }\n+                    });\n+\n+            Futures.exceptionallyComposeExpecting(streamSegmentInfo, ex -> Exceptions.unwrap(ex) instanceof StreamSegmentNotExistsException,\n+                    () -> container.registerSegment(segmentName, segmentLength, isSealed)).join();\n+        }\n+    }\n+\n+    /**\n+     * Deletes container-metadata segment and attribute segment of the container with given container Id.\n+     * @param storage       Long term storage to delete the segments from.\n+     * @param containerId   Id of the container for which the segments has to be deleted.\n+     */\n+    public static void deleteContainerMetadataSegments(Storage storage, int containerId) {\n+        String metadataSegmentName = NameUtils.getMetadataSegmentName(containerId);\n+        deleteSegment(storage, metadataSegmentName);\n+        String attributeSegmentName = NameUtils.getAttributeSegmentName(metadataSegmentName);\n+        deleteSegment(storage, attributeSegmentName);\n+    }\n+\n+    /**\n+     * Deletes the segment with given segment name from the given long term storage.\n+     * @param storage       Long term storage to delete the segment from.\n+     * @param segmentName   Name of the segment to be deleted.\n+     */\n+    private static void deleteSegment(Storage storage, String segmentName) {\n+        try {\n+            SegmentHandle segmentHandle = storage.openWrite(segmentName).join();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2NjY3Mw=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 166}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDA0MDc2OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/store/ServiceBuilder.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo1MDo0OFrOG_0GsA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNjowNzo0N1rOHAoqOw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2NzE1Mg==", "bodyText": "return createStorageFactory()", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469567152", "createdAt": "2020-08-12T21:50:48Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/store/ServiceBuilder.java", "diffHunk": "@@ -241,6 +241,14 @@ public void initialize() throws DurableDataLogException {\n         getSingleton(this.containerManager, this.segmentContainerManagerCreator).initialize();\n     }\n \n+    /**\n+     * To get the storageFactory after a ServiceBuilder has been initialized.\n+     * @return StorageFactory instance used to initialize ServiceBuilder.\n+     */\n+    public StorageFactory getStorageFactory() {\n+        return this.storageFactory.get();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyODIxOQ==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470428219", "createdAt": "2020-08-14T06:07:47Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/store/ServiceBuilder.java", "diffHunk": "@@ -241,6 +241,14 @@ public void initialize() throws DurableDataLogException {\n         getSingleton(this.containerManager, this.segmentContainerManagerCreator).initialize();\n     }\n \n+    /**\n+     * To get the storageFactory after a ServiceBuilder has been initialized.\n+     * @return StorageFactory instance used to initialize ServiceBuilder.\n+     */\n+    public StorageFactory getStorageFactory() {\n+        return this.storageFactory.get();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2NzE1Mg=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDA0MzI1OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/SegmentsTracker.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo1MTo1NFrOG_0IYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNjowODozMVrOHAoq8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2NzU4Nw==", "bodyText": "SegmentStoreWithSegmentTracker", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469567587", "createdAt": "2020-08-12T21:51:54Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/SegmentsTracker.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.BufferView;\n+import io.pravega.segmentstore.contracts.AttributeUpdate;\n+import io.pravega.segmentstore.contracts.MergeStreamSegmentResult;\n+import io.pravega.segmentstore.contracts.ReadResult;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableEntry;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.contracts.tables.TableStore;\n+import lombok.AccessLevel;\n+import lombok.Getter;\n+\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+/**\n+ * A wrapper class to StreamSegmentStore and TableStore to track the segments being created or deleted. The list of segments\n+ * obtained during this process is used in RestoreBackUpDataRecoveryTest to wait for segments to be flushed to the long term storage.\n+ */\n+public class SegmentsTracker implements StreamSegmentStore, TableStore {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 39}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyODQwMA==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470428400", "createdAt": "2020-08-14T06:08:31Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/SegmentsTracker.java", "diffHunk": "@@ -0,0 +1,179 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server;\n+\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.BufferView;\n+import io.pravega.segmentstore.contracts.AttributeUpdate;\n+import io.pravega.segmentstore.contracts.MergeStreamSegmentResult;\n+import io.pravega.segmentstore.contracts.ReadResult;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentStore;\n+import io.pravega.segmentstore.contracts.tables.IteratorArgs;\n+import io.pravega.segmentstore.contracts.tables.IteratorItem;\n+import io.pravega.segmentstore.contracts.tables.TableEntry;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.contracts.tables.TableStore;\n+import lombok.AccessLevel;\n+import lombok.Getter;\n+\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.UUID;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ConcurrentHashMap;\n+\n+/**\n+ * A wrapper class to StreamSegmentStore and TableStore to track the segments being created or deleted. The list of segments\n+ * obtained during this process is used in RestoreBackUpDataRecoveryTest to wait for segments to be flushed to the long term storage.\n+ */\n+public class SegmentsTracker implements StreamSegmentStore, TableStore {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2NzU4Nw=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDA0NTI0OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo1Mjo1MFrOG_0Jrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo1Mjo1MFrOG_0Jrg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2NzkxOA==", "bodyText": "Delete this. You can use executorService()", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469567918", "createdAt": "2020-08-12T21:52:50Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "diffHunk": "@@ -0,0 +1,315 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.mocks.InMemoryDurableDataLogFactory;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorage;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorageFactory;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.ByteArrayInputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+/**\n+ * Tests for DebugStreamSegmentContainer class.\n+ */\n+@Slf4j\n+public class DebugStreamSegmentContainerTests extends ThreadPooledTestSuite {\n+    private static final int MIN_SEGMENT_LENGTH = 0; // Used in randomly generating the length for a segment\n+    private static final int MAX_SEGMENT_LENGTH = 10100; // Used in randomly generating the length for a segment\n+    private static final int CONTAINER_ID = 1234567;\n+    private static final int EXPECTED_PINNED_SEGMENT_COUNT = 1;\n+    private static final int MAX_DATA_LOG_APPEND_SIZE = 100 * 1024;\n+    private static final int TEST_TIMEOUT_MILLIS = 60 * 1000;\n+    private static final Duration TIMEOUT = Duration.ofMillis(TEST_TIMEOUT_MILLIS);\n+    private static final Random RANDOM = new Random(1234);\n+    private static final int THREAD_POOL_COUNT = 30;\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, 10 * 60)\n+            .build();\n+\n+    private static final DurableLogConfig DEFAULT_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10 * 1024 * 1024L)\n+            .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+            .build();\n+\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig DEFAULT_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+            .build();\n+    private static final ContainerConfig CONTAINER_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+            .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 200 + EXPECTED_PINNED_SEGMENT_COUNT)\n+            .build();\n+    private ScheduledExecutorService executorService;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 115}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkzNDA0NjAwOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMlQyMTo1MzowOVrOG_0KHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQwNjowODo0NVrOHAorOg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2ODAyOQ==", "bodyText": "You do not need this. Remove these 2 methods.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r469568029", "createdAt": "2020-08-12T21:53:09Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "diffHunk": "@@ -0,0 +1,315 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.mocks.InMemoryDurableDataLogFactory;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorage;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorageFactory;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.ByteArrayInputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+/**\n+ * Tests for DebugStreamSegmentContainer class.\n+ */\n+@Slf4j\n+public class DebugStreamSegmentContainerTests extends ThreadPooledTestSuite {\n+    private static final int MIN_SEGMENT_LENGTH = 0; // Used in randomly generating the length for a segment\n+    private static final int MAX_SEGMENT_LENGTH = 10100; // Used in randomly generating the length for a segment\n+    private static final int CONTAINER_ID = 1234567;\n+    private static final int EXPECTED_PINNED_SEGMENT_COUNT = 1;\n+    private static final int MAX_DATA_LOG_APPEND_SIZE = 100 * 1024;\n+    private static final int TEST_TIMEOUT_MILLIS = 60 * 1000;\n+    private static final Duration TIMEOUT = Duration.ofMillis(TEST_TIMEOUT_MILLIS);\n+    private static final Random RANDOM = new Random(1234);\n+    private static final int THREAD_POOL_COUNT = 30;\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, 10 * 60)\n+            .build();\n+\n+    private static final DurableLogConfig DEFAULT_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10 * 1024 * 1024L)\n+            .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+            .build();\n+\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig DEFAULT_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+            .build();\n+    private static final ContainerConfig CONTAINER_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+            .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 200 + EXPECTED_PINNED_SEGMENT_COUNT)\n+            .build();\n+    private ScheduledExecutorService executorService;\n+\n+    @Rule\n+    public Timeout globalTimeout = Timeout.millis(TEST_TIMEOUT_MILLIS);\n+\n+    @Before\n+    public void setUp() {\n+        this.executorService = executorService();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDQyODQ3NA==", "bodyText": "Done.", "url": "https://github.com/pravega/pravega/pull/4716#discussion_r470428474", "createdAt": "2020-08-14T06:08:45Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/containers/DebugStreamSegmentContainerTests.java", "diffHunk": "@@ -0,0 +1,315 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.concurrent.Services;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.server.CacheManager;\n+import io.pravega.segmentstore.server.CachePolicy;\n+import io.pravega.segmentstore.server.OperationLogFactory;\n+import io.pravega.segmentstore.server.ReadIndexFactory;\n+import io.pravega.segmentstore.server.SegmentContainer;\n+import io.pravega.segmentstore.server.SegmentContainerFactory;\n+import io.pravega.segmentstore.server.WriterFactory;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexConfig;\n+import io.pravega.segmentstore.server.attributes.AttributeIndexFactory;\n+import io.pravega.segmentstore.server.attributes.ContainerAttributeIndexFactoryImpl;\n+import io.pravega.segmentstore.server.logs.DurableLogConfig;\n+import io.pravega.segmentstore.server.logs.DurableLogFactory;\n+import io.pravega.segmentstore.server.reading.ContainerReadIndexFactory;\n+import io.pravega.segmentstore.server.reading.ReadIndexConfig;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtension;\n+import io.pravega.segmentstore.server.tables.ContainerTableExtensionImpl;\n+import io.pravega.segmentstore.server.writer.StorageWriterFactory;\n+import io.pravega.segmentstore.server.writer.WriterConfig;\n+import io.pravega.segmentstore.storage.AsyncStorageWrapper;\n+import io.pravega.segmentstore.storage.DurableDataLogFactory;\n+import io.pravega.segmentstore.storage.SegmentRollingPolicy;\n+import io.pravega.segmentstore.storage.StorageFactory;\n+import io.pravega.segmentstore.storage.cache.CacheStorage;\n+import io.pravega.segmentstore.storage.cache.DirectMemoryCache;\n+import io.pravega.segmentstore.storage.mocks.InMemoryDurableDataLogFactory;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorage;\n+import io.pravega.segmentstore.storage.mocks.InMemoryStorageFactory;\n+import io.pravega.segmentstore.storage.rolling.RollingStorage;\n+import io.pravega.shared.segment.SegmentToContainerMapper;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import lombok.Cleanup;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+import org.junit.After;\n+import org.junit.Assert;\n+import org.junit.Before;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.Timeout;\n+\n+import java.io.ByteArrayInputStream;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+\n+/**\n+ * Tests for DebugStreamSegmentContainer class.\n+ */\n+@Slf4j\n+public class DebugStreamSegmentContainerTests extends ThreadPooledTestSuite {\n+    private static final int MIN_SEGMENT_LENGTH = 0; // Used in randomly generating the length for a segment\n+    private static final int MAX_SEGMENT_LENGTH = 10100; // Used in randomly generating the length for a segment\n+    private static final int CONTAINER_ID = 1234567;\n+    private static final int EXPECTED_PINNED_SEGMENT_COUNT = 1;\n+    private static final int MAX_DATA_LOG_APPEND_SIZE = 100 * 1024;\n+    private static final int TEST_TIMEOUT_MILLIS = 60 * 1000;\n+    private static final Duration TIMEOUT = Duration.ofMillis(TEST_TIMEOUT_MILLIS);\n+    private static final Random RANDOM = new Random(1234);\n+    private static final int THREAD_POOL_COUNT = 30;\n+    private static final ContainerConfig DEFAULT_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, 10 * 60)\n+            .build();\n+\n+    private static final DurableLogConfig DEFAULT_DURABLE_LOG_CONFIG = DurableLogConfig\n+            .builder()\n+            .with(DurableLogConfig.CHECKPOINT_MIN_COMMIT_COUNT, 1)\n+            .with(DurableLogConfig.CHECKPOINT_COMMIT_COUNT, 10)\n+            .with(DurableLogConfig.CHECKPOINT_TOTAL_COMMIT_LENGTH, 10 * 1024 * 1024L)\n+            .with(DurableLogConfig.START_RETRY_DELAY_MILLIS, 20)\n+            .build();\n+\n+    private static final ReadIndexConfig DEFAULT_READ_INDEX_CONFIG = ReadIndexConfig.builder().with(ReadIndexConfig.STORAGE_READ_ALIGNMENT, 1024).build();\n+\n+    private static final AttributeIndexConfig DEFAULT_ATTRIBUTE_INDEX_CONFIG = AttributeIndexConfig\n+            .builder()\n+            .with(AttributeIndexConfig.MAX_INDEX_PAGE_SIZE, 2 * 1024)\n+            .with(AttributeIndexConfig.ATTRIBUTE_SEGMENT_ROLLING_SIZE, 1000)\n+            .build();\n+\n+    private static final WriterConfig DEFAULT_WRITER_CONFIG = WriterConfig\n+            .builder()\n+            .with(WriterConfig.FLUSH_THRESHOLD_BYTES, 1)\n+            .with(WriterConfig.FLUSH_ATTRIBUTES_THRESHOLD, 3)\n+            .with(WriterConfig.FLUSH_THRESHOLD_MILLIS, 25L)\n+            .with(WriterConfig.MIN_READ_TIMEOUT_MILLIS, 10L)\n+            .with(WriterConfig.MAX_READ_TIMEOUT_MILLIS, 250L)\n+            .build();\n+    private static final ContainerConfig CONTAINER_CONFIG = ContainerConfig\n+            .builder()\n+            .with(ContainerConfig.SEGMENT_METADATA_EXPIRATION_SECONDS, (int) DEFAULT_CONFIG.getSegmentMetadataExpiration().getSeconds())\n+            .with(ContainerConfig.MAX_ACTIVE_SEGMENT_COUNT, 200 + EXPECTED_PINNED_SEGMENT_COUNT)\n+            .build();\n+    private ScheduledExecutorService executorService;\n+\n+    @Rule\n+    public Timeout globalTimeout = Timeout.millis(TEST_TIMEOUT_MILLIS);\n+\n+    @Before\n+    public void setUp() {\n+        this.executorService = executorService();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2OTU2ODAyOQ=="}, "originalCommit": {"oid": "644a58c4d832e2bfcc8bace1025eff68f12f7493"}, "originalPosition": 122}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4280, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}