{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDExNzQxMDI0", "number": 4763, "reviewThreads": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMDozMjozMFrOEAqlzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMzozNDoyOFrOEBvykg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MTMzMjYxOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/TableService.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMDozMjozMFrOGcFsfw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNTozOToyNlrOGch-8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEwNjYyMw==", "bodyText": "It is not entirely clear to me why we are making this optional. Could we make all table segments sorted rather than deciding whether it needs to be sorted upon creation?", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r432106623", "createdAt": "2020-05-28T20:32:30Z", "author": {"login": "fpj"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/TableService.java", "diffHunk": "@@ -48,9 +48,9 @@ public TableService(SegmentContainerRegistry segmentContainerRegistry, SegmentTo\n     //region TableStore Implementation\n \n     @Override\n-    public CompletableFuture<Void> createSegment(String segmentName, Duration timeout) {\n+    public CompletableFuture<Void> createSegment(String segmentName, boolean sorted, Duration timeout) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE3MDkyOA==", "bodyText": "We could very well do that, but sorting doesn't come for free. There will be additional space requirement (keep all keys in sorted order) and some computational overhead to maintain that sorted index. Not all Table Segments may want that. If all they want is a Hash-Table-like structure with no order between keys, then regular Table Segments will do the job without the overhead of sorted index.\nAdditionally, there is no upgrade path for Hash Table Segments to Sorted Table Segments. IF we converted all HTS into STS, then we'd have to auto-upgrade existing Segments. That would be another challenge by its own.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r432170928", "createdAt": "2020-05-28T23:04:14Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/TableService.java", "diffHunk": "@@ -48,9 +48,9 @@ public TableService(SegmentContainerRegistry segmentContainerRegistry, SegmentTo\n     //region TableStore Implementation\n \n     @Override\n-    public CompletableFuture<Void> createSegment(String segmentName, Duration timeout) {\n+    public CompletableFuture<Void> createSegment(String segmentName, boolean sorted, Duration timeout) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEwNjYyMw=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjQxODUzNg==", "bodyText": "I guess we don't have a concrete answer to this, but I'm wondering about the recommendation we would give about the use of STS, like what's a reasonable number of sorted tables or what is a reasonable amount of traffic to guide users.\nThe migration is indeed a concern.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r432418536", "createdAt": "2020-05-29T11:18:53Z", "author": {"login": "fpj"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/TableService.java", "diffHunk": "@@ -48,9 +48,9 @@ public TableService(SegmentContainerRegistry segmentContainerRegistry, SegmentTo\n     //region TableStore Implementation\n \n     @Override\n-    public CompletableFuture<Void> createSegment(String segmentName, Duration timeout) {\n+    public CompletableFuture<Void> createSegment(String segmentName, boolean sorted, Duration timeout) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEwNjYyMw=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU3MDA5Nw==", "bodyText": "For now, I think we should keep all metadata (Segment Store and Controller) in hash tables - there is no need for sorted keys there.\nFor KVTs, we will use STSs because we want to use iterators. In the (near) future, we can add an option at the creation of KVTs that says \"I don't need iterators\" which should result in the use of Hash Table Segments as backing store (the API is identical between STS and HTS).\n\nI say \"near future\" because KVTs are going out as Beta, which gives us certain freedoms in altering the API without need for backwards compatibility while having this tag.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r432570097", "createdAt": "2020-05-29T15:39:26Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/TableService.java", "diffHunk": "@@ -48,9 +48,9 @@ public TableService(SegmentContainerRegistry segmentContainerRegistry, SegmentTo\n     //region TableStore Implementation\n \n     @Override\n-    public CompletableFuture<Void> createSegment(String segmentName, Duration timeout) {\n+    public CompletableFuture<Void> createSegment(String segmentName, boolean sorted, Duration timeout) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEwNjYyMw=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MTM0MjM3OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/SegmentSortedKeyIndex.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMDozNTo0OVrOGcFyuw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxMToxNjowMVrOGcYp_w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEwODIxOQ==", "bodyText": "If the sorted key index is updated separately from the table segment index, then is there a chance that the user of this table sees an update in one but not in the other? For example, a regular lookup can return a key that is not present in a range query.\nPerhaps I'm missing an overall view of the flow for the calls in this interface.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r432108219", "createdAt": "2020-05-28T20:35:49Z", "author": {"login": "fpj"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/SegmentSortedKeyIndex.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.tables;\n+\n+import com.google.common.annotations.Beta;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import javax.annotation.Nullable;\n+import lombok.Data;\n+\n+/**\n+ * Defines an index that maintains a Table Segment's Keys in lexicographic bitwise order.\n+ */\n+@Beta\n+interface SegmentSortedKeyIndex {\n+    /**\n+     * Include and persist updates that have been included in a Table Segment Index.\n+     *\n+     * @param bucketUpdates A Collection of {@link BucketUpdate} instances that reflect what keys have been added and/or\n+     *                      removed.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation has completed.\n+     */\n+    CompletableFuture<Void> persistUpdate(Collection<BucketUpdate> bucketUpdates, Duration timeout);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE3MTg3NA==", "bodyText": "The Sorted Key Index (SKI) is updated just like the regular Container Key Index (CKI). There is a tail section (you found it below), which gets updated every time we get a user update. The non-tail section (indexed section) gets updated by the WriterTableProcessor when it indexes the next entries (to be indexed) from the tail. At this point, the tail is drained of those Keys, which are then only available in the indexed portion.\nWe update the SKI (via tail portion below) before we ack to the user. As such, the Key will be visible to the user both via the regular (get) API and in the Key/Entry iterator; there will be no inconsistency. The persistUpdate call is done in the background and is completely transparent to upstream code, and hence to the user.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r432171874", "createdAt": "2020-05-28T23:07:22Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/SegmentSortedKeyIndex.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.tables;\n+\n+import com.google.common.annotations.Beta;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import javax.annotation.Nullable;\n+import lombok.Data;\n+\n+/**\n+ * Defines an index that maintains a Table Segment's Keys in lexicographic bitwise order.\n+ */\n+@Beta\n+interface SegmentSortedKeyIndex {\n+    /**\n+     * Include and persist updates that have been included in a Table Segment Index.\n+     *\n+     * @param bucketUpdates A Collection of {@link BucketUpdate} instances that reflect what keys have been added and/or\n+     *                      removed.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation has completed.\n+     */\n+    CompletableFuture<Void> persistUpdate(Collection<BucketUpdate> bucketUpdates, Duration timeout);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEwODIxOQ=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjQxNzI3OQ==", "bodyText": "Ok, makes sense.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r432417279", "createdAt": "2020-05-29T11:16:01Z", "author": {"login": "fpj"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/SegmentSortedKeyIndex.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.tables;\n+\n+import com.google.common.annotations.Beta;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import javax.annotation.Nullable;\n+import lombok.Data;\n+\n+/**\n+ * Defines an index that maintains a Table Segment's Keys in lexicographic bitwise order.\n+ */\n+@Beta\n+interface SegmentSortedKeyIndex {\n+    /**\n+     * Include and persist updates that have been included in a Table Segment Index.\n+     *\n+     * @param bucketUpdates A Collection of {@link BucketUpdate} instances that reflect what keys have been added and/or\n+     *                      removed.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation has completed.\n+     */\n+    CompletableFuture<Void> persistUpdate(Collection<BucketUpdate> bucketUpdates, Duration timeout);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEwODIxOQ=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MTQ1MzU4OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/SegmentSortedKeyIndex.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMToxMTo0N1rOGcG4xA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNTo0MTowOVrOGciDUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEyNjE0OA==", "bodyText": "The next three calls are to maintain a tail cache, if I understand it right. By reading these, it is not clear to me the expected flow of these calls for a user of this interface, and it is also not not clear what the consistency guarantee is.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r432126148", "createdAt": "2020-05-28T21:11:47Z", "author": {"login": "fpj"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/SegmentSortedKeyIndex.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.tables;\n+\n+import com.google.common.annotations.Beta;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import javax.annotation.Nullable;\n+import lombok.Data;\n+\n+/**\n+ * Defines an index that maintains a Table Segment's Keys in lexicographic bitwise order.\n+ */\n+@Beta\n+interface SegmentSortedKeyIndex {\n+    /**\n+     * Include and persist updates that have been included in a Table Segment Index.\n+     *\n+     * @param bucketUpdates A Collection of {@link BucketUpdate} instances that reflect what keys have been added and/or\n+     *                      removed.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation has completed.\n+     */\n+    CompletableFuture<Void> persistUpdate(Collection<BucketUpdate> bucketUpdates, Duration timeout);\n+\n+    /**\n+     * Includes the given {@link TableKeyBatch} which contains Keys that have recently been updated or removed, but not\n+     * yet indexed. These will be stored in a memory data structure until {@link #updateSegmentIndexOffset} will be invoked\n+     * with an offset that exceeds their offset.\n+     *\n+     * @param batch              The {@link TableKeyBatch} to include.\n+     * @param batchSegmentOffset The offset of the {@link TableKeyBatch}.\n+     */\n+    void includeTailUpdate(TableKeyBatch batch, long batchSegmentOffset);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE3Mzg4Nw==", "bodyText": "This mirrors ContainerKeyIndex and ContainerKeyCache.\n\nincludeTailUpdate is called every time we insert, update or remove a key (from the API)\nincludeTailCache is called (optionally) after a container failover, if someone wants to access the Segment prior to it being fully indexed by the WriterTableProcessor (via persistUpdate.\n\nWhy is this important? Because the in-memory part is volatile; it is gone after a crash. We accumulate data into it via includeTailUpdate, but that information is gone after a crash. There is special code in the ContainerKeyIndex (pre-existing; not part of this PR) that auto-recovers a Table Segment if it is accessed immediately after a recovery but before it is indexed. (If you remember, we had a few problems last year when the Controller was complaining that Table Segment data was unavailable shortly after a recovery - this pre-recovery was introduced to handle that kind of problems).\n\n\nupdateSegmentIndexOffset is called by the WriterTableProcessor (indirectly, via ContainerKeyIndex to tell the SKI that everything prior to a certain offset has been indexed and it can remove it from the \"tail cache\" (i.e., if anyone needs that info, they can get it from the actual index, and no need to keep a heap data structure filled with redundant info).\n\nDoes this explanation make sense? These are all pre-existing concepts, and have just been applied to the SKI. The consumers of this interface are the ContainerKeyIndex, WriterTableProcessor which work in tandem to offer a unified view of the TableSegment to the upstream code.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r432173887", "createdAt": "2020-05-28T23:13:48Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/SegmentSortedKeyIndex.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.tables;\n+\n+import com.google.common.annotations.Beta;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import javax.annotation.Nullable;\n+import lombok.Data;\n+\n+/**\n+ * Defines an index that maintains a Table Segment's Keys in lexicographic bitwise order.\n+ */\n+@Beta\n+interface SegmentSortedKeyIndex {\n+    /**\n+     * Include and persist updates that have been included in a Table Segment Index.\n+     *\n+     * @param bucketUpdates A Collection of {@link BucketUpdate} instances that reflect what keys have been added and/or\n+     *                      removed.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation has completed.\n+     */\n+    CompletableFuture<Void> persistUpdate(Collection<BucketUpdate> bucketUpdates, Duration timeout);\n+\n+    /**\n+     * Includes the given {@link TableKeyBatch} which contains Keys that have recently been updated or removed, but not\n+     * yet indexed. These will be stored in a memory data structure until {@link #updateSegmentIndexOffset} will be invoked\n+     * with an offset that exceeds their offset.\n+     *\n+     * @param batch              The {@link TableKeyBatch} to include.\n+     * @param batchSegmentOffset The offset of the {@link TableKeyBatch}.\n+     */\n+    void includeTailUpdate(TableKeyBatch batch, long batchSegmentOffset);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEyNjE0OA=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjQxNjc3MQ==", "bodyText": "What actually got me to ask about it is whether this distinction between cached vs. indexed  belongs in the interface or whether it is an implementation detail leaking to the interface. But, if this is symmetric to an existing concept, then there is no point in arguing about it for this PR. At best, we should think about it separately.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r432416771", "createdAt": "2020-05-29T11:14:54Z", "author": {"login": "fpj"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/SegmentSortedKeyIndex.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.tables;\n+\n+import com.google.common.annotations.Beta;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import javax.annotation.Nullable;\n+import lombok.Data;\n+\n+/**\n+ * Defines an index that maintains a Table Segment's Keys in lexicographic bitwise order.\n+ */\n+@Beta\n+interface SegmentSortedKeyIndex {\n+    /**\n+     * Include and persist updates that have been included in a Table Segment Index.\n+     *\n+     * @param bucketUpdates A Collection of {@link BucketUpdate} instances that reflect what keys have been added and/or\n+     *                      removed.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation has completed.\n+     */\n+    CompletableFuture<Void> persistUpdate(Collection<BucketUpdate> bucketUpdates, Duration timeout);\n+\n+    /**\n+     * Includes the given {@link TableKeyBatch} which contains Keys that have recently been updated or removed, but not\n+     * yet indexed. These will be stored in a memory data structure until {@link #updateSegmentIndexOffset} will be invoked\n+     * with an offset that exceeds their offset.\n+     *\n+     * @param batch              The {@link TableKeyBatch} to include.\n+     * @param batchSegmentOffset The offset of the {@link TableKeyBatch}.\n+     */\n+    void includeTailUpdate(TableKeyBatch batch, long batchSegmentOffset);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEyNjE0OA=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU3MTIxOQ==", "bodyText": "While this is an interface, it's an internal one, and the whole point of it was to be able to mock it during unit tests. This is buried so deep within the stack that it will be hard for anyone to surface this if they want to.\nYes, it exposes details about the implementation, but this is the implementation.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r432571219", "createdAt": "2020-05-29T15:41:09Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/SegmentSortedKeyIndex.java", "diffHunk": "@@ -0,0 +1,141 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.tables;\n+\n+import com.google.common.annotations.Beta;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import java.time.Duration;\n+import java.util.Collection;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.concurrent.CompletableFuture;\n+import javax.annotation.Nullable;\n+import lombok.Data;\n+\n+/**\n+ * Defines an index that maintains a Table Segment's Keys in lexicographic bitwise order.\n+ */\n+@Beta\n+interface SegmentSortedKeyIndex {\n+    /**\n+     * Include and persist updates that have been included in a Table Segment Index.\n+     *\n+     * @param bucketUpdates A Collection of {@link BucketUpdate} instances that reflect what keys have been added and/or\n+     *                      removed.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation has completed.\n+     */\n+    CompletableFuture<Void> persistUpdate(Collection<BucketUpdate> bucketUpdates, Duration timeout);\n+\n+    /**\n+     * Includes the given {@link TableKeyBatch} which contains Keys that have recently been updated or removed, but not\n+     * yet indexed. These will be stored in a memory data structure until {@link #updateSegmentIndexOffset} will be invoked\n+     * with an offset that exceeds their offset.\n+     *\n+     * @param batch              The {@link TableKeyBatch} to include.\n+     * @param batchSegmentOffset The offset of the {@link TableKeyBatch}.\n+     */\n+    void includeTailUpdate(TableKeyBatch batch, long batchSegmentOffset);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjEyNjE0OA=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MzM5MjcxOnYy", "diffSide": "RIGHT", "path": "common/src/main/java/io/pravega/common/util/btree/sets/BTreeSet.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxMTo1ODozNlrOGcZwwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQwNzozMzowOFrOGdlgGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjQzNTM5Mw==", "bodyText": "I understand that there are different properties and API we want for this set implementation vs. the index implementation backed by a B+ tree. I'm wondering if there is any opportunity for consolidation as they are both backed by a B+ tree. I haven't done a close comparison, but at a high-level sounds possible. Any insight you can share?", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r432435393", "createdAt": "2020-05-29T11:58:36Z", "author": {"login": "fpj"}, "path": "common/src/main/java/io/pravega/common/util/btree/sets/BTreeSet.java", "diffHunk": "@@ -0,0 +1,414 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.common.util.btree.sets;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArrayComparator;\n+import java.time.Duration;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Supplier;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.NotThreadSafe;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+/**\n+ * A B+Tree-backed Set. Stores all items in a B+Tree Structure using a {@link ByteArrayComparator} for ordering them.\n+ *\n+ * NOTE: This component is in {@link Beta}. There are no guarantees about data or API compatibility with future versions.\n+ * Any component that is directly dependent on this one should either be in {@link Beta} as well.\n+ */\n+@NotThreadSafe\n+@Beta\n+@Slf4j\n+public class BTreeSet {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU3NTM2Mg==", "bodyText": "If I haven't already, we should file a follow-up issue to unify as much as we can between the existing BTreeIndex and this BTreeSet.\nI tried to do that during development of this class, however they are so different that I preferred to keep them apart.\nWhy are they different?\n\nBTreeIndex is a Key-Value index with all keys of the same length and all values of the same length. BTree set has no values and allows variable-length keys.\n\nThis allows BTreeIndex Nodes/Pages to more efficiently pack the data together (which is necessary (see next bullet)) and execute binary searches for lookups. For BTreeSet I have to keep a header with pointers to data so I can \"jump\" to an entry.\n\n\nBTreeIndex is meant to store data on an append-only medium, while BTreeSet couldn't care less (literally). BtreeIndex is stored directly on Tier2/LTS, while BTreeSet uses the Hash Table Segment itself to store nodes (each node is a Key-Value entry in the Table Segment it keeps keys for). As such, BTreeSet does a lot less work in managing data integrity and compaction (it does none of that!) since it delegates all of it to the Table Segment underneath it.\n\nDue to the fact that BTreeIndex stores on Append only media, it has to deal with write amplification so the amount of data that we need to write must be kept to a minimum, hence the branching factor of the node. BTreeIndex has a max node/page size of 32KB, which gives it a branching factor of about 1000 (more or less), thus reducing the depth of the tree and the size of the writes on updates.\nBTreeSet has no such limitation so its max page/node size is whatever the TableSegment allows as values (1MB as of today). Given that it has variable-length keys, the branching factor will vary, however there is a lower bound on it due to the fact that keys themselves have an upper bound on their length.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r432575362", "createdAt": "2020-05-29T15:48:16Z", "author": {"login": "andreipaduroiu"}, "path": "common/src/main/java/io/pravega/common/util/btree/sets/BTreeSet.java", "diffHunk": "@@ -0,0 +1,414 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.common.util.btree.sets;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArrayComparator;\n+import java.time.Duration;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Supplier;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.NotThreadSafe;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+/**\n+ * A B+Tree-backed Set. Stores all items in a B+Tree Structure using a {@link ByteArrayComparator} for ordering them.\n+ *\n+ * NOTE: This component is in {@link Beta}. There are no guarantees about data or API compatibility with future versions.\n+ * Any component that is directly dependent on this one should either be in {@link Beta} as well.\n+ */\n+@NotThreadSafe\n+@Beta\n+@Slf4j\n+public class BTreeSet {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjQzNTM5Mw=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzY3NjMxNA==", "bodyText": "Ok, I searched and couldn't find an issue, so I created #4840, and assigned it to you for now, feel free to reassign as needed. You can resolve this thread once you see this message, I want to make sure you see the created issue.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r433676314", "createdAt": "2020-06-02T07:33:08Z", "author": {"login": "fpj"}, "path": "common/src/main/java/io/pravega/common/util/btree/sets/BTreeSet.java", "diffHunk": "@@ -0,0 +1,414 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.common.util.btree.sets;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArrayComparator;\n+import java.time.Duration;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Supplier;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.NotThreadSafe;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+/**\n+ * A B+Tree-backed Set. Stores all items in a B+Tree Structure using a {@link ByteArrayComparator} for ordering them.\n+ *\n+ * NOTE: This component is in {@link Beta}. There are no guarantees about data or API compatibility with future versions.\n+ * Any component that is directly dependent on this one should either be in {@link Beta} as well.\n+ */\n+@NotThreadSafe\n+@Beta\n+@Slf4j\n+public class BTreeSet {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjQzNTM5Mw=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MzY4NTI0OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/ContainerTableExtensionImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxMzoyODo0OVrOGccofA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxNTo1MToyMFrOGcia5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjQ4MjQyOA==", "bodyText": "If ai understand the flow right, ContainerTableExtensionImpl implements ContainerTableExtension, which implements TableStore. keyIterator and entryIterator are calls in the TableStore interface, so that's how code using table segments use the sorted table segments.\nSegmentSortedKeyIndex is made available via ContainerKeyIndex, which is used in this class. This is how the TableStore interface connects with the SegmentSortedKeyIndex, is it right?", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r432482428", "createdAt": "2020-05-29T13:28:49Z", "author": {"login": "fpj"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/ContainerTableExtensionImpl.java", "diffHunk": "@@ -269,16 +311,49 @@ public void close() {\n         return builder.getResultFutures();\n     }\n \n+    @SuppressWarnings(\"unchecked\")\n+    private <T, V extends Collection<T>> V translateItems(V items, SegmentProperties segmentInfo, boolean isExternal,\n+                                                          BiFunction<KeyTranslator, T, T> translateFunction) {\n+        if (!ContainerSortedKeyIndex.isSortedTableSegment(segmentInfo)) {\n+            // Nothing to translate for non-sorted segments.\n+            return items;\n+        }\n+\n+        val t = isExternal ? SortedKeyIndexDataSource.EXTERNAL_TRANSLATOR : SortedKeyIndexDataSource.INTERNAL_TRANSLATOR;\n+        return (V) items.stream().map(i -> translateItem(i, t, translateFunction)).collect(Collectors.toList());\n+    }\n+\n+    private <T> T translateItem(T item, KeyTranslator translator, BiFunction<KeyTranslator, T, T> translateItem) {\n+        return item == null ? null : translateItem.apply(translator, item);\n+    }\n+\n     @Override\n     public CompletableFuture<AsyncIterator<IteratorItem<TableKey>>> keyIterator(String segmentName, IteratorArgs args) {\n-        logRequest(\"keyIterator\", segmentName);\n-        return newIterator(segmentName, args, TableBucketReader::key);\n+        return this.segmentContainer.forSegment(segmentName, args.getFetchTimeout())\n+                .thenComposeAsync(segment -> {\n+                    if (ContainerSortedKeyIndex.isSortedTableSegment(segment.getInfo())) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU3NzI1NQ==", "bodyText": "First paragraph is 100% spot on.\nSecond paragraph:\n\nSortedKeyIndex is indeed exposed via ContainerKeyIndex, but it is only used in ContainerTableExtensionImpl, which, depending on whether the Segment is Sorted or not, uses it or uses the default iterator.\nThe connection is done in the ContainerTableExtensionImpl and again (in the back-end) in WriterTableProcessor that invokes its persistUpdate API to insert or remove keys (only when keys are inserted or removed, but not on update).\n\n\nClarification in case I didn't explain it properly: the BTreeSet behind the SKI is only updated when a Key is inserted or removed. It is not touched when a Key is updated. Hence the frequency of writes to that data structure is a function of inserts or deletes, and not in-place updates.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r432577255", "createdAt": "2020-05-29T15:51:20Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/ContainerTableExtensionImpl.java", "diffHunk": "@@ -269,16 +311,49 @@ public void close() {\n         return builder.getResultFutures();\n     }\n \n+    @SuppressWarnings(\"unchecked\")\n+    private <T, V extends Collection<T>> V translateItems(V items, SegmentProperties segmentInfo, boolean isExternal,\n+                                                          BiFunction<KeyTranslator, T, T> translateFunction) {\n+        if (!ContainerSortedKeyIndex.isSortedTableSegment(segmentInfo)) {\n+            // Nothing to translate for non-sorted segments.\n+            return items;\n+        }\n+\n+        val t = isExternal ? SortedKeyIndexDataSource.EXTERNAL_TRANSLATOR : SortedKeyIndexDataSource.INTERNAL_TRANSLATOR;\n+        return (V) items.stream().map(i -> translateItem(i, t, translateFunction)).collect(Collectors.toList());\n+    }\n+\n+    private <T> T translateItem(T item, KeyTranslator translator, BiFunction<KeyTranslator, T, T> translateItem) {\n+        return item == null ? null : translateItem.apply(translator, item);\n+    }\n+\n     @Override\n     public CompletableFuture<AsyncIterator<IteratorItem<TableKey>>> keyIterator(String segmentName, IteratorArgs args) {\n-        logRequest(\"keyIterator\", segmentName);\n-        return newIterator(segmentName, args, TableBucketReader::key);\n+        return this.segmentContainer.forSegment(segmentName, args.getFetchTimeout())\n+                .thenComposeAsync(segment -> {\n+                    if (ContainerSortedKeyIndex.isSortedTableSegment(segment.getInfo())) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjQ4MjQyOA=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 213}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MzY5NTg1OnYy", "diffSide": "RIGHT", "path": "common/src/main/java/io/pravega/common/util/btree/sets/BTreeSet.java", "isResolved": true, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxMzozMTo1MVrOGccvkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzoxMTowOFrOGeljTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjQ4NDI0MQ==", "bodyText": "I'm curious, why you think it is a good idea to have booleans for inclusion rather than pick one option and go with it?", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r432484241", "createdAt": "2020-05-29T13:31:51Z", "author": {"login": "fpj"}, "path": "common/src/main/java/io/pravega/common/util/btree/sets/BTreeSet.java", "diffHunk": "@@ -0,0 +1,414 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.common.util.btree.sets;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArrayComparator;\n+import java.time.Duration;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Supplier;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.NotThreadSafe;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+/**\n+ * A B+Tree-backed Set. Stores all items in a B+Tree Structure using a {@link ByteArrayComparator} for ordering them.\n+ *\n+ * NOTE: This component is in {@link Beta}. There are no guarantees about data or API compatibility with future versions.\n+ * Any component that is directly dependent on this one should either be in {@link Beta} as well.\n+ */\n+@NotThreadSafe\n+@Beta\n+@Slf4j\n+public class BTreeSet {\n+    //region Members\n+\n+    public static final Comparator<ArrayView> COMPARATOR = new ByteArrayComparator()::compare;\n+    private static final Comparator<PagePointer> POINTER_COMPARATOR = PagePointer.getComparator(COMPARATOR);\n+\n+    private final int maxPageSize;\n+    private final int maxItemSize;\n+    @NonNull\n+    private final ReadPage read;\n+    @NonNull\n+    private final PersistPages update;\n+    @NonNull\n+    private final Executor executor;\n+    @NonNull\n+    private final String traceLogId;\n+\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the {@link BTreeSet} class.\n+     *\n+     * @param maxPageSize The maximum size, in bytes, of any page.\n+     * @param maxItemSize The maximum size, in bytes, of any single item in the {@link BTreeSet}.\n+     * @param read        A {@link ReadPage} function that can be used to fetch a single {@link BTreeSet} page from an\n+     *                    external data source.\n+     * @param update      A {@link PersistPages} function that can be used to store and delete multiple {@link BTreeSet}\n+     *                    pages to/from an external data source.\n+     * @param executor    Executor for async operations.\n+     * @param traceLogId  Trace id for logging.\n+     */\n+    public BTreeSet(int maxPageSize, int maxItemSize, @NonNull ReadPage read, @NonNull PersistPages update,\n+                    @NonNull Executor executor, String traceLogId) {\n+        Preconditions.checkArgument(maxItemSize < maxPageSize / 2, \"maxItemSize must be at most half of maxPageSize.\");\n+        this.maxItemSize = maxItemSize;\n+        this.maxPageSize = maxPageSize;\n+        this.read = read;\n+        this.update = update;\n+        this.executor = executor;\n+        this.traceLogId = traceLogId == null ? \"\" : traceLogId;\n+    }\n+\n+    //endregion\n+\n+    //region Updates\n+\n+    /**\n+     * Atomically inserts the items in 'toInsert' into the {@link BTreeSet} and removes the items in 'toRemove'\n+     * from the {@link BTreeSet}. No duplicates are allowed; the same item cannot exist multiple times in either 'toInsert'\n+     * or 'toRemove' or in both of them.\n+     *\n+     * @param toInsert      (Optional). A Collection of {@link ArrayView} instances representing the items to insert.\n+     *                      If an item is already present, it will not be reinserted (updates are idempotent).\n+     * @param toRemove      (Optional). A Collection of {@link ArrayView} instances representing the items to remove.\n+     * @param getNextPageId A Supplier that, when invoked, will return a unique number representing the Id of the next\n+     *                      {@link BTreeSet} page that has to be generated.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed normally, will indicate that the updates have been applied\n+     * successfully. If the operation failed, the Future will be completed with the appropriate exception.\n+     */\n+    public CompletableFuture<Void> update(@Nullable Collection<? extends ArrayView> toInsert, @Nullable Collection<? extends ArrayView> toRemove,\n+                                          @NonNull Supplier<Long> getNextPageId, @NonNull Duration timeout) {\n+        TimeoutTimer timer = new TimeoutTimer(timeout);\n+        val updates = new ArrayList<UpdateItem>();\n+        int insertCount = collectUpdates(toInsert, false, updates);\n+        int removeCount = collectUpdates(toRemove, true, updates);\n+        updates.sort(UpdateItem::compareTo);\n+        log.debug(\"{}: Update (Insert={}, Remove={}).\", this.traceLogId, insertCount, removeCount);\n+        if (updates.isEmpty()) {\n+            // Nothing to do.\n+            return CompletableFuture.completedFuture(null);\n+        }\n+\n+        // The updates are sorted, so any empty items will be placed first.\n+        Preconditions.checkArgument(updates.get(0).getItem().getLength() > 0, \"No empty items allowed.\");\n+        return applyUpdates(updates.iterator(), timer)\n+                .thenApply(pageCollection -> processModifiedPages(pageCollection, getNextPageId))\n+                .thenComposeAsync(pageCollection -> writePages(pageCollection, timer), this.executor);\n+    }\n+\n+    private int collectUpdates(Collection<? extends ArrayView> items, boolean isRemoval, List<UpdateItem> updates) {\n+        if (items == null) {\n+            return 0;\n+        }\n+\n+        for (val i : items) {\n+            Preconditions.checkArgument(i.getLength() <= this.maxItemSize,\n+                    \"Item exceeds maximum allowed length (%s).\", this.maxItemSize);\n+            updates.add(new UpdateItem(i, isRemoval));\n+        }\n+        return items.size();\n+    }\n+\n+    private CompletableFuture<PageCollection> applyUpdates(Iterator<UpdateItem> items, TimeoutTimer timer) {\n+        val pageCollection = new PageCollection();\n+        val lastPage = new AtomicReference<BTreeSetPage.LeafPage>(null);\n+        val lastPageUpdates = new ArrayList<UpdateItem>();\n+        return Futures.loop(\n+                items::hasNext,\n+                () -> {\n+                    // Locate the page where the update is to be executed. Do not apply it yet as it is more efficient\n+                    // to bulk-apply multiple at once. Collect all updates for each Page, and only apply them once we have\n+                    // \"moved on\" to another page.\n+                    val next = items.next();\n+                    return locatePage(next.getItem(), pageCollection, timer)\n+                            .thenAccept(page -> {\n+                                val last = lastPage.get();\n+                                if (page != last) {\n+                                    // This key goes to a different page than the one we were looking at.\n+                                    if (last != null) {\n+                                        // Commit the outstanding updates.\n+                                        last.update(lastPageUpdates);\n+                                    }\n+\n+                                    // Update the pointers.\n+                                    lastPage.set(page);\n+                                    lastPageUpdates.clear();\n+                                }\n+\n+                                // Record the current update.\n+                                lastPageUpdates.add(next);\n+                            });\n+                },\n+                this.executor)\n+                .thenApplyAsync(v -> {\n+                    // We need not forget to apply the last batch of updates from the last page.\n+                    if (lastPage.get() != null) {\n+                        lastPage.get().update(lastPageUpdates);\n+                    }\n+                    return pageCollection;\n+                }, this.executor);\n+    }\n+\n+    private PageCollection processModifiedPages(PageCollection pageCollection, Supplier<Long> getNewPageId) {\n+        Collection<BTreeSetPage> candidates = pageCollection.getLeafPages();\n+        while (!candidates.isEmpty()) {\n+            // Process each candidate and determine if it should be deleted or split into multiple pages.\n+            val tmc = new TreeModificationContext(pageCollection);\n+            for (BTreeSetPage p : candidates) {\n+                if (p.getItemCount() == 0) {\n+                    deletePage(p, tmc);\n+                } else {\n+                    splitPageIfNecessary(p, getNewPageId, tmc);\n+                }\n+            }\n+\n+            // Update those pages' parents.\n+            tmc.accept(BTreeSetPage.IndexPage::addChildren, BTreeSetPage.IndexPage::removeChildren, POINTER_COMPARATOR);\n+            candidates = tmc.getModifiedParents();\n+        }\n+\n+        pageCollection.getIndexPages().forEach(p -> {\n+            if (p.isModified()) {\n+                p.seal();\n+            }\n+        });\n+        return pageCollection;\n+    }\n+\n+    private void deletePage(BTreeSetPage p, TreeModificationContext context) {\n+        // Delete the page if it's empty, but only if it's not the root page.\n+        if (p.getPagePointer().hasParent()) {\n+            context.getPageCollection().pageDeleted(p);\n+            context.deleted(p.getPagePointer());\n+            log.debug(\"{}: Deleted empty page {}.\", this.traceLogId, p.getPagePointer());\n+        } else if (p.isIndexPage()) {\n+            p = BTreeSetPage.emptyLeafRoot();\n+            p.markModified();\n+            context.getPageCollection().pageUpdated(p);\n+            log.debug(\"{}: Replaced empty Index Root with empty Leaf Root.\", this.traceLogId);\n+        }\n+    }\n+\n+    private void splitPageIfNecessary(BTreeSetPage p, Supplier<Long> getNewPageId, TreeModificationContext context) {\n+        val splits = p.split(this.maxPageSize, getNewPageId);\n+        if (splits == null) {\n+            // No split necessary\n+            return;\n+        }\n+\n+        if (p.getPagePointer().hasParent()) {\n+            Preconditions.checkArgument(splits.get(0).getPagePointer().getPageId() == p.getPagePointer().getPageId(),\n+                    \"First split result (%s) not current page (%s).\", splits.get(0).getPagePointer(), p.getPagePointer());\n+        } else {\n+            // If we split the root, the new pages will already point to the root; we must create a blank\n+            // index root page, which will be updated in the next step.\n+            context.getPageCollection().pageUpdated(BTreeSetPage.emptyIndexRoot());\n+        }\n+\n+        splits.forEach(splitPage -> {\n+            context.getPageCollection().pageUpdated(splitPage);\n+            context.created(splitPage.getPagePointer());\n+        });\n+        log.debug(\"{}: Page '{}' split into {}: {}.\", this.traceLogId, p, splits.size(), splits);\n+    }\n+\n+    private CompletableFuture<Void> writePages(@NonNull PageCollection pageCollection, TimeoutTimer timer) {\n+        // Order the pages from bottom up. The upstream code may have limitations in how much it can update atomically,\n+        // so it may commit this in multiple non-atomic operations. If the process is interrupted mid-way then we want\n+        // to ensure that parent pages aren't updated before leaf pages (which would cause index corruptions - i.e., by\n+        // pointing to inexistent pages).\n+        val processedPageIds = new HashSet<Long>();\n+\n+        // First collect updates. Begin from the bottom (Leaf Pages).\n+        val toWrite = new ArrayList<Map.Entry<Long, ArrayView>>();\n+        collectWriteCandidates(pageCollection.getLeafPages(), toWrite, processedPageIds, pageCollection);\n+\n+        // Newly split pages may not be reachable from any modified Leaf Pages. Collect them too.\n+        collectWriteCandidates(pageCollection.getIndexPages(), toWrite, processedPageIds, pageCollection);\n+\n+        // Then collect deletions, making sure we also consider all their parents (which should be modified/deleted as well).\n+        collectWriteCandidates(pageCollection.getDeletedPagesParents(), toWrite, processedPageIds, pageCollection);\n+        log.debug(\"{}: Persist (Updates={}, Deletions={}).\", this.traceLogId, toWrite.size(), pageCollection.getDeletedPageIds().size());\n+        return this.update.apply(toWrite, pageCollection.getDeletedPageIds(), timer.getRemaining());\n+    }\n+\n+    private void collectWriteCandidates(Collection<BTreeSetPage> candidates, List<Map.Entry<Long, ArrayView>> toWrite,\n+                                        Set<Long> processedIds, PageCollection pageCollection) {\n+        while (!candidates.isEmpty()) {\n+            val next = new ArrayList<BTreeSetPage>();\n+            candidates.stream()\n+                    .filter(p -> p.isModified() && !processedIds.contains(p.getPagePointer().getPageId()))\n+                    .forEach(p -> {\n+                        toWrite.add(new AbstractMap.SimpleImmutableEntry<>(p.getPagePointer().getPageId(), p.getData()));\n+                        val parent = pageCollection.get(p.getPagePointer().getParentPageId());\n+                        assert p.getPagePointer().hasParent() == (parent != null);\n+                        processedIds.add(p.getPagePointer().getPageId());\n+                        if (parent != null) {\n+                            next.add(parent);\n+                        }\n+                    });\n+            candidates = next;\n+        }\n+    }\n+\n+    //endregion\n+\n+    //region Queries\n+\n+    /**\n+     * Returns an {@link AsyncIterator} that will iterate through all the items in this {@link BTreeSet} within the\n+     * specified bounds. All iterated items will be returned in lexicographic order (smallest to largest).\n+     * See {@link ByteArrayComparator} for ordering details.\n+     *\n+     * @param firstItem          An {@link ArrayView} indicating the first Item to iterate from. If null, the iteration\n+     *                           will begin with the first item in the index.\n+     * @param firstItemInclusive If true, firstIem will be included in the iteration (provided it exists), otherwise it", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 296}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU3ODY4Ng==", "bodyText": "Because sometimes I need to iterate starting with an element and sometimes I need to iterate immediately after an element (i.e., I know the last element I processed, so give me the next one).", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r432578686", "createdAt": "2020-05-29T15:53:23Z", "author": {"login": "andreipaduroiu"}, "path": "common/src/main/java/io/pravega/common/util/btree/sets/BTreeSet.java", "diffHunk": "@@ -0,0 +1,414 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.common.util.btree.sets;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArrayComparator;\n+import java.time.Duration;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Supplier;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.NotThreadSafe;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+/**\n+ * A B+Tree-backed Set. Stores all items in a B+Tree Structure using a {@link ByteArrayComparator} for ordering them.\n+ *\n+ * NOTE: This component is in {@link Beta}. There are no guarantees about data or API compatibility with future versions.\n+ * Any component that is directly dependent on this one should either be in {@link Beta} as well.\n+ */\n+@NotThreadSafe\n+@Beta\n+@Slf4j\n+public class BTreeSet {\n+    //region Members\n+\n+    public static final Comparator<ArrayView> COMPARATOR = new ByteArrayComparator()::compare;\n+    private static final Comparator<PagePointer> POINTER_COMPARATOR = PagePointer.getComparator(COMPARATOR);\n+\n+    private final int maxPageSize;\n+    private final int maxItemSize;\n+    @NonNull\n+    private final ReadPage read;\n+    @NonNull\n+    private final PersistPages update;\n+    @NonNull\n+    private final Executor executor;\n+    @NonNull\n+    private final String traceLogId;\n+\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the {@link BTreeSet} class.\n+     *\n+     * @param maxPageSize The maximum size, in bytes, of any page.\n+     * @param maxItemSize The maximum size, in bytes, of any single item in the {@link BTreeSet}.\n+     * @param read        A {@link ReadPage} function that can be used to fetch a single {@link BTreeSet} page from an\n+     *                    external data source.\n+     * @param update      A {@link PersistPages} function that can be used to store and delete multiple {@link BTreeSet}\n+     *                    pages to/from an external data source.\n+     * @param executor    Executor for async operations.\n+     * @param traceLogId  Trace id for logging.\n+     */\n+    public BTreeSet(int maxPageSize, int maxItemSize, @NonNull ReadPage read, @NonNull PersistPages update,\n+                    @NonNull Executor executor, String traceLogId) {\n+        Preconditions.checkArgument(maxItemSize < maxPageSize / 2, \"maxItemSize must be at most half of maxPageSize.\");\n+        this.maxItemSize = maxItemSize;\n+        this.maxPageSize = maxPageSize;\n+        this.read = read;\n+        this.update = update;\n+        this.executor = executor;\n+        this.traceLogId = traceLogId == null ? \"\" : traceLogId;\n+    }\n+\n+    //endregion\n+\n+    //region Updates\n+\n+    /**\n+     * Atomically inserts the items in 'toInsert' into the {@link BTreeSet} and removes the items in 'toRemove'\n+     * from the {@link BTreeSet}. No duplicates are allowed; the same item cannot exist multiple times in either 'toInsert'\n+     * or 'toRemove' or in both of them.\n+     *\n+     * @param toInsert      (Optional). A Collection of {@link ArrayView} instances representing the items to insert.\n+     *                      If an item is already present, it will not be reinserted (updates are idempotent).\n+     * @param toRemove      (Optional). A Collection of {@link ArrayView} instances representing the items to remove.\n+     * @param getNextPageId A Supplier that, when invoked, will return a unique number representing the Id of the next\n+     *                      {@link BTreeSet} page that has to be generated.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed normally, will indicate that the updates have been applied\n+     * successfully. If the operation failed, the Future will be completed with the appropriate exception.\n+     */\n+    public CompletableFuture<Void> update(@Nullable Collection<? extends ArrayView> toInsert, @Nullable Collection<? extends ArrayView> toRemove,\n+                                          @NonNull Supplier<Long> getNextPageId, @NonNull Duration timeout) {\n+        TimeoutTimer timer = new TimeoutTimer(timeout);\n+        val updates = new ArrayList<UpdateItem>();\n+        int insertCount = collectUpdates(toInsert, false, updates);\n+        int removeCount = collectUpdates(toRemove, true, updates);\n+        updates.sort(UpdateItem::compareTo);\n+        log.debug(\"{}: Update (Insert={}, Remove={}).\", this.traceLogId, insertCount, removeCount);\n+        if (updates.isEmpty()) {\n+            // Nothing to do.\n+            return CompletableFuture.completedFuture(null);\n+        }\n+\n+        // The updates are sorted, so any empty items will be placed first.\n+        Preconditions.checkArgument(updates.get(0).getItem().getLength() > 0, \"No empty items allowed.\");\n+        return applyUpdates(updates.iterator(), timer)\n+                .thenApply(pageCollection -> processModifiedPages(pageCollection, getNextPageId))\n+                .thenComposeAsync(pageCollection -> writePages(pageCollection, timer), this.executor);\n+    }\n+\n+    private int collectUpdates(Collection<? extends ArrayView> items, boolean isRemoval, List<UpdateItem> updates) {\n+        if (items == null) {\n+            return 0;\n+        }\n+\n+        for (val i : items) {\n+            Preconditions.checkArgument(i.getLength() <= this.maxItemSize,\n+                    \"Item exceeds maximum allowed length (%s).\", this.maxItemSize);\n+            updates.add(new UpdateItem(i, isRemoval));\n+        }\n+        return items.size();\n+    }\n+\n+    private CompletableFuture<PageCollection> applyUpdates(Iterator<UpdateItem> items, TimeoutTimer timer) {\n+        val pageCollection = new PageCollection();\n+        val lastPage = new AtomicReference<BTreeSetPage.LeafPage>(null);\n+        val lastPageUpdates = new ArrayList<UpdateItem>();\n+        return Futures.loop(\n+                items::hasNext,\n+                () -> {\n+                    // Locate the page where the update is to be executed. Do not apply it yet as it is more efficient\n+                    // to bulk-apply multiple at once. Collect all updates for each Page, and only apply them once we have\n+                    // \"moved on\" to another page.\n+                    val next = items.next();\n+                    return locatePage(next.getItem(), pageCollection, timer)\n+                            .thenAccept(page -> {\n+                                val last = lastPage.get();\n+                                if (page != last) {\n+                                    // This key goes to a different page than the one we were looking at.\n+                                    if (last != null) {\n+                                        // Commit the outstanding updates.\n+                                        last.update(lastPageUpdates);\n+                                    }\n+\n+                                    // Update the pointers.\n+                                    lastPage.set(page);\n+                                    lastPageUpdates.clear();\n+                                }\n+\n+                                // Record the current update.\n+                                lastPageUpdates.add(next);\n+                            });\n+                },\n+                this.executor)\n+                .thenApplyAsync(v -> {\n+                    // We need not forget to apply the last batch of updates from the last page.\n+                    if (lastPage.get() != null) {\n+                        lastPage.get().update(lastPageUpdates);\n+                    }\n+                    return pageCollection;\n+                }, this.executor);\n+    }\n+\n+    private PageCollection processModifiedPages(PageCollection pageCollection, Supplier<Long> getNewPageId) {\n+        Collection<BTreeSetPage> candidates = pageCollection.getLeafPages();\n+        while (!candidates.isEmpty()) {\n+            // Process each candidate and determine if it should be deleted or split into multiple pages.\n+            val tmc = new TreeModificationContext(pageCollection);\n+            for (BTreeSetPage p : candidates) {\n+                if (p.getItemCount() == 0) {\n+                    deletePage(p, tmc);\n+                } else {\n+                    splitPageIfNecessary(p, getNewPageId, tmc);\n+                }\n+            }\n+\n+            // Update those pages' parents.\n+            tmc.accept(BTreeSetPage.IndexPage::addChildren, BTreeSetPage.IndexPage::removeChildren, POINTER_COMPARATOR);\n+            candidates = tmc.getModifiedParents();\n+        }\n+\n+        pageCollection.getIndexPages().forEach(p -> {\n+            if (p.isModified()) {\n+                p.seal();\n+            }\n+        });\n+        return pageCollection;\n+    }\n+\n+    private void deletePage(BTreeSetPage p, TreeModificationContext context) {\n+        // Delete the page if it's empty, but only if it's not the root page.\n+        if (p.getPagePointer().hasParent()) {\n+            context.getPageCollection().pageDeleted(p);\n+            context.deleted(p.getPagePointer());\n+            log.debug(\"{}: Deleted empty page {}.\", this.traceLogId, p.getPagePointer());\n+        } else if (p.isIndexPage()) {\n+            p = BTreeSetPage.emptyLeafRoot();\n+            p.markModified();\n+            context.getPageCollection().pageUpdated(p);\n+            log.debug(\"{}: Replaced empty Index Root with empty Leaf Root.\", this.traceLogId);\n+        }\n+    }\n+\n+    private void splitPageIfNecessary(BTreeSetPage p, Supplier<Long> getNewPageId, TreeModificationContext context) {\n+        val splits = p.split(this.maxPageSize, getNewPageId);\n+        if (splits == null) {\n+            // No split necessary\n+            return;\n+        }\n+\n+        if (p.getPagePointer().hasParent()) {\n+            Preconditions.checkArgument(splits.get(0).getPagePointer().getPageId() == p.getPagePointer().getPageId(),\n+                    \"First split result (%s) not current page (%s).\", splits.get(0).getPagePointer(), p.getPagePointer());\n+        } else {\n+            // If we split the root, the new pages will already point to the root; we must create a blank\n+            // index root page, which will be updated in the next step.\n+            context.getPageCollection().pageUpdated(BTreeSetPage.emptyIndexRoot());\n+        }\n+\n+        splits.forEach(splitPage -> {\n+            context.getPageCollection().pageUpdated(splitPage);\n+            context.created(splitPage.getPagePointer());\n+        });\n+        log.debug(\"{}: Page '{}' split into {}: {}.\", this.traceLogId, p, splits.size(), splits);\n+    }\n+\n+    private CompletableFuture<Void> writePages(@NonNull PageCollection pageCollection, TimeoutTimer timer) {\n+        // Order the pages from bottom up. The upstream code may have limitations in how much it can update atomically,\n+        // so it may commit this in multiple non-atomic operations. If the process is interrupted mid-way then we want\n+        // to ensure that parent pages aren't updated before leaf pages (which would cause index corruptions - i.e., by\n+        // pointing to inexistent pages).\n+        val processedPageIds = new HashSet<Long>();\n+\n+        // First collect updates. Begin from the bottom (Leaf Pages).\n+        val toWrite = new ArrayList<Map.Entry<Long, ArrayView>>();\n+        collectWriteCandidates(pageCollection.getLeafPages(), toWrite, processedPageIds, pageCollection);\n+\n+        // Newly split pages may not be reachable from any modified Leaf Pages. Collect them too.\n+        collectWriteCandidates(pageCollection.getIndexPages(), toWrite, processedPageIds, pageCollection);\n+\n+        // Then collect deletions, making sure we also consider all their parents (which should be modified/deleted as well).\n+        collectWriteCandidates(pageCollection.getDeletedPagesParents(), toWrite, processedPageIds, pageCollection);\n+        log.debug(\"{}: Persist (Updates={}, Deletions={}).\", this.traceLogId, toWrite.size(), pageCollection.getDeletedPageIds().size());\n+        return this.update.apply(toWrite, pageCollection.getDeletedPageIds(), timer.getRemaining());\n+    }\n+\n+    private void collectWriteCandidates(Collection<BTreeSetPage> candidates, List<Map.Entry<Long, ArrayView>> toWrite,\n+                                        Set<Long> processedIds, PageCollection pageCollection) {\n+        while (!candidates.isEmpty()) {\n+            val next = new ArrayList<BTreeSetPage>();\n+            candidates.stream()\n+                    .filter(p -> p.isModified() && !processedIds.contains(p.getPagePointer().getPageId()))\n+                    .forEach(p -> {\n+                        toWrite.add(new AbstractMap.SimpleImmutableEntry<>(p.getPagePointer().getPageId(), p.getData()));\n+                        val parent = pageCollection.get(p.getPagePointer().getParentPageId());\n+                        assert p.getPagePointer().hasParent() == (parent != null);\n+                        processedIds.add(p.getPagePointer().getPageId());\n+                        if (parent != null) {\n+                            next.add(parent);\n+                        }\n+                    });\n+            candidates = next;\n+        }\n+    }\n+\n+    //endregion\n+\n+    //region Queries\n+\n+    /**\n+     * Returns an {@link AsyncIterator} that will iterate through all the items in this {@link BTreeSet} within the\n+     * specified bounds. All iterated items will be returned in lexicographic order (smallest to largest).\n+     * See {@link ByteArrayComparator} for ordering details.\n+     *\n+     * @param firstItem          An {@link ArrayView} indicating the first Item to iterate from. If null, the iteration\n+     *                           will begin with the first item in the index.\n+     * @param firstItemInclusive If true, firstIem will be included in the iteration (provided it exists), otherwise it", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjQ4NDI0MQ=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 296}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzgyOTg5Nw==", "bodyText": "There is probably something silly I'm missing, but they seem unnecessary, here is my reasoning:\n\nit should be always exclusive\nif the caller has the first item and wants it inclusive, then it can use the first item it already has.\nif the caller has the last item and wants it inclusive, then it can return the last item once there are no more items to read within the range of the iterator, then return the last one.\n\nWhat am I missing?", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r433829897", "createdAt": "2020-06-02T12:15:32Z", "author": {"login": "fpj"}, "path": "common/src/main/java/io/pravega/common/util/btree/sets/BTreeSet.java", "diffHunk": "@@ -0,0 +1,414 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.common.util.btree.sets;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArrayComparator;\n+import java.time.Duration;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Supplier;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.NotThreadSafe;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+/**\n+ * A B+Tree-backed Set. Stores all items in a B+Tree Structure using a {@link ByteArrayComparator} for ordering them.\n+ *\n+ * NOTE: This component is in {@link Beta}. There are no guarantees about data or API compatibility with future versions.\n+ * Any component that is directly dependent on this one should either be in {@link Beta} as well.\n+ */\n+@NotThreadSafe\n+@Beta\n+@Slf4j\n+public class BTreeSet {\n+    //region Members\n+\n+    public static final Comparator<ArrayView> COMPARATOR = new ByteArrayComparator()::compare;\n+    private static final Comparator<PagePointer> POINTER_COMPARATOR = PagePointer.getComparator(COMPARATOR);\n+\n+    private final int maxPageSize;\n+    private final int maxItemSize;\n+    @NonNull\n+    private final ReadPage read;\n+    @NonNull\n+    private final PersistPages update;\n+    @NonNull\n+    private final Executor executor;\n+    @NonNull\n+    private final String traceLogId;\n+\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the {@link BTreeSet} class.\n+     *\n+     * @param maxPageSize The maximum size, in bytes, of any page.\n+     * @param maxItemSize The maximum size, in bytes, of any single item in the {@link BTreeSet}.\n+     * @param read        A {@link ReadPage} function that can be used to fetch a single {@link BTreeSet} page from an\n+     *                    external data source.\n+     * @param update      A {@link PersistPages} function that can be used to store and delete multiple {@link BTreeSet}\n+     *                    pages to/from an external data source.\n+     * @param executor    Executor for async operations.\n+     * @param traceLogId  Trace id for logging.\n+     */\n+    public BTreeSet(int maxPageSize, int maxItemSize, @NonNull ReadPage read, @NonNull PersistPages update,\n+                    @NonNull Executor executor, String traceLogId) {\n+        Preconditions.checkArgument(maxItemSize < maxPageSize / 2, \"maxItemSize must be at most half of maxPageSize.\");\n+        this.maxItemSize = maxItemSize;\n+        this.maxPageSize = maxPageSize;\n+        this.read = read;\n+        this.update = update;\n+        this.executor = executor;\n+        this.traceLogId = traceLogId == null ? \"\" : traceLogId;\n+    }\n+\n+    //endregion\n+\n+    //region Updates\n+\n+    /**\n+     * Atomically inserts the items in 'toInsert' into the {@link BTreeSet} and removes the items in 'toRemove'\n+     * from the {@link BTreeSet}. No duplicates are allowed; the same item cannot exist multiple times in either 'toInsert'\n+     * or 'toRemove' or in both of them.\n+     *\n+     * @param toInsert      (Optional). A Collection of {@link ArrayView} instances representing the items to insert.\n+     *                      If an item is already present, it will not be reinserted (updates are idempotent).\n+     * @param toRemove      (Optional). A Collection of {@link ArrayView} instances representing the items to remove.\n+     * @param getNextPageId A Supplier that, when invoked, will return a unique number representing the Id of the next\n+     *                      {@link BTreeSet} page that has to be generated.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed normally, will indicate that the updates have been applied\n+     * successfully. If the operation failed, the Future will be completed with the appropriate exception.\n+     */\n+    public CompletableFuture<Void> update(@Nullable Collection<? extends ArrayView> toInsert, @Nullable Collection<? extends ArrayView> toRemove,\n+                                          @NonNull Supplier<Long> getNextPageId, @NonNull Duration timeout) {\n+        TimeoutTimer timer = new TimeoutTimer(timeout);\n+        val updates = new ArrayList<UpdateItem>();\n+        int insertCount = collectUpdates(toInsert, false, updates);\n+        int removeCount = collectUpdates(toRemove, true, updates);\n+        updates.sort(UpdateItem::compareTo);\n+        log.debug(\"{}: Update (Insert={}, Remove={}).\", this.traceLogId, insertCount, removeCount);\n+        if (updates.isEmpty()) {\n+            // Nothing to do.\n+            return CompletableFuture.completedFuture(null);\n+        }\n+\n+        // The updates are sorted, so any empty items will be placed first.\n+        Preconditions.checkArgument(updates.get(0).getItem().getLength() > 0, \"No empty items allowed.\");\n+        return applyUpdates(updates.iterator(), timer)\n+                .thenApply(pageCollection -> processModifiedPages(pageCollection, getNextPageId))\n+                .thenComposeAsync(pageCollection -> writePages(pageCollection, timer), this.executor);\n+    }\n+\n+    private int collectUpdates(Collection<? extends ArrayView> items, boolean isRemoval, List<UpdateItem> updates) {\n+        if (items == null) {\n+            return 0;\n+        }\n+\n+        for (val i : items) {\n+            Preconditions.checkArgument(i.getLength() <= this.maxItemSize,\n+                    \"Item exceeds maximum allowed length (%s).\", this.maxItemSize);\n+            updates.add(new UpdateItem(i, isRemoval));\n+        }\n+        return items.size();\n+    }\n+\n+    private CompletableFuture<PageCollection> applyUpdates(Iterator<UpdateItem> items, TimeoutTimer timer) {\n+        val pageCollection = new PageCollection();\n+        val lastPage = new AtomicReference<BTreeSetPage.LeafPage>(null);\n+        val lastPageUpdates = new ArrayList<UpdateItem>();\n+        return Futures.loop(\n+                items::hasNext,\n+                () -> {\n+                    // Locate the page where the update is to be executed. Do not apply it yet as it is more efficient\n+                    // to bulk-apply multiple at once. Collect all updates for each Page, and only apply them once we have\n+                    // \"moved on\" to another page.\n+                    val next = items.next();\n+                    return locatePage(next.getItem(), pageCollection, timer)\n+                            .thenAccept(page -> {\n+                                val last = lastPage.get();\n+                                if (page != last) {\n+                                    // This key goes to a different page than the one we were looking at.\n+                                    if (last != null) {\n+                                        // Commit the outstanding updates.\n+                                        last.update(lastPageUpdates);\n+                                    }\n+\n+                                    // Update the pointers.\n+                                    lastPage.set(page);\n+                                    lastPageUpdates.clear();\n+                                }\n+\n+                                // Record the current update.\n+                                lastPageUpdates.add(next);\n+                            });\n+                },\n+                this.executor)\n+                .thenApplyAsync(v -> {\n+                    // We need not forget to apply the last batch of updates from the last page.\n+                    if (lastPage.get() != null) {\n+                        lastPage.get().update(lastPageUpdates);\n+                    }\n+                    return pageCollection;\n+                }, this.executor);\n+    }\n+\n+    private PageCollection processModifiedPages(PageCollection pageCollection, Supplier<Long> getNewPageId) {\n+        Collection<BTreeSetPage> candidates = pageCollection.getLeafPages();\n+        while (!candidates.isEmpty()) {\n+            // Process each candidate and determine if it should be deleted or split into multiple pages.\n+            val tmc = new TreeModificationContext(pageCollection);\n+            for (BTreeSetPage p : candidates) {\n+                if (p.getItemCount() == 0) {\n+                    deletePage(p, tmc);\n+                } else {\n+                    splitPageIfNecessary(p, getNewPageId, tmc);\n+                }\n+            }\n+\n+            // Update those pages' parents.\n+            tmc.accept(BTreeSetPage.IndexPage::addChildren, BTreeSetPage.IndexPage::removeChildren, POINTER_COMPARATOR);\n+            candidates = tmc.getModifiedParents();\n+        }\n+\n+        pageCollection.getIndexPages().forEach(p -> {\n+            if (p.isModified()) {\n+                p.seal();\n+            }\n+        });\n+        return pageCollection;\n+    }\n+\n+    private void deletePage(BTreeSetPage p, TreeModificationContext context) {\n+        // Delete the page if it's empty, but only if it's not the root page.\n+        if (p.getPagePointer().hasParent()) {\n+            context.getPageCollection().pageDeleted(p);\n+            context.deleted(p.getPagePointer());\n+            log.debug(\"{}: Deleted empty page {}.\", this.traceLogId, p.getPagePointer());\n+        } else if (p.isIndexPage()) {\n+            p = BTreeSetPage.emptyLeafRoot();\n+            p.markModified();\n+            context.getPageCollection().pageUpdated(p);\n+            log.debug(\"{}: Replaced empty Index Root with empty Leaf Root.\", this.traceLogId);\n+        }\n+    }\n+\n+    private void splitPageIfNecessary(BTreeSetPage p, Supplier<Long> getNewPageId, TreeModificationContext context) {\n+        val splits = p.split(this.maxPageSize, getNewPageId);\n+        if (splits == null) {\n+            // No split necessary\n+            return;\n+        }\n+\n+        if (p.getPagePointer().hasParent()) {\n+            Preconditions.checkArgument(splits.get(0).getPagePointer().getPageId() == p.getPagePointer().getPageId(),\n+                    \"First split result (%s) not current page (%s).\", splits.get(0).getPagePointer(), p.getPagePointer());\n+        } else {\n+            // If we split the root, the new pages will already point to the root; we must create a blank\n+            // index root page, which will be updated in the next step.\n+            context.getPageCollection().pageUpdated(BTreeSetPage.emptyIndexRoot());\n+        }\n+\n+        splits.forEach(splitPage -> {\n+            context.getPageCollection().pageUpdated(splitPage);\n+            context.created(splitPage.getPagePointer());\n+        });\n+        log.debug(\"{}: Page '{}' split into {}: {}.\", this.traceLogId, p, splits.size(), splits);\n+    }\n+\n+    private CompletableFuture<Void> writePages(@NonNull PageCollection pageCollection, TimeoutTimer timer) {\n+        // Order the pages from bottom up. The upstream code may have limitations in how much it can update atomically,\n+        // so it may commit this in multiple non-atomic operations. If the process is interrupted mid-way then we want\n+        // to ensure that parent pages aren't updated before leaf pages (which would cause index corruptions - i.e., by\n+        // pointing to inexistent pages).\n+        val processedPageIds = new HashSet<Long>();\n+\n+        // First collect updates. Begin from the bottom (Leaf Pages).\n+        val toWrite = new ArrayList<Map.Entry<Long, ArrayView>>();\n+        collectWriteCandidates(pageCollection.getLeafPages(), toWrite, processedPageIds, pageCollection);\n+\n+        // Newly split pages may not be reachable from any modified Leaf Pages. Collect them too.\n+        collectWriteCandidates(pageCollection.getIndexPages(), toWrite, processedPageIds, pageCollection);\n+\n+        // Then collect deletions, making sure we also consider all their parents (which should be modified/deleted as well).\n+        collectWriteCandidates(pageCollection.getDeletedPagesParents(), toWrite, processedPageIds, pageCollection);\n+        log.debug(\"{}: Persist (Updates={}, Deletions={}).\", this.traceLogId, toWrite.size(), pageCollection.getDeletedPageIds().size());\n+        return this.update.apply(toWrite, pageCollection.getDeletedPageIds(), timer.getRemaining());\n+    }\n+\n+    private void collectWriteCandidates(Collection<BTreeSetPage> candidates, List<Map.Entry<Long, ArrayView>> toWrite,\n+                                        Set<Long> processedIds, PageCollection pageCollection) {\n+        while (!candidates.isEmpty()) {\n+            val next = new ArrayList<BTreeSetPage>();\n+            candidates.stream()\n+                    .filter(p -> p.isModified() && !processedIds.contains(p.getPagePointer().getPageId()))\n+                    .forEach(p -> {\n+                        toWrite.add(new AbstractMap.SimpleImmutableEntry<>(p.getPagePointer().getPageId(), p.getData()));\n+                        val parent = pageCollection.get(p.getPagePointer().getParentPageId());\n+                        assert p.getPagePointer().hasParent() == (parent != null);\n+                        processedIds.add(p.getPagePointer().getPageId());\n+                        if (parent != null) {\n+                            next.add(parent);\n+                        }\n+                    });\n+            candidates = next;\n+        }\n+    }\n+\n+    //endregion\n+\n+    //region Queries\n+\n+    /**\n+     * Returns an {@link AsyncIterator} that will iterate through all the items in this {@link BTreeSet} within the\n+     * specified bounds. All iterated items will be returned in lexicographic order (smallest to largest).\n+     * See {@link ByteArrayComparator} for ordering details.\n+     *\n+     * @param firstItem          An {@link ArrayView} indicating the first Item to iterate from. If null, the iteration\n+     *                           will begin with the first item in the index.\n+     * @param firstItemInclusive If true, firstIem will be included in the iteration (provided it exists), otherwise it", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjQ4NDI0MQ=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 296}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzk1MzMwNQ==", "bodyText": "if the caller has the first item and wants it inclusive, then it can use the first item it already has.\n\nThe caller may have the first item, but the first item may not be part of the set. If we always make it exclusive, the caller may erroneously include that first item in the result even if it is not part of the set.\nThis is a low-level API. I don't see a problem with giving this type of granularity to the user; in the future we may have other use cases that warrant this flexibility.\nAnother reason would be to make it consistent with the similar method in BTreeIndex, which has uses for both inclusive and exclusive.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r433953305", "createdAt": "2020-06-02T15:12:50Z", "author": {"login": "andreipaduroiu"}, "path": "common/src/main/java/io/pravega/common/util/btree/sets/BTreeSet.java", "diffHunk": "@@ -0,0 +1,414 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.common.util.btree.sets;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArrayComparator;\n+import java.time.Duration;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Supplier;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.NotThreadSafe;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+/**\n+ * A B+Tree-backed Set. Stores all items in a B+Tree Structure using a {@link ByteArrayComparator} for ordering them.\n+ *\n+ * NOTE: This component is in {@link Beta}. There are no guarantees about data or API compatibility with future versions.\n+ * Any component that is directly dependent on this one should either be in {@link Beta} as well.\n+ */\n+@NotThreadSafe\n+@Beta\n+@Slf4j\n+public class BTreeSet {\n+    //region Members\n+\n+    public static final Comparator<ArrayView> COMPARATOR = new ByteArrayComparator()::compare;\n+    private static final Comparator<PagePointer> POINTER_COMPARATOR = PagePointer.getComparator(COMPARATOR);\n+\n+    private final int maxPageSize;\n+    private final int maxItemSize;\n+    @NonNull\n+    private final ReadPage read;\n+    @NonNull\n+    private final PersistPages update;\n+    @NonNull\n+    private final Executor executor;\n+    @NonNull\n+    private final String traceLogId;\n+\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the {@link BTreeSet} class.\n+     *\n+     * @param maxPageSize The maximum size, in bytes, of any page.\n+     * @param maxItemSize The maximum size, in bytes, of any single item in the {@link BTreeSet}.\n+     * @param read        A {@link ReadPage} function that can be used to fetch a single {@link BTreeSet} page from an\n+     *                    external data source.\n+     * @param update      A {@link PersistPages} function that can be used to store and delete multiple {@link BTreeSet}\n+     *                    pages to/from an external data source.\n+     * @param executor    Executor for async operations.\n+     * @param traceLogId  Trace id for logging.\n+     */\n+    public BTreeSet(int maxPageSize, int maxItemSize, @NonNull ReadPage read, @NonNull PersistPages update,\n+                    @NonNull Executor executor, String traceLogId) {\n+        Preconditions.checkArgument(maxItemSize < maxPageSize / 2, \"maxItemSize must be at most half of maxPageSize.\");\n+        this.maxItemSize = maxItemSize;\n+        this.maxPageSize = maxPageSize;\n+        this.read = read;\n+        this.update = update;\n+        this.executor = executor;\n+        this.traceLogId = traceLogId == null ? \"\" : traceLogId;\n+    }\n+\n+    //endregion\n+\n+    //region Updates\n+\n+    /**\n+     * Atomically inserts the items in 'toInsert' into the {@link BTreeSet} and removes the items in 'toRemove'\n+     * from the {@link BTreeSet}. No duplicates are allowed; the same item cannot exist multiple times in either 'toInsert'\n+     * or 'toRemove' or in both of them.\n+     *\n+     * @param toInsert      (Optional). A Collection of {@link ArrayView} instances representing the items to insert.\n+     *                      If an item is already present, it will not be reinserted (updates are idempotent).\n+     * @param toRemove      (Optional). A Collection of {@link ArrayView} instances representing the items to remove.\n+     * @param getNextPageId A Supplier that, when invoked, will return a unique number representing the Id of the next\n+     *                      {@link BTreeSet} page that has to be generated.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed normally, will indicate that the updates have been applied\n+     * successfully. If the operation failed, the Future will be completed with the appropriate exception.\n+     */\n+    public CompletableFuture<Void> update(@Nullable Collection<? extends ArrayView> toInsert, @Nullable Collection<? extends ArrayView> toRemove,\n+                                          @NonNull Supplier<Long> getNextPageId, @NonNull Duration timeout) {\n+        TimeoutTimer timer = new TimeoutTimer(timeout);\n+        val updates = new ArrayList<UpdateItem>();\n+        int insertCount = collectUpdates(toInsert, false, updates);\n+        int removeCount = collectUpdates(toRemove, true, updates);\n+        updates.sort(UpdateItem::compareTo);\n+        log.debug(\"{}: Update (Insert={}, Remove={}).\", this.traceLogId, insertCount, removeCount);\n+        if (updates.isEmpty()) {\n+            // Nothing to do.\n+            return CompletableFuture.completedFuture(null);\n+        }\n+\n+        // The updates are sorted, so any empty items will be placed first.\n+        Preconditions.checkArgument(updates.get(0).getItem().getLength() > 0, \"No empty items allowed.\");\n+        return applyUpdates(updates.iterator(), timer)\n+                .thenApply(pageCollection -> processModifiedPages(pageCollection, getNextPageId))\n+                .thenComposeAsync(pageCollection -> writePages(pageCollection, timer), this.executor);\n+    }\n+\n+    private int collectUpdates(Collection<? extends ArrayView> items, boolean isRemoval, List<UpdateItem> updates) {\n+        if (items == null) {\n+            return 0;\n+        }\n+\n+        for (val i : items) {\n+            Preconditions.checkArgument(i.getLength() <= this.maxItemSize,\n+                    \"Item exceeds maximum allowed length (%s).\", this.maxItemSize);\n+            updates.add(new UpdateItem(i, isRemoval));\n+        }\n+        return items.size();\n+    }\n+\n+    private CompletableFuture<PageCollection> applyUpdates(Iterator<UpdateItem> items, TimeoutTimer timer) {\n+        val pageCollection = new PageCollection();\n+        val lastPage = new AtomicReference<BTreeSetPage.LeafPage>(null);\n+        val lastPageUpdates = new ArrayList<UpdateItem>();\n+        return Futures.loop(\n+                items::hasNext,\n+                () -> {\n+                    // Locate the page where the update is to be executed. Do not apply it yet as it is more efficient\n+                    // to bulk-apply multiple at once. Collect all updates for each Page, and only apply them once we have\n+                    // \"moved on\" to another page.\n+                    val next = items.next();\n+                    return locatePage(next.getItem(), pageCollection, timer)\n+                            .thenAccept(page -> {\n+                                val last = lastPage.get();\n+                                if (page != last) {\n+                                    // This key goes to a different page than the one we were looking at.\n+                                    if (last != null) {\n+                                        // Commit the outstanding updates.\n+                                        last.update(lastPageUpdates);\n+                                    }\n+\n+                                    // Update the pointers.\n+                                    lastPage.set(page);\n+                                    lastPageUpdates.clear();\n+                                }\n+\n+                                // Record the current update.\n+                                lastPageUpdates.add(next);\n+                            });\n+                },\n+                this.executor)\n+                .thenApplyAsync(v -> {\n+                    // We need not forget to apply the last batch of updates from the last page.\n+                    if (lastPage.get() != null) {\n+                        lastPage.get().update(lastPageUpdates);\n+                    }\n+                    return pageCollection;\n+                }, this.executor);\n+    }\n+\n+    private PageCollection processModifiedPages(PageCollection pageCollection, Supplier<Long> getNewPageId) {\n+        Collection<BTreeSetPage> candidates = pageCollection.getLeafPages();\n+        while (!candidates.isEmpty()) {\n+            // Process each candidate and determine if it should be deleted or split into multiple pages.\n+            val tmc = new TreeModificationContext(pageCollection);\n+            for (BTreeSetPage p : candidates) {\n+                if (p.getItemCount() == 0) {\n+                    deletePage(p, tmc);\n+                } else {\n+                    splitPageIfNecessary(p, getNewPageId, tmc);\n+                }\n+            }\n+\n+            // Update those pages' parents.\n+            tmc.accept(BTreeSetPage.IndexPage::addChildren, BTreeSetPage.IndexPage::removeChildren, POINTER_COMPARATOR);\n+            candidates = tmc.getModifiedParents();\n+        }\n+\n+        pageCollection.getIndexPages().forEach(p -> {\n+            if (p.isModified()) {\n+                p.seal();\n+            }\n+        });\n+        return pageCollection;\n+    }\n+\n+    private void deletePage(BTreeSetPage p, TreeModificationContext context) {\n+        // Delete the page if it's empty, but only if it's not the root page.\n+        if (p.getPagePointer().hasParent()) {\n+            context.getPageCollection().pageDeleted(p);\n+            context.deleted(p.getPagePointer());\n+            log.debug(\"{}: Deleted empty page {}.\", this.traceLogId, p.getPagePointer());\n+        } else if (p.isIndexPage()) {\n+            p = BTreeSetPage.emptyLeafRoot();\n+            p.markModified();\n+            context.getPageCollection().pageUpdated(p);\n+            log.debug(\"{}: Replaced empty Index Root with empty Leaf Root.\", this.traceLogId);\n+        }\n+    }\n+\n+    private void splitPageIfNecessary(BTreeSetPage p, Supplier<Long> getNewPageId, TreeModificationContext context) {\n+        val splits = p.split(this.maxPageSize, getNewPageId);\n+        if (splits == null) {\n+            // No split necessary\n+            return;\n+        }\n+\n+        if (p.getPagePointer().hasParent()) {\n+            Preconditions.checkArgument(splits.get(0).getPagePointer().getPageId() == p.getPagePointer().getPageId(),\n+                    \"First split result (%s) not current page (%s).\", splits.get(0).getPagePointer(), p.getPagePointer());\n+        } else {\n+            // If we split the root, the new pages will already point to the root; we must create a blank\n+            // index root page, which will be updated in the next step.\n+            context.getPageCollection().pageUpdated(BTreeSetPage.emptyIndexRoot());\n+        }\n+\n+        splits.forEach(splitPage -> {\n+            context.getPageCollection().pageUpdated(splitPage);\n+            context.created(splitPage.getPagePointer());\n+        });\n+        log.debug(\"{}: Page '{}' split into {}: {}.\", this.traceLogId, p, splits.size(), splits);\n+    }\n+\n+    private CompletableFuture<Void> writePages(@NonNull PageCollection pageCollection, TimeoutTimer timer) {\n+        // Order the pages from bottom up. The upstream code may have limitations in how much it can update atomically,\n+        // so it may commit this in multiple non-atomic operations. If the process is interrupted mid-way then we want\n+        // to ensure that parent pages aren't updated before leaf pages (which would cause index corruptions - i.e., by\n+        // pointing to inexistent pages).\n+        val processedPageIds = new HashSet<Long>();\n+\n+        // First collect updates. Begin from the bottom (Leaf Pages).\n+        val toWrite = new ArrayList<Map.Entry<Long, ArrayView>>();\n+        collectWriteCandidates(pageCollection.getLeafPages(), toWrite, processedPageIds, pageCollection);\n+\n+        // Newly split pages may not be reachable from any modified Leaf Pages. Collect them too.\n+        collectWriteCandidates(pageCollection.getIndexPages(), toWrite, processedPageIds, pageCollection);\n+\n+        // Then collect deletions, making sure we also consider all their parents (which should be modified/deleted as well).\n+        collectWriteCandidates(pageCollection.getDeletedPagesParents(), toWrite, processedPageIds, pageCollection);\n+        log.debug(\"{}: Persist (Updates={}, Deletions={}).\", this.traceLogId, toWrite.size(), pageCollection.getDeletedPageIds().size());\n+        return this.update.apply(toWrite, pageCollection.getDeletedPageIds(), timer.getRemaining());\n+    }\n+\n+    private void collectWriteCandidates(Collection<BTreeSetPage> candidates, List<Map.Entry<Long, ArrayView>> toWrite,\n+                                        Set<Long> processedIds, PageCollection pageCollection) {\n+        while (!candidates.isEmpty()) {\n+            val next = new ArrayList<BTreeSetPage>();\n+            candidates.stream()\n+                    .filter(p -> p.isModified() && !processedIds.contains(p.getPagePointer().getPageId()))\n+                    .forEach(p -> {\n+                        toWrite.add(new AbstractMap.SimpleImmutableEntry<>(p.getPagePointer().getPageId(), p.getData()));\n+                        val parent = pageCollection.get(p.getPagePointer().getParentPageId());\n+                        assert p.getPagePointer().hasParent() == (parent != null);\n+                        processedIds.add(p.getPagePointer().getPageId());\n+                        if (parent != null) {\n+                            next.add(parent);\n+                        }\n+                    });\n+            candidates = next;\n+        }\n+    }\n+\n+    //endregion\n+\n+    //region Queries\n+\n+    /**\n+     * Returns an {@link AsyncIterator} that will iterate through all the items in this {@link BTreeSet} within the\n+     * specified bounds. All iterated items will be returned in lexicographic order (smallest to largest).\n+     * See {@link ByteArrayComparator} for ordering details.\n+     *\n+     * @param firstItem          An {@link ArrayView} indicating the first Item to iterate from. If null, the iteration\n+     *                           will begin with the first item in the index.\n+     * @param firstItemInclusive If true, firstIem will be included in the iteration (provided it exists), otherwise it", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjQ4NDI0MQ=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 296}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDUxODgxMw==", "bodyText": "The caller may have the first item, but the first item may not be part of the set. If we always make it exclusive, the caller may erroneously include that first item in the result even if it is not part of the set.\n\nIt is correct that making it always exclusive requires a different code path to add the first or last when you want it inclusive, and that makes it more prone to errors. The developer can always set the flag incorrectly, though.\n\nThis is a low-level API.\n\nThis is part of common, which is principle should be usable even outside Pravega. I'm curious, is this common in the implementation of iterators?  Do you know of other examples outside Pravega that implement iterators like this?\n\nAnother reason would be to make it consistent\n\nWe should make it consistent, at least for this PR as we are not really changing BTreeIndex.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r434518813", "createdAt": "2020-06-03T12:10:22Z", "author": {"login": "fpj"}, "path": "common/src/main/java/io/pravega/common/util/btree/sets/BTreeSet.java", "diffHunk": "@@ -0,0 +1,414 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.common.util.btree.sets;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArrayComparator;\n+import java.time.Duration;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Supplier;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.NotThreadSafe;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+/**\n+ * A B+Tree-backed Set. Stores all items in a B+Tree Structure using a {@link ByteArrayComparator} for ordering them.\n+ *\n+ * NOTE: This component is in {@link Beta}. There are no guarantees about data or API compatibility with future versions.\n+ * Any component that is directly dependent on this one should either be in {@link Beta} as well.\n+ */\n+@NotThreadSafe\n+@Beta\n+@Slf4j\n+public class BTreeSet {\n+    //region Members\n+\n+    public static final Comparator<ArrayView> COMPARATOR = new ByteArrayComparator()::compare;\n+    private static final Comparator<PagePointer> POINTER_COMPARATOR = PagePointer.getComparator(COMPARATOR);\n+\n+    private final int maxPageSize;\n+    private final int maxItemSize;\n+    @NonNull\n+    private final ReadPage read;\n+    @NonNull\n+    private final PersistPages update;\n+    @NonNull\n+    private final Executor executor;\n+    @NonNull\n+    private final String traceLogId;\n+\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the {@link BTreeSet} class.\n+     *\n+     * @param maxPageSize The maximum size, in bytes, of any page.\n+     * @param maxItemSize The maximum size, in bytes, of any single item in the {@link BTreeSet}.\n+     * @param read        A {@link ReadPage} function that can be used to fetch a single {@link BTreeSet} page from an\n+     *                    external data source.\n+     * @param update      A {@link PersistPages} function that can be used to store and delete multiple {@link BTreeSet}\n+     *                    pages to/from an external data source.\n+     * @param executor    Executor for async operations.\n+     * @param traceLogId  Trace id for logging.\n+     */\n+    public BTreeSet(int maxPageSize, int maxItemSize, @NonNull ReadPage read, @NonNull PersistPages update,\n+                    @NonNull Executor executor, String traceLogId) {\n+        Preconditions.checkArgument(maxItemSize < maxPageSize / 2, \"maxItemSize must be at most half of maxPageSize.\");\n+        this.maxItemSize = maxItemSize;\n+        this.maxPageSize = maxPageSize;\n+        this.read = read;\n+        this.update = update;\n+        this.executor = executor;\n+        this.traceLogId = traceLogId == null ? \"\" : traceLogId;\n+    }\n+\n+    //endregion\n+\n+    //region Updates\n+\n+    /**\n+     * Atomically inserts the items in 'toInsert' into the {@link BTreeSet} and removes the items in 'toRemove'\n+     * from the {@link BTreeSet}. No duplicates are allowed; the same item cannot exist multiple times in either 'toInsert'\n+     * or 'toRemove' or in both of them.\n+     *\n+     * @param toInsert      (Optional). A Collection of {@link ArrayView} instances representing the items to insert.\n+     *                      If an item is already present, it will not be reinserted (updates are idempotent).\n+     * @param toRemove      (Optional). A Collection of {@link ArrayView} instances representing the items to remove.\n+     * @param getNextPageId A Supplier that, when invoked, will return a unique number representing the Id of the next\n+     *                      {@link BTreeSet} page that has to be generated.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed normally, will indicate that the updates have been applied\n+     * successfully. If the operation failed, the Future will be completed with the appropriate exception.\n+     */\n+    public CompletableFuture<Void> update(@Nullable Collection<? extends ArrayView> toInsert, @Nullable Collection<? extends ArrayView> toRemove,\n+                                          @NonNull Supplier<Long> getNextPageId, @NonNull Duration timeout) {\n+        TimeoutTimer timer = new TimeoutTimer(timeout);\n+        val updates = new ArrayList<UpdateItem>();\n+        int insertCount = collectUpdates(toInsert, false, updates);\n+        int removeCount = collectUpdates(toRemove, true, updates);\n+        updates.sort(UpdateItem::compareTo);\n+        log.debug(\"{}: Update (Insert={}, Remove={}).\", this.traceLogId, insertCount, removeCount);\n+        if (updates.isEmpty()) {\n+            // Nothing to do.\n+            return CompletableFuture.completedFuture(null);\n+        }\n+\n+        // The updates are sorted, so any empty items will be placed first.\n+        Preconditions.checkArgument(updates.get(0).getItem().getLength() > 0, \"No empty items allowed.\");\n+        return applyUpdates(updates.iterator(), timer)\n+                .thenApply(pageCollection -> processModifiedPages(pageCollection, getNextPageId))\n+                .thenComposeAsync(pageCollection -> writePages(pageCollection, timer), this.executor);\n+    }\n+\n+    private int collectUpdates(Collection<? extends ArrayView> items, boolean isRemoval, List<UpdateItem> updates) {\n+        if (items == null) {\n+            return 0;\n+        }\n+\n+        for (val i : items) {\n+            Preconditions.checkArgument(i.getLength() <= this.maxItemSize,\n+                    \"Item exceeds maximum allowed length (%s).\", this.maxItemSize);\n+            updates.add(new UpdateItem(i, isRemoval));\n+        }\n+        return items.size();\n+    }\n+\n+    private CompletableFuture<PageCollection> applyUpdates(Iterator<UpdateItem> items, TimeoutTimer timer) {\n+        val pageCollection = new PageCollection();\n+        val lastPage = new AtomicReference<BTreeSetPage.LeafPage>(null);\n+        val lastPageUpdates = new ArrayList<UpdateItem>();\n+        return Futures.loop(\n+                items::hasNext,\n+                () -> {\n+                    // Locate the page where the update is to be executed. Do not apply it yet as it is more efficient\n+                    // to bulk-apply multiple at once. Collect all updates for each Page, and only apply them once we have\n+                    // \"moved on\" to another page.\n+                    val next = items.next();\n+                    return locatePage(next.getItem(), pageCollection, timer)\n+                            .thenAccept(page -> {\n+                                val last = lastPage.get();\n+                                if (page != last) {\n+                                    // This key goes to a different page than the one we were looking at.\n+                                    if (last != null) {\n+                                        // Commit the outstanding updates.\n+                                        last.update(lastPageUpdates);\n+                                    }\n+\n+                                    // Update the pointers.\n+                                    lastPage.set(page);\n+                                    lastPageUpdates.clear();\n+                                }\n+\n+                                // Record the current update.\n+                                lastPageUpdates.add(next);\n+                            });\n+                },\n+                this.executor)\n+                .thenApplyAsync(v -> {\n+                    // We need not forget to apply the last batch of updates from the last page.\n+                    if (lastPage.get() != null) {\n+                        lastPage.get().update(lastPageUpdates);\n+                    }\n+                    return pageCollection;\n+                }, this.executor);\n+    }\n+\n+    private PageCollection processModifiedPages(PageCollection pageCollection, Supplier<Long> getNewPageId) {\n+        Collection<BTreeSetPage> candidates = pageCollection.getLeafPages();\n+        while (!candidates.isEmpty()) {\n+            // Process each candidate and determine if it should be deleted or split into multiple pages.\n+            val tmc = new TreeModificationContext(pageCollection);\n+            for (BTreeSetPage p : candidates) {\n+                if (p.getItemCount() == 0) {\n+                    deletePage(p, tmc);\n+                } else {\n+                    splitPageIfNecessary(p, getNewPageId, tmc);\n+                }\n+            }\n+\n+            // Update those pages' parents.\n+            tmc.accept(BTreeSetPage.IndexPage::addChildren, BTreeSetPage.IndexPage::removeChildren, POINTER_COMPARATOR);\n+            candidates = tmc.getModifiedParents();\n+        }\n+\n+        pageCollection.getIndexPages().forEach(p -> {\n+            if (p.isModified()) {\n+                p.seal();\n+            }\n+        });\n+        return pageCollection;\n+    }\n+\n+    private void deletePage(BTreeSetPage p, TreeModificationContext context) {\n+        // Delete the page if it's empty, but only if it's not the root page.\n+        if (p.getPagePointer().hasParent()) {\n+            context.getPageCollection().pageDeleted(p);\n+            context.deleted(p.getPagePointer());\n+            log.debug(\"{}: Deleted empty page {}.\", this.traceLogId, p.getPagePointer());\n+        } else if (p.isIndexPage()) {\n+            p = BTreeSetPage.emptyLeafRoot();\n+            p.markModified();\n+            context.getPageCollection().pageUpdated(p);\n+            log.debug(\"{}: Replaced empty Index Root with empty Leaf Root.\", this.traceLogId);\n+        }\n+    }\n+\n+    private void splitPageIfNecessary(BTreeSetPage p, Supplier<Long> getNewPageId, TreeModificationContext context) {\n+        val splits = p.split(this.maxPageSize, getNewPageId);\n+        if (splits == null) {\n+            // No split necessary\n+            return;\n+        }\n+\n+        if (p.getPagePointer().hasParent()) {\n+            Preconditions.checkArgument(splits.get(0).getPagePointer().getPageId() == p.getPagePointer().getPageId(),\n+                    \"First split result (%s) not current page (%s).\", splits.get(0).getPagePointer(), p.getPagePointer());\n+        } else {\n+            // If we split the root, the new pages will already point to the root; we must create a blank\n+            // index root page, which will be updated in the next step.\n+            context.getPageCollection().pageUpdated(BTreeSetPage.emptyIndexRoot());\n+        }\n+\n+        splits.forEach(splitPage -> {\n+            context.getPageCollection().pageUpdated(splitPage);\n+            context.created(splitPage.getPagePointer());\n+        });\n+        log.debug(\"{}: Page '{}' split into {}: {}.\", this.traceLogId, p, splits.size(), splits);\n+    }\n+\n+    private CompletableFuture<Void> writePages(@NonNull PageCollection pageCollection, TimeoutTimer timer) {\n+        // Order the pages from bottom up. The upstream code may have limitations in how much it can update atomically,\n+        // so it may commit this in multiple non-atomic operations. If the process is interrupted mid-way then we want\n+        // to ensure that parent pages aren't updated before leaf pages (which would cause index corruptions - i.e., by\n+        // pointing to inexistent pages).\n+        val processedPageIds = new HashSet<Long>();\n+\n+        // First collect updates. Begin from the bottom (Leaf Pages).\n+        val toWrite = new ArrayList<Map.Entry<Long, ArrayView>>();\n+        collectWriteCandidates(pageCollection.getLeafPages(), toWrite, processedPageIds, pageCollection);\n+\n+        // Newly split pages may not be reachable from any modified Leaf Pages. Collect them too.\n+        collectWriteCandidates(pageCollection.getIndexPages(), toWrite, processedPageIds, pageCollection);\n+\n+        // Then collect deletions, making sure we also consider all their parents (which should be modified/deleted as well).\n+        collectWriteCandidates(pageCollection.getDeletedPagesParents(), toWrite, processedPageIds, pageCollection);\n+        log.debug(\"{}: Persist (Updates={}, Deletions={}).\", this.traceLogId, toWrite.size(), pageCollection.getDeletedPageIds().size());\n+        return this.update.apply(toWrite, pageCollection.getDeletedPageIds(), timer.getRemaining());\n+    }\n+\n+    private void collectWriteCandidates(Collection<BTreeSetPage> candidates, List<Map.Entry<Long, ArrayView>> toWrite,\n+                                        Set<Long> processedIds, PageCollection pageCollection) {\n+        while (!candidates.isEmpty()) {\n+            val next = new ArrayList<BTreeSetPage>();\n+            candidates.stream()\n+                    .filter(p -> p.isModified() && !processedIds.contains(p.getPagePointer().getPageId()))\n+                    .forEach(p -> {\n+                        toWrite.add(new AbstractMap.SimpleImmutableEntry<>(p.getPagePointer().getPageId(), p.getData()));\n+                        val parent = pageCollection.get(p.getPagePointer().getParentPageId());\n+                        assert p.getPagePointer().hasParent() == (parent != null);\n+                        processedIds.add(p.getPagePointer().getPageId());\n+                        if (parent != null) {\n+                            next.add(parent);\n+                        }\n+                    });\n+            candidates = next;\n+        }\n+    }\n+\n+    //endregion\n+\n+    //region Queries\n+\n+    /**\n+     * Returns an {@link AsyncIterator} that will iterate through all the items in this {@link BTreeSet} within the\n+     * specified bounds. All iterated items will be returned in lexicographic order (smallest to largest).\n+     * See {@link ByteArrayComparator} for ordering details.\n+     *\n+     * @param firstItem          An {@link ArrayView} indicating the first Item to iterate from. If null, the iteration\n+     *                           will begin with the first item in the index.\n+     * @param firstItemInclusive If true, firstIem will be included in the iteration (provided it exists), otherwise it", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjQ4NDI0MQ=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 296}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcyNTcxMQ==", "bodyText": "Some examples I could quickly came up with:\n\nJava IntStream.range(start, end) and IntStream.rangeClosed(start, end)\nJava NavigableSet\n\nheadSet\nsubSet\nSimilarly for NavigableMap\n\n\n\nNavigableSet and NavigableMap are what I based the APIs for BTreeSet and BTreeIndex on (more or less).", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r434725711", "createdAt": "2020-06-03T17:11:08Z", "author": {"login": "andreipaduroiu"}, "path": "common/src/main/java/io/pravega/common/util/btree/sets/BTreeSet.java", "diffHunk": "@@ -0,0 +1,414 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.common.util.btree.sets;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArrayComparator;\n+import java.time.Duration;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Supplier;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.NotThreadSafe;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+/**\n+ * A B+Tree-backed Set. Stores all items in a B+Tree Structure using a {@link ByteArrayComparator} for ordering them.\n+ *\n+ * NOTE: This component is in {@link Beta}. There are no guarantees about data or API compatibility with future versions.\n+ * Any component that is directly dependent on this one should either be in {@link Beta} as well.\n+ */\n+@NotThreadSafe\n+@Beta\n+@Slf4j\n+public class BTreeSet {\n+    //region Members\n+\n+    public static final Comparator<ArrayView> COMPARATOR = new ByteArrayComparator()::compare;\n+    private static final Comparator<PagePointer> POINTER_COMPARATOR = PagePointer.getComparator(COMPARATOR);\n+\n+    private final int maxPageSize;\n+    private final int maxItemSize;\n+    @NonNull\n+    private final ReadPage read;\n+    @NonNull\n+    private final PersistPages update;\n+    @NonNull\n+    private final Executor executor;\n+    @NonNull\n+    private final String traceLogId;\n+\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the {@link BTreeSet} class.\n+     *\n+     * @param maxPageSize The maximum size, in bytes, of any page.\n+     * @param maxItemSize The maximum size, in bytes, of any single item in the {@link BTreeSet}.\n+     * @param read        A {@link ReadPage} function that can be used to fetch a single {@link BTreeSet} page from an\n+     *                    external data source.\n+     * @param update      A {@link PersistPages} function that can be used to store and delete multiple {@link BTreeSet}\n+     *                    pages to/from an external data source.\n+     * @param executor    Executor for async operations.\n+     * @param traceLogId  Trace id for logging.\n+     */\n+    public BTreeSet(int maxPageSize, int maxItemSize, @NonNull ReadPage read, @NonNull PersistPages update,\n+                    @NonNull Executor executor, String traceLogId) {\n+        Preconditions.checkArgument(maxItemSize < maxPageSize / 2, \"maxItemSize must be at most half of maxPageSize.\");\n+        this.maxItemSize = maxItemSize;\n+        this.maxPageSize = maxPageSize;\n+        this.read = read;\n+        this.update = update;\n+        this.executor = executor;\n+        this.traceLogId = traceLogId == null ? \"\" : traceLogId;\n+    }\n+\n+    //endregion\n+\n+    //region Updates\n+\n+    /**\n+     * Atomically inserts the items in 'toInsert' into the {@link BTreeSet} and removes the items in 'toRemove'\n+     * from the {@link BTreeSet}. No duplicates are allowed; the same item cannot exist multiple times in either 'toInsert'\n+     * or 'toRemove' or in both of them.\n+     *\n+     * @param toInsert      (Optional). A Collection of {@link ArrayView} instances representing the items to insert.\n+     *                      If an item is already present, it will not be reinserted (updates are idempotent).\n+     * @param toRemove      (Optional). A Collection of {@link ArrayView} instances representing the items to remove.\n+     * @param getNextPageId A Supplier that, when invoked, will return a unique number representing the Id of the next\n+     *                      {@link BTreeSet} page that has to be generated.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed normally, will indicate that the updates have been applied\n+     * successfully. If the operation failed, the Future will be completed with the appropriate exception.\n+     */\n+    public CompletableFuture<Void> update(@Nullable Collection<? extends ArrayView> toInsert, @Nullable Collection<? extends ArrayView> toRemove,\n+                                          @NonNull Supplier<Long> getNextPageId, @NonNull Duration timeout) {\n+        TimeoutTimer timer = new TimeoutTimer(timeout);\n+        val updates = new ArrayList<UpdateItem>();\n+        int insertCount = collectUpdates(toInsert, false, updates);\n+        int removeCount = collectUpdates(toRemove, true, updates);\n+        updates.sort(UpdateItem::compareTo);\n+        log.debug(\"{}: Update (Insert={}, Remove={}).\", this.traceLogId, insertCount, removeCount);\n+        if (updates.isEmpty()) {\n+            // Nothing to do.\n+            return CompletableFuture.completedFuture(null);\n+        }\n+\n+        // The updates are sorted, so any empty items will be placed first.\n+        Preconditions.checkArgument(updates.get(0).getItem().getLength() > 0, \"No empty items allowed.\");\n+        return applyUpdates(updates.iterator(), timer)\n+                .thenApply(pageCollection -> processModifiedPages(pageCollection, getNextPageId))\n+                .thenComposeAsync(pageCollection -> writePages(pageCollection, timer), this.executor);\n+    }\n+\n+    private int collectUpdates(Collection<? extends ArrayView> items, boolean isRemoval, List<UpdateItem> updates) {\n+        if (items == null) {\n+            return 0;\n+        }\n+\n+        for (val i : items) {\n+            Preconditions.checkArgument(i.getLength() <= this.maxItemSize,\n+                    \"Item exceeds maximum allowed length (%s).\", this.maxItemSize);\n+            updates.add(new UpdateItem(i, isRemoval));\n+        }\n+        return items.size();\n+    }\n+\n+    private CompletableFuture<PageCollection> applyUpdates(Iterator<UpdateItem> items, TimeoutTimer timer) {\n+        val pageCollection = new PageCollection();\n+        val lastPage = new AtomicReference<BTreeSetPage.LeafPage>(null);\n+        val lastPageUpdates = new ArrayList<UpdateItem>();\n+        return Futures.loop(\n+                items::hasNext,\n+                () -> {\n+                    // Locate the page where the update is to be executed. Do not apply it yet as it is more efficient\n+                    // to bulk-apply multiple at once. Collect all updates for each Page, and only apply them once we have\n+                    // \"moved on\" to another page.\n+                    val next = items.next();\n+                    return locatePage(next.getItem(), pageCollection, timer)\n+                            .thenAccept(page -> {\n+                                val last = lastPage.get();\n+                                if (page != last) {\n+                                    // This key goes to a different page than the one we were looking at.\n+                                    if (last != null) {\n+                                        // Commit the outstanding updates.\n+                                        last.update(lastPageUpdates);\n+                                    }\n+\n+                                    // Update the pointers.\n+                                    lastPage.set(page);\n+                                    lastPageUpdates.clear();\n+                                }\n+\n+                                // Record the current update.\n+                                lastPageUpdates.add(next);\n+                            });\n+                },\n+                this.executor)\n+                .thenApplyAsync(v -> {\n+                    // We need not forget to apply the last batch of updates from the last page.\n+                    if (lastPage.get() != null) {\n+                        lastPage.get().update(lastPageUpdates);\n+                    }\n+                    return pageCollection;\n+                }, this.executor);\n+    }\n+\n+    private PageCollection processModifiedPages(PageCollection pageCollection, Supplier<Long> getNewPageId) {\n+        Collection<BTreeSetPage> candidates = pageCollection.getLeafPages();\n+        while (!candidates.isEmpty()) {\n+            // Process each candidate and determine if it should be deleted or split into multiple pages.\n+            val tmc = new TreeModificationContext(pageCollection);\n+            for (BTreeSetPage p : candidates) {\n+                if (p.getItemCount() == 0) {\n+                    deletePage(p, tmc);\n+                } else {\n+                    splitPageIfNecessary(p, getNewPageId, tmc);\n+                }\n+            }\n+\n+            // Update those pages' parents.\n+            tmc.accept(BTreeSetPage.IndexPage::addChildren, BTreeSetPage.IndexPage::removeChildren, POINTER_COMPARATOR);\n+            candidates = tmc.getModifiedParents();\n+        }\n+\n+        pageCollection.getIndexPages().forEach(p -> {\n+            if (p.isModified()) {\n+                p.seal();\n+            }\n+        });\n+        return pageCollection;\n+    }\n+\n+    private void deletePage(BTreeSetPage p, TreeModificationContext context) {\n+        // Delete the page if it's empty, but only if it's not the root page.\n+        if (p.getPagePointer().hasParent()) {\n+            context.getPageCollection().pageDeleted(p);\n+            context.deleted(p.getPagePointer());\n+            log.debug(\"{}: Deleted empty page {}.\", this.traceLogId, p.getPagePointer());\n+        } else if (p.isIndexPage()) {\n+            p = BTreeSetPage.emptyLeafRoot();\n+            p.markModified();\n+            context.getPageCollection().pageUpdated(p);\n+            log.debug(\"{}: Replaced empty Index Root with empty Leaf Root.\", this.traceLogId);\n+        }\n+    }\n+\n+    private void splitPageIfNecessary(BTreeSetPage p, Supplier<Long> getNewPageId, TreeModificationContext context) {\n+        val splits = p.split(this.maxPageSize, getNewPageId);\n+        if (splits == null) {\n+            // No split necessary\n+            return;\n+        }\n+\n+        if (p.getPagePointer().hasParent()) {\n+            Preconditions.checkArgument(splits.get(0).getPagePointer().getPageId() == p.getPagePointer().getPageId(),\n+                    \"First split result (%s) not current page (%s).\", splits.get(0).getPagePointer(), p.getPagePointer());\n+        } else {\n+            // If we split the root, the new pages will already point to the root; we must create a blank\n+            // index root page, which will be updated in the next step.\n+            context.getPageCollection().pageUpdated(BTreeSetPage.emptyIndexRoot());\n+        }\n+\n+        splits.forEach(splitPage -> {\n+            context.getPageCollection().pageUpdated(splitPage);\n+            context.created(splitPage.getPagePointer());\n+        });\n+        log.debug(\"{}: Page '{}' split into {}: {}.\", this.traceLogId, p, splits.size(), splits);\n+    }\n+\n+    private CompletableFuture<Void> writePages(@NonNull PageCollection pageCollection, TimeoutTimer timer) {\n+        // Order the pages from bottom up. The upstream code may have limitations in how much it can update atomically,\n+        // so it may commit this in multiple non-atomic operations. If the process is interrupted mid-way then we want\n+        // to ensure that parent pages aren't updated before leaf pages (which would cause index corruptions - i.e., by\n+        // pointing to inexistent pages).\n+        val processedPageIds = new HashSet<Long>();\n+\n+        // First collect updates. Begin from the bottom (Leaf Pages).\n+        val toWrite = new ArrayList<Map.Entry<Long, ArrayView>>();\n+        collectWriteCandidates(pageCollection.getLeafPages(), toWrite, processedPageIds, pageCollection);\n+\n+        // Newly split pages may not be reachable from any modified Leaf Pages. Collect them too.\n+        collectWriteCandidates(pageCollection.getIndexPages(), toWrite, processedPageIds, pageCollection);\n+\n+        // Then collect deletions, making sure we also consider all their parents (which should be modified/deleted as well).\n+        collectWriteCandidates(pageCollection.getDeletedPagesParents(), toWrite, processedPageIds, pageCollection);\n+        log.debug(\"{}: Persist (Updates={}, Deletions={}).\", this.traceLogId, toWrite.size(), pageCollection.getDeletedPageIds().size());\n+        return this.update.apply(toWrite, pageCollection.getDeletedPageIds(), timer.getRemaining());\n+    }\n+\n+    private void collectWriteCandidates(Collection<BTreeSetPage> candidates, List<Map.Entry<Long, ArrayView>> toWrite,\n+                                        Set<Long> processedIds, PageCollection pageCollection) {\n+        while (!candidates.isEmpty()) {\n+            val next = new ArrayList<BTreeSetPage>();\n+            candidates.stream()\n+                    .filter(p -> p.isModified() && !processedIds.contains(p.getPagePointer().getPageId()))\n+                    .forEach(p -> {\n+                        toWrite.add(new AbstractMap.SimpleImmutableEntry<>(p.getPagePointer().getPageId(), p.getData()));\n+                        val parent = pageCollection.get(p.getPagePointer().getParentPageId());\n+                        assert p.getPagePointer().hasParent() == (parent != null);\n+                        processedIds.add(p.getPagePointer().getPageId());\n+                        if (parent != null) {\n+                            next.add(parent);\n+                        }\n+                    });\n+            candidates = next;\n+        }\n+    }\n+\n+    //endregion\n+\n+    //region Queries\n+\n+    /**\n+     * Returns an {@link AsyncIterator} that will iterate through all the items in this {@link BTreeSet} within the\n+     * specified bounds. All iterated items will be returned in lexicographic order (smallest to largest).\n+     * See {@link ByteArrayComparator} for ordering details.\n+     *\n+     * @param firstItem          An {@link ArrayView} indicating the first Item to iterate from. If null, the iteration\n+     *                           will begin with the first item in the index.\n+     * @param firstItemInclusive If true, firstIem will be included in the iteration (provided it exists), otherwise it", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjQ4NDI0MQ=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 296}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MzcyMTA2OnYy", "diffSide": "RIGHT", "path": "common/src/main/java/io/pravega/common/util/btree/sets/BTreeSetPage.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOVQxMzozODo0MlrOGcc_iA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQxNzoyNzozN1rOGf3DAA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjQ4ODMyOA==", "bodyText": "It is not great that we have to create an exception for the serialization versioning. I understand that the concern is the overhead of serializing, but could you elaborate on why you believe this is concern that justifies creating an exception for not using VersionedSerializer? Is there a way of using VersionedSerializer that could solve the issue, e.g., changing a version such that it is not backwards compatible when the overhead is high?", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r432488328", "createdAt": "2020-05-29T13:38:42Z", "author": {"login": "fpj"}, "path": "common/src/main/java/io/pravega/common/util/btree/sets/BTreeSetPage.java", "diffHunk": "@@ -0,0 +1,879 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.common.util.btree.sets;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.BitConverter;\n+import io.pravega.common.util.ByteArrayComparator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.common.util.IllegalDataFormatException;\n+import io.pravega.common.util.btree.SearchResult;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.function.Supplier;\n+import javax.annotation.concurrent.NotThreadSafe;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.RequiredArgsConstructor;\n+import lombok.val;\n+\n+/**\n+ * Represents a Page (Node) within a {@link BTreeSet}. Pages can be of type {@link IndexPage} or {@link LeafPage}.\n+ */\n+@NotThreadSafe\n+abstract class BTreeSetPage {\n+    //region Serialization format\n+\n+    /**\n+     * Format Version related fields. The version itself is the first byte of the serialization. When we will have to\n+     * support multiple versions, we will need to read this byte and choose the appropriate deserialization approach.\n+     * We cannot use VersionedSerializer in here - doing so would prevent us from efficiently querying and modifying the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjU4MDUxNQ==", "bodyText": "The problem with VersionedSerializer is that it forces me to read the whole input (buffer) and store it in another buffer. That is, it reads the data before it gives it to me.\nIn this case, since I'm always dealing with huge chunks of data, reading it (and copying it, since that's what VersionedSerializer does) will add extra CPU and GC overhead that will slow down the whole process.\nThis reason alone was enough for me to choose not to use the convenience of VersionedSerializer and use a version flag instead.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r432580515", "createdAt": "2020-05-29T15:55:17Z", "author": {"login": "andreipaduroiu"}, "path": "common/src/main/java/io/pravega/common/util/btree/sets/BTreeSetPage.java", "diffHunk": "@@ -0,0 +1,879 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.common.util.btree.sets;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.BitConverter;\n+import io.pravega.common.util.ByteArrayComparator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.common.util.IllegalDataFormatException;\n+import io.pravega.common.util.btree.SearchResult;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.function.Supplier;\n+import javax.annotation.concurrent.NotThreadSafe;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.RequiredArgsConstructor;\n+import lombok.val;\n+\n+/**\n+ * Represents a Page (Node) within a {@link BTreeSet}. Pages can be of type {@link IndexPage} or {@link LeafPage}.\n+ */\n+@NotThreadSafe\n+abstract class BTreeSetPage {\n+    //region Serialization format\n+\n+    /**\n+     * Format Version related fields. The version itself is the first byte of the serialization. When we will have to\n+     * support multiple versions, we will need to read this byte and choose the appropriate deserialization approach.\n+     * We cannot use VersionedSerializer in here - doing so would prevent us from efficiently querying and modifying the", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjQ4ODMyOA=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDUxMzk5MQ==", "bodyText": "I'm concerned that at this point I don't understand when it is acceptable to not comply and version directly instead of using the versioned serializer. One of the main goals of versioned serializer was to make sure that we are versioning persistent and external data structures across Pravega in a consistent manner.\nIt might be acceptable to do it due to the performance concern, but I'm wondering what the criteria is for exceptions for other PRs.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r434513991", "createdAt": "2020-06-03T12:01:08Z", "author": {"login": "fpj"}, "path": "common/src/main/java/io/pravega/common/util/btree/sets/BTreeSetPage.java", "diffHunk": "@@ -0,0 +1,879 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.common.util.btree.sets;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.BitConverter;\n+import io.pravega.common.util.ByteArrayComparator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.common.util.IllegalDataFormatException;\n+import io.pravega.common.util.btree.SearchResult;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.function.Supplier;\n+import javax.annotation.concurrent.NotThreadSafe;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.RequiredArgsConstructor;\n+import lombok.val;\n+\n+/**\n+ * Represents a Page (Node) within a {@link BTreeSet}. Pages can be of type {@link IndexPage} or {@link LeafPage}.\n+ */\n+@NotThreadSafe\n+abstract class BTreeSetPage {\n+    //region Serialization format\n+\n+    /**\n+     * Format Version related fields. The version itself is the first byte of the serialization. When we will have to\n+     * support multiple versions, we will need to read this byte and choose the appropriate deserialization approach.\n+     * We cannot use VersionedSerializer in here - doing so would prevent us from efficiently querying and modifying the", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjQ4ODMyOA=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDczNDQ5OQ==", "bodyText": "Every other place where we use VersionedSerializer we have very small amounts of data to store and the act of writing/reading is rare (i.e., we read data from somewhere and store it as objects in memory and we occasionally need to persist some updates).\nThis data structure contains very little metadata and a lot of external (user) data which is opaque to us. We do a lot of frequent reads and we should not be holding that data in the Java heap long-term. When we read such a node from wherever we store it we deal with a lot of data (1MB per read) that we want to avoid having to re-read and copy in memory. VersionedSerializer would force that copy. By doing it with a version flag we can avoid that.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r434734499", "createdAt": "2020-06-03T17:26:18Z", "author": {"login": "andreipaduroiu"}, "path": "common/src/main/java/io/pravega/common/util/btree/sets/BTreeSetPage.java", "diffHunk": "@@ -0,0 +1,879 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.common.util.btree.sets;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.BitConverter;\n+import io.pravega.common.util.ByteArrayComparator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.common.util.IllegalDataFormatException;\n+import io.pravega.common.util.btree.SearchResult;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.function.Supplier;\n+import javax.annotation.concurrent.NotThreadSafe;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.RequiredArgsConstructor;\n+import lombok.val;\n+\n+/**\n+ * Represents a Page (Node) within a {@link BTreeSet}. Pages can be of type {@link IndexPage} or {@link LeafPage}.\n+ */\n+@NotThreadSafe\n+abstract class BTreeSetPage {\n+    //region Serialization format\n+\n+    /**\n+     * Format Version related fields. The version itself is the first byte of the serialization. When we will have to\n+     * support multiple versions, we will need to read this byte and choose the appropriate deserialization approach.\n+     * We cannot use VersionedSerializer in here - doing so would prevent us from efficiently querying and modifying the", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjQ4ODMyOA=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjA2MDkyOA==", "bodyText": "(copy-pasting offline conversation conclusion)\nThe way to perform an upgrade with this format is as follows:\n\nWe can create new serialization versions by adding new fields at the end of the current serialization.\nThere isn't anything to do at this time in terms of accepting a larger input for BTreeSetPage. The format encodes the length of the BTreeSetPage and the parsing logic validates that the given buffer isn't smaller.\n\nThe page format is: {Header}{ItemOffsets}{ItemData}{PageId}.\nThe length of an item is calculated by getting the difference between that item's offset and the next item's offset.\nThen line 152 slices the contents buffer to the size encoded in the header.\n\n\nSo in case of a backwards compatible upgrade (from Version 1), it would work like this:\n\nNew code defines Version 2, but does not use it yet (it just understands it)\nNew code will serialize using Version 1, but encode its additional data after the end of the buffer. It will make sure that the size encoded in the header ends where the old code expects it to.\nOld code will read this size and slice the input, essentially dropping new data.\nNew code will be programmed to read past this (if anything) and process any additional fields.\nEventually, in a subsequent release, we will change the code to serialize to Version 2, thus completing the upgrade.\n\n\n\nNot as smooth as VersionedSerializer (which was doing all this legwork for us), but this is the tradeoff I'm willing to accept for better perf.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r436060928", "createdAt": "2020-06-05T17:27:37Z", "author": {"login": "andreipaduroiu"}, "path": "common/src/main/java/io/pravega/common/util/btree/sets/BTreeSetPage.java", "diffHunk": "@@ -0,0 +1,879 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.common.util.btree.sets;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.BitConverter;\n+import io.pravega.common.util.ByteArrayComparator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.common.util.IllegalDataFormatException;\n+import io.pravega.common.util.btree.SearchResult;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.function.Supplier;\n+import javax.annotation.concurrent.NotThreadSafe;\n+import lombok.Getter;\n+import lombok.NonNull;\n+import lombok.RequiredArgsConstructor;\n+import lombok.val;\n+\n+/**\n+ * Represents a Page (Node) within a {@link BTreeSet}. Pages can be of type {@link IndexPage} or {@link LeafPage}.\n+ */\n+@NotThreadSafe\n+abstract class BTreeSetPage {\n+    //region Serialization format\n+\n+    /**\n+     * Format Version related fields. The version itself is the first byte of the serialization. When we will have to\n+     * support multiple versions, we will need to read this byte and choose the appropriate deserialization approach.\n+     * We cannot use VersionedSerializer in here - doing so would prevent us from efficiently querying and modifying the", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjQ4ODMyOA=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwMTQ0MjI1OnYy", "diffSide": "RIGHT", "path": "common/src/main/java/io/pravega/common/util/btree/sets/BTreeSet.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQwNzozOToyOFrOGdls3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQwNzozOToyOFrOGdls3Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzY3OTU4MQ==", "bodyText": "There is a typo in firstIem.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r433679581", "createdAt": "2020-06-02T07:39:28Z", "author": {"login": "fpj"}, "path": "common/src/main/java/io/pravega/common/util/btree/sets/BTreeSet.java", "diffHunk": "@@ -0,0 +1,414 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.common.util.btree.sets;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArrayComparator;\n+import java.time.Duration;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Supplier;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.NotThreadSafe;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+/**\n+ * A B+Tree-backed Set. Stores all items in a B+Tree Structure using a {@link ByteArrayComparator} for ordering them.\n+ *\n+ * NOTE: This component is in {@link Beta}. There are no guarantees about data or API compatibility with future versions.\n+ * Any component that is directly dependent on this one should either be in {@link Beta} as well.\n+ */\n+@NotThreadSafe\n+@Beta\n+@Slf4j\n+public class BTreeSet {\n+    //region Members\n+\n+    public static final Comparator<ArrayView> COMPARATOR = new ByteArrayComparator()::compare;\n+    private static final Comparator<PagePointer> POINTER_COMPARATOR = PagePointer.getComparator(COMPARATOR);\n+\n+    private final int maxPageSize;\n+    private final int maxItemSize;\n+    @NonNull\n+    private final ReadPage read;\n+    @NonNull\n+    private final PersistPages update;\n+    @NonNull\n+    private final Executor executor;\n+    @NonNull\n+    private final String traceLogId;\n+\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the {@link BTreeSet} class.\n+     *\n+     * @param maxPageSize The maximum size, in bytes, of any page.\n+     * @param maxItemSize The maximum size, in bytes, of any single item in the {@link BTreeSet}.\n+     * @param read        A {@link ReadPage} function that can be used to fetch a single {@link BTreeSet} page from an\n+     *                    external data source.\n+     * @param update      A {@link PersistPages} function that can be used to store and delete multiple {@link BTreeSet}\n+     *                    pages to/from an external data source.\n+     * @param executor    Executor for async operations.\n+     * @param traceLogId  Trace id for logging.\n+     */\n+    public BTreeSet(int maxPageSize, int maxItemSize, @NonNull ReadPage read, @NonNull PersistPages update,\n+                    @NonNull Executor executor, String traceLogId) {\n+        Preconditions.checkArgument(maxItemSize < maxPageSize / 2, \"maxItemSize must be at most half of maxPageSize.\");\n+        this.maxItemSize = maxItemSize;\n+        this.maxPageSize = maxPageSize;\n+        this.read = read;\n+        this.update = update;\n+        this.executor = executor;\n+        this.traceLogId = traceLogId == null ? \"\" : traceLogId;\n+    }\n+\n+    //endregion\n+\n+    //region Updates\n+\n+    /**\n+     * Atomically inserts the items in 'toInsert' into the {@link BTreeSet} and removes the items in 'toRemove'\n+     * from the {@link BTreeSet}. No duplicates are allowed; the same item cannot exist multiple times in either 'toInsert'\n+     * or 'toRemove' or in both of them.\n+     *\n+     * @param toInsert      (Optional). A Collection of {@link ArrayView} instances representing the items to insert.\n+     *                      If an item is already present, it will not be reinserted (updates are idempotent).\n+     * @param toRemove      (Optional). A Collection of {@link ArrayView} instances representing the items to remove.\n+     * @param getNextPageId A Supplier that, when invoked, will return a unique number representing the Id of the next\n+     *                      {@link BTreeSet} page that has to be generated.\n+     * @param timeout       Timeout for the operation.\n+     * @return A CompletableFuture that, when completed normally, will indicate that the updates have been applied\n+     * successfully. If the operation failed, the Future will be completed with the appropriate exception.\n+     */\n+    public CompletableFuture<Void> update(@Nullable Collection<? extends ArrayView> toInsert, @Nullable Collection<? extends ArrayView> toRemove,\n+                                          @NonNull Supplier<Long> getNextPageId, @NonNull Duration timeout) {\n+        TimeoutTimer timer = new TimeoutTimer(timeout);\n+        val updates = new ArrayList<UpdateItem>();\n+        int insertCount = collectUpdates(toInsert, false, updates);\n+        int removeCount = collectUpdates(toRemove, true, updates);\n+        updates.sort(UpdateItem::compareTo);\n+        log.debug(\"{}: Update (Insert={}, Remove={}).\", this.traceLogId, insertCount, removeCount);\n+        if (updates.isEmpty()) {\n+            // Nothing to do.\n+            return CompletableFuture.completedFuture(null);\n+        }\n+\n+        // The updates are sorted, so any empty items will be placed first.\n+        Preconditions.checkArgument(updates.get(0).getItem().getLength() > 0, \"No empty items allowed.\");\n+        return applyUpdates(updates.iterator(), timer)\n+                .thenApply(pageCollection -> processModifiedPages(pageCollection, getNextPageId))\n+                .thenComposeAsync(pageCollection -> writePages(pageCollection, timer), this.executor);\n+    }\n+\n+    private int collectUpdates(Collection<? extends ArrayView> items, boolean isRemoval, List<UpdateItem> updates) {\n+        if (items == null) {\n+            return 0;\n+        }\n+\n+        for (val i : items) {\n+            Preconditions.checkArgument(i.getLength() <= this.maxItemSize,\n+                    \"Item exceeds maximum allowed length (%s).\", this.maxItemSize);\n+            updates.add(new UpdateItem(i, isRemoval));\n+        }\n+        return items.size();\n+    }\n+\n+    private CompletableFuture<PageCollection> applyUpdates(Iterator<UpdateItem> items, TimeoutTimer timer) {\n+        val pageCollection = new PageCollection();\n+        val lastPage = new AtomicReference<BTreeSetPage.LeafPage>(null);\n+        val lastPageUpdates = new ArrayList<UpdateItem>();\n+        return Futures.loop(\n+                items::hasNext,\n+                () -> {\n+                    // Locate the page where the update is to be executed. Do not apply it yet as it is more efficient\n+                    // to bulk-apply multiple at once. Collect all updates for each Page, and only apply them once we have\n+                    // \"moved on\" to another page.\n+                    val next = items.next();\n+                    return locatePage(next.getItem(), pageCollection, timer)\n+                            .thenAccept(page -> {\n+                                val last = lastPage.get();\n+                                if (page != last) {\n+                                    // This key goes to a different page than the one we were looking at.\n+                                    if (last != null) {\n+                                        // Commit the outstanding updates.\n+                                        last.update(lastPageUpdates);\n+                                    }\n+\n+                                    // Update the pointers.\n+                                    lastPage.set(page);\n+                                    lastPageUpdates.clear();\n+                                }\n+\n+                                // Record the current update.\n+                                lastPageUpdates.add(next);\n+                            });\n+                },\n+                this.executor)\n+                .thenApplyAsync(v -> {\n+                    // We need not forget to apply the last batch of updates from the last page.\n+                    if (lastPage.get() != null) {\n+                        lastPage.get().update(lastPageUpdates);\n+                    }\n+                    return pageCollection;\n+                }, this.executor);\n+    }\n+\n+    private PageCollection processModifiedPages(PageCollection pageCollection, Supplier<Long> getNewPageId) {\n+        Collection<BTreeSetPage> candidates = pageCollection.getLeafPages();\n+        while (!candidates.isEmpty()) {\n+            // Process each candidate and determine if it should be deleted or split into multiple pages.\n+            val tmc = new TreeModificationContext(pageCollection);\n+            for (BTreeSetPage p : candidates) {\n+                if (p.getItemCount() == 0) {\n+                    deletePage(p, tmc);\n+                } else {\n+                    splitPageIfNecessary(p, getNewPageId, tmc);\n+                }\n+            }\n+\n+            // Update those pages' parents.\n+            tmc.accept(BTreeSetPage.IndexPage::addChildren, BTreeSetPage.IndexPage::removeChildren, POINTER_COMPARATOR);\n+            candidates = tmc.getModifiedParents();\n+        }\n+\n+        pageCollection.getIndexPages().forEach(p -> {\n+            if (p.isModified()) {\n+                p.seal();\n+            }\n+        });\n+        return pageCollection;\n+    }\n+\n+    private void deletePage(BTreeSetPage p, TreeModificationContext context) {\n+        // Delete the page if it's empty, but only if it's not the root page.\n+        if (p.getPagePointer().hasParent()) {\n+            context.getPageCollection().pageDeleted(p);\n+            context.deleted(p.getPagePointer());\n+            log.debug(\"{}: Deleted empty page {}.\", this.traceLogId, p.getPagePointer());\n+        } else if (p.isIndexPage()) {\n+            p = BTreeSetPage.emptyLeafRoot();\n+            p.markModified();\n+            context.getPageCollection().pageUpdated(p);\n+            log.debug(\"{}: Replaced empty Index Root with empty Leaf Root.\", this.traceLogId);\n+        }\n+    }\n+\n+    private void splitPageIfNecessary(BTreeSetPage p, Supplier<Long> getNewPageId, TreeModificationContext context) {\n+        val splits = p.split(this.maxPageSize, getNewPageId);\n+        if (splits == null) {\n+            // No split necessary\n+            return;\n+        }\n+\n+        if (p.getPagePointer().hasParent()) {\n+            Preconditions.checkArgument(splits.get(0).getPagePointer().getPageId() == p.getPagePointer().getPageId(),\n+                    \"First split result (%s) not current page (%s).\", splits.get(0).getPagePointer(), p.getPagePointer());\n+        } else {\n+            // If we split the root, the new pages will already point to the root; we must create a blank\n+            // index root page, which will be updated in the next step.\n+            context.getPageCollection().pageUpdated(BTreeSetPage.emptyIndexRoot());\n+        }\n+\n+        splits.forEach(splitPage -> {\n+            context.getPageCollection().pageUpdated(splitPage);\n+            context.created(splitPage.getPagePointer());\n+        });\n+        log.debug(\"{}: Page '{}' split into {}: {}.\", this.traceLogId, p, splits.size(), splits);\n+    }\n+\n+    private CompletableFuture<Void> writePages(@NonNull PageCollection pageCollection, TimeoutTimer timer) {\n+        // Order the pages from bottom up. The upstream code may have limitations in how much it can update atomically,\n+        // so it may commit this in multiple non-atomic operations. If the process is interrupted mid-way then we want\n+        // to ensure that parent pages aren't updated before leaf pages (which would cause index corruptions - i.e., by\n+        // pointing to inexistent pages).\n+        val processedPageIds = new HashSet<Long>();\n+\n+        // First collect updates. Begin from the bottom (Leaf Pages).\n+        val toWrite = new ArrayList<Map.Entry<Long, ArrayView>>();\n+        collectWriteCandidates(pageCollection.getLeafPages(), toWrite, processedPageIds, pageCollection);\n+\n+        // Newly split pages may not be reachable from any modified Leaf Pages. Collect them too.\n+        collectWriteCandidates(pageCollection.getIndexPages(), toWrite, processedPageIds, pageCollection);\n+\n+        // Then collect deletions, making sure we also consider all their parents (which should be modified/deleted as well).\n+        collectWriteCandidates(pageCollection.getDeletedPagesParents(), toWrite, processedPageIds, pageCollection);\n+        log.debug(\"{}: Persist (Updates={}, Deletions={}).\", this.traceLogId, toWrite.size(), pageCollection.getDeletedPageIds().size());\n+        return this.update.apply(toWrite, pageCollection.getDeletedPageIds(), timer.getRemaining());\n+    }\n+\n+    private void collectWriteCandidates(Collection<BTreeSetPage> candidates, List<Map.Entry<Long, ArrayView>> toWrite,\n+                                        Set<Long> processedIds, PageCollection pageCollection) {\n+        while (!candidates.isEmpty()) {\n+            val next = new ArrayList<BTreeSetPage>();\n+            candidates.stream()\n+                    .filter(p -> p.isModified() && !processedIds.contains(p.getPagePointer().getPageId()))\n+                    .forEach(p -> {\n+                        toWrite.add(new AbstractMap.SimpleImmutableEntry<>(p.getPagePointer().getPageId(), p.getData()));\n+                        val parent = pageCollection.get(p.getPagePointer().getParentPageId());\n+                        assert p.getPagePointer().hasParent() == (parent != null);\n+                        processedIds.add(p.getPagePointer().getPageId());\n+                        if (parent != null) {\n+                            next.add(parent);\n+                        }\n+                    });\n+            candidates = next;\n+        }\n+    }\n+\n+    //endregion\n+\n+    //region Queries\n+\n+    /**\n+     * Returns an {@link AsyncIterator} that will iterate through all the items in this {@link BTreeSet} within the\n+     * specified bounds. All iterated items will be returned in lexicographic order (smallest to largest).\n+     * See {@link ByteArrayComparator} for ordering details.\n+     *\n+     * @param firstItem          An {@link ArrayView} indicating the first Item to iterate from. If null, the iteration\n+     *                           will begin with the first item in the index.\n+     * @param firstItemInclusive If true, firstIem will be included in the iteration (provided it exists), otherwise it", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 296}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwMjM4Mjc5OnYy", "diffSide": "RIGHT", "path": "common/src/main/java/io/pravega/common/util/btree/sets/BTreeSet.java", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMjoyMDozMVrOGdvCNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzoyOTowMFrOGemL4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzgzMjUwMA==", "bodyText": "When it says \"No duplicates are allowed\", does it mean that a duplicate would cause the execution of the method to return an error? What happens if there is a duplicate in the collection?", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r433832500", "createdAt": "2020-06-02T12:20:31Z", "author": {"login": "fpj"}, "path": "common/src/main/java/io/pravega/common/util/btree/sets/BTreeSet.java", "diffHunk": "@@ -0,0 +1,414 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.common.util.btree.sets;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArrayComparator;\n+import java.time.Duration;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Supplier;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.NotThreadSafe;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+/**\n+ * A B+Tree-backed Set. Stores all items in a B+Tree Structure using a {@link ByteArrayComparator} for ordering them.\n+ *\n+ * NOTE: This component is in {@link Beta}. There are no guarantees about data or API compatibility with future versions.\n+ * Any component that is directly dependent on this one should either be in {@link Beta} as well.\n+ */\n+@NotThreadSafe\n+@Beta\n+@Slf4j\n+public class BTreeSet {\n+    //region Members\n+\n+    public static final Comparator<ArrayView> COMPARATOR = new ByteArrayComparator()::compare;\n+    private static final Comparator<PagePointer> POINTER_COMPARATOR = PagePointer.getComparator(COMPARATOR);\n+\n+    private final int maxPageSize;\n+    private final int maxItemSize;\n+    @NonNull\n+    private final ReadPage read;\n+    @NonNull\n+    private final PersistPages update;\n+    @NonNull\n+    private final Executor executor;\n+    @NonNull\n+    private final String traceLogId;\n+\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the {@link BTreeSet} class.\n+     *\n+     * @param maxPageSize The maximum size, in bytes, of any page.\n+     * @param maxItemSize The maximum size, in bytes, of any single item in the {@link BTreeSet}.\n+     * @param read        A {@link ReadPage} function that can be used to fetch a single {@link BTreeSet} page from an\n+     *                    external data source.\n+     * @param update      A {@link PersistPages} function that can be used to store and delete multiple {@link BTreeSet}\n+     *                    pages to/from an external data source.\n+     * @param executor    Executor for async operations.\n+     * @param traceLogId  Trace id for logging.\n+     */\n+    public BTreeSet(int maxPageSize, int maxItemSize, @NonNull ReadPage read, @NonNull PersistPages update,\n+                    @NonNull Executor executor, String traceLogId) {\n+        Preconditions.checkArgument(maxItemSize < maxPageSize / 2, \"maxItemSize must be at most half of maxPageSize.\");\n+        this.maxItemSize = maxItemSize;\n+        this.maxPageSize = maxPageSize;\n+        this.read = read;\n+        this.update = update;\n+        this.executor = executor;\n+        this.traceLogId = traceLogId == null ? \"\" : traceLogId;\n+    }\n+\n+    //endregion\n+\n+    //region Updates\n+\n+    /**\n+     * Atomically inserts the items in 'toInsert' into the {@link BTreeSet} and removes the items in 'toRemove'\n+     * from the {@link BTreeSet}. No duplicates are allowed; the same item cannot exist multiple times in either 'toInsert'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzk1Njc5Ng==", "bodyText": "An IllegalArgumentException is thrown.\nFor efficiency reasons, this is bubbled up from BTreeSetPage.preProcessUpdate because the inbound collection may not be sorted. I need to sort it and them pass slices of it to the appropriate BTreeSetPage which makes the determination (it's easy to spot duplicates in a sorted list without extra overhead).", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r433956796", "createdAt": "2020-06-02T15:17:45Z", "author": {"login": "andreipaduroiu"}, "path": "common/src/main/java/io/pravega/common/util/btree/sets/BTreeSet.java", "diffHunk": "@@ -0,0 +1,414 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.common.util.btree.sets;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArrayComparator;\n+import java.time.Duration;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Supplier;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.NotThreadSafe;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+/**\n+ * A B+Tree-backed Set. Stores all items in a B+Tree Structure using a {@link ByteArrayComparator} for ordering them.\n+ *\n+ * NOTE: This component is in {@link Beta}. There are no guarantees about data or API compatibility with future versions.\n+ * Any component that is directly dependent on this one should either be in {@link Beta} as well.\n+ */\n+@NotThreadSafe\n+@Beta\n+@Slf4j\n+public class BTreeSet {\n+    //region Members\n+\n+    public static final Comparator<ArrayView> COMPARATOR = new ByteArrayComparator()::compare;\n+    private static final Comparator<PagePointer> POINTER_COMPARATOR = PagePointer.getComparator(COMPARATOR);\n+\n+    private final int maxPageSize;\n+    private final int maxItemSize;\n+    @NonNull\n+    private final ReadPage read;\n+    @NonNull\n+    private final PersistPages update;\n+    @NonNull\n+    private final Executor executor;\n+    @NonNull\n+    private final String traceLogId;\n+\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the {@link BTreeSet} class.\n+     *\n+     * @param maxPageSize The maximum size, in bytes, of any page.\n+     * @param maxItemSize The maximum size, in bytes, of any single item in the {@link BTreeSet}.\n+     * @param read        A {@link ReadPage} function that can be used to fetch a single {@link BTreeSet} page from an\n+     *                    external data source.\n+     * @param update      A {@link PersistPages} function that can be used to store and delete multiple {@link BTreeSet}\n+     *                    pages to/from an external data source.\n+     * @param executor    Executor for async operations.\n+     * @param traceLogId  Trace id for logging.\n+     */\n+    public BTreeSet(int maxPageSize, int maxItemSize, @NonNull ReadPage read, @NonNull PersistPages update,\n+                    @NonNull Executor executor, String traceLogId) {\n+        Preconditions.checkArgument(maxItemSize < maxPageSize / 2, \"maxItemSize must be at most half of maxPageSize.\");\n+        this.maxItemSize = maxItemSize;\n+        this.maxPageSize = maxPageSize;\n+        this.read = read;\n+        this.update = update;\n+        this.executor = executor;\n+        this.traceLogId = traceLogId == null ? \"\" : traceLogId;\n+    }\n+\n+    //endregion\n+\n+    //region Updates\n+\n+    /**\n+     * Atomically inserts the items in 'toInsert' into the {@link BTreeSet} and removes the items in 'toRemove'\n+     * from the {@link BTreeSet}. No duplicates are allowed; the same item cannot exist multiple times in either 'toInsert'", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzgzMjUwMA=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDUwODM1OA==", "bodyText": "Could we update the comment to reflect what happens in the presence of a duplicate?", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r434508358", "createdAt": "2020-06-03T11:50:35Z", "author": {"login": "fpj"}, "path": "common/src/main/java/io/pravega/common/util/btree/sets/BTreeSet.java", "diffHunk": "@@ -0,0 +1,414 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.common.util.btree.sets;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArrayComparator;\n+import java.time.Duration;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Supplier;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.NotThreadSafe;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+/**\n+ * A B+Tree-backed Set. Stores all items in a B+Tree Structure using a {@link ByteArrayComparator} for ordering them.\n+ *\n+ * NOTE: This component is in {@link Beta}. There are no guarantees about data or API compatibility with future versions.\n+ * Any component that is directly dependent on this one should either be in {@link Beta} as well.\n+ */\n+@NotThreadSafe\n+@Beta\n+@Slf4j\n+public class BTreeSet {\n+    //region Members\n+\n+    public static final Comparator<ArrayView> COMPARATOR = new ByteArrayComparator()::compare;\n+    private static final Comparator<PagePointer> POINTER_COMPARATOR = PagePointer.getComparator(COMPARATOR);\n+\n+    private final int maxPageSize;\n+    private final int maxItemSize;\n+    @NonNull\n+    private final ReadPage read;\n+    @NonNull\n+    private final PersistPages update;\n+    @NonNull\n+    private final Executor executor;\n+    @NonNull\n+    private final String traceLogId;\n+\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the {@link BTreeSet} class.\n+     *\n+     * @param maxPageSize The maximum size, in bytes, of any page.\n+     * @param maxItemSize The maximum size, in bytes, of any single item in the {@link BTreeSet}.\n+     * @param read        A {@link ReadPage} function that can be used to fetch a single {@link BTreeSet} page from an\n+     *                    external data source.\n+     * @param update      A {@link PersistPages} function that can be used to store and delete multiple {@link BTreeSet}\n+     *                    pages to/from an external data source.\n+     * @param executor    Executor for async operations.\n+     * @param traceLogId  Trace id for logging.\n+     */\n+    public BTreeSet(int maxPageSize, int maxItemSize, @NonNull ReadPage read, @NonNull PersistPages update,\n+                    @NonNull Executor executor, String traceLogId) {\n+        Preconditions.checkArgument(maxItemSize < maxPageSize / 2, \"maxItemSize must be at most half of maxPageSize.\");\n+        this.maxItemSize = maxItemSize;\n+        this.maxPageSize = maxPageSize;\n+        this.read = read;\n+        this.update = update;\n+        this.executor = executor;\n+        this.traceLogId = traceLogId == null ? \"\" : traceLogId;\n+    }\n+\n+    //endregion\n+\n+    //region Updates\n+\n+    /**\n+     * Atomically inserts the items in 'toInsert' into the {@link BTreeSet} and removes the items in 'toRemove'\n+     * from the {@link BTreeSet}. No duplicates are allowed; the same item cannot exist multiple times in either 'toInsert'", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzgzMjUwMA=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 98}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDczNjA5Nw==", "bodyText": "Added a @throws tag.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r434736097", "createdAt": "2020-06-03T17:29:00Z", "author": {"login": "andreipaduroiu"}, "path": "common/src/main/java/io/pravega/common/util/btree/sets/BTreeSet.java", "diffHunk": "@@ -0,0 +1,414 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.common.util.btree.sets;\n+\n+import com.google.common.annotations.Beta;\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArrayComparator;\n+import java.time.Duration;\n+import java.util.AbstractMap;\n+import java.util.ArrayList;\n+import java.util.Collection;\n+import java.util.Comparator;\n+import java.util.HashSet;\n+import java.util.Iterator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Set;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.Executor;\n+import java.util.concurrent.atomic.AtomicReference;\n+import java.util.function.Supplier;\n+import javax.annotation.Nullable;\n+import javax.annotation.concurrent.NotThreadSafe;\n+import lombok.NonNull;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+/**\n+ * A B+Tree-backed Set. Stores all items in a B+Tree Structure using a {@link ByteArrayComparator} for ordering them.\n+ *\n+ * NOTE: This component is in {@link Beta}. There are no guarantees about data or API compatibility with future versions.\n+ * Any component that is directly dependent on this one should either be in {@link Beta} as well.\n+ */\n+@NotThreadSafe\n+@Beta\n+@Slf4j\n+public class BTreeSet {\n+    //region Members\n+\n+    public static final Comparator<ArrayView> COMPARATOR = new ByteArrayComparator()::compare;\n+    private static final Comparator<PagePointer> POINTER_COMPARATOR = PagePointer.getComparator(COMPARATOR);\n+\n+    private final int maxPageSize;\n+    private final int maxItemSize;\n+    @NonNull\n+    private final ReadPage read;\n+    @NonNull\n+    private final PersistPages update;\n+    @NonNull\n+    private final Executor executor;\n+    @NonNull\n+    private final String traceLogId;\n+\n+    //endregion\n+\n+    //region Constructor\n+\n+    /**\n+     * Creates a new instance of the {@link BTreeSet} class.\n+     *\n+     * @param maxPageSize The maximum size, in bytes, of any page.\n+     * @param maxItemSize The maximum size, in bytes, of any single item in the {@link BTreeSet}.\n+     * @param read        A {@link ReadPage} function that can be used to fetch a single {@link BTreeSet} page from an\n+     *                    external data source.\n+     * @param update      A {@link PersistPages} function that can be used to store and delete multiple {@link BTreeSet}\n+     *                    pages to/from an external data source.\n+     * @param executor    Executor for async operations.\n+     * @param traceLogId  Trace id for logging.\n+     */\n+    public BTreeSet(int maxPageSize, int maxItemSize, @NonNull ReadPage read, @NonNull PersistPages update,\n+                    @NonNull Executor executor, String traceLogId) {\n+        Preconditions.checkArgument(maxItemSize < maxPageSize / 2, \"maxItemSize must be at most half of maxPageSize.\");\n+        this.maxItemSize = maxItemSize;\n+        this.maxPageSize = maxPageSize;\n+        this.read = read;\n+        this.update = update;\n+        this.executor = executor;\n+        this.traceLogId = traceLogId == null ? \"\" : traceLogId;\n+    }\n+\n+    //endregion\n+\n+    //region Updates\n+\n+    /**\n+     * Atomically inserts the items in 'toInsert' into the {@link BTreeSet} and removes the items in 'toRemove'\n+     * from the {@link BTreeSet}. No duplicates are allowed; the same item cannot exist multiple times in either 'toInsert'", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzgzMjUwMA=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 98}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwMjU3OTEzOnYy", "diffSide": "RIGHT", "path": "segmentstore/contracts/src/main/java/io/pravega/segmentstore/contracts/tables/IteratorArgs.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMzoxMzo0MFrOGdw9Rg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxNToyMzoxMFrOGd23Ug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg2NDAwNg==", "bodyText": "To clarify, is it only the prefixFilter that is experimental? Basically, getpPrefixFilter returns an ArrayView that is not guaranteed to be correct? Do you know whether this is displayed correctly with javadocs (because of the @DaTa lombok annotation)?", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r433864006", "createdAt": "2020-06-02T13:13:40Z", "author": {"login": "fpj"}, "path": "segmentstore/contracts/src/main/java/io/pravega/segmentstore/contracts/tables/IteratorArgs.java", "diffHunk": "@@ -24,9 +24,11 @@\n @Builder\n public class IteratorArgs {\n     /**\n+     * EXPERIMENTAL!", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzk2MDc4Ng==", "bodyText": "We do not generate or publish Javadocs for internal services since everything is behind the wire protocol, so this is just for someone looking at the code.\nMy IDE correctly displays Javadoc when I hover over a method, so I am not worried about it here.\nAnd yes, it's only the prefixFilter that is experimental, since the whole Sorted Table Segment is experimental too.\n\ngetpPrefixFilter returns an ArrayView that is not guaranteed to be correct\n\nNo. Making use of this feature is experimental and the API and/or underlying behavior could change without notice. There is nothing here that indicates something isn't going to be correct.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r433960786", "createdAt": "2020-06-02T15:23:10Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/contracts/src/main/java/io/pravega/segmentstore/contracts/tables/IteratorArgs.java", "diffHunk": "@@ -24,9 +24,11 @@\n @Builder\n public class IteratorArgs {\n     /**\n+     * EXPERIMENTAL!", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg2NDAwNg=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwMjYwMjA3OnYy", "diffSide": "RIGHT", "path": "segmentstore/contracts/src/main/java/io/pravega/segmentstore/contracts/tables/TableStore.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMzoxOToxNlrOGdxLwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxNToyNDoyN1rOGd27Jw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg2NzcxMg==", "bodyText": "Do you want to add that this additional space is proportional to the number and length of keys, but not the size of the value?", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r433867712", "createdAt": "2020-06-02T13:19:16Z", "author": {"login": "fpj"}, "path": "segmentstore/contracts/src/main/java/io/pravega/segmentstore/contracts/tables/TableStore.java", "diffHunk": "@@ -41,8 +39,19 @@\n  * will be atomically checked-and-applied.\n  * * Unconditional Updates (insert, update, remove) will take effect regardless of what the current Key version exists in\n  * the Table Segment.\n+ *\n+ * Sorted vs Non-Sorted Table Segments:\n+ * * All Table Segments are a Hash-Table-like data structure, where Keys are mapped to Values.\n+ * * Non-Sorted Table Segments provide no ordering guarantees for {@link #keyIterator} or {@link #entryIterator}.\n+ * * Sorted Table Segments store additional information about the Keys and will return results for {@link #keyIterator}\n+ * or {@link #entryIterator} in lexicographic bitwise order. All other contracts are identical to the Non-Sorted variant.\n+ * * Sorted Table Segments will require additional storage space to store the ordered Keys and may require additional", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzk2MTc2Nw==", "bodyText": "Good point. Done.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r433961767", "createdAt": "2020-06-02T15:24:27Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/contracts/src/main/java/io/pravega/segmentstore/contracts/tables/TableStore.java", "diffHunk": "@@ -41,8 +39,19 @@\n  * will be atomically checked-and-applied.\n  * * Unconditional Updates (insert, update, remove) will take effect regardless of what the current Key version exists in\n  * the Table Segment.\n+ *\n+ * Sorted vs Non-Sorted Table Segments:\n+ * * All Table Segments are a Hash-Table-like data structure, where Keys are mapped to Values.\n+ * * Non-Sorted Table Segments provide no ordering guarantees for {@link #keyIterator} or {@link #entryIterator}.\n+ * * Sorted Table Segments store additional information about the Keys and will return results for {@link #keyIterator}\n+ * or {@link #entryIterator} in lexicographic bitwise order. All other contracts are identical to the Non-Sorted variant.\n+ * * Sorted Table Segments will require additional storage space to store the ordered Keys and may require additional", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg2NzcxMg=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwMjYyMjQzOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/ContainerKeyIndex.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMzoyMzo0MlrOGdxXzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxNToyNjoxNlrOGd3AEQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg3MDc5OQ==", "bodyText": "This Future will wait on any Segment-specific recovery to complete before executing.\n\nThis is a good information to have, but it made me wonder if that's not true of any segment-related operation. Is there anything special about this call to get the sorted key index with respect to recovery?", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r433870799", "createdAt": "2020-06-02T13:23:42Z", "author": {"login": "fpj"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/ContainerKeyIndex.java", "diffHunk": "@@ -538,6 +546,25 @@ void notifyIndexOffsetChanged(long segmentId, long indexOffset) {\n                 ignored -> CompletableFuture.completedFuture(this.cache.getTailHashes(segment.getSegmentId())));\n     }\n \n+    /**\n+     * Gets the {@link SegmentSortedKeyIndex} associated with the given Segment, as provided by\n+     * {@link ContainerSortedKeyIndex#getSortedKeyIndex}. This can be used to safely iterate through Keys of a Sorted\n+     * Table Segment while including both fully indexed and tail updates.\n+     *\n+     * Note: this will return the same result as {@link ContainerSortedKeyIndex#getSortedKeyIndex}, however this method\n+     * will wait on any Segment-specific recovery (and trigger it) to complete before executing, which should enable a\n+     * safe iteration for recently recovered Table Segments.\n+     *\n+     * @param segment A {@link DirectSegmentAccess} representing the Segment for which to get the {@link SegmentSortedKeyIndex}.\n+     * @return A CompletableFuture that, when completed, will contain the desired result. This Future will wait on any", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzk2MzAyNQ==", "bodyText": "It's true of any segment-related operation that have this phrase attached to them. Not all operations need to wait on that. Conditional updates must wait, but unconditional updates do not. Retrievals and iterators must wait.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r433963025", "createdAt": "2020-06-02T15:26:16Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/ContainerKeyIndex.java", "diffHunk": "@@ -538,6 +546,25 @@ void notifyIndexOffsetChanged(long segmentId, long indexOffset) {\n                 ignored -> CompletableFuture.completedFuture(this.cache.getTailHashes(segment.getSegmentId())));\n     }\n \n+    /**\n+     * Gets the {@link SegmentSortedKeyIndex} associated with the given Segment, as provided by\n+     * {@link ContainerSortedKeyIndex#getSortedKeyIndex}. This can be used to safely iterate through Keys of a Sorted\n+     * Table Segment while including both fully indexed and tail updates.\n+     *\n+     * Note: this will return the same result as {@link ContainerSortedKeyIndex#getSortedKeyIndex}, however this method\n+     * will wait on any Segment-specific recovery (and trigger it) to complete before executing, which should enable a\n+     * safe iteration for recently recovered Table Segments.\n+     *\n+     * @param segment A {@link DirectSegmentAccess} representing the Segment for which to get the {@link SegmentSortedKeyIndex}.\n+     * @return A CompletableFuture that, when completed, will contain the desired result. This Future will wait on any", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg3MDc5OQ=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwMjY0MTUxOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/ContainerSortedKeyIndex.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMzoyODowMlrOGdxj3Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxNToyODowNVrOGd3FRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg3Mzg4NQ==", "bodyText": "I'm not sure what this notification is about, could you elaborate?", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r433873885", "createdAt": "2020-06-02T13:28:02Z", "author": {"login": "fpj"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/ContainerSortedKeyIndex.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.tables;\n+\n+import io.pravega.segmentstore.contracts.Attributes;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.tables.TableAttributes;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executor;\n+import lombok.NonNull;\n+import lombok.RequiredArgsConstructor;\n+import lombok.val;\n+\n+/**\n+ * Manages {@link SegmentSortedKeyIndex} instances.\n+ */\n+@RequiredArgsConstructor\n+class ContainerSortedKeyIndex {\n+    //region Members\n+\n+    private final ConcurrentHashMap<Long, SegmentSortedKeyIndex> sortedKeyIndices = new ConcurrentHashMap<>();\n+    @NonNull\n+    private final SortedKeyIndexDataSource dataSource;\n+    @NonNull\n+    private final Executor executor;\n+\n+    //endregion\n+\n+    //region Operations\n+\n+    /**\n+     * Determines whether the given {@link SegmentProperties} instance indicates the associated Table Segment is a sorted one.\n+     *\n+     * @param info The {@link SegmentProperties} to query.\n+     * @return True if Sorted Table Segment, false otherwise.\n+     */\n+    static boolean isSortedTableSegment(SegmentProperties info) {\n+        return info.getAttributes().getOrDefault(TableAttributes.SORTED, Attributes.BOOLEAN_FALSE) == Attributes.BOOLEAN_TRUE;\n+    }\n+\n+    /**\n+     * Gets a {@link SegmentSortedKeyIndex} instance for the given Segment. If there is no {@link SegmentSortedKeyIndex}\n+     * currently associated with the given segment, it will be associated (and the same instance will be returned later).\n+     *\n+     * @param segmentId   The Id of the Segment.\n+     * @param segmentInfo A {@link SegmentProperties} associated with the segment.\n+     * @return A {@link SegmentSortedKeyIndex} if segmentInfo indicates a Sorted Table Segment, or\n+     * {@link SegmentSortedKeyIndex#noop()} otherwise.\n+     */\n+    SegmentSortedKeyIndex getSortedKeyIndex(long segmentId, SegmentProperties segmentInfo) {\n+        if (isSortedTableSegment(segmentInfo)) {\n+            return this.sortedKeyIndices.computeIfAbsent(segmentId, id -> createSortedKeyIndex(segmentInfo.getName()));\n+        } else {\n+            // Not a Sorted Table Segment.\n+            return SegmentSortedKeyIndex.noop();\n+        }\n+    }\n+\n+    /**\n+     * Notifies that the indexed offset for a particular Segment Id has been changed.\n+     *\n+     * @param segmentId   The Segment Id whose indexed offset has changed.\n+     * @param indexOffset The new indexed offset. If -1, and if the given Segment is currently registered, it will be\n+     *                    de-registered (since -1 indicates it has been evicted).\n+     */\n+    void notifyIndexOffsetChanged(long segmentId, long indexOffset) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 73}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzk2NDM1OQ==", "bodyText": "It notifies that \"IndexedOffset\" has changed for this segment. If you look at the method with same name in SegmentSortedKeyIndex, you'll see that this is used to clear the tail cache.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r433964359", "createdAt": "2020-06-02T15:28:05Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/ContainerSortedKeyIndex.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.tables;\n+\n+import io.pravega.segmentstore.contracts.Attributes;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.tables.TableAttributes;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.concurrent.Executor;\n+import lombok.NonNull;\n+import lombok.RequiredArgsConstructor;\n+import lombok.val;\n+\n+/**\n+ * Manages {@link SegmentSortedKeyIndex} instances.\n+ */\n+@RequiredArgsConstructor\n+class ContainerSortedKeyIndex {\n+    //region Members\n+\n+    private final ConcurrentHashMap<Long, SegmentSortedKeyIndex> sortedKeyIndices = new ConcurrentHashMap<>();\n+    @NonNull\n+    private final SortedKeyIndexDataSource dataSource;\n+    @NonNull\n+    private final Executor executor;\n+\n+    //endregion\n+\n+    //region Operations\n+\n+    /**\n+     * Determines whether the given {@link SegmentProperties} instance indicates the associated Table Segment is a sorted one.\n+     *\n+     * @param info The {@link SegmentProperties} to query.\n+     * @return True if Sorted Table Segment, false otherwise.\n+     */\n+    static boolean isSortedTableSegment(SegmentProperties info) {\n+        return info.getAttributes().getOrDefault(TableAttributes.SORTED, Attributes.BOOLEAN_FALSE) == Attributes.BOOLEAN_TRUE;\n+    }\n+\n+    /**\n+     * Gets a {@link SegmentSortedKeyIndex} instance for the given Segment. If there is no {@link SegmentSortedKeyIndex}\n+     * currently associated with the given segment, it will be associated (and the same instance will be returned later).\n+     *\n+     * @param segmentId   The Id of the Segment.\n+     * @param segmentInfo A {@link SegmentProperties} associated with the segment.\n+     * @return A {@link SegmentSortedKeyIndex} if segmentInfo indicates a Sorted Table Segment, or\n+     * {@link SegmentSortedKeyIndex#noop()} otherwise.\n+     */\n+    SegmentSortedKeyIndex getSortedKeyIndex(long segmentId, SegmentProperties segmentInfo) {\n+        if (isSortedTableSegment(segmentInfo)) {\n+            return this.sortedKeyIndices.computeIfAbsent(segmentId, id -> createSortedKeyIndex(segmentInfo.getName()));\n+        } else {\n+            // Not a Sorted Table Segment.\n+            return SegmentSortedKeyIndex.noop();\n+        }\n+    }\n+\n+    /**\n+     * Notifies that the indexed offset for a particular Segment Id has been changed.\n+     *\n+     * @param segmentId   The Segment Id whose indexed offset has changed.\n+     * @param indexOffset The new indexed offset. If -1, and if the given Segment is currently registered, it will be\n+     *                    de-registered (since -1 indicates it has been evicted).\n+     */\n+    void notifyIndexOffsetChanged(long segmentId, long indexOffset) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg3Mzg4NQ=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwMjY1NzU5OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/KeyTranslator.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMzozMTozMlrOGdxtyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxNTozMToyOVrOGd3Owg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg3NjQyNA==", "bodyText": "What's this KeyTranslator about?", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r433876424", "createdAt": "2020-06-02T13:31:32Z", "author": {"login": "fpj"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/KeyTranslator.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.tables;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.tables.TableEntry;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import lombok.RequiredArgsConstructor;\n+\n+/**\n+ * Translates Table Segment Keys from an external form into an internal one and back.\n+ */\n+abstract class KeyTranslator {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzk2Njc4Ng==", "bodyText": "I need to separate user keys from the Sorted Key Index keys (since I store the Sorted Key Index alongside the rest of the keys). I do not want internal and external keys to clash, or for a user to (accidentally) modify a SKI key-value or viceversa.\nFor Sorted Table Segments only, this translator adds a 1-byte prefix to each key, as follows:\n\nUser-keys: byte equivalent of E (for External)\nSKI keys: byte equivalent of I (for Internal).\n\nFor Hash Table Segments, the no-op translator is used which doesn't do anything (so no effect on existing keys).\nCurrently this prepend is inefficient due to a mandatory array copy. However with #4837 this will be a simple, zero-copy composition which has no overhead.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r433966786", "createdAt": "2020-06-02T15:31:29Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/tables/KeyTranslator.java", "diffHunk": "@@ -0,0 +1,207 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.tables;\n+\n+import com.google.common.base.Preconditions;\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.segmentstore.contracts.tables.TableEntry;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import lombok.RequiredArgsConstructor;\n+\n+/**\n+ * Translates Table Segment Keys from an external form into an internal one and back.\n+ */\n+abstract class KeyTranslator {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg3NjQyNA=="}, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwMjY3MDI2OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/tables/ContainerSortedKeyIndexTests.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMzozNDoyOFrOGdx2Dw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wMlQxMzozNDoyOFrOGdx2Dw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMzg3ODU0Mw==", "bodyText": "Typo.", "url": "https://github.com/pravega/pravega/pull/4763#discussion_r433878543", "createdAt": "2020-06-02T13:34:28Z", "author": {"login": "fpj"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/tables/ContainerSortedKeyIndexTests.java", "diffHunk": "@@ -0,0 +1,392 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.tables;\n+\n+import io.pravega.common.util.ArrayView;\n+import io.pravega.common.util.AsyncIterator;\n+import io.pravega.common.util.ByteArraySegment;\n+import io.pravega.common.util.HashedArray;\n+import io.pravega.segmentstore.contracts.Attributes;\n+import io.pravega.segmentstore.contracts.SegmentProperties;\n+import io.pravega.segmentstore.contracts.StreamSegmentInformation;\n+import io.pravega.segmentstore.contracts.tables.TableAttributes;\n+import io.pravega.segmentstore.contracts.tables.TableKey;\n+import io.pravega.segmentstore.server.TableStoreMock;\n+import io.pravega.test.common.AssertExtensions;\n+import io.pravega.test.common.ThreadPooledTestSuite;\n+import java.time.Duration;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.HashMap;\n+import java.util.HashSet;\n+import java.util.List;\n+import java.util.Random;\n+import java.util.Set;\n+import java.util.TreeSet;\n+import java.util.UUID;\n+import java.util.function.Function;\n+import java.util.stream.Collectors;\n+import lombok.RequiredArgsConstructor;\n+import lombok.val;\n+import org.junit.Assert;\n+import org.junit.Test;\n+\n+/**\n+ * Unit tess for {@link ContainerSortedKeyIndex} and {@link SegmentSortedKeyIndexImpl}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e23c68e3087386a7ba7090d86218ad916e2dabac"}, "originalPosition": 43}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4304, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}