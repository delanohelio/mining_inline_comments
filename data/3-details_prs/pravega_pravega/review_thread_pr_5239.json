{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk5NTE5NzMz", "number": 5239, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxOTozOTowOFrOEswViA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwODozODowNFrOEs7dxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MzY0NzQ0OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/MetadataCleaner.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxOTozOTowOFrOHgK5hg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQyMTozMjo0OFrOHgN1yQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQ5NTA0Ng==", "bodyText": "Not sure why we don't want to flush deleted and merged segment.", "url": "https://github.com/pravega/pravega/pull/5239#discussion_r503495046", "createdAt": "2020-10-12T19:39:08Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/MetadataCleaner.java", "diffHunk": "@@ -100,6 +101,24 @@ protected void doStop() {\n \n     //endregion\n \n+    /**\n+     * Persists the metadata of all active Segments from the Container's metadata into the {@link MetadataStore}.\n+     * This method does not evict or otherwise perform any cleanup tasks on the Container or its Metadata, nor does it\n+     * interfere with the regular operation of {@link #runOnce()}.\n+     *\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, indicates that the operation completed.\n+     */\n+    CompletableFuture<Void> persistAll(Duration timeout) {\n+        val tasks = this.metadata.getAllStreamSegmentIds().stream()\n+                .map(this.metadata::getStreamSegmentMetadata)\n+                .filter(Objects::nonNull)\n+                .filter(sm -> !sm.isDeleted() && !sm.isMerged())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1820301b71b8963f78b51b1f39524199654c4fc"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzU0MzI0MQ==", "bodyText": "The previous step in LogFlusher (before this is invoked) tells the StorageWriter to apply all the outstanding changes to Storage. That includes any deletions or mergers. Those segments' metadata may still linger in memory (not evicted yet) and should be treated as garbage. No need to write them down here.\nIf you look in the runOnceInternal method in this class (the one that does evictions), you'll see similar logic.", "url": "https://github.com/pravega/pravega/pull/5239#discussion_r503543241", "createdAt": "2020-10-12T21:32:48Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/MetadataCleaner.java", "diffHunk": "@@ -100,6 +101,24 @@ protected void doStop() {\n \n     //endregion\n \n+    /**\n+     * Persists the metadata of all active Segments from the Container's metadata into the {@link MetadataStore}.\n+     * This method does not evict or otherwise perform any cleanup tasks on the Container or its Metadata, nor does it\n+     * interfere with the regular operation of {@link #runOnce()}.\n+     *\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, indicates that the operation completed.\n+     */\n+    CompletableFuture<Void> persistAll(Duration timeout) {\n+        val tasks = this.metadata.getAllStreamSegmentIds().stream()\n+                .map(this.metadata::getStreamSegmentMetadata)\n+                .filter(Objects::nonNull)\n+                .filter(sm -> !sm.isDeleted() && !sm.isMerged())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQ5NTA0Ng=="}, "originalCommit": {"oid": "d1820301b71b8963f78b51b1f39524199654c4fc"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1MzY3MTA2OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/writer/WriterState.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQxOTo0ODozMVrOHgLHtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xMlQyMTozMzo1M1rOHgN5iQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQ5ODY3Nw==", "bodyText": "What happens when there is nothing to flush?", "url": "https://github.com/pravega/pravega/pull/5239#discussion_r503498677", "createdAt": "2020-10-12T19:48:31Z", "author": {"login": "sachin-j-joshi"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/writer/WriterState.java", "diffHunk": "@@ -122,6 +129,57 @@ long getLastReadSequenceNumber() {\n     void setLastReadSequenceNumber(long value) {\n         Preconditions.checkArgument(value >= this.lastReadSequenceNumber.get(), \"New LastReadSequenceNumber cannot be smaller than the previous one.\");\n         this.lastReadSequenceNumber.set(value);\n+        recordReadComplete();\n+    }\n+\n+    /**\n+     * Indicates the fact that the {@link StorageWriter} has completed reading.\n+     */\n+    void recordReadComplete() {\n+        val ffc = this.forceFlushContext.get();\n+        if (ffc != null) {\n+            ffc.setLastReadSequenceNumber(this.lastReadSequenceNumber.get());\n+        }\n+    }\n+\n+    /**\n+     * Indicates the fact that the {@link StorageWriter} has completed a flush stage.\n+     *\n+     * @param result The {@link WriterFlushResult} summarizing the flush stage.\n+     */\n+    void recordFlushComplete(WriterFlushResult result) {\n+        val ffc = this.forceFlushContext.get();\n+        if (ffc != null && ffc.flushComplete(result)) {\n+            this.forceFlushContext.set(null);\n+            ffc.getCompletion().complete(ffc.isAnythingFlushed());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1820301b71b8963f78b51b1f39524199654c4fc"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzU0NDIwMQ==", "bodyText": "The StorageWriter may execute multiple iterations until it is able to complete this. When requested to force flush, it sets a context, and as long as that context is present, the Storage Writer will forcefully try to flush everything, read more data, flush, etc., until the requested \"flush-to\" sequence number has been processed.", "url": "https://github.com/pravega/pravega/pull/5239#discussion_r503544201", "createdAt": "2020-10-12T21:33:53Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/writer/WriterState.java", "diffHunk": "@@ -122,6 +129,57 @@ long getLastReadSequenceNumber() {\n     void setLastReadSequenceNumber(long value) {\n         Preconditions.checkArgument(value >= this.lastReadSequenceNumber.get(), \"New LastReadSequenceNumber cannot be smaller than the previous one.\");\n         this.lastReadSequenceNumber.set(value);\n+        recordReadComplete();\n+    }\n+\n+    /**\n+     * Indicates the fact that the {@link StorageWriter} has completed reading.\n+     */\n+    void recordReadComplete() {\n+        val ffc = this.forceFlushContext.get();\n+        if (ffc != null) {\n+            ffc.setLastReadSequenceNumber(this.lastReadSequenceNumber.get());\n+        }\n+    }\n+\n+    /**\n+     * Indicates the fact that the {@link StorageWriter} has completed a flush stage.\n+     *\n+     * @param result The {@link WriterFlushResult} summarizing the flush stage.\n+     */\n+    void recordFlushComplete(WriterFlushResult result) {\n+        val ffc = this.forceFlushContext.get();\n+        if (ffc != null && ffc.flushComplete(result)) {\n+            this.forceFlushContext.set(null);\n+            ffc.getCompletion().complete(ffc.isAnythingFlushed());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzQ5ODY3Nw=="}, "originalCommit": {"oid": "d1820301b71b8963f78b51b1f39524199654c4fc"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1NDg0Mjg2OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/WriterSegmentProcessor.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwNToyMTozM1rOHgVx_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNzoyOTo1OFrOHgx7Ag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzY3MzM0Mg==", "bodyText": "Should it be mustFlush instead of mustFlush()?", "url": "https://github.com/pravega/pravega/pull/5239#discussion_r503673342", "createdAt": "2020-10-13T05:21:33Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/WriterSegmentProcessor.java", "diffHunk": "@@ -54,9 +54,22 @@\n     /**\n      * Flushes the contents of the Processor.\n      *\n+     * @param force   If true, force-flushes everything accumulated in the {@link WriterSegmentProcessor}, regardless of\n+     *                the value returned by {@link #mustFlush()}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1820301b71b8963f78b51b1f39524199654c4fc"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDEzNDQwMg==", "bodyText": "Both are acceptable for Javadoc generation. The former will link to any method matching that name, while the latter will link to a specific method overload (looking at parameters, signature, etc.).", "url": "https://github.com/pravega/pravega/pull/5239#discussion_r504134402", "createdAt": "2020-10-13T17:29:58Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/WriterSegmentProcessor.java", "diffHunk": "@@ -54,9 +54,22 @@\n     /**\n      * Flushes the contents of the Processor.\n      *\n+     * @param force   If true, force-flushes everything accumulated in the {@link WriterSegmentProcessor}, regardless of\n+     *                the value returned by {@link #mustFlush()}.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzY3MzM0Mg=="}, "originalCommit": {"oid": "d1820301b71b8963f78b51b1f39524199654c4fc"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1NDk3NzUyOnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/LogFlusher.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwNjoyMzozN1rOHgXBrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNzozMzoxMFrOHgyB6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzY5Mzc0MA==", "bodyText": "Will MAX_FLUSH_ATTEMPTS be reached before flushing all operations? I mean, what if there is still something to flush after attemptNo has reached to MAX_FLUSH_ATTEMPTS.", "url": "https://github.com/pravega/pravega/pull/5239#discussion_r503693740", "createdAt": "2020-10-13T06:23:37Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/LogFlusher.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.RetriesExhaustedException;\n+import io.pravega.segmentstore.server.OperationLog;\n+import io.pravega.segmentstore.server.Writer;\n+import java.time.Duration;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import lombok.NonNull;\n+import lombok.RequiredArgsConstructor;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+/**\n+ * Utility class that helps the {@link StreamSegmentContainer} to force-flush all the data to the underlying Storage.\n+ */\n+@RequiredArgsConstructor\n+@Slf4j\n+class LogFlusher {\n+    /**\n+     * Maximum number of {@link Writer} flushes to attempt until no more flush progress is expected to be made.\n+     */\n+    @VisibleForTesting\n+    static final int MAX_FLUSH_ATTEMPTS = 10;\n+    private final int containerId;\n+    @NonNull\n+    private final OperationLog durableLog;\n+    @NonNull\n+    private final Writer writer;\n+    @NonNull\n+    private final MetadataCleaner metadataCleaner;\n+    @NonNull\n+    private final ScheduledExecutorService executor;\n+\n+    /**\n+     * Flushes every outstanding Operation in the Container's {@link OperationLog} to Storage. When this method completes:\n+     * - Every Operation that has been initiated in the {@link OperationLog} prior to the invocation of this method\n+     * will be flushed to the Storage via the {@link Writer}.\n+     * - The effects of such Operations on the Container's Metadata will be persisted to the Container's Metadata Store.\n+     * - The Container's Metadata Store will be persisted (and fully indexed) in Storage (it will contain all changes\n+     * from the previous step).\n+     *\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed successfully. If the\n+     * operation failed, it will be failed with the appropriate exception.\n+     */\n+    public CompletableFuture<Void> flushToStorage(Duration timeout) {\n+        // 1. Flush everything we have so far.\n+        // 2. Flush all in-memory Segment metadata to the Metadata Store.\n+        // 3. Flush everything we have so far (again) - to make sure step 2 is persisted in Storage.\n+        TimeoutTimer timer = new TimeoutTimer(timeout);\n+        log.info(\"LogFlusher[{}]: Flushing outstanding data.\", this.containerId);\n+        return flushAll(timer)\n+                .thenComposeAsync(v -> {\n+                    log.info(\"LogFlusher[{}]: Persisting active segment metadata.\", this.containerId);\n+                    return this.metadataCleaner.persistAll(timer.getRemaining());\n+                }, this.executor)\n+                .thenComposeAsync(v -> {\n+                    log.info(\"LogFlusher[{}]: Flushing metadata store.\", this.containerId);\n+                    return flushAll(timer);\n+                }, this.executor);\n+    }\n+\n+    private CompletableFuture<Void> flushAll(TimeoutTimer timer) {\n+        // 1. Queue a checkpoint and get its SeqNo. This is poor man's way of ensuring all initiated ops are in.\n+        // 2. Tell StorageWriter to flush all (new API, with seqNo). Includes: segment data and table segment indexing.\n+        // 3. Repeat 1+2 until StorageWriter claims there is nothing more to flush.\n+        val flushAgain = new AtomicBoolean(true);\n+        val attemptNo = new AtomicInteger(0);\n+        return Futures.loop(\n+                () -> flushAgain.get() && attemptNo.getAndIncrement() < MAX_FLUSH_ATTEMPTS,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1820301b71b8963f78b51b1f39524199654c4fc"}, "originalPosition": 85}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDEzNjE3MA==", "bodyText": "MAX_FLUSH_ATTEMPTS is set to 10. At each attempt, it records a checkpoint into the Tier 1 log, gets its sequence number, and tells the Storage Writer to flush everything up to and including that seq no. The Storage Writer will not complete until it has finished doing so. This qualifies as one \"attempt\".\nHowever, a Storage Writer iteration may produce side effects that get added to Tier 1 (i.e., for Table Segments, new data and/or attributes may be written) - we must make sure that all these get persisted to Storage as well. These should take at most one or two extra iterations, but to add a bit of buffer, I gave it up to 10 attempts to converge.", "url": "https://github.com/pravega/pravega/pull/5239#discussion_r504136170", "createdAt": "2020-10-13T17:33:10Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/containers/LogFlusher.java", "diffHunk": "@@ -0,0 +1,104 @@\n+/**\n+ * Copyright (c) Dell Inc., or its subsidiaries. All Rights Reserved.\n+ *\n+ * Licensed under the Apache License, Version 2.0 (the \"License\");\n+ * you may not use this file except in compliance with the License.\n+ * You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ */\n+package io.pravega.segmentstore.server.containers;\n+\n+import com.google.common.annotations.VisibleForTesting;\n+import io.pravega.common.TimeoutTimer;\n+import io.pravega.common.concurrent.Futures;\n+import io.pravega.common.util.RetriesExhaustedException;\n+import io.pravega.segmentstore.server.OperationLog;\n+import io.pravega.segmentstore.server.Writer;\n+import java.time.Duration;\n+import java.util.concurrent.CompletableFuture;\n+import java.util.concurrent.ScheduledExecutorService;\n+import java.util.concurrent.atomic.AtomicBoolean;\n+import java.util.concurrent.atomic.AtomicInteger;\n+import lombok.NonNull;\n+import lombok.RequiredArgsConstructor;\n+import lombok.extern.slf4j.Slf4j;\n+import lombok.val;\n+\n+/**\n+ * Utility class that helps the {@link StreamSegmentContainer} to force-flush all the data to the underlying Storage.\n+ */\n+@RequiredArgsConstructor\n+@Slf4j\n+class LogFlusher {\n+    /**\n+     * Maximum number of {@link Writer} flushes to attempt until no more flush progress is expected to be made.\n+     */\n+    @VisibleForTesting\n+    static final int MAX_FLUSH_ATTEMPTS = 10;\n+    private final int containerId;\n+    @NonNull\n+    private final OperationLog durableLog;\n+    @NonNull\n+    private final Writer writer;\n+    @NonNull\n+    private final MetadataCleaner metadataCleaner;\n+    @NonNull\n+    private final ScheduledExecutorService executor;\n+\n+    /**\n+     * Flushes every outstanding Operation in the Container's {@link OperationLog} to Storage. When this method completes:\n+     * - Every Operation that has been initiated in the {@link OperationLog} prior to the invocation of this method\n+     * will be flushed to the Storage via the {@link Writer}.\n+     * - The effects of such Operations on the Container's Metadata will be persisted to the Container's Metadata Store.\n+     * - The Container's Metadata Store will be persisted (and fully indexed) in Storage (it will contain all changes\n+     * from the previous step).\n+     *\n+     * @param timeout Timeout for the operation.\n+     * @return A CompletableFuture that, when completed, will indicate that the operation completed successfully. If the\n+     * operation failed, it will be failed with the appropriate exception.\n+     */\n+    public CompletableFuture<Void> flushToStorage(Duration timeout) {\n+        // 1. Flush everything we have so far.\n+        // 2. Flush all in-memory Segment metadata to the Metadata Store.\n+        // 3. Flush everything we have so far (again) - to make sure step 2 is persisted in Storage.\n+        TimeoutTimer timer = new TimeoutTimer(timeout);\n+        log.info(\"LogFlusher[{}]: Flushing outstanding data.\", this.containerId);\n+        return flushAll(timer)\n+                .thenComposeAsync(v -> {\n+                    log.info(\"LogFlusher[{}]: Persisting active segment metadata.\", this.containerId);\n+                    return this.metadataCleaner.persistAll(timer.getRemaining());\n+                }, this.executor)\n+                .thenComposeAsync(v -> {\n+                    log.info(\"LogFlusher[{}]: Flushing metadata store.\", this.containerId);\n+                    return flushAll(timer);\n+                }, this.executor);\n+    }\n+\n+    private CompletableFuture<Void> flushAll(TimeoutTimer timer) {\n+        // 1. Queue a checkpoint and get its SeqNo. This is poor man's way of ensuring all initiated ops are in.\n+        // 2. Tell StorageWriter to flush all (new API, with seqNo). Includes: segment data and table segment indexing.\n+        // 3. Repeat 1+2 until StorageWriter claims there is nothing more to flush.\n+        val flushAgain = new AtomicBoolean(true);\n+        val attemptNo = new AtomicInteger(0);\n+        return Futures.loop(\n+                () -> flushAgain.get() && attemptNo.getAndIncrement() < MAX_FLUSH_ATTEMPTS,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzY5Mzc0MA=="}, "originalCommit": {"oid": "d1820301b71b8963f78b51b1f39524199654c4fc"}, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1NTA0MjU1OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/writer/AttributeAggregator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwNjo0Njo1OFrOHgXn1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNzozMzoyOFrOHgyChg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzcwMzUwOQ==", "bodyText": "#mustFlush, as I see in other places.", "url": "https://github.com/pravega/pravega/pull/5239#discussion_r503703509", "createdAt": "2020-10-13T06:46:58Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/writer/AttributeAggregator.java", "diffHunk": "@@ -190,14 +190,16 @@ public boolean mustFlush() {\n     /**\n      * Flushes the contents of the Aggregator to the Storage.\n      *\n+     * @param force   If true, force-flushes everything accumulated in the {@link AttributeAggregator}, regardless of\n+     *                the value returned by {@link #mustFlush()}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1820301b71b8963f78b51b1f39524199654c4fc"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDEzNjMyNg==", "bodyText": "See above.", "url": "https://github.com/pravega/pravega/pull/5239#discussion_r504136326", "createdAt": "2020-10-13T17:33:28Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/main/java/io/pravega/segmentstore/server/writer/AttributeAggregator.java", "diffHunk": "@@ -190,14 +190,16 @@ public boolean mustFlush() {\n     /**\n      * Flushes the contents of the Aggregator to the Storage.\n      *\n+     * @param force   If true, force-flushes everything accumulated in the {@link AttributeAggregator}, regardless of\n+     *                the value returned by {@link #mustFlush()}.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzcwMzUwOQ=="}, "originalCommit": {"oid": "d1820301b71b8963f78b51b1f39524199654c4fc"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE1NTQ3MDc2OnYy", "diffSide": "RIGHT", "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/writer/SegmentAggregatorTests.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QwODozODowNFrOHgbsBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xM1QxNzozNTo0OVrOHgyHVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzc3MDExNg==", "bodyText": "One question: why didn't you use INFREQUENT_FLUSH_WRITER_CONFIG?", "url": "https://github.com/pravega/pravega/pull/5239#discussion_r503770116", "createdAt": "2020-10-13T08:38:04Z", "author": {"login": "ManishKumarKeshri"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/writer/SegmentAggregatorTests.java", "diffHunk": "@@ -683,6 +683,55 @@ public void testFlushEmptyAppend() throws Exception {\n         verifySegmentData(expectedData, context);\n     }\n \n+    /**\n+     * Tests the flush() method with the force flag set.\n+     * Verifies both length-based and time-based flush triggers, as well as flushing rather large operations.\n+     */\n+    @Test\n+    public void testFlushForced() throws Exception {\n+        final WriterConfig config = DEFAULT_CONFIG;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d1820301b71b8963f78b51b1f39524199654c4fc"}, "originalPosition": 10}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNDEzNzU1Ng==", "bodyText": "That one isn't defined here.\nIf you look below (lines 714-719), it doesn't matter. I invoke flush (normal) and validate it had no effect. Then I invoked flush (forced), without changing anything else, and validate it flushed everything.", "url": "https://github.com/pravega/pravega/pull/5239#discussion_r504137556", "createdAt": "2020-10-13T17:35:49Z", "author": {"login": "andreipaduroiu"}, "path": "segmentstore/server/src/test/java/io/pravega/segmentstore/server/writer/SegmentAggregatorTests.java", "diffHunk": "@@ -683,6 +683,55 @@ public void testFlushEmptyAppend() throws Exception {\n         verifySegmentData(expectedData, context);\n     }\n \n+    /**\n+     * Tests the flush() method with the force flag set.\n+     * Verifies both length-based and time-based flush triggers, as well as flushing rather large operations.\n+     */\n+    @Test\n+    public void testFlushForced() throws Exception {\n+        final WriterConfig config = DEFAULT_CONFIG;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMzc3MDExNg=="}, "originalCommit": {"oid": "d1820301b71b8963f78b51b1f39524199654c4fc"}, "originalPosition": 10}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4731, "cost": 1, "resetAt": "2021-11-13T12:26:42Z"}}}