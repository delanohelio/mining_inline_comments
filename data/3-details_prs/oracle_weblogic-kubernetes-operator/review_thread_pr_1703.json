{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI3MzE3NDI5", "number": 1703, "reviewThreads": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzoyMzozMFrOECQEDw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzo1ODoxNVrOECQyig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNzk1NzkxOnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/node-heating.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzoyMzozMFrOGel_TA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzoyMzozMFrOGel_TA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDczMjg3Ng==", "bodyText": "WebLogic Operator -> WebLogic Server Kubernetes Operator (when we use operator capitalized, I prefer that we use the product name, either WebLogic Server Kubernetes Operator or Oracle WebLogic Server Kubernetes Operator. Please make this change globally, if you want to use the full product name, or use lowercase operator.)", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1703#discussion_r434732876", "createdAt": "2020-06-03T17:23:30Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/node-heating.md", "diffHunk": "@@ -0,0 +1,56 @@\n+---\n+title: \"Node heating problem\"\n+date: 2020-06-03T08:08:19-04:00\n+draft: false\n+weight: 22\n+---\n+\n+The WebLogic Operator creates a Pod for each WebLogic Server instance that is started. The [Kubernetes Scheduler](https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/) then selects a Node for each Pod. Because the default scheduling algorithm gives substantial weight to selecting a Node where the necessary Docker images have already been pulled, this often results in Kubernetes running many of the Pods for WebLogic Server instances on the same Node while other Nodes are not fairly utilized.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2270ed288a33f1521b34e5ab3bbcb21c3e52c25"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNzk2Njc5OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/node-heating.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzoyNjowMFrOGemFAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzoyNjowMFrOGemFAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDczNDMzOQ==", "bodyText": "WebLogic Operator Team -> WebLogic Server Kubernetes Operator team (team does not need to be capitalized, like a product name)", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1703#discussion_r434734339", "createdAt": "2020-06-03T17:26:00Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/node-heating.md", "diffHunk": "@@ -0,0 +1,56 @@\n+---\n+title: \"Node heating problem\"\n+date: 2020-06-03T08:08:19-04:00\n+draft: false\n+weight: 22\n+---\n+\n+The WebLogic Operator creates a Pod for each WebLogic Server instance that is started. The [Kubernetes Scheduler](https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/) then selects a Node for each Pod. Because the default scheduling algorithm gives substantial weight to selecting a Node where the necessary Docker images have already been pulled, this often results in Kubernetes running many of the Pods for WebLogic Server instances on the same Node while other Nodes are not fairly utilized.\n+\n+This is commonly known as the \"Node heating problem.\" One solution is to ensure that all necessary Docker images are available on worker Nodes as part of node provisioning. When the necessary Docker images are available on each worker Node, the Kubernetes Scheduler will instead select a Node based on other factors such as available CPU and memory or a simple round-robin.\n+\n+The WebLogic Operator Team recommends a different solution that is based on [inter-pod affinity and anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity). This solution has the advantage of both resolving the Node heating problem and of explicitly directing the Kubernetes Scheduler to spread the Pods for WebLogic Server instances from a given cluster or domain more widely across the available Nodes. Inter-pod affinity and anti-affinity are features of the Kubernetes Scheduler that allow the scheduler to choose a Node for a new Pod based on details of the Pods that are already running. For WebLogic Server use cases, the intent will often be for anti-affinity with the Pods for other WebLogic Server instances so that server instances spread over the available Nodes.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2270ed288a33f1521b34e5ab3bbcb21c3e52c25"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNzk4MjA2OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/node-heating.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzozMDoyN1rOGemPVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzozMDoyN1rOGemPVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDczNjk4Mg==", "bodyText": "the new Pod avoiding as much as possible Nodes -> the new Pod avoiding, as much as possible, Nodes (the commas add clarity to a long sentence)", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1703#discussion_r434736982", "createdAt": "2020-06-03T17:30:27Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/node-heating.md", "diffHunk": "@@ -0,0 +1,56 @@\n+---\n+title: \"Node heating problem\"\n+date: 2020-06-03T08:08:19-04:00\n+draft: false\n+weight: 22\n+---\n+\n+The WebLogic Operator creates a Pod for each WebLogic Server instance that is started. The [Kubernetes Scheduler](https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/) then selects a Node for each Pod. Because the default scheduling algorithm gives substantial weight to selecting a Node where the necessary Docker images have already been pulled, this often results in Kubernetes running many of the Pods for WebLogic Server instances on the same Node while other Nodes are not fairly utilized.\n+\n+This is commonly known as the \"Node heating problem.\" One solution is to ensure that all necessary Docker images are available on worker Nodes as part of node provisioning. When the necessary Docker images are available on each worker Node, the Kubernetes Scheduler will instead select a Node based on other factors such as available CPU and memory or a simple round-robin.\n+\n+The WebLogic Operator Team recommends a different solution that is based on [inter-pod affinity and anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity). This solution has the advantage of both resolving the Node heating problem and of explicitly directing the Kubernetes Scheduler to spread the Pods for WebLogic Server instances from a given cluster or domain more widely across the available Nodes. Inter-pod affinity and anti-affinity are features of the Kubernetes Scheduler that allow the scheduler to choose a Node for a new Pod based on details of the Pods that are already running. For WebLogic Server use cases, the intent will often be for anti-affinity with the Pods for other WebLogic Server instances so that server instances spread over the available Nodes.\n+\n+To use these features, edit the Domain Custom Resource to add content to the `serverPod` element, in this case at the scope of a cluster, as shown in the following example:\n+\n+```\n+clusters:\n+- clusterName: cluster-1\n+  serverStartState: \"RUNNING\"\n+  serverPod:\n+    affinity:\n+      podAntiAffinity:\n+        preferredDuringSchedulingIgnoredDuringExecution:\n+          - weight: 100\n+            podAffinityTerm:\n+              labelSelector:\n+                matchExpressions:\n+                  - key: \"weblogic.clusterName\"\n+                    operator: In\n+                    values:\n+                    - $(CLUSTER_NAME)\n+              topologyKey: \"kubernetes.io/hostname\"\n+```\n+\n+Because the `serverPod` element here is scoped to a cluster, the content of the `affinity` element will be added to the Pod generated for each WebLogic Server instance that is a member of this WebLogic cluster. This inter-pod anti-affinity statement expresses a preference that the scheduler select a Node for the new Pod avoiding as much as possible Nodes that already have Pods with the label \"weblogic.clusterName\" and the name of this cluster. Note that the `weight` is set to `100`, which is the maximum weight, so that this term will outweigh any possible preference for a Node based on availability of Docker images.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2270ed288a33f1521b34e5ab3bbcb21c3e52c25"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwODAwNDA3OnYy", "diffSide": "RIGHT", "path": "docs-source/content/userguide/managing-domains/domain-resource.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzozNjo0NlrOGemdqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzozNjo0NlrOGemdqw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc0MDY1MQ==", "bodyText": "your own domain resource which can be used -> your own domain resource, which can be used (you need a comma (before which) to set off a non-restrictive clause)", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1703#discussion_r434740651", "createdAt": "2020-06-03T17:36:46Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/userguide/managing-domains/domain-resource.md", "diffHunk": "@@ -6,9 +6,9 @@ pre = \"<b> </b>\"\n +++\n \n \n-Use this document to set up and configure your own [domain resource](https://github.com/oracle/weblogic-kubernetes-operator/blob/master/docs/domains/Domain.md) which can be used to configure the operation of your WebLogic domain. The domain resource does not replace the traditional configuration of WebLogic domains found in the domain configuration files, but instead cooperates with those files to describe the Kubernetes artifacts of the corresponding domain.  For instance, the WebLogic domain configuration will still specify deployed applications, data sources, and most other details about the domain while the domain resource will specify the number of cluster members currently running or the persistent volumes that will be mounted into the containers running WebLogic Server instances.\n+Use this document to set up and configure your own [domain resource](https://github.com/oracle/weblogic-kubernetes-operator/blob/master/docs/domains/Domain.md) which can be used to configure the operation of your WebLogic Server domain. The domain resource does not replace the traditional domain configuration files, but instead cooperates with those files to describe the Kubernetes artifacts of the corresponding domain.  For instance, the domain configuration will still specify deployed applications, data sources, and most other details about the domain while the domain resource will specify the number of cluster members currently running or the persistent volumes that will be mounted into the containers running WebLogic Server instances.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2270ed288a33f1521b34e5ab3bbcb21c3e52c25"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwODAxMzcwOnYy", "diffSide": "RIGHT", "path": "docs-source/content/userguide/managing-domains/domain-resource.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzozOTo0NVrOGemkHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzozOTo0NVrOGemkHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc0MjMwMA==", "bodyText": "are scheduled as much as possible on different Nodes -> are scheduled, as much as possible, on different Nodes", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1703#discussion_r434742300", "createdAt": "2020-06-03T17:39:45Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/userguide/managing-domains/domain-resource.md", "diffHunk": "@@ -164,18 +164,27 @@ spec:\n \n ### Pod generation\n \n-The operator creates a pod for each running WebLogic Server instance.  This pod will have a container based on the Docker image specified by the `image` field.  Additional pod or container content can be specified using the elements under `serverPod`.  This includes Kubernetes sidecar and init containers, labels, annotations, volumes, volume mounts, scheduling constraints, including anti-affinity, [resource requirements](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/), or [security context](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/).\n+The operator creates a Pod for each running WebLogic Server instance.  This Pod will have a container, named `weblogic-server`, based on the Docker image specified by the `image` field.  Additional Pod or container content can be specified using the elements under `serverPod`. This includes Kubernetes sidecar and init containers, labels, annotations, volumes, volume mounts, scheduling constraints, including anti-affinity, [resource requirements](https://kubernetes.io/docs/concepts/configuration/manage-compute-resources-container/), or [security context](https://kubernetes.io/docs/tasks/configure-pod-container/security-context/).\n \n-Prior to creating a pod, the operator replaces variable references allowing the pod content to be templates.  The format of these variable references is `$(VARIABLE_NAME)` where `VARIABLE_NAME` is one of the variable names available in the container for the WebLogic Server instance.  The default set of environment variables includes:\n+Customer provided labels and annotations may not begin with \"weblogic\" and the operator will generate the following labels:\n \n-* `DOMAIN_NAME`: The WebLogic domain name.\n+* `weblogic.createdByOperator: \"true\"`\n+* `weblogic.domainName: <domain-name>`, where `<domain-name>` is the name of the WebLogic domain\n+* `weblogic.domainUID: <uid>`, where `<uid>` is the domain UID from the domain resource\n+* `weblogic.serverName: <server-name>`, where `<server-name>` is the name of the WebLogic Server instance\n+* `weblogic.clusterName: <cluster-name>`, where `<cluster-name>` is the name of the cluster of which this instance is a member, if any\n+* `weblogic.resourceVersion: <version>`, where `<version>` is the value of the `resourceVersion` field from the domain resource\n+\n+Prior to creating a Pod, the operator replaces variable references allowing the Pod content to be templates.  The format of these variable references is `$(VARIABLE_NAME)` where `VARIABLE_NAME` is one of the variable names available in the container for the WebLogic Server instance. The default set of environment variables includes:\n+\n+* `DOMAIN_NAME`: The WebLogic Server domain name.\n * `DOMAIN_UID`: The domain unique identifier.\n * `DOMAIN_HOME`: The domain home location as a file system path within the container.\n-* `SERVER_NAME`: The WebLogic Server name.\n+* `SERVER_NAME`: The WebLogic Server instance name.\n * `CLUSTER_NAME`: The WebLogic cluster name, if this is a cluster member.\n * `LOG_HOME`: The WebLogic log location as a file system path within the container.\n \n-This example domain YAML file specifies that pods for WebLogic Server instances in the `cluster-1` cluster will have a per-Managed Server volume and volume mount (similar to a Kubernetes `StatefulSet`), an `init` container to initialize some files in that volume, and anti-affinity scheduling so that the server instances are scheduled as much as possible on different nodes:\n+This example domain YAML file specifies that Pods for WebLogic Server instances in the `cluster-1` cluster will have a per-Managed Server volume and volume mount (similar to a Kubernetes `StatefulSet`), an `init` container to initialize some files in that volume, and anti-affinity scheduling so that the server instances are scheduled as much as possible on different Nodes:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2270ed288a33f1521b34e5ab3bbcb21c3e52c25"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwODAxNDYwOnYy", "diffSide": "RIGHT", "path": "kubernetes/samples/scripts/create-weblogic-domain/model-in-image/domain-resources/JRF/mii-initial-d1-JRF-v1.yaml", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzozOTo1OVrOGemkuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxODo0MzowN1rOGeoxaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc0MjQ1Ng==", "bodyText": "This file and the other domain resource files in the MII sample are generated from a template and checked-in.\nSo the corresponding file 'mii-domain.yaml.template-JRF' and also the WLS template file 'mii-domain.yaml.template-WLS' also need to be modified.\nIf you want to test that templates and sample files match, run 'cd src/integration-tests/model-in-image/mii-sample-wrapper  ; ./generate-sample-doc.sh '.  This will generate YAML from the templates and compare it to the files checked into source.  It should pass.   (The same script is called as part of the MII sample testing - which isn't merged into develop yet.)", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1703#discussion_r434742456", "createdAt": "2020-06-03T17:39:59Z", "author": {"login": "tbarnes-us"}, "path": "kubernetes/samples/scripts/create-weblogic-domain/model-in-image/domain-resources/JRF/mii-initial-d1-JRF-v1.yaml", "diffHunk": "@@ -92,6 +92,21 @@ spec:\n   clusters:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2270ed288a33f1521b34e5ab3bbcb21c3e52c25"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc2NTY5Ng==", "bodyText": "I ran this on my Linux box and it succeeded; however, it's not clear that it compared what is currently in the tree -- where would I see that?  It ends like this:\n@@\n@@ Info: Finished 'generate-sample-doc.sh'! See target directory '/tmp/mii-sample'.\n@@", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1703#discussion_r434765696", "createdAt": "2020-06-03T18:20:31Z", "author": {"login": "rjeberhard"}, "path": "kubernetes/samples/scripts/create-weblogic-domain/model-in-image/domain-resources/JRF/mii-initial-d1-JRF-v1.yaml", "diffHunk": "@@ -92,6 +92,21 @@ spec:\n   clusters:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc0MjQ1Ng=="}, "originalCommit": {"oid": "e2270ed288a33f1521b34e5ab3bbcb21c3e52c25"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc3ODQ3NQ==", "bodyText": "The script is silent in  'develop' when the compare passes.  (The 'mii-sample-it' branch version happens to include an extra echo when the compare passes.)", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1703#discussion_r434778475", "createdAt": "2020-06-03T18:43:07Z", "author": {"login": "tbarnes-us"}, "path": "kubernetes/samples/scripts/create-weblogic-domain/model-in-image/domain-resources/JRF/mii-initial-d1-JRF-v1.yaml", "diffHunk": "@@ -92,6 +92,21 @@ spec:\n   clusters:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc0MjQ1Ng=="}, "originalCommit": {"oid": "e2270ed288a33f1521b34e5ab3bbcb21c3e52c25"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwODA0Mzg1OnYy", "diffSide": "RIGHT", "path": "kubernetes/samples/scripts/create-weblogic-domain/manually-create-domain/domain.yaml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzo0ODo0NVrOGem4Mg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzo1NzowMFrOGenLQA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc0NzQ0Mg==", "bodyText": "Is podAntiAffinity necessary for the sample? It complicates the MII sample's domain resource YAML.\nIf it is necessary, can there instead be a single spec.configuration.podAntiAffinityEnabled or somesuch option to add the podAntiAffinity expression automagically?  With default true?  (If it is necessary it seems like it'd be a common enough issue we'd even want it be easy to configure and on by default.)", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1703#discussion_r434747442", "createdAt": "2020-06-03T17:48:45Z", "author": {"login": "tbarnes-us"}, "path": "kubernetes/samples/scripts/create-weblogic-domain/manually-create-domain/domain.yaml", "diffHunk": "@@ -99,6 +99,21 @@ spec:\n   clusters:\n   - clusterName: cluster-1\n     serverStartState: \"RUNNING\"\n+    serverPod:\n+      # Instructs Kubernetes scheduler to prefer nodes for new cluster members where there are not\n+      # already members of the same cluster.\n+      affinity:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2270ed288a33f1521b34e5ab3bbcb21c3e52c25"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc1MjMyMA==", "bodyText": "Necessary is, of course, debatable. The reasons I went with adding the content are that a lot of customers have already hit this issue, customers blindly use samples to create their own YAML, and there aren't any functional downsides to including the content.\nI don't really like the automatic insertion idea because that means this content is either hidden in a script or the template is even more complicated.  If you look at the other places where we conditionally include content in a script, the content is there in the script always but is, by default, commented out until we use sed to remove the commenting.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1703#discussion_r434752320", "createdAt": "2020-06-03T17:57:00Z", "author": {"login": "rjeberhard"}, "path": "kubernetes/samples/scripts/create-weblogic-domain/manually-create-domain/domain.yaml", "diffHunk": "@@ -99,6 +99,21 @@ spec:\n   clusters:\n   - clusterName: cluster-1\n     serverStartState: \"RUNNING\"\n+    serverPod:\n+      # Instructs Kubernetes scheduler to prefer nodes for new cluster members where there are not\n+      # already members of the same cluster.\n+      affinity:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc0NzQ0Mg=="}, "originalCommit": {"oid": "e2270ed288a33f1521b34e5ab3bbcb21c3e52c25"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwODA1MjcxOnYy", "diffSide": "RIGHT", "path": "kubernetes/samples/scripts/create-weblogic-domain/manually-create-domain/domain.yaml", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzo1MTowNlrOGem-AA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzo1MTowNlrOGem-AA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc0ODkyOA==", "bodyText": "Do the MII sample tests pass with this change?  See the WLS instructions for running the stand-alone version in 'src/integration-tests/model-in-image/README'.  (I've only tested on OCI boxes - might not work directly on a mac.)    This will run the sample and also run the 'check generated source' test I describe below in my other comment.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1703#discussion_r434748928", "createdAt": "2020-06-03T17:51:06Z", "author": {"login": "tbarnes-us"}, "path": "kubernetes/samples/scripts/create-weblogic-domain/manually-create-domain/domain.yaml", "diffHunk": "@@ -99,6 +99,21 @@ spec:\n   clusters:\n   - clusterName: cluster-1\n     serverStartState: \"RUNNING\"\n+    serverPod:\n+      # Instructs Kubernetes scheduler to prefer nodes for new cluster members where there are not\n+      # already members of the same cluster.\n+      affinity:\n+        podAntiAffinity:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2270ed288a33f1521b34e5ab3bbcb21c3e52c25"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwODA3NjkwOnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/node-heating.md", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzo1ODoxNVrOGenN6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxODoxOTo0N1rOGen9yw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc1MzAwMA==", "bodyText": "It could be useful to to have an example that covers all WL pods - not just for a particular domain-uid or cluster.   Eg. Any pod with a weblogic.domainUID - not just a particular domainUID.\nAlso, what is the precedence hierarchy for anti-affinity? In other words, what happens when multiple anti-affinity expressions are in effect?  E.g. any && domain && cluster", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1703#discussion_r434753000", "createdAt": "2020-06-03T17:58:15Z", "author": {"login": "tbarnes-us"}, "path": "docs-source/content/faq/node-heating.md", "diffHunk": "@@ -0,0 +1,56 @@\n+---\n+title: \"Node heating problem\"\n+date: 2020-06-03T08:08:19-04:00\n+draft: false\n+weight: 22\n+---\n+\n+The WebLogic Operator creates a Pod for each WebLogic Server instance that is started. The [Kubernetes Scheduler](https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/) then selects a Node for each Pod. Because the default scheduling algorithm gives substantial weight to selecting a Node where the necessary Docker images have already been pulled, this often results in Kubernetes running many of the Pods for WebLogic Server instances on the same Node while other Nodes are not fairly utilized.\n+\n+This is commonly known as the \"Node heating problem.\" One solution is to ensure that all necessary Docker images are available on worker Nodes as part of node provisioning. When the necessary Docker images are available on each worker Node, the Kubernetes Scheduler will instead select a Node based on other factors such as available CPU and memory or a simple round-robin.\n+\n+The WebLogic Operator Team recommends a different solution that is based on [inter-pod affinity and anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity). This solution has the advantage of both resolving the Node heating problem and of explicitly directing the Kubernetes Scheduler to spread the Pods for WebLogic Server instances from a given cluster or domain more widely across the available Nodes. Inter-pod affinity and anti-affinity are features of the Kubernetes Scheduler that allow the scheduler to choose a Node for a new Pod based on details of the Pods that are already running. For WebLogic Server use cases, the intent will often be for anti-affinity with the Pods for other WebLogic Server instances so that server instances spread over the available Nodes.\n+\n+To use these features, edit the Domain Custom Resource to add content to the `serverPod` element, in this case at the scope of a cluster, as shown in the following example:\n+\n+```\n+clusters:\n+- clusterName: cluster-1\n+  serverStartState: \"RUNNING\"\n+  serverPod:\n+    affinity:\n+      podAntiAffinity:\n+        preferredDuringSchedulingIgnoredDuringExecution:\n+          - weight: 100\n+            podAffinityTerm:\n+              labelSelector:\n+                matchExpressions:\n+                  - key: \"weblogic.clusterName\"\n+                    operator: In\n+                    values:\n+                    - $(CLUSTER_NAME)\n+              topologyKey: \"kubernetes.io/hostname\"\n+```\n+\n+Because the `serverPod` element here is scoped to a cluster, the content of the `affinity` element will be added to the Pod generated for each WebLogic Server instance that is a member of this WebLogic cluster. This inter-pod anti-affinity statement expresses a preference that the scheduler select a Node for the new Pod avoiding as much as possible Nodes that already have Pods with the label \"weblogic.clusterName\" and the name of this cluster. Note that the `weight` is set to `100`, which is the maximum weight, so that this term will outweigh any possible preference for a Node based on availability of Docker images.\n+\n+It is possible to express many other scheduling preferences or constraints. The following example similarly expresses an anti-affinity, but changes the test to have all WebLogic Server instances in the domain prefer to run on Nodes where there is not already a Pod for a running instance:\n+\n+```\n+serverPod:\n+  affinity:\n+    podAntiAffinity:\n+      preferredDuringSchedulingIgnoredDuringExecution:\n+      - weight: 100\n+        podAffinityTerm:\n+          labelSelector:\n+            matchExpressions:\n+            - key: \"weblogic.domainUID\"\n+              operator: In\n+              values:\n+              - $(DOMAIN_UID)\n+          topologyKey: \"kubernetes.io/hostname\"\n+```\n+\n+Details about how the WebLogic Operator generates Pods for WebLogic Server instances, including details about labels and variable substitution, are available [here]({{< relref \"/userguide/managing-domains/domain-resource#pod-generation\" >}}).\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e2270ed288a33f1521b34e5ab3bbcb21c3e52c25"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc1NjY1Ng==", "bodyText": "Affinity content from the various scopes (domain, admin, cluster, etc.) are merged, so you get content with all of the terms from any of the scopes that apply.  So, yes, you can do use cases where cluster members have anti-affinity with each other and where all servers in the domain have some other kind of affinity or anti-affinity with other workloads.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1703#discussion_r434756656", "createdAt": "2020-06-03T18:04:22Z", "author": {"login": "rjeberhard"}, "path": "docs-source/content/faq/node-heating.md", "diffHunk": "@@ -0,0 +1,56 @@\n+---\n+title: \"Node heating problem\"\n+date: 2020-06-03T08:08:19-04:00\n+draft: false\n+weight: 22\n+---\n+\n+The WebLogic Operator creates a Pod for each WebLogic Server instance that is started. The [Kubernetes Scheduler](https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/) then selects a Node for each Pod. Because the default scheduling algorithm gives substantial weight to selecting a Node where the necessary Docker images have already been pulled, this often results in Kubernetes running many of the Pods for WebLogic Server instances on the same Node while other Nodes are not fairly utilized.\n+\n+This is commonly known as the \"Node heating problem.\" One solution is to ensure that all necessary Docker images are available on worker Nodes as part of node provisioning. When the necessary Docker images are available on each worker Node, the Kubernetes Scheduler will instead select a Node based on other factors such as available CPU and memory or a simple round-robin.\n+\n+The WebLogic Operator Team recommends a different solution that is based on [inter-pod affinity and anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity). This solution has the advantage of both resolving the Node heating problem and of explicitly directing the Kubernetes Scheduler to spread the Pods for WebLogic Server instances from a given cluster or domain more widely across the available Nodes. Inter-pod affinity and anti-affinity are features of the Kubernetes Scheduler that allow the scheduler to choose a Node for a new Pod based on details of the Pods that are already running. For WebLogic Server use cases, the intent will often be for anti-affinity with the Pods for other WebLogic Server instances so that server instances spread over the available Nodes.\n+\n+To use these features, edit the Domain Custom Resource to add content to the `serverPod` element, in this case at the scope of a cluster, as shown in the following example:\n+\n+```\n+clusters:\n+- clusterName: cluster-1\n+  serverStartState: \"RUNNING\"\n+  serverPod:\n+    affinity:\n+      podAntiAffinity:\n+        preferredDuringSchedulingIgnoredDuringExecution:\n+          - weight: 100\n+            podAffinityTerm:\n+              labelSelector:\n+                matchExpressions:\n+                  - key: \"weblogic.clusterName\"\n+                    operator: In\n+                    values:\n+                    - $(CLUSTER_NAME)\n+              topologyKey: \"kubernetes.io/hostname\"\n+```\n+\n+Because the `serverPod` element here is scoped to a cluster, the content of the `affinity` element will be added to the Pod generated for each WebLogic Server instance that is a member of this WebLogic cluster. This inter-pod anti-affinity statement expresses a preference that the scheduler select a Node for the new Pod avoiding as much as possible Nodes that already have Pods with the label \"weblogic.clusterName\" and the name of this cluster. Note that the `weight` is set to `100`, which is the maximum weight, so that this term will outweigh any possible preference for a Node based on availability of Docker images.\n+\n+It is possible to express many other scheduling preferences or constraints. The following example similarly expresses an anti-affinity, but changes the test to have all WebLogic Server instances in the domain prefer to run on Nodes where there is not already a Pod for a running instance:\n+\n+```\n+serverPod:\n+  affinity:\n+    podAntiAffinity:\n+      preferredDuringSchedulingIgnoredDuringExecution:\n+      - weight: 100\n+        podAffinityTerm:\n+          labelSelector:\n+            matchExpressions:\n+            - key: \"weblogic.domainUID\"\n+              operator: In\n+              values:\n+              - $(DOMAIN_UID)\n+          topologyKey: \"kubernetes.io/hostname\"\n+```\n+\n+Details about how the WebLogic Operator generates Pods for WebLogic Server instances, including details about labels and variable substitution, are available [here]({{< relref \"/userguide/managing-domains/domain-resource#pod-generation\" >}}).\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc1MzAwMA=="}, "originalCommit": {"oid": "e2270ed288a33f1521b34e5ab3bbcb21c3e52c25"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc1NzAzNQ==", "bodyText": "I'm not sure that I want more examples here unless we have a use case customers have described.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1703#discussion_r434757035", "createdAt": "2020-06-03T18:05:01Z", "author": {"login": "rjeberhard"}, "path": "docs-source/content/faq/node-heating.md", "diffHunk": "@@ -0,0 +1,56 @@\n+---\n+title: \"Node heating problem\"\n+date: 2020-06-03T08:08:19-04:00\n+draft: false\n+weight: 22\n+---\n+\n+The WebLogic Operator creates a Pod for each WebLogic Server instance that is started. The [Kubernetes Scheduler](https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/) then selects a Node for each Pod. Because the default scheduling algorithm gives substantial weight to selecting a Node where the necessary Docker images have already been pulled, this often results in Kubernetes running many of the Pods for WebLogic Server instances on the same Node while other Nodes are not fairly utilized.\n+\n+This is commonly known as the \"Node heating problem.\" One solution is to ensure that all necessary Docker images are available on worker Nodes as part of node provisioning. When the necessary Docker images are available on each worker Node, the Kubernetes Scheduler will instead select a Node based on other factors such as available CPU and memory or a simple round-robin.\n+\n+The WebLogic Operator Team recommends a different solution that is based on [inter-pod affinity and anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity). This solution has the advantage of both resolving the Node heating problem and of explicitly directing the Kubernetes Scheduler to spread the Pods for WebLogic Server instances from a given cluster or domain more widely across the available Nodes. Inter-pod affinity and anti-affinity are features of the Kubernetes Scheduler that allow the scheduler to choose a Node for a new Pod based on details of the Pods that are already running. For WebLogic Server use cases, the intent will often be for anti-affinity with the Pods for other WebLogic Server instances so that server instances spread over the available Nodes.\n+\n+To use these features, edit the Domain Custom Resource to add content to the `serverPod` element, in this case at the scope of a cluster, as shown in the following example:\n+\n+```\n+clusters:\n+- clusterName: cluster-1\n+  serverStartState: \"RUNNING\"\n+  serverPod:\n+    affinity:\n+      podAntiAffinity:\n+        preferredDuringSchedulingIgnoredDuringExecution:\n+          - weight: 100\n+            podAffinityTerm:\n+              labelSelector:\n+                matchExpressions:\n+                  - key: \"weblogic.clusterName\"\n+                    operator: In\n+                    values:\n+                    - $(CLUSTER_NAME)\n+              topologyKey: \"kubernetes.io/hostname\"\n+```\n+\n+Because the `serverPod` element here is scoped to a cluster, the content of the `affinity` element will be added to the Pod generated for each WebLogic Server instance that is a member of this WebLogic cluster. This inter-pod anti-affinity statement expresses a preference that the scheduler select a Node for the new Pod avoiding as much as possible Nodes that already have Pods with the label \"weblogic.clusterName\" and the name of this cluster. Note that the `weight` is set to `100`, which is the maximum weight, so that this term will outweigh any possible preference for a Node based on availability of Docker images.\n+\n+It is possible to express many other scheduling preferences or constraints. The following example similarly expresses an anti-affinity, but changes the test to have all WebLogic Server instances in the domain prefer to run on Nodes where there is not already a Pod for a running instance:\n+\n+```\n+serverPod:\n+  affinity:\n+    podAntiAffinity:\n+      preferredDuringSchedulingIgnoredDuringExecution:\n+      - weight: 100\n+        podAffinityTerm:\n+          labelSelector:\n+            matchExpressions:\n+            - key: \"weblogic.domainUID\"\n+              operator: In\n+              values:\n+              - $(DOMAIN_UID)\n+          topologyKey: \"kubernetes.io/hostname\"\n+```\n+\n+Details about how the WebLogic Operator generates Pods for WebLogic Server instances, including details about labels and variable substitution, are available [here]({{< relref \"/userguide/managing-domains/domain-resource#pod-generation\" >}}).\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc1MzAwMA=="}, "originalCommit": {"oid": "e2270ed288a33f1521b34e5ab3bbcb21c3e52c25"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc2NDk3Nw==", "bodyText": "I think the most common use case is most likely 'all'.  You have domain-uid and cluster covered, but not 'all'.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1703#discussion_r434764977", "createdAt": "2020-06-03T18:19:18Z", "author": {"login": "tbarnes-us"}, "path": "docs-source/content/faq/node-heating.md", "diffHunk": "@@ -0,0 +1,56 @@\n+---\n+title: \"Node heating problem\"\n+date: 2020-06-03T08:08:19-04:00\n+draft: false\n+weight: 22\n+---\n+\n+The WebLogic Operator creates a Pod for each WebLogic Server instance that is started. The [Kubernetes Scheduler](https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/) then selects a Node for each Pod. Because the default scheduling algorithm gives substantial weight to selecting a Node where the necessary Docker images have already been pulled, this often results in Kubernetes running many of the Pods for WebLogic Server instances on the same Node while other Nodes are not fairly utilized.\n+\n+This is commonly known as the \"Node heating problem.\" One solution is to ensure that all necessary Docker images are available on worker Nodes as part of node provisioning. When the necessary Docker images are available on each worker Node, the Kubernetes Scheduler will instead select a Node based on other factors such as available CPU and memory or a simple round-robin.\n+\n+The WebLogic Operator Team recommends a different solution that is based on [inter-pod affinity and anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity). This solution has the advantage of both resolving the Node heating problem and of explicitly directing the Kubernetes Scheduler to spread the Pods for WebLogic Server instances from a given cluster or domain more widely across the available Nodes. Inter-pod affinity and anti-affinity are features of the Kubernetes Scheduler that allow the scheduler to choose a Node for a new Pod based on details of the Pods that are already running. For WebLogic Server use cases, the intent will often be for anti-affinity with the Pods for other WebLogic Server instances so that server instances spread over the available Nodes.\n+\n+To use these features, edit the Domain Custom Resource to add content to the `serverPod` element, in this case at the scope of a cluster, as shown in the following example:\n+\n+```\n+clusters:\n+- clusterName: cluster-1\n+  serverStartState: \"RUNNING\"\n+  serverPod:\n+    affinity:\n+      podAntiAffinity:\n+        preferredDuringSchedulingIgnoredDuringExecution:\n+          - weight: 100\n+            podAffinityTerm:\n+              labelSelector:\n+                matchExpressions:\n+                  - key: \"weblogic.clusterName\"\n+                    operator: In\n+                    values:\n+                    - $(CLUSTER_NAME)\n+              topologyKey: \"kubernetes.io/hostname\"\n+```\n+\n+Because the `serverPod` element here is scoped to a cluster, the content of the `affinity` element will be added to the Pod generated for each WebLogic Server instance that is a member of this WebLogic cluster. This inter-pod anti-affinity statement expresses a preference that the scheduler select a Node for the new Pod avoiding as much as possible Nodes that already have Pods with the label \"weblogic.clusterName\" and the name of this cluster. Note that the `weight` is set to `100`, which is the maximum weight, so that this term will outweigh any possible preference for a Node based on availability of Docker images.\n+\n+It is possible to express many other scheduling preferences or constraints. The following example similarly expresses an anti-affinity, but changes the test to have all WebLogic Server instances in the domain prefer to run on Nodes where there is not already a Pod for a running instance:\n+\n+```\n+serverPod:\n+  affinity:\n+    podAntiAffinity:\n+      preferredDuringSchedulingIgnoredDuringExecution:\n+      - weight: 100\n+        podAffinityTerm:\n+          labelSelector:\n+            matchExpressions:\n+            - key: \"weblogic.domainUID\"\n+              operator: In\n+              values:\n+              - $(DOMAIN_UID)\n+          topologyKey: \"kubernetes.io/hostname\"\n+```\n+\n+Details about how the WebLogic Operator generates Pods for WebLogic Server instances, including details about labels and variable substitution, are available [here]({{< relref \"/userguide/managing-domains/domain-resource#pod-generation\" >}}).\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc1MzAwMA=="}, "originalCommit": {"oid": "e2270ed288a33f1521b34e5ab3bbcb21c3e52c25"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc2NTI1OQ==", "bodyText": "So why not have the samples and FAQ emphasize 'all'?", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1703#discussion_r434765259", "createdAt": "2020-06-03T18:19:47Z", "author": {"login": "tbarnes-us"}, "path": "docs-source/content/faq/node-heating.md", "diffHunk": "@@ -0,0 +1,56 @@\n+---\n+title: \"Node heating problem\"\n+date: 2020-06-03T08:08:19-04:00\n+draft: false\n+weight: 22\n+---\n+\n+The WebLogic Operator creates a Pod for each WebLogic Server instance that is started. The [Kubernetes Scheduler](https://kubernetes.io/docs/concepts/scheduling-eviction/kube-scheduler/) then selects a Node for each Pod. Because the default scheduling algorithm gives substantial weight to selecting a Node where the necessary Docker images have already been pulled, this often results in Kubernetes running many of the Pods for WebLogic Server instances on the same Node while other Nodes are not fairly utilized.\n+\n+This is commonly known as the \"Node heating problem.\" One solution is to ensure that all necessary Docker images are available on worker Nodes as part of node provisioning. When the necessary Docker images are available on each worker Node, the Kubernetes Scheduler will instead select a Node based on other factors such as available CPU and memory or a simple round-robin.\n+\n+The WebLogic Operator Team recommends a different solution that is based on [inter-pod affinity and anti-affinity](https://kubernetes.io/docs/concepts/scheduling-eviction/assign-pod-node/#inter-pod-affinity-and-anti-affinity). This solution has the advantage of both resolving the Node heating problem and of explicitly directing the Kubernetes Scheduler to spread the Pods for WebLogic Server instances from a given cluster or domain more widely across the available Nodes. Inter-pod affinity and anti-affinity are features of the Kubernetes Scheduler that allow the scheduler to choose a Node for a new Pod based on details of the Pods that are already running. For WebLogic Server use cases, the intent will often be for anti-affinity with the Pods for other WebLogic Server instances so that server instances spread over the available Nodes.\n+\n+To use these features, edit the Domain Custom Resource to add content to the `serverPod` element, in this case at the scope of a cluster, as shown in the following example:\n+\n+```\n+clusters:\n+- clusterName: cluster-1\n+  serverStartState: \"RUNNING\"\n+  serverPod:\n+    affinity:\n+      podAntiAffinity:\n+        preferredDuringSchedulingIgnoredDuringExecution:\n+          - weight: 100\n+            podAffinityTerm:\n+              labelSelector:\n+                matchExpressions:\n+                  - key: \"weblogic.clusterName\"\n+                    operator: In\n+                    values:\n+                    - $(CLUSTER_NAME)\n+              topologyKey: \"kubernetes.io/hostname\"\n+```\n+\n+Because the `serverPod` element here is scoped to a cluster, the content of the `affinity` element will be added to the Pod generated for each WebLogic Server instance that is a member of this WebLogic cluster. This inter-pod anti-affinity statement expresses a preference that the scheduler select a Node for the new Pod avoiding as much as possible Nodes that already have Pods with the label \"weblogic.clusterName\" and the name of this cluster. Note that the `weight` is set to `100`, which is the maximum weight, so that this term will outweigh any possible preference for a Node based on availability of Docker images.\n+\n+It is possible to express many other scheduling preferences or constraints. The following example similarly expresses an anti-affinity, but changes the test to have all WebLogic Server instances in the domain prefer to run on Nodes where there is not already a Pod for a running instance:\n+\n+```\n+serverPod:\n+  affinity:\n+    podAntiAffinity:\n+      preferredDuringSchedulingIgnoredDuringExecution:\n+      - weight: 100\n+        podAffinityTerm:\n+          labelSelector:\n+            matchExpressions:\n+            - key: \"weblogic.domainUID\"\n+              operator: In\n+              values:\n+              - $(DOMAIN_UID)\n+          topologyKey: \"kubernetes.io/hostname\"\n+```\n+\n+Details about how the WebLogic Operator generates Pods for WebLogic Server instances, including details about labels and variable substitution, are available [here]({{< relref \"/userguide/managing-domains/domain-resource#pod-generation\" >}}).\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc1MzAwMA=="}, "originalCommit": {"oid": "e2270ed288a33f1521b34e5ab3bbcb21c3e52c25"}, "originalPosition": 56}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4293, "cost": 1, "resetAt": "2021-11-12T12:57:47Z"}}}