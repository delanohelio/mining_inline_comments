{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQyMDE3Njky", "number": 1775, "reviewThreads": {"totalCount": 51, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzo0Mzo1MlrOEKGpfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNToxOToyNlrOEK8Qyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDMwMTQxOnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzo0Mzo1MlrOGq9CCg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzo0Mzo1MlrOGq9CCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzY5MzMyMg==", "bodyText": "Considerations for Pod Resource (Memory and CPU) Requests and Limits -> Considerations for Pod resource (memory and CPU) requests and limits  (we use sentence capitalization instead of title capitalization; appears more user friendly)", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447693322", "createdAt": "2020-06-30T13:43:52Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 2}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDMxMDM2OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzo0NTo1M1rOGq9Hpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzo0NTo1M1rOGq9Hpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzY5NDc1OQ==", "bodyText": "pod -> Pod (globally, if you are referring to a Kubernetes resource)\nIt.s -> It's (typo)", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447694759", "createdAt": "2020-06-30T13:45:53Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDMyMDg4OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzo0ODoxMFrOGq9OHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzo0ODoxMFrOGq9OHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzY5NjQxNQ==", "bodyText": "node -> Node (globally, if you are referring to a Kubernetes resource)\nIt.s -> It's (typo, globally)\nrouge -> rogue\nhas memory leak -> has a memory leak", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447696415", "createdAt": "2020-06-30T13:48:10Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDM0MzAyOnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzo1Mjo1MFrOGq9b6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzo1Mjo1MFrOGq9b6A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzY5OTk0NA==", "bodyText": "pod.s -> pod's (typo, globally; your apostrophes are periods)\nof resource -> of resources\nIt also allows users to plan -> Also, it lets you plan\npod.s priority and the Quality of Service (QoS) that pod receives -> the pod's priority and Quality of Service (QoS) that the pod receives\nare specified or not. -> are specified.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447699944", "createdAt": "2020-06-30T13:52:50Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. \n+\n+A pod.s resource requests and limit parameters can be used to solve these problems. Setting resource limits prevents an application from using more than it.s share of resource. Thus, limiting resources improves reliability and stability of applications.  It also allows users to plan for the hardware capacity. Additionally, pod.s priority and the Quality of Service (QoS) that pod receives is affected by whether resource requests and limits are specified or not.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 11}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDM0NjEyOnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzo1MzozMVrOGq9d3A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzo1MzozMVrOGq9d3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzcwMDQ0NA==", "bodyText": "Prioritization -> prioritization", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447700444", "createdAt": "2020-06-30T13:53:31Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. \n+\n+A pod.s resource requests and limit parameters can be used to solve these problems. Setting resource limits prevents an application from using more than it.s share of resource. Thus, limiting resources improves reliability and stability of applications.  It also allows users to plan for the hardware capacity. Additionally, pod.s priority and the Quality of Service (QoS) that pod receives is affected by whether resource requests and limits are specified or not.\n+\n+## Pod Quality Of Service (QoS) and Prioritization", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDM1NzgwOnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzo1NTo1MlrOGq9lLg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzo1NTo1MlrOGq9lLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzcwMjMxOA==", "bodyText": "Pod.s Quality of Service (QoS) -> A Pod's QoS (you don't have to repeat the spelled out acronym more than once)\nwhether pod.s resource -> whether the pod's resource\nare configured or not -> are configured", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447702318", "createdAt": "2020-06-30T13:55:52Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. \n+\n+A pod.s resource requests and limit parameters can be used to solve these problems. Setting resource limits prevents an application from using more than it.s share of resource. Thus, limiting resources improves reliability and stability of applications.  It also allows users to plan for the hardware capacity. Additionally, pod.s priority and the Quality of Service (QoS) that pod receives is affected by whether resource requests and limits are specified or not.\n+\n+## Pod Quality Of Service (QoS) and Prioritization\n+Pod.s Quality of Service (QoS) and priority is determined based on whether pod.s resource requests and limits are configured or not and how they.re configured.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDM2NzQwOnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzo1Nzo1N1rOGq9rJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxMzo1Nzo1N1rOGq9rJg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzcwMzg0Ng==", "bodyText": "Best Effort QoS -> Best-Effort QoS (IF you hyphenate it in the description, you must hyphenate it in the title)\nlimits, pod receives .best-effort. QoS -> limits, then the pod receives \"best-effort\" QoS\nwhere node -> where a Node", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447703846", "createdAt": "2020-06-30T13:57:57Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. \n+\n+A pod.s resource requests and limit parameters can be used to solve these problems. Setting resource limits prevents an application from using more than it.s share of resource. Thus, limiting resources improves reliability and stability of applications.  It also allows users to plan for the hardware capacity. Additionally, pod.s priority and the Quality of Service (QoS) that pod receives is affected by whether resource requests and limits are specified or not.\n+\n+## Pod Quality Of Service (QoS) and Prioritization\n+Pod.s Quality of Service (QoS) and priority is determined based on whether pod.s resource requests and limits are configured or not and how they.re configured.\n+\n+**Best Effort QoS**: If you don.t configure requests and limits, pod receives .best-effort. QoS and pod has the **lowest priority**. In cases where node runs out of non-shareable resources, kubelet.s out-of-resource eviction policy evicts/kills the pods with best-effort QoS first.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDQzMTE3OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNDoxMToyMVrOGq-TJw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNDoxMToyMVrOGq-TJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzcxNDA4Nw==", "bodyText": "values, pod will have .Guranteed. -> values, then the Pod will have \"guaranteed\"\nand pod will be considered as of the top -> and it will be considered as the top\nsettings indicates ->  settings indicate\nresources, Kubernetes -> resources, then Kubernetes", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447714087", "createdAt": "2020-06-30T14:11:21Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. \n+\n+A pod.s resource requests and limit parameters can be used to solve these problems. Setting resource limits prevents an application from using more than it.s share of resource. Thus, limiting resources improves reliability and stability of applications.  It also allows users to plan for the hardware capacity. Additionally, pod.s priority and the Quality of Service (QoS) that pod receives is affected by whether resource requests and limits are specified or not.\n+\n+## Pod Quality Of Service (QoS) and Prioritization\n+Pod.s Quality of Service (QoS) and priority is determined based on whether pod.s resource requests and limits are configured or not and how they.re configured.\n+\n+**Best Effort QoS**: If you don.t configure requests and limits, pod receives .best-effort. QoS and pod has the **lowest priority**. In cases where node runs out of non-shareable resources, kubelet.s out-of-resource eviction policy evicts/kills the pods with best-effort QoS first.\n+\n+**Burstable QoS**: If you configure both resource requests and limits, and set the requests to be less than the limit, pod.s QoS will be .Burstable.. Similarly when you only configure the resource requests (without limits), the pod QoS is .Burstable.. When the node runs out of non-shareable resources, kubelet will kill .Burstable. Pods only when there are no more .best-effort. pods running. The Burstable pod receives **medium priority**.\n+\n+**Guaranteed QoS**:  If you set the requests and the limits to equal values, pod will have .Guranteed. QoS and pod will be considered as of the top most priority. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, Kubernetes will kill the best-effort and the burstable Pods first before terminating these Guaranteed QoS Pods. These are the **highest priority** pods.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDQ1MTA2OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNDoxNToxNlrOGq-fXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNDoxNToxNlrOGq-fXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzcxNzIxMg==", "bodyText": "limit, pod.s QoS -> limit, then the pod's QoS\n.Burstable. -> \"burstable\" (does not need to be capitalized in the paragraph; the choice is to capitalize best-effort, burstable, and guaranteed consistently in the paras) or to keep them all lower case.)", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447717212", "createdAt": "2020-06-30T14:15:16Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. \n+\n+A pod.s resource requests and limit parameters can be used to solve these problems. Setting resource limits prevents an application from using more than it.s share of resource. Thus, limiting resources improves reliability and stability of applications.  It also allows users to plan for the hardware capacity. Additionally, pod.s priority and the Quality of Service (QoS) that pod receives is affected by whether resource requests and limits are specified or not.\n+\n+## Pod Quality Of Service (QoS) and Prioritization\n+Pod.s Quality of Service (QoS) and priority is determined based on whether pod.s resource requests and limits are configured or not and how they.re configured.\n+\n+**Best Effort QoS**: If you don.t configure requests and limits, pod receives .best-effort. QoS and pod has the **lowest priority**. In cases where node runs out of non-shareable resources, kubelet.s out-of-resource eviction policy evicts/kills the pods with best-effort QoS first.\n+\n+**Burstable QoS**: If you configure both resource requests and limits, and set the requests to be less than the limit, pod.s QoS will be .Burstable.. Similarly when you only configure the resource requests (without limits), the pod QoS is .Burstable.. When the node runs out of non-shareable resources, kubelet will kill .Burstable. Pods only when there are no more .best-effort. pods running. The Burstable pod receives **medium priority**.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDQ4NTI0OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNDoyMjoxNFrOGq-0RQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNDoyMjoxNFrOGq-0RQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzcyMjU2NQ==", "bodyText": "to set correct heap -> to set the correct heap\nIf available memory on node or memory allocated to container -> If the available memory on a Node or the memory allocated to a container\nfor specified JVM heap arguments -> for the specified JVM heap arguments\nWL process  -> WebLogic processes\nthat configured heap sizes -> that the configured heap sizes", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447722565", "createdAt": "2020-06-30T14:22:14Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. \n+\n+A pod.s resource requests and limit parameters can be used to solve these problems. Setting resource limits prevents an application from using more than it.s share of resource. Thus, limiting resources improves reliability and stability of applications.  It also allows users to plan for the hardware capacity. Additionally, pod.s priority and the Quality of Service (QoS) that pod receives is affected by whether resource requests and limits are specified or not.\n+\n+## Pod Quality Of Service (QoS) and Prioritization\n+Pod.s Quality of Service (QoS) and priority is determined based on whether pod.s resource requests and limits are configured or not and how they.re configured.\n+\n+**Best Effort QoS**: If you don.t configure requests and limits, pod receives .best-effort. QoS and pod has the **lowest priority**. In cases where node runs out of non-shareable resources, kubelet.s out-of-resource eviction policy evicts/kills the pods with best-effort QoS first.\n+\n+**Burstable QoS**: If you configure both resource requests and limits, and set the requests to be less than the limit, pod.s QoS will be .Burstable.. Similarly when you only configure the resource requests (without limits), the pod QoS is .Burstable.. When the node runs out of non-shareable resources, kubelet will kill .Burstable. Pods only when there are no more .best-effort. pods running. The Burstable pod receives **medium priority**.\n+\n+**Guaranteed QoS**:  If you set the requests and the limits to equal values, pod will have .Guranteed. QoS and pod will be considered as of the top most priority. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, Kubernetes will kill the best-effort and the burstable Pods first before terminating these Guaranteed QoS Pods. These are the **highest priority** pods.\n+\n+## Java heap size and pod memory request/limit considerations\n+It.s extremely important to set correct heap size for JVM-based applications.  If available memory on node or memory allocated to container is not sufficient for specified JVM heap arguments (and additional off-heap memory), it is possible for WL process to run out of memory. In order to avoid this, you will need to make sure that configured heap sizes are not too big and that the pod is scheduled on the node with sufficient memory.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDc5ODQ2OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNToyNTo0OVrOGrB2ng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNToyNTo0OVrOGrB2ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc3MjMxOA==", "bodyText": "Replace every \".s\" in this document with \"'s\".", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447772318", "createdAt": "2020-06-30T15:25:49Z", "author": {"login": "tbarnes-us"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDgwNjcxOnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNToyNzozNFrOGrB7ww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNToyNzozNFrOGrB7ww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc3MzYzNQ==", "bodyText": "rouge -> rogue (took me a bit to figure that one out - interesting autocorrect :-) )", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447773635", "createdAt": "2020-06-30T15:27:34Z", "author": {"login": "tbarnes-us"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDgyNzIzOnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNTozMTo0M1rOGrCIbw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNTozMTo0M1rOGrCIbw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc3Njg3OQ==", "bodyText": "Additionally, pod.s ... --> Additionally, a pod's priority and Quality of Service (QoS) is affected by whether or not it specifies resource requests and limits.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447776879", "createdAt": "2020-06-30T15:31:43Z", "author": {"login": "tbarnes-us"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. \n+\n+A pod.s resource requests and limit parameters can be used to solve these problems. Setting resource limits prevents an application from using more than it.s share of resource. Thus, limiting resources improves reliability and stability of applications.  It also allows users to plan for the hardware capacity. Additionally, pod.s priority and the Quality of Service (QoS) that pod receives is affected by whether resource requests and limits are specified or not.\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDg0NDA4OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNTozNToyNVrOGrCSoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNTozNToyNVrOGrCSoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc3OTQ4OQ==", "bodyText": "heap sizes (-Xms and -Xmx), JVM will -> heap sizes (-Xms and -Xmx), then the JVM will\nof container memory limit -> of the container memory limit\nof limit value -> of the limit value", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447779489", "createdAt": "2020-06-30T15:35:25Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. \n+\n+A pod.s resource requests and limit parameters can be used to solve these problems. Setting resource limits prevents an application from using more than it.s share of resource. Thus, limiting resources improves reliability and stability of applications.  It also allows users to plan for the hardware capacity. Additionally, pod.s priority and the Quality of Service (QoS) that pod receives is affected by whether resource requests and limits are specified or not.\n+\n+## Pod Quality Of Service (QoS) and Prioritization\n+Pod.s Quality of Service (QoS) and priority is determined based on whether pod.s resource requests and limits are configured or not and how they.re configured.\n+\n+**Best Effort QoS**: If you don.t configure requests and limits, pod receives .best-effort. QoS and pod has the **lowest priority**. In cases where node runs out of non-shareable resources, kubelet.s out-of-resource eviction policy evicts/kills the pods with best-effort QoS first.\n+\n+**Burstable QoS**: If you configure both resource requests and limits, and set the requests to be less than the limit, pod.s QoS will be .Burstable.. Similarly when you only configure the resource requests (without limits), the pod QoS is .Burstable.. When the node runs out of non-shareable resources, kubelet will kill .Burstable. Pods only when there are no more .best-effort. pods running. The Burstable pod receives **medium priority**.\n+\n+**Guaranteed QoS**:  If you set the requests and the limits to equal values, pod will have .Guranteed. QoS and pod will be considered as of the top most priority. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, Kubernetes will kill the best-effort and the burstable Pods first before terminating these Guaranteed QoS Pods. These are the **highest priority** pods.\n+\n+## Java heap size and pod memory request/limit considerations\n+It.s extremely important to set correct heap size for JVM-based applications.  If available memory on node or memory allocated to container is not sufficient for specified JVM heap arguments (and additional off-heap memory), it is possible for WL process to run out of memory. In order to avoid this, you will need to make sure that configured heap sizes are not too big and that the pod is scheduled on the node with sufficient memory.\n+With the latest Java version, it.s possible to rely on the default JVM heap settings which are safe but quite conservative. If you configure the memory limit for a container but don.t configure heap sizes (-Xms and -Xmx), JVM will configure max heap size to 25% (1/4th) of container memory limit by default. The minimum heap size is configured to 1.56% (1/64th) of limit value.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDg1MjEzOnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNTozNzoxMlrOGrCXig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNTozNzoxMlrOGrCXig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc4MDc0Ng==", "bodyText": "configure default -> configure the default\nWebLogic server java process -> WebLogic Server Java processes\nusing USER_MEM_ARG -> using the USER_MEM_ARG", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447780746", "createdAt": "2020-06-30T15:37:12Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. \n+\n+A pod.s resource requests and limit parameters can be used to solve these problems. Setting resource limits prevents an application from using more than it.s share of resource. Thus, limiting resources improves reliability and stability of applications.  It also allows users to plan for the hardware capacity. Additionally, pod.s priority and the Quality of Service (QoS) that pod receives is affected by whether resource requests and limits are specified or not.\n+\n+## Pod Quality Of Service (QoS) and Prioritization\n+Pod.s Quality of Service (QoS) and priority is determined based on whether pod.s resource requests and limits are configured or not and how they.re configured.\n+\n+**Best Effort QoS**: If you don.t configure requests and limits, pod receives .best-effort. QoS and pod has the **lowest priority**. In cases where node runs out of non-shareable resources, kubelet.s out-of-resource eviction policy evicts/kills the pods with best-effort QoS first.\n+\n+**Burstable QoS**: If you configure both resource requests and limits, and set the requests to be less than the limit, pod.s QoS will be .Burstable.. Similarly when you only configure the resource requests (without limits), the pod QoS is .Burstable.. When the node runs out of non-shareable resources, kubelet will kill .Burstable. Pods only when there are no more .best-effort. pods running. The Burstable pod receives **medium priority**.\n+\n+**Guaranteed QoS**:  If you set the requests and the limits to equal values, pod will have .Guranteed. QoS and pod will be considered as of the top most priority. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, Kubernetes will kill the best-effort and the burstable Pods first before terminating these Guaranteed QoS Pods. These are the **highest priority** pods.\n+\n+## Java heap size and pod memory request/limit considerations\n+It.s extremely important to set correct heap size for JVM-based applications.  If available memory on node or memory allocated to container is not sufficient for specified JVM heap arguments (and additional off-heap memory), it is possible for WL process to run out of memory. In order to avoid this, you will need to make sure that configured heap sizes are not too big and that the pod is scheduled on the node with sufficient memory.\n+With the latest Java version, it.s possible to rely on the default JVM heap settings which are safe but quite conservative. If you configure the memory limit for a container but don.t configure heap sizes (-Xms and -Xmx), JVM will configure max heap size to 25% (1/4th) of container memory limit by default. The minimum heap size is configured to 1.56% (1/64th) of limit value.\n+\n+**Default heap sizes and resource request values for sample WebLogic Server Pods**:\n+The WLS samples configure default min and max heap size for WebLogic server java process to 256MB and 512MB respectively. This can be changed using USER_MEM_ARGS environment variable. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDg1NzU0OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNTozODoyOVrOGrCbCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNTozODoyOVrOGrCbCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc4MTY0Mw==", "bodyText": "for node-manager process -> for the node-manager process\nusing NODEMGR_MEM_ARGS -> using the NODEMGR_MEM_ARGS", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447781643", "createdAt": "2020-06-30T15:38:29Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. \n+\n+A pod.s resource requests and limit parameters can be used to solve these problems. Setting resource limits prevents an application from using more than it.s share of resource. Thus, limiting resources improves reliability and stability of applications.  It also allows users to plan for the hardware capacity. Additionally, pod.s priority and the Quality of Service (QoS) that pod receives is affected by whether resource requests and limits are specified or not.\n+\n+## Pod Quality Of Service (QoS) and Prioritization\n+Pod.s Quality of Service (QoS) and priority is determined based on whether pod.s resource requests and limits are configured or not and how they.re configured.\n+\n+**Best Effort QoS**: If you don.t configure requests and limits, pod receives .best-effort. QoS and pod has the **lowest priority**. In cases where node runs out of non-shareable resources, kubelet.s out-of-resource eviction policy evicts/kills the pods with best-effort QoS first.\n+\n+**Burstable QoS**: If you configure both resource requests and limits, and set the requests to be less than the limit, pod.s QoS will be .Burstable.. Similarly when you only configure the resource requests (without limits), the pod QoS is .Burstable.. When the node runs out of non-shareable resources, kubelet will kill .Burstable. Pods only when there are no more .best-effort. pods running. The Burstable pod receives **medium priority**.\n+\n+**Guaranteed QoS**:  If you set the requests and the limits to equal values, pod will have .Guranteed. QoS and pod will be considered as of the top most priority. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, Kubernetes will kill the best-effort and the burstable Pods first before terminating these Guaranteed QoS Pods. These are the **highest priority** pods.\n+\n+## Java heap size and pod memory request/limit considerations\n+It.s extremely important to set correct heap size for JVM-based applications.  If available memory on node or memory allocated to container is not sufficient for specified JVM heap arguments (and additional off-heap memory), it is possible for WL process to run out of memory. In order to avoid this, you will need to make sure that configured heap sizes are not too big and that the pod is scheduled on the node with sufficient memory.\n+With the latest Java version, it.s possible to rely on the default JVM heap settings which are safe but quite conservative. If you configure the memory limit for a container but don.t configure heap sizes (-Xms and -Xmx), JVM will configure max heap size to 25% (1/4th) of container memory limit by default. The minimum heap size is configured to 1.56% (1/64th) of limit value.\n+\n+**Default heap sizes and resource request values for sample WebLogic Server Pods**:\n+The WLS samples configure default min and max heap size for WebLogic server java process to 256MB and 512MB respectively. This can be changed using USER_MEM_ARGS environment variable. \n+```\n+    resources:\n+      env:\n+      - name: \"USER_MEM_ARGS\"\n+        value: \"-Xms256m -Xmx512m -Djava.security.egd=file:/dev/./urandom\"\n+```        \n+\n+The default min and max heap size for node-manager process is 64MB and 100MB. This can be changed by using NODEMGR_MEM_ARGS environment variable. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDg2MTcwOnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNTozOTozMVrOGrCdyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNTozOTozMVrOGrCdyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc4MjM0Ng==", "bodyText": "and default CPU request -> and the default CPU request\nThe requests values -> The request values\nin resources section. -> in the resources section.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447782346", "createdAt": "2020-06-30T15:39:31Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. \n+\n+A pod.s resource requests and limit parameters can be used to solve these problems. Setting resource limits prevents an application from using more than it.s share of resource. Thus, limiting resources improves reliability and stability of applications.  It also allows users to plan for the hardware capacity. Additionally, pod.s priority and the Quality of Service (QoS) that pod receives is affected by whether resource requests and limits are specified or not.\n+\n+## Pod Quality Of Service (QoS) and Prioritization\n+Pod.s Quality of Service (QoS) and priority is determined based on whether pod.s resource requests and limits are configured or not and how they.re configured.\n+\n+**Best Effort QoS**: If you don.t configure requests and limits, pod receives .best-effort. QoS and pod has the **lowest priority**. In cases where node runs out of non-shareable resources, kubelet.s out-of-resource eviction policy evicts/kills the pods with best-effort QoS first.\n+\n+**Burstable QoS**: If you configure both resource requests and limits, and set the requests to be less than the limit, pod.s QoS will be .Burstable.. Similarly when you only configure the resource requests (without limits), the pod QoS is .Burstable.. When the node runs out of non-shareable resources, kubelet will kill .Burstable. Pods only when there are no more .best-effort. pods running. The Burstable pod receives **medium priority**.\n+\n+**Guaranteed QoS**:  If you set the requests and the limits to equal values, pod will have .Guranteed. QoS and pod will be considered as of the top most priority. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, Kubernetes will kill the best-effort and the burstable Pods first before terminating these Guaranteed QoS Pods. These are the **highest priority** pods.\n+\n+## Java heap size and pod memory request/limit considerations\n+It.s extremely important to set correct heap size for JVM-based applications.  If available memory on node or memory allocated to container is not sufficient for specified JVM heap arguments (and additional off-heap memory), it is possible for WL process to run out of memory. In order to avoid this, you will need to make sure that configured heap sizes are not too big and that the pod is scheduled on the node with sufficient memory.\n+With the latest Java version, it.s possible to rely on the default JVM heap settings which are safe but quite conservative. If you configure the memory limit for a container but don.t configure heap sizes (-Xms and -Xmx), JVM will configure max heap size to 25% (1/4th) of container memory limit by default. The minimum heap size is configured to 1.56% (1/64th) of limit value.\n+\n+**Default heap sizes and resource request values for sample WebLogic Server Pods**:\n+The WLS samples configure default min and max heap size for WebLogic server java process to 256MB and 512MB respectively. This can be changed using USER_MEM_ARGS environment variable. \n+```\n+    resources:\n+      env:\n+      - name: \"USER_MEM_ARGS\"\n+        value: \"-Xms256m -Xmx512m -Djava.security.egd=file:/dev/./urandom\"\n+```        \n+\n+The default min and max heap size for node-manager process is 64MB and 100MB. This can be changed by using NODEMGR_MEM_ARGS environment variable. \n+\n+The default pod memory request in WLS samples is 768MB and default CPU request is 250m. The requests values can be changed in resources section.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDg3MjczOnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNTo0MTo1OFrOGrCknQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNTo0MTo1OFrOGrCknQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc4NDA5Mw==", "bodyText": "by default in samples and default -> by default in the samples and the default\nWebLogic server pod -> WebLogic Server pods (always capitalize WebLogic Server, both words; please fix globally)\ndetermine optimal -> determine the optimal", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447784093", "createdAt": "2020-06-30T15:41:58Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. \n+\n+A pod.s resource requests and limit parameters can be used to solve these problems. Setting resource limits prevents an application from using more than it.s share of resource. Thus, limiting resources improves reliability and stability of applications.  It also allows users to plan for the hardware capacity. Additionally, pod.s priority and the Quality of Service (QoS) that pod receives is affected by whether resource requests and limits are specified or not.\n+\n+## Pod Quality Of Service (QoS) and Prioritization\n+Pod.s Quality of Service (QoS) and priority is determined based on whether pod.s resource requests and limits are configured or not and how they.re configured.\n+\n+**Best Effort QoS**: If you don.t configure requests and limits, pod receives .best-effort. QoS and pod has the **lowest priority**. In cases where node runs out of non-shareable resources, kubelet.s out-of-resource eviction policy evicts/kills the pods with best-effort QoS first.\n+\n+**Burstable QoS**: If you configure both resource requests and limits, and set the requests to be less than the limit, pod.s QoS will be .Burstable.. Similarly when you only configure the resource requests (without limits), the pod QoS is .Burstable.. When the node runs out of non-shareable resources, kubelet will kill .Burstable. Pods only when there are no more .best-effort. pods running. The Burstable pod receives **medium priority**.\n+\n+**Guaranteed QoS**:  If you set the requests and the limits to equal values, pod will have .Guranteed. QoS and pod will be considered as of the top most priority. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, Kubernetes will kill the best-effort and the burstable Pods first before terminating these Guaranteed QoS Pods. These are the **highest priority** pods.\n+\n+## Java heap size and pod memory request/limit considerations\n+It.s extremely important to set correct heap size for JVM-based applications.  If available memory on node or memory allocated to container is not sufficient for specified JVM heap arguments (and additional off-heap memory), it is possible for WL process to run out of memory. In order to avoid this, you will need to make sure that configured heap sizes are not too big and that the pod is scheduled on the node with sufficient memory.\n+With the latest Java version, it.s possible to rely on the default JVM heap settings which are safe but quite conservative. If you configure the memory limit for a container but don.t configure heap sizes (-Xms and -Xmx), JVM will configure max heap size to 25% (1/4th) of container memory limit by default. The minimum heap size is configured to 1.56% (1/64th) of limit value.\n+\n+**Default heap sizes and resource request values for sample WebLogic Server Pods**:\n+The WLS samples configure default min and max heap size for WebLogic server java process to 256MB and 512MB respectively. This can be changed using USER_MEM_ARGS environment variable. \n+```\n+    resources:\n+      env:\n+      - name: \"USER_MEM_ARGS\"\n+        value: \"-Xms256m -Xmx512m -Djava.security.egd=file:/dev/./urandom\"\n+```        \n+\n+The default min and max heap size for node-manager process is 64MB and 100MB. This can be changed by using NODEMGR_MEM_ARGS environment variable. \n+\n+The default pod memory request in WLS samples is 768MB and default CPU request is 250m. The requests values can be changed in resources section.\n+```\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+```\n+\n+There.s no memory or CPU limit configured by default in samples and default QoS for WebLogic server pod is Burstable. If your use-case and workload requires higher QoS and priority, this can be achieved by setting memory and CPU limits. You.ll need to run tests and experiment with different memory/CPU limits to determine optimal limit values.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 44}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDg5MDQ1OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNTo0NjowMlrOGrCvmw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNTo0NjowMlrOGrCvmw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc4NjkwNw==", "bodyText": "specify pod memory limit, it's  -> specify a pod memory limit, then it's\nrecommended to configure heap size -> recommended that you configure the heap size\nnote . they -> note that they\n\"Thanks to it changing container memory settings will not break anything.\" I cannot figure out what it is that you want to say in this sentence, so I can't figure out how to help you clarify it, but whatever it is, it is not clear.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447786907", "createdAt": "2020-06-30T15:46:02Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. \n+\n+A pod.s resource requests and limit parameters can be used to solve these problems. Setting resource limits prevents an application from using more than it.s share of resource. Thus, limiting resources improves reliability and stability of applications.  It also allows users to plan for the hardware capacity. Additionally, pod.s priority and the Quality of Service (QoS) that pod receives is affected by whether resource requests and limits are specified or not.\n+\n+## Pod Quality Of Service (QoS) and Prioritization\n+Pod.s Quality of Service (QoS) and priority is determined based on whether pod.s resource requests and limits are configured or not and how they.re configured.\n+\n+**Best Effort QoS**: If you don.t configure requests and limits, pod receives .best-effort. QoS and pod has the **lowest priority**. In cases where node runs out of non-shareable resources, kubelet.s out-of-resource eviction policy evicts/kills the pods with best-effort QoS first.\n+\n+**Burstable QoS**: If you configure both resource requests and limits, and set the requests to be less than the limit, pod.s QoS will be .Burstable.. Similarly when you only configure the resource requests (without limits), the pod QoS is .Burstable.. When the node runs out of non-shareable resources, kubelet will kill .Burstable. Pods only when there are no more .best-effort. pods running. The Burstable pod receives **medium priority**.\n+\n+**Guaranteed QoS**:  If you set the requests and the limits to equal values, pod will have .Guranteed. QoS and pod will be considered as of the top most priority. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, Kubernetes will kill the best-effort and the burstable Pods first before terminating these Guaranteed QoS Pods. These are the **highest priority** pods.\n+\n+## Java heap size and pod memory request/limit considerations\n+It.s extremely important to set correct heap size for JVM-based applications.  If available memory on node or memory allocated to container is not sufficient for specified JVM heap arguments (and additional off-heap memory), it is possible for WL process to run out of memory. In order to avoid this, you will need to make sure that configured heap sizes are not too big and that the pod is scheduled on the node with sufficient memory.\n+With the latest Java version, it.s possible to rely on the default JVM heap settings which are safe but quite conservative. If you configure the memory limit for a container but don.t configure heap sizes (-Xms and -Xmx), JVM will configure max heap size to 25% (1/4th) of container memory limit by default. The minimum heap size is configured to 1.56% (1/64th) of limit value.\n+\n+**Default heap sizes and resource request values for sample WebLogic Server Pods**:\n+The WLS samples configure default min and max heap size for WebLogic server java process to 256MB and 512MB respectively. This can be changed using USER_MEM_ARGS environment variable. \n+```\n+    resources:\n+      env:\n+      - name: \"USER_MEM_ARGS\"\n+        value: \"-Xms256m -Xmx512m -Djava.security.egd=file:/dev/./urandom\"\n+```        \n+\n+The default min and max heap size for node-manager process is 64MB and 100MB. This can be changed by using NODEMGR_MEM_ARGS environment variable. \n+\n+The default pod memory request in WLS samples is 768MB and default CPU request is 250m. The requests values can be changed in resources section.\n+```\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+```\n+\n+There.s no memory or CPU limit configured by default in samples and default QoS for WebLogic server pod is Burstable. If your use-case and workload requires higher QoS and priority, this can be achieved by setting memory and CPU limits. You.ll need to run tests and experiment with different memory/CPU limits to determine optimal limit values.\n+```\n+      limits:\n+        cpu: 2\n+        memory: \"2048Mi\"\n+```\n+\n+### Configure min/max heap size in percentages using \"-XX:MinRAMPercentage\" and \"-XX:MaxRAMPercentage\"\n+If you specify pod memory limit, it's recommended to configure heap size as a percentage of the total RAM (memory) specified in the pod memory limit. These parameters allow you to fine-tune the heap size. Please note . they set the percentage, not the fixed values. Thanks to it changing container memory settings will not break anything. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDkyODkzOnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNTo1Mzo0M1rOGrDHZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNTo1Mzo0M1rOGrDHZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc5Mjk5OA==", "bodyText": "the limit is sufficiently big -> the limit is big enough\nbut it's not too big to waste memory resource. -> but not too big, which would waste memory resource.\nSince pod memory -> Because pod memory\nif JVM's memory -> if the JVM's memory\nsum of heap -> sum of the heap\nlimit, JVM process -> limit, the JVM process\nto out-of-memory error -> to an out-of-memory error\nand WebLogic container -> and the WebLogic container\ndue to liveness probe -> due to a liveness probe\nrunning in same container -> running in the same container\nnode manager -> Node Manager\nusing .NODEMGR_JAVA_OPTIONS.  -> using the NODEMGR_JAVA_OPTIONS", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447792998", "createdAt": "2020-06-30T15:53:43Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. \n+\n+A pod.s resource requests and limit parameters can be used to solve these problems. Setting resource limits prevents an application from using more than it.s share of resource. Thus, limiting resources improves reliability and stability of applications.  It also allows users to plan for the hardware capacity. Additionally, pod.s priority and the Quality of Service (QoS) that pod receives is affected by whether resource requests and limits are specified or not.\n+\n+## Pod Quality Of Service (QoS) and Prioritization\n+Pod.s Quality of Service (QoS) and priority is determined based on whether pod.s resource requests and limits are configured or not and how they.re configured.\n+\n+**Best Effort QoS**: If you don.t configure requests and limits, pod receives .best-effort. QoS and pod has the **lowest priority**. In cases where node runs out of non-shareable resources, kubelet.s out-of-resource eviction policy evicts/kills the pods with best-effort QoS first.\n+\n+**Burstable QoS**: If you configure both resource requests and limits, and set the requests to be less than the limit, pod.s QoS will be .Burstable.. Similarly when you only configure the resource requests (without limits), the pod QoS is .Burstable.. When the node runs out of non-shareable resources, kubelet will kill .Burstable. Pods only when there are no more .best-effort. pods running. The Burstable pod receives **medium priority**.\n+\n+**Guaranteed QoS**:  If you set the requests and the limits to equal values, pod will have .Guranteed. QoS and pod will be considered as of the top most priority. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, Kubernetes will kill the best-effort and the burstable Pods first before terminating these Guaranteed QoS Pods. These are the **highest priority** pods.\n+\n+## Java heap size and pod memory request/limit considerations\n+It.s extremely important to set correct heap size for JVM-based applications.  If available memory on node or memory allocated to container is not sufficient for specified JVM heap arguments (and additional off-heap memory), it is possible for WL process to run out of memory. In order to avoid this, you will need to make sure that configured heap sizes are not too big and that the pod is scheduled on the node with sufficient memory.\n+With the latest Java version, it.s possible to rely on the default JVM heap settings which are safe but quite conservative. If you configure the memory limit for a container but don.t configure heap sizes (-Xms and -Xmx), JVM will configure max heap size to 25% (1/4th) of container memory limit by default. The minimum heap size is configured to 1.56% (1/64th) of limit value.\n+\n+**Default heap sizes and resource request values for sample WebLogic Server Pods**:\n+The WLS samples configure default min and max heap size for WebLogic server java process to 256MB and 512MB respectively. This can be changed using USER_MEM_ARGS environment variable. \n+```\n+    resources:\n+      env:\n+      - name: \"USER_MEM_ARGS\"\n+        value: \"-Xms256m -Xmx512m -Djava.security.egd=file:/dev/./urandom\"\n+```        \n+\n+The default min and max heap size for node-manager process is 64MB and 100MB. This can be changed by using NODEMGR_MEM_ARGS environment variable. \n+\n+The default pod memory request in WLS samples is 768MB and default CPU request is 250m. The requests values can be changed in resources section.\n+```\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+```\n+\n+There.s no memory or CPU limit configured by default in samples and default QoS for WebLogic server pod is Burstable. If your use-case and workload requires higher QoS and priority, this can be achieved by setting memory and CPU limits. You.ll need to run tests and experiment with different memory/CPU limits to determine optimal limit values.\n+```\n+      limits:\n+        cpu: 2\n+        memory: \"2048Mi\"\n+```\n+\n+### Configure min/max heap size in percentages using \"-XX:MinRAMPercentage\" and \"-XX:MaxRAMPercentage\"\n+If you specify pod memory limit, it's recommended to configure heap size as a percentage of the total RAM (memory) specified in the pod memory limit. These parameters allow you to fine-tune the heap size. Please note . they set the percentage, not the fixed values. Thanks to it changing container memory settings will not break anything. \n+```\n+    resources:\n+      env:\n+      - name: JAVA_OPTIONS\n+        value: \"--XX:MinRAMPercentage=25.0 --XX:MaxRAMPercentage=50.0 -Dweblogic.StdoutDebugEnabled=false\"\n+```\n+When configuring memory limits, it.s important to make sure that the limit is sufficiently big to accommodate the configured heap (and off-heap) requirements, but it's not too big to waste memory resource. Since pod memory will never go above the limit, if JVM's memory usage (sum of heap and native memory) goes above the limit, JVM process will be killed due to out-of-memory error and WebLogic container will be restarted due to liveness probe failure.   Additionally there's also a node-manager process that.s running in same container and it has it's own heap and off-heap requirements. You can also fine tune the node manager heap size in percentages by setting \"-XX:MinRAMPercentage\" and \"-XX:MaxRAMPercentage\" using .NODEMGR_JAVA_OPTIONS. environment variable. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDk0MDY2OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNTo1NjoxNVrOGrDOqg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNTo1NjoxNVrOGrDOqg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc5NDg1OA==", "bodyText": "can use traditional approach -> can use the traditional approach", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447794858", "createdAt": "2020-06-30T15:56:15Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. \n+\n+A pod.s resource requests and limit parameters can be used to solve these problems. Setting resource limits prevents an application from using more than it.s share of resource. Thus, limiting resources improves reliability and stability of applications.  It also allows users to plan for the hardware capacity. Additionally, pod.s priority and the Quality of Service (QoS) that pod receives is affected by whether resource requests and limits are specified or not.\n+\n+## Pod Quality Of Service (QoS) and Prioritization\n+Pod.s Quality of Service (QoS) and priority is determined based on whether pod.s resource requests and limits are configured or not and how they.re configured.\n+\n+**Best Effort QoS**: If you don.t configure requests and limits, pod receives .best-effort. QoS and pod has the **lowest priority**. In cases where node runs out of non-shareable resources, kubelet.s out-of-resource eviction policy evicts/kills the pods with best-effort QoS first.\n+\n+**Burstable QoS**: If you configure both resource requests and limits, and set the requests to be less than the limit, pod.s QoS will be .Burstable.. Similarly when you only configure the resource requests (without limits), the pod QoS is .Burstable.. When the node runs out of non-shareable resources, kubelet will kill .Burstable. Pods only when there are no more .best-effort. pods running. The Burstable pod receives **medium priority**.\n+\n+**Guaranteed QoS**:  If you set the requests and the limits to equal values, pod will have .Guranteed. QoS and pod will be considered as of the top most priority. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, Kubernetes will kill the best-effort and the burstable Pods first before terminating these Guaranteed QoS Pods. These are the **highest priority** pods.\n+\n+## Java heap size and pod memory request/limit considerations\n+It.s extremely important to set correct heap size for JVM-based applications.  If available memory on node or memory allocated to container is not sufficient for specified JVM heap arguments (and additional off-heap memory), it is possible for WL process to run out of memory. In order to avoid this, you will need to make sure that configured heap sizes are not too big and that the pod is scheduled on the node with sufficient memory.\n+With the latest Java version, it.s possible to rely on the default JVM heap settings which are safe but quite conservative. If you configure the memory limit for a container but don.t configure heap sizes (-Xms and -Xmx), JVM will configure max heap size to 25% (1/4th) of container memory limit by default. The minimum heap size is configured to 1.56% (1/64th) of limit value.\n+\n+**Default heap sizes and resource request values for sample WebLogic Server Pods**:\n+The WLS samples configure default min and max heap size for WebLogic server java process to 256MB and 512MB respectively. This can be changed using USER_MEM_ARGS environment variable. \n+```\n+    resources:\n+      env:\n+      - name: \"USER_MEM_ARGS\"\n+        value: \"-Xms256m -Xmx512m -Djava.security.egd=file:/dev/./urandom\"\n+```        \n+\n+The default min and max heap size for node-manager process is 64MB and 100MB. This can be changed by using NODEMGR_MEM_ARGS environment variable. \n+\n+The default pod memory request in WLS samples is 768MB and default CPU request is 250m. The requests values can be changed in resources section.\n+```\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+```\n+\n+There.s no memory or CPU limit configured by default in samples and default QoS for WebLogic server pod is Burstable. If your use-case and workload requires higher QoS and priority, this can be achieved by setting memory and CPU limits. You.ll need to run tests and experiment with different memory/CPU limits to determine optimal limit values.\n+```\n+      limits:\n+        cpu: 2\n+        memory: \"2048Mi\"\n+```\n+\n+### Configure min/max heap size in percentages using \"-XX:MinRAMPercentage\" and \"-XX:MaxRAMPercentage\"\n+If you specify pod memory limit, it's recommended to configure heap size as a percentage of the total RAM (memory) specified in the pod memory limit. These parameters allow you to fine-tune the heap size. Please note . they set the percentage, not the fixed values. Thanks to it changing container memory settings will not break anything. \n+```\n+    resources:\n+      env:\n+      - name: JAVA_OPTIONS\n+        value: \"--XX:MinRAMPercentage=25.0 --XX:MaxRAMPercentage=50.0 -Dweblogic.StdoutDebugEnabled=false\"\n+```\n+When configuring memory limits, it.s important to make sure that the limit is sufficiently big to accommodate the configured heap (and off-heap) requirements, but it's not too big to waste memory resource. Since pod memory will never go above the limit, if JVM's memory usage (sum of heap and native memory) goes above the limit, JVM process will be killed due to out-of-memory error and WebLogic container will be restarted due to liveness probe failure.   Additionally there's also a node-manager process that.s running in same container and it has it's own heap and off-heap requirements. You can also fine tune the node manager heap size in percentages by setting \"-XX:MinRAMPercentage\" and \"-XX:MaxRAMPercentage\" using .NODEMGR_JAVA_OPTIONS. environment variable. \n+\n+### Using \"-Xms\" and \"-Xmx\" parameters when not configuring limits \n+In some cases, it.s difficult to come up with a hard limit for the container and you might only want to configure memory requests but not configure memory limits. In such scenarios, you can use traditional approach to set min/max heap size using .-Xms. and .-Xmx..", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 62}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDk2NjM4OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNjowMjowN1rOGrDesg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNjowMjowN1rOGrDesg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc5ODk2Mg==", "bodyText": "applications performance -> application performance\nYou also don't want to set CPU requests -> Also you don't want to set the CPU requests\napplication don't need -> application doesn't need\nSince CPU -> Because CPU\nall CPU resources -> all the CPU resources\non node -> on the node", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447798962", "createdAt": "2020-06-30T16:02:07Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. \n+\n+A pod.s resource requests and limit parameters can be used to solve these problems. Setting resource limits prevents an application from using more than it.s share of resource. Thus, limiting resources improves reliability and stability of applications.  It also allows users to plan for the hardware capacity. Additionally, pod.s priority and the Quality of Service (QoS) that pod receives is affected by whether resource requests and limits are specified or not.\n+\n+## Pod Quality Of Service (QoS) and Prioritization\n+Pod.s Quality of Service (QoS) and priority is determined based on whether pod.s resource requests and limits are configured or not and how they.re configured.\n+\n+**Best Effort QoS**: If you don.t configure requests and limits, pod receives .best-effort. QoS and pod has the **lowest priority**. In cases where node runs out of non-shareable resources, kubelet.s out-of-resource eviction policy evicts/kills the pods with best-effort QoS first.\n+\n+**Burstable QoS**: If you configure both resource requests and limits, and set the requests to be less than the limit, pod.s QoS will be .Burstable.. Similarly when you only configure the resource requests (without limits), the pod QoS is .Burstable.. When the node runs out of non-shareable resources, kubelet will kill .Burstable. Pods only when there are no more .best-effort. pods running. The Burstable pod receives **medium priority**.\n+\n+**Guaranteed QoS**:  If you set the requests and the limits to equal values, pod will have .Guranteed. QoS and pod will be considered as of the top most priority. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, Kubernetes will kill the best-effort and the burstable Pods first before terminating these Guaranteed QoS Pods. These are the **highest priority** pods.\n+\n+## Java heap size and pod memory request/limit considerations\n+It.s extremely important to set correct heap size for JVM-based applications.  If available memory on node or memory allocated to container is not sufficient for specified JVM heap arguments (and additional off-heap memory), it is possible for WL process to run out of memory. In order to avoid this, you will need to make sure that configured heap sizes are not too big and that the pod is scheduled on the node with sufficient memory.\n+With the latest Java version, it.s possible to rely on the default JVM heap settings which are safe but quite conservative. If you configure the memory limit for a container but don.t configure heap sizes (-Xms and -Xmx), JVM will configure max heap size to 25% (1/4th) of container memory limit by default. The minimum heap size is configured to 1.56% (1/64th) of limit value.\n+\n+**Default heap sizes and resource request values for sample WebLogic Server Pods**:\n+The WLS samples configure default min and max heap size for WebLogic server java process to 256MB and 512MB respectively. This can be changed using USER_MEM_ARGS environment variable. \n+```\n+    resources:\n+      env:\n+      - name: \"USER_MEM_ARGS\"\n+        value: \"-Xms256m -Xmx512m -Djava.security.egd=file:/dev/./urandom\"\n+```        \n+\n+The default min and max heap size for node-manager process is 64MB and 100MB. This can be changed by using NODEMGR_MEM_ARGS environment variable. \n+\n+The default pod memory request in WLS samples is 768MB and default CPU request is 250m. The requests values can be changed in resources section.\n+```\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+```\n+\n+There.s no memory or CPU limit configured by default in samples and default QoS for WebLogic server pod is Burstable. If your use-case and workload requires higher QoS and priority, this can be achieved by setting memory and CPU limits. You.ll need to run tests and experiment with different memory/CPU limits to determine optimal limit values.\n+```\n+      limits:\n+        cpu: 2\n+        memory: \"2048Mi\"\n+```\n+\n+### Configure min/max heap size in percentages using \"-XX:MinRAMPercentage\" and \"-XX:MaxRAMPercentage\"\n+If you specify pod memory limit, it's recommended to configure heap size as a percentage of the total RAM (memory) specified in the pod memory limit. These parameters allow you to fine-tune the heap size. Please note . they set the percentage, not the fixed values. Thanks to it changing container memory settings will not break anything. \n+```\n+    resources:\n+      env:\n+      - name: JAVA_OPTIONS\n+        value: \"--XX:MinRAMPercentage=25.0 --XX:MaxRAMPercentage=50.0 -Dweblogic.StdoutDebugEnabled=false\"\n+```\n+When configuring memory limits, it.s important to make sure that the limit is sufficiently big to accommodate the configured heap (and off-heap) requirements, but it's not too big to waste memory resource. Since pod memory will never go above the limit, if JVM's memory usage (sum of heap and native memory) goes above the limit, JVM process will be killed due to out-of-memory error and WebLogic container will be restarted due to liveness probe failure.   Additionally there's also a node-manager process that.s running in same container and it has it's own heap and off-heap requirements. You can also fine tune the node manager heap size in percentages by setting \"-XX:MinRAMPercentage\" and \"-XX:MaxRAMPercentage\" using .NODEMGR_JAVA_OPTIONS. environment variable. \n+\n+### Using \"-Xms\" and \"-Xmx\" parameters when not configuring limits \n+In some cases, it.s difficult to come up with a hard limit for the container and you might only want to configure memory requests but not configure memory limits. In such scenarios, you can use traditional approach to set min/max heap size using .-Xms. and .-Xmx..\n+\n+### CPU requests and limits \n+It.s important that the containers running WebLogic applications have enough CPU resources, otherwise applications performance can suffer. You also don't want to set CPU requests and limit too high if your application don't need or use allocated CPU resources. Since CPU is a shared resource, if the amount of CPU that you reserve is more than required by your application, the CPU cycles will go unused and be wasted. If no CPU request and limit is configured, it can end up using all CPU resources available on node. This can starve other containers from using shareable CPU cycles. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDk3MjAwOnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNjowMzozMFrOGrDiTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNjowMzozMFrOGrDiTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Nzc5OTg4Ng==", "bodyText": "if pod CPU limit -> if the pod CPU limit\nspecify container -> specify the container\nto incorrect number -> to an incorrect number", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447799886", "createdAt": "2020-06-30T16:03:30Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. \n+\n+A pod.s resource requests and limit parameters can be used to solve these problems. Setting resource limits prevents an application from using more than it.s share of resource. Thus, limiting resources improves reliability and stability of applications.  It also allows users to plan for the hardware capacity. Additionally, pod.s priority and the Quality of Service (QoS) that pod receives is affected by whether resource requests and limits are specified or not.\n+\n+## Pod Quality Of Service (QoS) and Prioritization\n+Pod.s Quality of Service (QoS) and priority is determined based on whether pod.s resource requests and limits are configured or not and how they.re configured.\n+\n+**Best Effort QoS**: If you don.t configure requests and limits, pod receives .best-effort. QoS and pod has the **lowest priority**. In cases where node runs out of non-shareable resources, kubelet.s out-of-resource eviction policy evicts/kills the pods with best-effort QoS first.\n+\n+**Burstable QoS**: If you configure both resource requests and limits, and set the requests to be less than the limit, pod.s QoS will be .Burstable.. Similarly when you only configure the resource requests (without limits), the pod QoS is .Burstable.. When the node runs out of non-shareable resources, kubelet will kill .Burstable. Pods only when there are no more .best-effort. pods running. The Burstable pod receives **medium priority**.\n+\n+**Guaranteed QoS**:  If you set the requests and the limits to equal values, pod will have .Guranteed. QoS and pod will be considered as of the top most priority. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, Kubernetes will kill the best-effort and the burstable Pods first before terminating these Guaranteed QoS Pods. These are the **highest priority** pods.\n+\n+## Java heap size and pod memory request/limit considerations\n+It.s extremely important to set correct heap size for JVM-based applications.  If available memory on node or memory allocated to container is not sufficient for specified JVM heap arguments (and additional off-heap memory), it is possible for WL process to run out of memory. In order to avoid this, you will need to make sure that configured heap sizes are not too big and that the pod is scheduled on the node with sufficient memory.\n+With the latest Java version, it.s possible to rely on the default JVM heap settings which are safe but quite conservative. If you configure the memory limit for a container but don.t configure heap sizes (-Xms and -Xmx), JVM will configure max heap size to 25% (1/4th) of container memory limit by default. The minimum heap size is configured to 1.56% (1/64th) of limit value.\n+\n+**Default heap sizes and resource request values for sample WebLogic Server Pods**:\n+The WLS samples configure default min and max heap size for WebLogic server java process to 256MB and 512MB respectively. This can be changed using USER_MEM_ARGS environment variable. \n+```\n+    resources:\n+      env:\n+      - name: \"USER_MEM_ARGS\"\n+        value: \"-Xms256m -Xmx512m -Djava.security.egd=file:/dev/./urandom\"\n+```        \n+\n+The default min and max heap size for node-manager process is 64MB and 100MB. This can be changed by using NODEMGR_MEM_ARGS environment variable. \n+\n+The default pod memory request in WLS samples is 768MB and default CPU request is 250m. The requests values can be changed in resources section.\n+```\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+```\n+\n+There.s no memory or CPU limit configured by default in samples and default QoS for WebLogic server pod is Burstable. If your use-case and workload requires higher QoS and priority, this can be achieved by setting memory and CPU limits. You.ll need to run tests and experiment with different memory/CPU limits to determine optimal limit values.\n+```\n+      limits:\n+        cpu: 2\n+        memory: \"2048Mi\"\n+```\n+\n+### Configure min/max heap size in percentages using \"-XX:MinRAMPercentage\" and \"-XX:MaxRAMPercentage\"\n+If you specify pod memory limit, it's recommended to configure heap size as a percentage of the total RAM (memory) specified in the pod memory limit. These parameters allow you to fine-tune the heap size. Please note . they set the percentage, not the fixed values. Thanks to it changing container memory settings will not break anything. \n+```\n+    resources:\n+      env:\n+      - name: JAVA_OPTIONS\n+        value: \"--XX:MinRAMPercentage=25.0 --XX:MaxRAMPercentage=50.0 -Dweblogic.StdoutDebugEnabled=false\"\n+```\n+When configuring memory limits, it.s important to make sure that the limit is sufficiently big to accommodate the configured heap (and off-heap) requirements, but it's not too big to waste memory resource. Since pod memory will never go above the limit, if JVM's memory usage (sum of heap and native memory) goes above the limit, JVM process will be killed due to out-of-memory error and WebLogic container will be restarted due to liveness probe failure.   Additionally there's also a node-manager process that.s running in same container and it has it's own heap and off-heap requirements. You can also fine tune the node manager heap size in percentages by setting \"-XX:MinRAMPercentage\" and \"-XX:MaxRAMPercentage\" using .NODEMGR_JAVA_OPTIONS. environment variable. \n+\n+### Using \"-Xms\" and \"-Xmx\" parameters when not configuring limits \n+In some cases, it.s difficult to come up with a hard limit for the container and you might only want to configure memory requests but not configure memory limits. In such scenarios, you can use traditional approach to set min/max heap size using .-Xms. and .-Xmx..\n+\n+### CPU requests and limits \n+It.s important that the containers running WebLogic applications have enough CPU resources, otherwise applications performance can suffer. You also don't want to set CPU requests and limit too high if your application don't need or use allocated CPU resources. Since CPU is a shared resource, if the amount of CPU that you reserve is more than required by your application, the CPU cycles will go unused and be wasted. If no CPU request and limit is configured, it can end up using all CPU resources available on node. This can starve other containers from using shareable CPU cycles. \n+\n+One other thing to keep in mind is that if pod CPU limit is not configured, it might lead to incorrect garbage collection (GC) strategy selection. WebLogic self-tuning work-manager uses pod CPU limit to configure the  number of threads in a default thread pool. If you don.t specify container CPU limit, the performance might be affected due to incorrect number of GC threads or wrong WebLogic server thread pool size. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDk4Nzg1OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNjowNzozMVrOGrDsZQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNjowNzozMVrOGrDsZQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgwMjQ2OQ==", "bodyText": "than core count -> than the core count\nkubernetes -> Kubernetes (always capitalized; please fix globally)\nare normally designed -> are typically designed\nconsidered as a -> considered a\napps  -> applications (globally)", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447802469", "createdAt": "2020-06-30T16:07:31Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. \n+\n+A pod.s resource requests and limit parameters can be used to solve these problems. Setting resource limits prevents an application from using more than it.s share of resource. Thus, limiting resources improves reliability and stability of applications.  It also allows users to plan for the hardware capacity. Additionally, pod.s priority and the Quality of Service (QoS) that pod receives is affected by whether resource requests and limits are specified or not.\n+\n+## Pod Quality Of Service (QoS) and Prioritization\n+Pod.s Quality of Service (QoS) and priority is determined based on whether pod.s resource requests and limits are configured or not and how they.re configured.\n+\n+**Best Effort QoS**: If you don.t configure requests and limits, pod receives .best-effort. QoS and pod has the **lowest priority**. In cases where node runs out of non-shareable resources, kubelet.s out-of-resource eviction policy evicts/kills the pods with best-effort QoS first.\n+\n+**Burstable QoS**: If you configure both resource requests and limits, and set the requests to be less than the limit, pod.s QoS will be .Burstable.. Similarly when you only configure the resource requests (without limits), the pod QoS is .Burstable.. When the node runs out of non-shareable resources, kubelet will kill .Burstable. Pods only when there are no more .best-effort. pods running. The Burstable pod receives **medium priority**.\n+\n+**Guaranteed QoS**:  If you set the requests and the limits to equal values, pod will have .Guranteed. QoS and pod will be considered as of the top most priority. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, Kubernetes will kill the best-effort and the burstable Pods first before terminating these Guaranteed QoS Pods. These are the **highest priority** pods.\n+\n+## Java heap size and pod memory request/limit considerations\n+It.s extremely important to set correct heap size for JVM-based applications.  If available memory on node or memory allocated to container is not sufficient for specified JVM heap arguments (and additional off-heap memory), it is possible for WL process to run out of memory. In order to avoid this, you will need to make sure that configured heap sizes are not too big and that the pod is scheduled on the node with sufficient memory.\n+With the latest Java version, it.s possible to rely on the default JVM heap settings which are safe but quite conservative. If you configure the memory limit for a container but don.t configure heap sizes (-Xms and -Xmx), JVM will configure max heap size to 25% (1/4th) of container memory limit by default. The minimum heap size is configured to 1.56% (1/64th) of limit value.\n+\n+**Default heap sizes and resource request values for sample WebLogic Server Pods**:\n+The WLS samples configure default min and max heap size for WebLogic server java process to 256MB and 512MB respectively. This can be changed using USER_MEM_ARGS environment variable. \n+```\n+    resources:\n+      env:\n+      - name: \"USER_MEM_ARGS\"\n+        value: \"-Xms256m -Xmx512m -Djava.security.egd=file:/dev/./urandom\"\n+```        \n+\n+The default min and max heap size for node-manager process is 64MB and 100MB. This can be changed by using NODEMGR_MEM_ARGS environment variable. \n+\n+The default pod memory request in WLS samples is 768MB and default CPU request is 250m. The requests values can be changed in resources section.\n+```\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+```\n+\n+There.s no memory or CPU limit configured by default in samples and default QoS for WebLogic server pod is Burstable. If your use-case and workload requires higher QoS and priority, this can be achieved by setting memory and CPU limits. You.ll need to run tests and experiment with different memory/CPU limits to determine optimal limit values.\n+```\n+      limits:\n+        cpu: 2\n+        memory: \"2048Mi\"\n+```\n+\n+### Configure min/max heap size in percentages using \"-XX:MinRAMPercentage\" and \"-XX:MaxRAMPercentage\"\n+If you specify pod memory limit, it's recommended to configure heap size as a percentage of the total RAM (memory) specified in the pod memory limit. These parameters allow you to fine-tune the heap size. Please note . they set the percentage, not the fixed values. Thanks to it changing container memory settings will not break anything. \n+```\n+    resources:\n+      env:\n+      - name: JAVA_OPTIONS\n+        value: \"--XX:MinRAMPercentage=25.0 --XX:MaxRAMPercentage=50.0 -Dweblogic.StdoutDebugEnabled=false\"\n+```\n+When configuring memory limits, it.s important to make sure that the limit is sufficiently big to accommodate the configured heap (and off-heap) requirements, but it's not too big to waste memory resource. Since pod memory will never go above the limit, if JVM's memory usage (sum of heap and native memory) goes above the limit, JVM process will be killed due to out-of-memory error and WebLogic container will be restarted due to liveness probe failure.   Additionally there's also a node-manager process that.s running in same container and it has it's own heap and off-heap requirements. You can also fine tune the node manager heap size in percentages by setting \"-XX:MinRAMPercentage\" and \"-XX:MaxRAMPercentage\" using .NODEMGR_JAVA_OPTIONS. environment variable. \n+\n+### Using \"-Xms\" and \"-Xmx\" parameters when not configuring limits \n+In some cases, it.s difficult to come up with a hard limit for the container and you might only want to configure memory requests but not configure memory limits. In such scenarios, you can use traditional approach to set min/max heap size using .-Xms. and .-Xmx..\n+\n+### CPU requests and limits \n+It.s important that the containers running WebLogic applications have enough CPU resources, otherwise applications performance can suffer. You also don't want to set CPU requests and limit too high if your application don't need or use allocated CPU resources. Since CPU is a shared resource, if the amount of CPU that you reserve is more than required by your application, the CPU cycles will go unused and be wasted. If no CPU request and limit is configured, it can end up using all CPU resources available on node. This can starve other containers from using shareable CPU cycles. \n+\n+One other thing to keep in mind is that if pod CPU limit is not configured, it might lead to incorrect garbage collection (GC) strategy selection. WebLogic self-tuning work-manager uses pod CPU limit to configure the  number of threads in a default thread pool. If you don.t specify container CPU limit, the performance might be affected due to incorrect number of GC threads or wrong WebLogic server thread pool size. \n+\n+## Beware of setting resource limits too high\n+It.s important to keep in mind that if you set a value of CPU core count that.s larger than core count of the biggest node, then the pod will never be scheduled. Let.s say you have a pod that needs 4 cores but you have a kubernetes cluster that.s comprised of 2 core VMs. In this case, your pod will never be scheduled.  WebLogic applications are normally designed to take advantage of multiple cores and should be given CPU requests as such. CPUs are considered as a compressible resource. If your apps are hitting CPU limits, kubernetes will start to throttle your container. This means your CPU will be artificially restricted, giving your app potentially worse performance. However it won.t be terminated or evicted. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 70}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDk4OTkzOnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNjowNzo1OFrOGrDtnQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNjowNzo1OFrOGrDtnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgwMjc4MQ==", "bodyText": "than amount -> than he amount", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447802781", "createdAt": "2020-06-30T16:07:58Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. \n+\n+A pod.s resource requests and limit parameters can be used to solve these problems. Setting resource limits prevents an application from using more than it.s share of resource. Thus, limiting resources improves reliability and stability of applications.  It also allows users to plan for the hardware capacity. Additionally, pod.s priority and the Quality of Service (QoS) that pod receives is affected by whether resource requests and limits are specified or not.\n+\n+## Pod Quality Of Service (QoS) and Prioritization\n+Pod.s Quality of Service (QoS) and priority is determined based on whether pod.s resource requests and limits are configured or not and how they.re configured.\n+\n+**Best Effort QoS**: If you don.t configure requests and limits, pod receives .best-effort. QoS and pod has the **lowest priority**. In cases where node runs out of non-shareable resources, kubelet.s out-of-resource eviction policy evicts/kills the pods with best-effort QoS first.\n+\n+**Burstable QoS**: If you configure both resource requests and limits, and set the requests to be less than the limit, pod.s QoS will be .Burstable.. Similarly when you only configure the resource requests (without limits), the pod QoS is .Burstable.. When the node runs out of non-shareable resources, kubelet will kill .Burstable. Pods only when there are no more .best-effort. pods running. The Burstable pod receives **medium priority**.\n+\n+**Guaranteed QoS**:  If you set the requests and the limits to equal values, pod will have .Guranteed. QoS and pod will be considered as of the top most priority. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, Kubernetes will kill the best-effort and the burstable Pods first before terminating these Guaranteed QoS Pods. These are the **highest priority** pods.\n+\n+## Java heap size and pod memory request/limit considerations\n+It.s extremely important to set correct heap size for JVM-based applications.  If available memory on node or memory allocated to container is not sufficient for specified JVM heap arguments (and additional off-heap memory), it is possible for WL process to run out of memory. In order to avoid this, you will need to make sure that configured heap sizes are not too big and that the pod is scheduled on the node with sufficient memory.\n+With the latest Java version, it.s possible to rely on the default JVM heap settings which are safe but quite conservative. If you configure the memory limit for a container but don.t configure heap sizes (-Xms and -Xmx), JVM will configure max heap size to 25% (1/4th) of container memory limit by default. The minimum heap size is configured to 1.56% (1/64th) of limit value.\n+\n+**Default heap sizes and resource request values for sample WebLogic Server Pods**:\n+The WLS samples configure default min and max heap size for WebLogic server java process to 256MB and 512MB respectively. This can be changed using USER_MEM_ARGS environment variable. \n+```\n+    resources:\n+      env:\n+      - name: \"USER_MEM_ARGS\"\n+        value: \"-Xms256m -Xmx512m -Djava.security.egd=file:/dev/./urandom\"\n+```        \n+\n+The default min and max heap size for node-manager process is 64MB and 100MB. This can be changed by using NODEMGR_MEM_ARGS environment variable. \n+\n+The default pod memory request in WLS samples is 768MB and default CPU request is 250m. The requests values can be changed in resources section.\n+```\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+```\n+\n+There.s no memory or CPU limit configured by default in samples and default QoS for WebLogic server pod is Burstable. If your use-case and workload requires higher QoS and priority, this can be achieved by setting memory and CPU limits. You.ll need to run tests and experiment with different memory/CPU limits to determine optimal limit values.\n+```\n+      limits:\n+        cpu: 2\n+        memory: \"2048Mi\"\n+```\n+\n+### Configure min/max heap size in percentages using \"-XX:MinRAMPercentage\" and \"-XX:MaxRAMPercentage\"\n+If you specify pod memory limit, it's recommended to configure heap size as a percentage of the total RAM (memory) specified in the pod memory limit. These parameters allow you to fine-tune the heap size. Please note . they set the percentage, not the fixed values. Thanks to it changing container memory settings will not break anything. \n+```\n+    resources:\n+      env:\n+      - name: JAVA_OPTIONS\n+        value: \"--XX:MinRAMPercentage=25.0 --XX:MaxRAMPercentage=50.0 -Dweblogic.StdoutDebugEnabled=false\"\n+```\n+When configuring memory limits, it.s important to make sure that the limit is sufficiently big to accommodate the configured heap (and off-heap) requirements, but it's not too big to waste memory resource. Since pod memory will never go above the limit, if JVM's memory usage (sum of heap and native memory) goes above the limit, JVM process will be killed due to out-of-memory error and WebLogic container will be restarted due to liveness probe failure.   Additionally there's also a node-manager process that.s running in same container and it has it's own heap and off-heap requirements. You can also fine tune the node manager heap size in percentages by setting \"-XX:MinRAMPercentage\" and \"-XX:MaxRAMPercentage\" using .NODEMGR_JAVA_OPTIONS. environment variable. \n+\n+### Using \"-Xms\" and \"-Xmx\" parameters when not configuring limits \n+In some cases, it.s difficult to come up with a hard limit for the container and you might only want to configure memory requests but not configure memory limits. In such scenarios, you can use traditional approach to set min/max heap size using .-Xms. and .-Xmx..\n+\n+### CPU requests and limits \n+It.s important that the containers running WebLogic applications have enough CPU resources, otherwise applications performance can suffer. You also don't want to set CPU requests and limit too high if your application don't need or use allocated CPU resources. Since CPU is a shared resource, if the amount of CPU that you reserve is more than required by your application, the CPU cycles will go unused and be wasted. If no CPU request and limit is configured, it can end up using all CPU resources available on node. This can starve other containers from using shareable CPU cycles. \n+\n+One other thing to keep in mind is that if pod CPU limit is not configured, it might lead to incorrect garbage collection (GC) strategy selection. WebLogic self-tuning work-manager uses pod CPU limit to configure the  number of threads in a default thread pool. If you don.t specify container CPU limit, the performance might be affected due to incorrect number of GC threads or wrong WebLogic server thread pool size. \n+\n+## Beware of setting resource limits too high\n+It.s important to keep in mind that if you set a value of CPU core count that.s larger than core count of the biggest node, then the pod will never be scheduled. Let.s say you have a pod that needs 4 cores but you have a kubernetes cluster that.s comprised of 2 core VMs. In this case, your pod will never be scheduled.  WebLogic applications are normally designed to take advantage of multiple cores and should be given CPU requests as such. CPUs are considered as a compressible resource. If your apps are hitting CPU limits, kubernetes will start to throttle your container. This means your CPU will be artificially restricted, giving your app potentially worse performance. However it won.t be terminated or evicted. \n+Just like CPU, if you put a memory request that.s larger than amount of memory on your nodes, the pod will never be scheduled.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDk5MzM4OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNjowODo0OVrOGrDv2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNjowODo0OVrOGrDv2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgwMzM1NQ==", "bodyText": "Affinity  -> affinity\nk8s -> Kubernetes (globally)", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447803355", "createdAt": "2020-06-30T16:08:49Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. \n+\n+A pod.s resource requests and limit parameters can be used to solve these problems. Setting resource limits prevents an application from using more than it.s share of resource. Thus, limiting resources improves reliability and stability of applications.  It also allows users to plan for the hardware capacity. Additionally, pod.s priority and the Quality of Service (QoS) that pod receives is affected by whether resource requests and limits are specified or not.\n+\n+## Pod Quality Of Service (QoS) and Prioritization\n+Pod.s Quality of Service (QoS) and priority is determined based on whether pod.s resource requests and limits are configured or not and how they.re configured.\n+\n+**Best Effort QoS**: If you don.t configure requests and limits, pod receives .best-effort. QoS and pod has the **lowest priority**. In cases where node runs out of non-shareable resources, kubelet.s out-of-resource eviction policy evicts/kills the pods with best-effort QoS first.\n+\n+**Burstable QoS**: If you configure both resource requests and limits, and set the requests to be less than the limit, pod.s QoS will be .Burstable.. Similarly when you only configure the resource requests (without limits), the pod QoS is .Burstable.. When the node runs out of non-shareable resources, kubelet will kill .Burstable. Pods only when there are no more .best-effort. pods running. The Burstable pod receives **medium priority**.\n+\n+**Guaranteed QoS**:  If you set the requests and the limits to equal values, pod will have .Guranteed. QoS and pod will be considered as of the top most priority. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, Kubernetes will kill the best-effort and the burstable Pods first before terminating these Guaranteed QoS Pods. These are the **highest priority** pods.\n+\n+## Java heap size and pod memory request/limit considerations\n+It.s extremely important to set correct heap size for JVM-based applications.  If available memory on node or memory allocated to container is not sufficient for specified JVM heap arguments (and additional off-heap memory), it is possible for WL process to run out of memory. In order to avoid this, you will need to make sure that configured heap sizes are not too big and that the pod is scheduled on the node with sufficient memory.\n+With the latest Java version, it.s possible to rely on the default JVM heap settings which are safe but quite conservative. If you configure the memory limit for a container but don.t configure heap sizes (-Xms and -Xmx), JVM will configure max heap size to 25% (1/4th) of container memory limit by default. The minimum heap size is configured to 1.56% (1/64th) of limit value.\n+\n+**Default heap sizes and resource request values for sample WebLogic Server Pods**:\n+The WLS samples configure default min and max heap size for WebLogic server java process to 256MB and 512MB respectively. This can be changed using USER_MEM_ARGS environment variable. \n+```\n+    resources:\n+      env:\n+      - name: \"USER_MEM_ARGS\"\n+        value: \"-Xms256m -Xmx512m -Djava.security.egd=file:/dev/./urandom\"\n+```        \n+\n+The default min and max heap size for node-manager process is 64MB and 100MB. This can be changed by using NODEMGR_MEM_ARGS environment variable. \n+\n+The default pod memory request in WLS samples is 768MB and default CPU request is 250m. The requests values can be changed in resources section.\n+```\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+```\n+\n+There.s no memory or CPU limit configured by default in samples and default QoS for WebLogic server pod is Burstable. If your use-case and workload requires higher QoS and priority, this can be achieved by setting memory and CPU limits. You.ll need to run tests and experiment with different memory/CPU limits to determine optimal limit values.\n+```\n+      limits:\n+        cpu: 2\n+        memory: \"2048Mi\"\n+```\n+\n+### Configure min/max heap size in percentages using \"-XX:MinRAMPercentage\" and \"-XX:MaxRAMPercentage\"\n+If you specify pod memory limit, it's recommended to configure heap size as a percentage of the total RAM (memory) specified in the pod memory limit. These parameters allow you to fine-tune the heap size. Please note . they set the percentage, not the fixed values. Thanks to it changing container memory settings will not break anything. \n+```\n+    resources:\n+      env:\n+      - name: JAVA_OPTIONS\n+        value: \"--XX:MinRAMPercentage=25.0 --XX:MaxRAMPercentage=50.0 -Dweblogic.StdoutDebugEnabled=false\"\n+```\n+When configuring memory limits, it.s important to make sure that the limit is sufficiently big to accommodate the configured heap (and off-heap) requirements, but it's not too big to waste memory resource. Since pod memory will never go above the limit, if JVM's memory usage (sum of heap and native memory) goes above the limit, JVM process will be killed due to out-of-memory error and WebLogic container will be restarted due to liveness probe failure.   Additionally there's also a node-manager process that.s running in same container and it has it's own heap and off-heap requirements. You can also fine tune the node manager heap size in percentages by setting \"-XX:MinRAMPercentage\" and \"-XX:MaxRAMPercentage\" using .NODEMGR_JAVA_OPTIONS. environment variable. \n+\n+### Using \"-Xms\" and \"-Xmx\" parameters when not configuring limits \n+In some cases, it.s difficult to come up with a hard limit for the container and you might only want to configure memory requests but not configure memory limits. In such scenarios, you can use traditional approach to set min/max heap size using .-Xms. and .-Xmx..\n+\n+### CPU requests and limits \n+It.s important that the containers running WebLogic applications have enough CPU resources, otherwise applications performance can suffer. You also don't want to set CPU requests and limit too high if your application don't need or use allocated CPU resources. Since CPU is a shared resource, if the amount of CPU that you reserve is more than required by your application, the CPU cycles will go unused and be wasted. If no CPU request and limit is configured, it can end up using all CPU resources available on node. This can starve other containers from using shareable CPU cycles. \n+\n+One other thing to keep in mind is that if pod CPU limit is not configured, it might lead to incorrect garbage collection (GC) strategy selection. WebLogic self-tuning work-manager uses pod CPU limit to configure the  number of threads in a default thread pool. If you don.t specify container CPU limit, the performance might be affected due to incorrect number of GC threads or wrong WebLogic server thread pool size. \n+\n+## Beware of setting resource limits too high\n+It.s important to keep in mind that if you set a value of CPU core count that.s larger than core count of the biggest node, then the pod will never be scheduled. Let.s say you have a pod that needs 4 cores but you have a kubernetes cluster that.s comprised of 2 core VMs. In this case, your pod will never be scheduled.  WebLogic applications are normally designed to take advantage of multiple cores and should be given CPU requests as such. CPUs are considered as a compressible resource. If your apps are hitting CPU limits, kubernetes will start to throttle your container. This means your CPU will be artificially restricted, giving your app potentially worse performance. However it won.t be terminated or evicted. \n+Just like CPU, if you put a memory request that.s larger than amount of memory on your nodes, the pod will never be scheduled.\n+## CPU Affinity and lock contention in k8s", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MDk5OTEwOnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNjoxMDowOVrOGrDzSw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNjoxMDowOVrOGrDzSw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgwNDIzNQ==", "bodyText": "env -> environment (globally)\nseem -> seems", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447804235", "createdAt": "2020-06-30T16:10:09Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. \n+\n+A pod.s resource requests and limit parameters can be used to solve these problems. Setting resource limits prevents an application from using more than it.s share of resource. Thus, limiting resources improves reliability and stability of applications.  It also allows users to plan for the hardware capacity. Additionally, pod.s priority and the Quality of Service (QoS) that pod receives is affected by whether resource requests and limits are specified or not.\n+\n+## Pod Quality Of Service (QoS) and Prioritization\n+Pod.s Quality of Service (QoS) and priority is determined based on whether pod.s resource requests and limits are configured or not and how they.re configured.\n+\n+**Best Effort QoS**: If you don.t configure requests and limits, pod receives .best-effort. QoS and pod has the **lowest priority**. In cases where node runs out of non-shareable resources, kubelet.s out-of-resource eviction policy evicts/kills the pods with best-effort QoS first.\n+\n+**Burstable QoS**: If you configure both resource requests and limits, and set the requests to be less than the limit, pod.s QoS will be .Burstable.. Similarly when you only configure the resource requests (without limits), the pod QoS is .Burstable.. When the node runs out of non-shareable resources, kubelet will kill .Burstable. Pods only when there are no more .best-effort. pods running. The Burstable pod receives **medium priority**.\n+\n+**Guaranteed QoS**:  If you set the requests and the limits to equal values, pod will have .Guranteed. QoS and pod will be considered as of the top most priority. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, Kubernetes will kill the best-effort and the burstable Pods first before terminating these Guaranteed QoS Pods. These are the **highest priority** pods.\n+\n+## Java heap size and pod memory request/limit considerations\n+It.s extremely important to set correct heap size for JVM-based applications.  If available memory on node or memory allocated to container is not sufficient for specified JVM heap arguments (and additional off-heap memory), it is possible for WL process to run out of memory. In order to avoid this, you will need to make sure that configured heap sizes are not too big and that the pod is scheduled on the node with sufficient memory.\n+With the latest Java version, it.s possible to rely on the default JVM heap settings which are safe but quite conservative. If you configure the memory limit for a container but don.t configure heap sizes (-Xms and -Xmx), JVM will configure max heap size to 25% (1/4th) of container memory limit by default. The minimum heap size is configured to 1.56% (1/64th) of limit value.\n+\n+**Default heap sizes and resource request values for sample WebLogic Server Pods**:\n+The WLS samples configure default min and max heap size for WebLogic server java process to 256MB and 512MB respectively. This can be changed using USER_MEM_ARGS environment variable. \n+```\n+    resources:\n+      env:\n+      - name: \"USER_MEM_ARGS\"\n+        value: \"-Xms256m -Xmx512m -Djava.security.egd=file:/dev/./urandom\"\n+```        \n+\n+The default min and max heap size for node-manager process is 64MB and 100MB. This can be changed by using NODEMGR_MEM_ARGS environment variable. \n+\n+The default pod memory request in WLS samples is 768MB and default CPU request is 250m. The requests values can be changed in resources section.\n+```\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+```\n+\n+There.s no memory or CPU limit configured by default in samples and default QoS for WebLogic server pod is Burstable. If your use-case and workload requires higher QoS and priority, this can be achieved by setting memory and CPU limits. You.ll need to run tests and experiment with different memory/CPU limits to determine optimal limit values.\n+```\n+      limits:\n+        cpu: 2\n+        memory: \"2048Mi\"\n+```\n+\n+### Configure min/max heap size in percentages using \"-XX:MinRAMPercentage\" and \"-XX:MaxRAMPercentage\"\n+If you specify pod memory limit, it's recommended to configure heap size as a percentage of the total RAM (memory) specified in the pod memory limit. These parameters allow you to fine-tune the heap size. Please note . they set the percentage, not the fixed values. Thanks to it changing container memory settings will not break anything. \n+```\n+    resources:\n+      env:\n+      - name: JAVA_OPTIONS\n+        value: \"--XX:MinRAMPercentage=25.0 --XX:MaxRAMPercentage=50.0 -Dweblogic.StdoutDebugEnabled=false\"\n+```\n+When configuring memory limits, it.s important to make sure that the limit is sufficiently big to accommodate the configured heap (and off-heap) requirements, but it's not too big to waste memory resource. Since pod memory will never go above the limit, if JVM's memory usage (sum of heap and native memory) goes above the limit, JVM process will be killed due to out-of-memory error and WebLogic container will be restarted due to liveness probe failure.   Additionally there's also a node-manager process that.s running in same container and it has it's own heap and off-heap requirements. You can also fine tune the node manager heap size in percentages by setting \"-XX:MinRAMPercentage\" and \"-XX:MaxRAMPercentage\" using .NODEMGR_JAVA_OPTIONS. environment variable. \n+\n+### Using \"-Xms\" and \"-Xmx\" parameters when not configuring limits \n+In some cases, it.s difficult to come up with a hard limit for the container and you might only want to configure memory requests but not configure memory limits. In such scenarios, you can use traditional approach to set min/max heap size using .-Xms. and .-Xmx..\n+\n+### CPU requests and limits \n+It.s important that the containers running WebLogic applications have enough CPU resources, otherwise applications performance can suffer. You also don't want to set CPU requests and limit too high if your application don't need or use allocated CPU resources. Since CPU is a shared resource, if the amount of CPU that you reserve is more than required by your application, the CPU cycles will go unused and be wasted. If no CPU request and limit is configured, it can end up using all CPU resources available on node. This can starve other containers from using shareable CPU cycles. \n+\n+One other thing to keep in mind is that if pod CPU limit is not configured, it might lead to incorrect garbage collection (GC) strategy selection. WebLogic self-tuning work-manager uses pod CPU limit to configure the  number of threads in a default thread pool. If you don.t specify container CPU limit, the performance might be affected due to incorrect number of GC threads or wrong WebLogic server thread pool size. \n+\n+## Beware of setting resource limits too high\n+It.s important to keep in mind that if you set a value of CPU core count that.s larger than core count of the biggest node, then the pod will never be scheduled. Let.s say you have a pod that needs 4 cores but you have a kubernetes cluster that.s comprised of 2 core VMs. In this case, your pod will never be scheduled.  WebLogic applications are normally designed to take advantage of multiple cores and should be given CPU requests as such. CPUs are considered as a compressible resource. If your apps are hitting CPU limits, kubernetes will start to throttle your container. This means your CPU will be artificially restricted, giving your app potentially worse performance. However it won.t be terminated or evicted. \n+Just like CPU, if you put a memory request that.s larger than amount of memory on your nodes, the pod will never be scheduled.\n+## CPU Affinity and lock contention in k8s\n+We observed much higher lock contention in k8s env when running some workloads in kubernetes as compared to traditional env. The lock contention seem to be caused by the lack of CPU cache affinity and/or scheduling latency when the workload moves to different CPU cores.  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 73}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MTAwNzI0OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNjoxMTo0N1rOGrD4Bg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNjoxMTo0N1rOGrD4Bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgwNTQ0Ng==", "bodyText": "environment -> environments\njava -> Java (always capitalized)\nusing taskset -> using the taskset", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447805446", "createdAt": "2020-06-30T16:11:47Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. \n+\n+A pod.s resource requests and limit parameters can be used to solve these problems. Setting resource limits prevents an application from using more than it.s share of resource. Thus, limiting resources improves reliability and stability of applications.  It also allows users to plan for the hardware capacity. Additionally, pod.s priority and the Quality of Service (QoS) that pod receives is affected by whether resource requests and limits are specified or not.\n+\n+## Pod Quality Of Service (QoS) and Prioritization\n+Pod.s Quality of Service (QoS) and priority is determined based on whether pod.s resource requests and limits are configured or not and how they.re configured.\n+\n+**Best Effort QoS**: If you don.t configure requests and limits, pod receives .best-effort. QoS and pod has the **lowest priority**. In cases where node runs out of non-shareable resources, kubelet.s out-of-resource eviction policy evicts/kills the pods with best-effort QoS first.\n+\n+**Burstable QoS**: If you configure both resource requests and limits, and set the requests to be less than the limit, pod.s QoS will be .Burstable.. Similarly when you only configure the resource requests (without limits), the pod QoS is .Burstable.. When the node runs out of non-shareable resources, kubelet will kill .Burstable. Pods only when there are no more .best-effort. pods running. The Burstable pod receives **medium priority**.\n+\n+**Guaranteed QoS**:  If you set the requests and the limits to equal values, pod will have .Guranteed. QoS and pod will be considered as of the top most priority. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, Kubernetes will kill the best-effort and the burstable Pods first before terminating these Guaranteed QoS Pods. These are the **highest priority** pods.\n+\n+## Java heap size and pod memory request/limit considerations\n+It.s extremely important to set correct heap size for JVM-based applications.  If available memory on node or memory allocated to container is not sufficient for specified JVM heap arguments (and additional off-heap memory), it is possible for WL process to run out of memory. In order to avoid this, you will need to make sure that configured heap sizes are not too big and that the pod is scheduled on the node with sufficient memory.\n+With the latest Java version, it.s possible to rely on the default JVM heap settings which are safe but quite conservative. If you configure the memory limit for a container but don.t configure heap sizes (-Xms and -Xmx), JVM will configure max heap size to 25% (1/4th) of container memory limit by default. The minimum heap size is configured to 1.56% (1/64th) of limit value.\n+\n+**Default heap sizes and resource request values for sample WebLogic Server Pods**:\n+The WLS samples configure default min and max heap size for WebLogic server java process to 256MB and 512MB respectively. This can be changed using USER_MEM_ARGS environment variable. \n+```\n+    resources:\n+      env:\n+      - name: \"USER_MEM_ARGS\"\n+        value: \"-Xms256m -Xmx512m -Djava.security.egd=file:/dev/./urandom\"\n+```        \n+\n+The default min and max heap size for node-manager process is 64MB and 100MB. This can be changed by using NODEMGR_MEM_ARGS environment variable. \n+\n+The default pod memory request in WLS samples is 768MB and default CPU request is 250m. The requests values can be changed in resources section.\n+```\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+```\n+\n+There.s no memory or CPU limit configured by default in samples and default QoS for WebLogic server pod is Burstable. If your use-case and workload requires higher QoS and priority, this can be achieved by setting memory and CPU limits. You.ll need to run tests and experiment with different memory/CPU limits to determine optimal limit values.\n+```\n+      limits:\n+        cpu: 2\n+        memory: \"2048Mi\"\n+```\n+\n+### Configure min/max heap size in percentages using \"-XX:MinRAMPercentage\" and \"-XX:MaxRAMPercentage\"\n+If you specify pod memory limit, it's recommended to configure heap size as a percentage of the total RAM (memory) specified in the pod memory limit. These parameters allow you to fine-tune the heap size. Please note . they set the percentage, not the fixed values. Thanks to it changing container memory settings will not break anything. \n+```\n+    resources:\n+      env:\n+      - name: JAVA_OPTIONS\n+        value: \"--XX:MinRAMPercentage=25.0 --XX:MaxRAMPercentage=50.0 -Dweblogic.StdoutDebugEnabled=false\"\n+```\n+When configuring memory limits, it.s important to make sure that the limit is sufficiently big to accommodate the configured heap (and off-heap) requirements, but it's not too big to waste memory resource. Since pod memory will never go above the limit, if JVM's memory usage (sum of heap and native memory) goes above the limit, JVM process will be killed due to out-of-memory error and WebLogic container will be restarted due to liveness probe failure.   Additionally there's also a node-manager process that.s running in same container and it has it's own heap and off-heap requirements. You can also fine tune the node manager heap size in percentages by setting \"-XX:MinRAMPercentage\" and \"-XX:MaxRAMPercentage\" using .NODEMGR_JAVA_OPTIONS. environment variable. \n+\n+### Using \"-Xms\" and \"-Xmx\" parameters when not configuring limits \n+In some cases, it.s difficult to come up with a hard limit for the container and you might only want to configure memory requests but not configure memory limits. In such scenarios, you can use traditional approach to set min/max heap size using .-Xms. and .-Xmx..\n+\n+### CPU requests and limits \n+It.s important that the containers running WebLogic applications have enough CPU resources, otherwise applications performance can suffer. You also don't want to set CPU requests and limit too high if your application don't need or use allocated CPU resources. Since CPU is a shared resource, if the amount of CPU that you reserve is more than required by your application, the CPU cycles will go unused and be wasted. If no CPU request and limit is configured, it can end up using all CPU resources available on node. This can starve other containers from using shareable CPU cycles. \n+\n+One other thing to keep in mind is that if pod CPU limit is not configured, it might lead to incorrect garbage collection (GC) strategy selection. WebLogic self-tuning work-manager uses pod CPU limit to configure the  number of threads in a default thread pool. If you don.t specify container CPU limit, the performance might be affected due to incorrect number of GC threads or wrong WebLogic server thread pool size. \n+\n+## Beware of setting resource limits too high\n+It.s important to keep in mind that if you set a value of CPU core count that.s larger than core count of the biggest node, then the pod will never be scheduled. Let.s say you have a pod that needs 4 cores but you have a kubernetes cluster that.s comprised of 2 core VMs. In this case, your pod will never be scheduled.  WebLogic applications are normally designed to take advantage of multiple cores and should be given CPU requests as such. CPUs are considered as a compressible resource. If your apps are hitting CPU limits, kubernetes will start to throttle your container. This means your CPU will be artificially restricted, giving your app potentially worse performance. However it won.t be terminated or evicted. \n+Just like CPU, if you put a memory request that.s larger than amount of memory on your nodes, the pod will never be scheduled.\n+## CPU Affinity and lock contention in k8s\n+We observed much higher lock contention in k8s env when running some workloads in kubernetes as compared to traditional env. The lock contention seem to be caused by the lack of CPU cache affinity and/or scheduling latency when the workload moves to different CPU cores.  \n+\n+In traditional (non-k8s) environment, often tests are run with CPU affinity achieved by binding WLS java process to particular CPU core(s) (using taskset command). This results in reduced lock contention and better performance. ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 75}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MTAxMTkyOnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNjoxMjozNFrOGrD6lQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQxNjoxMjozNFrOGrD6lQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NzgwNjEwMQ==", "bodyText": "when CPU manager policy -> when the CPU manager policy\nQOS -> QoS", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r447806101", "createdAt": "2020-06-30T16:12:34Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,84 @@\n+---\n+title: \"Considerations for Pod Resource (Memory and CPU) Requests and Limits\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: true\n+weight: 40\n+---\n+The operator creates a pod for each running WebLogic Server instance and each pod will have a container. It.s important that containers have enough resources in order for applications to run efficiently and expeditiously. \n+\n+If a pod is scheduled on a node with limited resources, it.s possible for the node to run out of memory or CPU resources, and for applications to stop working properly or have degraded performance. It.s also possible for a rouge application to use all available memory and/or CPU, which makes other containers running on the same system unresponsive. The same problem can happen if an application has memory leak or bad configuration. \n+\n+A pod.s resource requests and limit parameters can be used to solve these problems. Setting resource limits prevents an application from using more than it.s share of resource. Thus, limiting resources improves reliability and stability of applications.  It also allows users to plan for the hardware capacity. Additionally, pod.s priority and the Quality of Service (QoS) that pod receives is affected by whether resource requests and limits are specified or not.\n+\n+## Pod Quality Of Service (QoS) and Prioritization\n+Pod.s Quality of Service (QoS) and priority is determined based on whether pod.s resource requests and limits are configured or not and how they.re configured.\n+\n+**Best Effort QoS**: If you don.t configure requests and limits, pod receives .best-effort. QoS and pod has the **lowest priority**. In cases where node runs out of non-shareable resources, kubelet.s out-of-resource eviction policy evicts/kills the pods with best-effort QoS first.\n+\n+**Burstable QoS**: If you configure both resource requests and limits, and set the requests to be less than the limit, pod.s QoS will be .Burstable.. Similarly when you only configure the resource requests (without limits), the pod QoS is .Burstable.. When the node runs out of non-shareable resources, kubelet will kill .Burstable. Pods only when there are no more .best-effort. pods running. The Burstable pod receives **medium priority**.\n+\n+**Guaranteed QoS**:  If you set the requests and the limits to equal values, pod will have .Guranteed. QoS and pod will be considered as of the top most priority. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, Kubernetes will kill the best-effort and the burstable Pods first before terminating these Guaranteed QoS Pods. These are the **highest priority** pods.\n+\n+## Java heap size and pod memory request/limit considerations\n+It.s extremely important to set correct heap size for JVM-based applications.  If available memory on node or memory allocated to container is not sufficient for specified JVM heap arguments (and additional off-heap memory), it is possible for WL process to run out of memory. In order to avoid this, you will need to make sure that configured heap sizes are not too big and that the pod is scheduled on the node with sufficient memory.\n+With the latest Java version, it.s possible to rely on the default JVM heap settings which are safe but quite conservative. If you configure the memory limit for a container but don.t configure heap sizes (-Xms and -Xmx), JVM will configure max heap size to 25% (1/4th) of container memory limit by default. The minimum heap size is configured to 1.56% (1/64th) of limit value.\n+\n+**Default heap sizes and resource request values for sample WebLogic Server Pods**:\n+The WLS samples configure default min and max heap size for WebLogic server java process to 256MB and 512MB respectively. This can be changed using USER_MEM_ARGS environment variable. \n+```\n+    resources:\n+      env:\n+      - name: \"USER_MEM_ARGS\"\n+        value: \"-Xms256m -Xmx512m -Djava.security.egd=file:/dev/./urandom\"\n+```        \n+\n+The default min and max heap size for node-manager process is 64MB and 100MB. This can be changed by using NODEMGR_MEM_ARGS environment variable. \n+\n+The default pod memory request in WLS samples is 768MB and default CPU request is 250m. The requests values can be changed in resources section.\n+```\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+```\n+\n+There.s no memory or CPU limit configured by default in samples and default QoS for WebLogic server pod is Burstable. If your use-case and workload requires higher QoS and priority, this can be achieved by setting memory and CPU limits. You.ll need to run tests and experiment with different memory/CPU limits to determine optimal limit values.\n+```\n+      limits:\n+        cpu: 2\n+        memory: \"2048Mi\"\n+```\n+\n+### Configure min/max heap size in percentages using \"-XX:MinRAMPercentage\" and \"-XX:MaxRAMPercentage\"\n+If you specify pod memory limit, it's recommended to configure heap size as a percentage of the total RAM (memory) specified in the pod memory limit. These parameters allow you to fine-tune the heap size. Please note . they set the percentage, not the fixed values. Thanks to it changing container memory settings will not break anything. \n+```\n+    resources:\n+      env:\n+      - name: JAVA_OPTIONS\n+        value: \"--XX:MinRAMPercentage=25.0 --XX:MaxRAMPercentage=50.0 -Dweblogic.StdoutDebugEnabled=false\"\n+```\n+When configuring memory limits, it.s important to make sure that the limit is sufficiently big to accommodate the configured heap (and off-heap) requirements, but it's not too big to waste memory resource. Since pod memory will never go above the limit, if JVM's memory usage (sum of heap and native memory) goes above the limit, JVM process will be killed due to out-of-memory error and WebLogic container will be restarted due to liveness probe failure.   Additionally there's also a node-manager process that.s running in same container and it has it's own heap and off-heap requirements. You can also fine tune the node manager heap size in percentages by setting \"-XX:MinRAMPercentage\" and \"-XX:MaxRAMPercentage\" using .NODEMGR_JAVA_OPTIONS. environment variable. \n+\n+### Using \"-Xms\" and \"-Xmx\" parameters when not configuring limits \n+In some cases, it.s difficult to come up with a hard limit for the container and you might only want to configure memory requests but not configure memory limits. In such scenarios, you can use traditional approach to set min/max heap size using .-Xms. and .-Xmx..\n+\n+### CPU requests and limits \n+It.s important that the containers running WebLogic applications have enough CPU resources, otherwise applications performance can suffer. You also don't want to set CPU requests and limit too high if your application don't need or use allocated CPU resources. Since CPU is a shared resource, if the amount of CPU that you reserve is more than required by your application, the CPU cycles will go unused and be wasted. If no CPU request and limit is configured, it can end up using all CPU resources available on node. This can starve other containers from using shareable CPU cycles. \n+\n+One other thing to keep in mind is that if pod CPU limit is not configured, it might lead to incorrect garbage collection (GC) strategy selection. WebLogic self-tuning work-manager uses pod CPU limit to configure the  number of threads in a default thread pool. If you don.t specify container CPU limit, the performance might be affected due to incorrect number of GC threads or wrong WebLogic server thread pool size. \n+\n+## Beware of setting resource limits too high\n+It.s important to keep in mind that if you set a value of CPU core count that.s larger than core count of the biggest node, then the pod will never be scheduled. Let.s say you have a pod that needs 4 cores but you have a kubernetes cluster that.s comprised of 2 core VMs. In this case, your pod will never be scheduled.  WebLogic applications are normally designed to take advantage of multiple cores and should be given CPU requests as such. CPUs are considered as a compressible resource. If your apps are hitting CPU limits, kubernetes will start to throttle your container. This means your CPU will be artificially restricted, giving your app potentially worse performance. However it won.t be terminated or evicted. \n+Just like CPU, if you put a memory request that.s larger than amount of memory on your nodes, the pod will never be scheduled.\n+## CPU Affinity and lock contention in k8s\n+We observed much higher lock contention in k8s env when running some workloads in kubernetes as compared to traditional env. The lock contention seem to be caused by the lack of CPU cache affinity and/or scheduling latency when the workload moves to different CPU cores.  \n+\n+In traditional (non-k8s) environment, often tests are run with CPU affinity achieved by binding WLS java process to particular CPU core(s) (using taskset command). This results in reduced lock contention and better performance. \n+\n+In k8s environment. when CPU manager policy is configured to be \"static\" and QOS is \"Guaranteed\" for WLS pods, we see reduced lock contention and better performance. The default CPU manager policy is \"none\" (default). Please refer to controlling CPU management policies for more details.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a23d465d277d44c2f5b22ee39ff66f06948acfa1"}, "originalPosition": 77}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MjM5ODQ1OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQyMjo1NDo0MVrOGrRLzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxOTowNzo0MVrOGryEeg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAyMzUwMg==", "bodyText": "The phrasing, \"It's also possible for a rogue application...\" sounds scarier than I think we need. Customers understand that application and server usage of resources need to be controlled and limited. We might borrow phrases from the current WebLogic sizing guide.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r448023502", "createdAt": "2020-06-30T22:54:41Z", "author": {"login": "rjeberhard"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,162 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Overview](#overview)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. It's important that the container has enough resources in order for WebLogic to run efficiently.\n+\n+If a pod is scheduled on a node with limited resources, then it's possible for node to run out of memory or CPU resources, and for the pod's applications to stop working properly or have degraded performance. It's also possible for a rogue application to use all of a node's available memory and/or CPU, which makes other containers running on the same node unresponsive. The same problem can happen if an application has a memory leak.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0477ff166bc9e02dd33e555ab283d981f0eafeef"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU2MjI5OA==", "bodyText": "We have rephrased the introduction section and removed the phrasing about \"rogue application using up all available resources\".", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r448562298", "createdAt": "2020-07-01T19:07:41Z", "author": {"login": "ankedia"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,162 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Overview](#overview)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. It's important that the container has enough resources in order for WebLogic to run efficiently.\n+\n+If a pod is scheduled on a node with limited resources, then it's possible for node to run out of memory or CPU resources, and for the pod's applications to stop working properly or have degraded performance. It's also possible for a rogue application to use all of a node's available memory and/or CPU, which makes other containers running on the same node unresponsive. The same problem can happen if an application has a memory leak.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAyMzUwMg=="}, "originalCommit": {"oid": "0477ff166bc9e02dd33e555ab283d981f0eafeef"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MjQwMjc1OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQyMjo1NjozOFrOGrROXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxOTowODo0OFrOGryGhA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAyNDE1Nw==", "bodyText": "This is good. Let's also link to the standard Kubernetes doc. topics https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource/ and https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource/.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r448024157", "createdAt": "2020-06-30T22:56:38Z", "author": {"login": "rjeberhard"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,162 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Overview](#overview)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. It's important that the container has enough resources in order for WebLogic to run efficiently.\n+\n+If a pod is scheduled on a node with limited resources, then it's possible for node to run out of memory or CPU resources, and for the pod's applications to stop working properly or have degraded performance. It's also possible for a rogue application to use all of a node's available memory and/or CPU, which makes other containers running on the same node unresponsive. The same problem can happen if an application has a memory leak.\n+\n+You can solve these problems by configuring resource requests and limits for your pods. A resource limit prevents a pod from using more than its share of a resource. Thus, it improves reliability, stability, and helps with hardware capacity planning. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0477ff166bc9e02dd33e555ab283d981f0eafeef"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU2MjgyMA==", "bodyText": "Included above additional links for Kubernetes doc.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r448562820", "createdAt": "2020-07-01T19:08:48Z", "author": {"login": "ankedia"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,162 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Overview](#overview)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. It's important that the container has enough resources in order for WebLogic to run efficiently.\n+\n+If a pod is scheduled on a node with limited resources, then it's possible for node to run out of memory or CPU resources, and for the pod's applications to stop working properly or have degraded performance. It's also possible for a rogue application to use all of a node's available memory and/or CPU, which makes other containers running on the same node unresponsive. The same problem can happen if an application has a memory leak.\n+\n+You can solve these problems by configuring resource requests and limits for your pods. A resource limit prevents a pod from using more than its share of a resource. Thus, it improves reliability, stability, and helps with hardware capacity planning. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAyNDE1Nw=="}, "originalCommit": {"oid": "0477ff166bc9e02dd33e555ab283d981f0eafeef"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MjQwNzQ2OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQyMjo1OTowMVrOGrRRPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxOToxMToxOFrOGryLyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAyNDg5NQ==", "bodyText": "Yes, we have serverPod.priorityClassName.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r448024895", "createdAt": "2020-06-30T22:59:01Z", "author": {"login": "rjeberhard"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,162 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Overview](#overview)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. It's important that the container has enough resources in order for WebLogic to run efficiently.\n+\n+If a pod is scheduled on a node with limited resources, then it's possible for node to run out of memory or CPU resources, and for the pod's applications to stop working properly or have degraded performance. It's also possible for a rogue application to use all of a node's available memory and/or CPU, which makes other containers running on the same node unresponsive. The same problem can happen if an application has a memory leak.\n+\n+You can solve these problems by configuring resource requests and limits for your pods. A resource limit prevents a pod from using more than its share of a resource. Thus, it improves reliability, stability, and helps with hardware capacity planning. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: \n+\n+```\n+  spec:\n+    serverPod:\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+      limits:\n+        cpu: \"2\"\n+        memory: \"2Gi\"\n+```\n+\n+Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An `m` suffix in a CPU attribute indicates 'milli-CPU', so `250m` is 25% of a CPU. \n+\n+Memory can be expressed in various units, where one `Mi` is one IEC unit mega-byte (1024^2), and one `Gi` is one IEC unit giga-byte (1024^3).\n+\n+See also [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/) in the Kubernetes documentation.\n+\n+### Determining pod Quality Of Service\n+\n+A pod's Quality of Service (QoS) is based on whether it's configured with resource requests and limits:\n+\n+- **Best Effort QoS** (lowest priority): If you don't configure requests and limits for a pod, then the pod is given a `best-effort` QoS. In cases where a node runs out of non-shareable resources, a Kubernetes `kubelet` default out-of-resource eviction policy evicts running pods with the `best-effort` QoS first.\n+\n+- **Burstable QoS** (medium priority): If you configure both resource requests and limits for a pod, and set the requests to be less than their respective limits, then the pod will be given a `burstable` QoS. Similarly, if you only configure resource requests (without limits) for a pod, then the pod QoS is also `burstable`. If a node runs out of non-shareable resources, the node's `kubelet` will evict `burstable` pods only when there are no more running `best-effort` pods.\n+\n+- **Guaranteed QoS** (highest priority): If you set a pod's requests and the limits to equal values, then the pod will have a `guaranteed` QoS. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, then a Kubernetes node's `kubelet` will evict `best-effort` and `burstable` QoS pods before terminating `guaranteed` QoS pods.\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring WebLogic pods with memory and CPU requests and limits, and furthermore setting requests equal to their respective limits in order to ensure a `guaranteed` QoS.\n+{{% /notice %}}\n+\n+In newer version of Kubernetes, it is possible to fine tune scheduling and eviction policies using [Pod Priority Preemption](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/). TBD Ryan - is it possible to change the priority class of a pod?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0477ff166bc9e02dd33e555ab283d981f0eafeef"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU2NDE3MA==", "bodyText": "Thanks. We have included wording about priority class tuning and two PriorityClasses shipped with Kubernetes.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r448564170", "createdAt": "2020-07-01T19:11:18Z", "author": {"login": "ankedia"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,162 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Overview](#overview)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. It's important that the container has enough resources in order for WebLogic to run efficiently.\n+\n+If a pod is scheduled on a node with limited resources, then it's possible for node to run out of memory or CPU resources, and for the pod's applications to stop working properly or have degraded performance. It's also possible for a rogue application to use all of a node's available memory and/or CPU, which makes other containers running on the same node unresponsive. The same problem can happen if an application has a memory leak.\n+\n+You can solve these problems by configuring resource requests and limits for your pods. A resource limit prevents a pod from using more than its share of a resource. Thus, it improves reliability, stability, and helps with hardware capacity planning. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: \n+\n+```\n+  spec:\n+    serverPod:\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+      limits:\n+        cpu: \"2\"\n+        memory: \"2Gi\"\n+```\n+\n+Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An `m` suffix in a CPU attribute indicates 'milli-CPU', so `250m` is 25% of a CPU. \n+\n+Memory can be expressed in various units, where one `Mi` is one IEC unit mega-byte (1024^2), and one `Gi` is one IEC unit giga-byte (1024^3).\n+\n+See also [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/) in the Kubernetes documentation.\n+\n+### Determining pod Quality Of Service\n+\n+A pod's Quality of Service (QoS) is based on whether it's configured with resource requests and limits:\n+\n+- **Best Effort QoS** (lowest priority): If you don't configure requests and limits for a pod, then the pod is given a `best-effort` QoS. In cases where a node runs out of non-shareable resources, a Kubernetes `kubelet` default out-of-resource eviction policy evicts running pods with the `best-effort` QoS first.\n+\n+- **Burstable QoS** (medium priority): If you configure both resource requests and limits for a pod, and set the requests to be less than their respective limits, then the pod will be given a `burstable` QoS. Similarly, if you only configure resource requests (without limits) for a pod, then the pod QoS is also `burstable`. If a node runs out of non-shareable resources, the node's `kubelet` will evict `burstable` pods only when there are no more running `best-effort` pods.\n+\n+- **Guaranteed QoS** (highest priority): If you set a pod's requests and the limits to equal values, then the pod will have a `guaranteed` QoS. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, then a Kubernetes node's `kubelet` will evict `best-effort` and `burstable` QoS pods before terminating `guaranteed` QoS pods.\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring WebLogic pods with memory and CPU requests and limits, and furthermore setting requests equal to their respective limits in order to ensure a `guaranteed` QoS.\n+{{% /notice %}}\n+\n+In newer version of Kubernetes, it is possible to fine tune scheduling and eviction policies using [Pod Priority Preemption](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/). TBD Ryan - is it possible to change the priority class of a pod?", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAyNDg5NQ=="}, "originalCommit": {"oid": "0477ff166bc9e02dd33e555ab283d981f0eafeef"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MjQzMjQ4OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQyMzoxMDo1MVrOGrRgKg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxOToxMzowMVrOGryPCQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAyODcxNA==", "bodyText": "I think the topic here is \"Importance of setting heap sizes\"", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r448028714", "createdAt": "2020-06-30T23:10:51Z", "author": {"login": "rjeberhard"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,162 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Overview](#overview)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. It's important that the container has enough resources in order for WebLogic to run efficiently.\n+\n+If a pod is scheduled on a node with limited resources, then it's possible for node to run out of memory or CPU resources, and for the pod's applications to stop working properly or have degraded performance. It's also possible for a rogue application to use all of a node's available memory and/or CPU, which makes other containers running on the same node unresponsive. The same problem can happen if an application has a memory leak.\n+\n+You can solve these problems by configuring resource requests and limits for your pods. A resource limit prevents a pod from using more than its share of a resource. Thus, it improves reliability, stability, and helps with hardware capacity planning. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: \n+\n+```\n+  spec:\n+    serverPod:\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+      limits:\n+        cpu: \"2\"\n+        memory: \"2Gi\"\n+```\n+\n+Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An `m` suffix in a CPU attribute indicates 'milli-CPU', so `250m` is 25% of a CPU. \n+\n+Memory can be expressed in various units, where one `Mi` is one IEC unit mega-byte (1024^2), and one `Gi` is one IEC unit giga-byte (1024^3).\n+\n+See also [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/) in the Kubernetes documentation.\n+\n+### Determining pod Quality Of Service\n+\n+A pod's Quality of Service (QoS) is based on whether it's configured with resource requests and limits:\n+\n+- **Best Effort QoS** (lowest priority): If you don't configure requests and limits for a pod, then the pod is given a `best-effort` QoS. In cases where a node runs out of non-shareable resources, a Kubernetes `kubelet` default out-of-resource eviction policy evicts running pods with the `best-effort` QoS first.\n+\n+- **Burstable QoS** (medium priority): If you configure both resource requests and limits for a pod, and set the requests to be less than their respective limits, then the pod will be given a `burstable` QoS. Similarly, if you only configure resource requests (without limits) for a pod, then the pod QoS is also `burstable`. If a node runs out of non-shareable resources, the node's `kubelet` will evict `burstable` pods only when there are no more running `best-effort` pods.\n+\n+- **Guaranteed QoS** (highest priority): If you set a pod's requests and the limits to equal values, then the pod will have a `guaranteed` QoS. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, then a Kubernetes node's `kubelet` will evict `best-effort` and `burstable` QoS pods before terminating `guaranteed` QoS pods.\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring WebLogic pods with memory and CPU requests and limits, and furthermore setting requests equal to their respective limits in order to ensure a `guaranteed` QoS.\n+{{% /notice %}}\n+\n+In newer version of Kubernetes, it is possible to fine tune scheduling and eviction policies using [Pod Priority Preemption](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/). TBD Ryan - is it possible to change the priority class of a pod?\n+\n+### Java heap size and memory resource considerations\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring Java heap sizes for WebLogic pods instead of relying on defaults.\n+{{% /notice %}}\n+\n+#### Overview\n+\n+It's extremely important to set correct heap sizes, memory requests, and memory limits for WebLogic JVMs and Pods. \n+\n+A WebLogic JVM heap must be sufficiently sized to run its applications and services, but should not be sized too large so as not to waste memory resources.\n+\n+A pod memory limit must be sufficiently sized to accommodate the configured heap (and off-heap) requirements, but  not too big to waste memory resources. If a JVM's memory usage (sum of heap and native memory) exceeds its pod's limit, then the JVM process will be abruptly killed due to an out-of-memory error and the WebLogic container will consequently automatically restart due to a liveness probe failure.  \n+\n+{{% notice warning %}}\n+If resource requests and resource limits are set too high, then your pods may not be scheduled due to lack of node resources, will unnecessarily use up CPU shared resources that could be used by other pods, or may prevent other pods from running.\n+{{% /notice %}}\n+\n+#### Default heap sizes", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0477ff166bc9e02dd33e555ab283d981f0eafeef"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAyOTM5Mw==", "bodyText": "Or, put another way, we should end this section more strongly to say that customers must set the heap min and max (or the maximum heap percentage) and at least container memory request.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r448029393", "createdAt": "2020-06-30T23:13:05Z", "author": {"login": "rjeberhard"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,162 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Overview](#overview)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. It's important that the container has enough resources in order for WebLogic to run efficiently.\n+\n+If a pod is scheduled on a node with limited resources, then it's possible for node to run out of memory or CPU resources, and for the pod's applications to stop working properly or have degraded performance. It's also possible for a rogue application to use all of a node's available memory and/or CPU, which makes other containers running on the same node unresponsive. The same problem can happen if an application has a memory leak.\n+\n+You can solve these problems by configuring resource requests and limits for your pods. A resource limit prevents a pod from using more than its share of a resource. Thus, it improves reliability, stability, and helps with hardware capacity planning. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: \n+\n+```\n+  spec:\n+    serverPod:\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+      limits:\n+        cpu: \"2\"\n+        memory: \"2Gi\"\n+```\n+\n+Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An `m` suffix in a CPU attribute indicates 'milli-CPU', so `250m` is 25% of a CPU. \n+\n+Memory can be expressed in various units, where one `Mi` is one IEC unit mega-byte (1024^2), and one `Gi` is one IEC unit giga-byte (1024^3).\n+\n+See also [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/) in the Kubernetes documentation.\n+\n+### Determining pod Quality Of Service\n+\n+A pod's Quality of Service (QoS) is based on whether it's configured with resource requests and limits:\n+\n+- **Best Effort QoS** (lowest priority): If you don't configure requests and limits for a pod, then the pod is given a `best-effort` QoS. In cases where a node runs out of non-shareable resources, a Kubernetes `kubelet` default out-of-resource eviction policy evicts running pods with the `best-effort` QoS first.\n+\n+- **Burstable QoS** (medium priority): If you configure both resource requests and limits for a pod, and set the requests to be less than their respective limits, then the pod will be given a `burstable` QoS. Similarly, if you only configure resource requests (without limits) for a pod, then the pod QoS is also `burstable`. If a node runs out of non-shareable resources, the node's `kubelet` will evict `burstable` pods only when there are no more running `best-effort` pods.\n+\n+- **Guaranteed QoS** (highest priority): If you set a pod's requests and the limits to equal values, then the pod will have a `guaranteed` QoS. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, then a Kubernetes node's `kubelet` will evict `best-effort` and `burstable` QoS pods before terminating `guaranteed` QoS pods.\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring WebLogic pods with memory and CPU requests and limits, and furthermore setting requests equal to their respective limits in order to ensure a `guaranteed` QoS.\n+{{% /notice %}}\n+\n+In newer version of Kubernetes, it is possible to fine tune scheduling and eviction policies using [Pod Priority Preemption](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/). TBD Ryan - is it possible to change the priority class of a pod?\n+\n+### Java heap size and memory resource considerations\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring Java heap sizes for WebLogic pods instead of relying on defaults.\n+{{% /notice %}}\n+\n+#### Overview\n+\n+It's extremely important to set correct heap sizes, memory requests, and memory limits for WebLogic JVMs and Pods. \n+\n+A WebLogic JVM heap must be sufficiently sized to run its applications and services, but should not be sized too large so as not to waste memory resources.\n+\n+A pod memory limit must be sufficiently sized to accommodate the configured heap (and off-heap) requirements, but  not too big to waste memory resources. If a JVM's memory usage (sum of heap and native memory) exceeds its pod's limit, then the JVM process will be abruptly killed due to an out-of-memory error and the WebLogic container will consequently automatically restart due to a liveness probe failure.  \n+\n+{{% notice warning %}}\n+If resource requests and resource limits are set too high, then your pods may not be scheduled due to lack of node resources, will unnecessarily use up CPU shared resources that could be used by other pods, or may prevent other pods from running.\n+{{% /notice %}}\n+\n+#### Default heap sizes", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAyODcxNA=="}, "originalCommit": {"oid": "0477ff166bc9e02dd33e555ab283d981f0eafeef"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU2NTAwMQ==", "bodyText": "Changed the title and added recommendation for setting heap min and max and container memory request.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r448565001", "createdAt": "2020-07-01T19:13:01Z", "author": {"login": "ankedia"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,162 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Overview](#overview)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. It's important that the container has enough resources in order for WebLogic to run efficiently.\n+\n+If a pod is scheduled on a node with limited resources, then it's possible for node to run out of memory or CPU resources, and for the pod's applications to stop working properly or have degraded performance. It's also possible for a rogue application to use all of a node's available memory and/or CPU, which makes other containers running on the same node unresponsive. The same problem can happen if an application has a memory leak.\n+\n+You can solve these problems by configuring resource requests and limits for your pods. A resource limit prevents a pod from using more than its share of a resource. Thus, it improves reliability, stability, and helps with hardware capacity planning. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: \n+\n+```\n+  spec:\n+    serverPod:\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+      limits:\n+        cpu: \"2\"\n+        memory: \"2Gi\"\n+```\n+\n+Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An `m` suffix in a CPU attribute indicates 'milli-CPU', so `250m` is 25% of a CPU. \n+\n+Memory can be expressed in various units, where one `Mi` is one IEC unit mega-byte (1024^2), and one `Gi` is one IEC unit giga-byte (1024^3).\n+\n+See also [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/) in the Kubernetes documentation.\n+\n+### Determining pod Quality Of Service\n+\n+A pod's Quality of Service (QoS) is based on whether it's configured with resource requests and limits:\n+\n+- **Best Effort QoS** (lowest priority): If you don't configure requests and limits for a pod, then the pod is given a `best-effort` QoS. In cases where a node runs out of non-shareable resources, a Kubernetes `kubelet` default out-of-resource eviction policy evicts running pods with the `best-effort` QoS first.\n+\n+- **Burstable QoS** (medium priority): If you configure both resource requests and limits for a pod, and set the requests to be less than their respective limits, then the pod will be given a `burstable` QoS. Similarly, if you only configure resource requests (without limits) for a pod, then the pod QoS is also `burstable`. If a node runs out of non-shareable resources, the node's `kubelet` will evict `burstable` pods only when there are no more running `best-effort` pods.\n+\n+- **Guaranteed QoS** (highest priority): If you set a pod's requests and the limits to equal values, then the pod will have a `guaranteed` QoS. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, then a Kubernetes node's `kubelet` will evict `best-effort` and `burstable` QoS pods before terminating `guaranteed` QoS pods.\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring WebLogic pods with memory and CPU requests and limits, and furthermore setting requests equal to their respective limits in order to ensure a `guaranteed` QoS.\n+{{% /notice %}}\n+\n+In newer version of Kubernetes, it is possible to fine tune scheduling and eviction policies using [Pod Priority Preemption](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/). TBD Ryan - is it possible to change the priority class of a pod?\n+\n+### Java heap size and memory resource considerations\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring Java heap sizes for WebLogic pods instead of relying on defaults.\n+{{% /notice %}}\n+\n+#### Overview\n+\n+It's extremely important to set correct heap sizes, memory requests, and memory limits for WebLogic JVMs and Pods. \n+\n+A WebLogic JVM heap must be sufficiently sized to run its applications and services, but should not be sized too large so as not to waste memory resources.\n+\n+A pod memory limit must be sufficiently sized to accommodate the configured heap (and off-heap) requirements, but  not too big to waste memory resources. If a JVM's memory usage (sum of heap and native memory) exceeds its pod's limit, then the JVM process will be abruptly killed due to an out-of-memory error and the WebLogic container will consequently automatically restart due to a liveness probe failure.  \n+\n+{{% notice warning %}}\n+If resource requests and resource limits are set too high, then your pods may not be scheduled due to lack of node resources, will unnecessarily use up CPU shared resources that could be used by other pods, or may prevent other pods from running.\n+{{% /notice %}}\n+\n+#### Default heap sizes", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAyODcxNA=="}, "originalCommit": {"oid": "0477ff166bc9e02dd33e555ab283d981f0eafeef"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MjQzODc2OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQyMzoxNDowOFrOGrRj8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxOToxMzoyN1rOGryPvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAyOTY4Mg==", "bodyText": "Will readers understand \"off-heap\" requirements? I've heard this described as native memory.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r448029682", "createdAt": "2020-06-30T23:14:08Z", "author": {"login": "rjeberhard"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,162 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Overview](#overview)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. It's important that the container has enough resources in order for WebLogic to run efficiently.\n+\n+If a pod is scheduled on a node with limited resources, then it's possible for node to run out of memory or CPU resources, and for the pod's applications to stop working properly or have degraded performance. It's also possible for a rogue application to use all of a node's available memory and/or CPU, which makes other containers running on the same node unresponsive. The same problem can happen if an application has a memory leak.\n+\n+You can solve these problems by configuring resource requests and limits for your pods. A resource limit prevents a pod from using more than its share of a resource. Thus, it improves reliability, stability, and helps with hardware capacity planning. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: \n+\n+```\n+  spec:\n+    serverPod:\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+      limits:\n+        cpu: \"2\"\n+        memory: \"2Gi\"\n+```\n+\n+Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An `m` suffix in a CPU attribute indicates 'milli-CPU', so `250m` is 25% of a CPU. \n+\n+Memory can be expressed in various units, where one `Mi` is one IEC unit mega-byte (1024^2), and one `Gi` is one IEC unit giga-byte (1024^3).\n+\n+See also [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/) in the Kubernetes documentation.\n+\n+### Determining pod Quality Of Service\n+\n+A pod's Quality of Service (QoS) is based on whether it's configured with resource requests and limits:\n+\n+- **Best Effort QoS** (lowest priority): If you don't configure requests and limits for a pod, then the pod is given a `best-effort` QoS. In cases where a node runs out of non-shareable resources, a Kubernetes `kubelet` default out-of-resource eviction policy evicts running pods with the `best-effort` QoS first.\n+\n+- **Burstable QoS** (medium priority): If you configure both resource requests and limits for a pod, and set the requests to be less than their respective limits, then the pod will be given a `burstable` QoS. Similarly, if you only configure resource requests (without limits) for a pod, then the pod QoS is also `burstable`. If a node runs out of non-shareable resources, the node's `kubelet` will evict `burstable` pods only when there are no more running `best-effort` pods.\n+\n+- **Guaranteed QoS** (highest priority): If you set a pod's requests and the limits to equal values, then the pod will have a `guaranteed` QoS. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, then a Kubernetes node's `kubelet` will evict `best-effort` and `burstable` QoS pods before terminating `guaranteed` QoS pods.\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring WebLogic pods with memory and CPU requests and limits, and furthermore setting requests equal to their respective limits in order to ensure a `guaranteed` QoS.\n+{{% /notice %}}\n+\n+In newer version of Kubernetes, it is possible to fine tune scheduling and eviction policies using [Pod Priority Preemption](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/). TBD Ryan - is it possible to change the priority class of a pod?\n+\n+### Java heap size and memory resource considerations\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring Java heap sizes for WebLogic pods instead of relying on defaults.\n+{{% /notice %}}\n+\n+#### Overview\n+\n+It's extremely important to set correct heap sizes, memory requests, and memory limits for WebLogic JVMs and Pods. \n+\n+A WebLogic JVM heap must be sufficiently sized to run its applications and services, but should not be sized too large so as not to waste memory resources.\n+\n+A pod memory limit must be sufficiently sized to accommodate the configured heap (and off-heap) requirements, but  not too big to waste memory resources. If a JVM's memory usage (sum of heap and native memory) exceeds its pod's limit, then the JVM process will be abruptly killed due to an out-of-memory error and the WebLogic container will consequently automatically restart due to a liveness probe failure.  \n+\n+{{% notice warning %}}\n+If resource requests and resource limits are set too high, then your pods may not be scheduled due to lack of node resources, will unnecessarily use up CPU shared resources that could be used by other pods, or may prevent other pods from running.\n+{{% /notice %}}\n+\n+#### Default heap sizes\n+\n+With the latest Java versions, Java 8 update 191 and onwards or Java 11, then if you don't configure a heap size (no '-Xms' or '-Xms') the default heap size is dynamically determined:\n+- If you configure the memory limit for a container, then the JVM default maximum heap size will be 25% (1/4th) of container memory limit and the default minimum heap size will be 1.56% (1/64th) of the limit value. \n+\n+  The default JVM heap settings in this case are often too conservative because the WebLogic JVM is the only major process running in the container.\n+\n+- If no memory limit is configured, then the JVM default maximum heap size will be  25% (1/4th) of the its node's machine RAM and the default minimum heap size will be 1.56% (1/64th) of the RAM.\n+\n+  The default JVM heap settings in this case can have undesirable behavior, including using unnecessary amounts of memory to the point where it might affect other pods that run on the same node.\n+\n+#### Configuring heap size\n+\n+If you specify pod memory limits, Oracle recommends configuring WebLogic Server heap sizes as a percentage. The JVM will interpret the percentage as a fraction of the limit. This is done using the JVM `-XX:MinRAMPercentage` and `-XX:MaxRAMPercentage` options in the `USER_MEM_ARGS` [domain resource environment variable]({{< relref \"/userguide/managing-domains/domain-resource#jvm-memory-and-java-option-environment-variables\" >}}).  For example:\n+\n+```\n+  spec:\n+    resources:\n+      env:\n+      - name: USER_MEM_ARGS\n+        value: \"--XX:MinRAMPercentage=25.0 --XX:MaxRAMPercentage=50.0 -Djava.security.egd=file:/dev/./urandom\"\n+```\n+\n+Additionally there's also a node-manager process that's running in the same container as the WebLogic Server which has its own heap and off-heap requirements. Its heap is tuned by using `-Xms` and `-Xmx` in the `NODEMGR_MEM_ARGS` environment variable. Oracle recommends setting the node manager heap memory to fixed sizes, instead of percentages, where [the default tuning]({{< relref \"/userguide/managing-domains/domain-resource#jvm-memory-and-java-option-environment-variables\" >}}) is usually sufficient.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0477ff166bc9e02dd33e555ab283d981f0eafeef"}, "originalPosition": 109}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU2NTE4Mw==", "bodyText": "Changed \"off-heap\" to \"native memory\".", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r448565183", "createdAt": "2020-07-01T19:13:27Z", "author": {"login": "ankedia"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,162 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Overview](#overview)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. It's important that the container has enough resources in order for WebLogic to run efficiently.\n+\n+If a pod is scheduled on a node with limited resources, then it's possible for node to run out of memory or CPU resources, and for the pod's applications to stop working properly or have degraded performance. It's also possible for a rogue application to use all of a node's available memory and/or CPU, which makes other containers running on the same node unresponsive. The same problem can happen if an application has a memory leak.\n+\n+You can solve these problems by configuring resource requests and limits for your pods. A resource limit prevents a pod from using more than its share of a resource. Thus, it improves reliability, stability, and helps with hardware capacity planning. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: \n+\n+```\n+  spec:\n+    serverPod:\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+      limits:\n+        cpu: \"2\"\n+        memory: \"2Gi\"\n+```\n+\n+Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An `m` suffix in a CPU attribute indicates 'milli-CPU', so `250m` is 25% of a CPU. \n+\n+Memory can be expressed in various units, where one `Mi` is one IEC unit mega-byte (1024^2), and one `Gi` is one IEC unit giga-byte (1024^3).\n+\n+See also [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/) in the Kubernetes documentation.\n+\n+### Determining pod Quality Of Service\n+\n+A pod's Quality of Service (QoS) is based on whether it's configured with resource requests and limits:\n+\n+- **Best Effort QoS** (lowest priority): If you don't configure requests and limits for a pod, then the pod is given a `best-effort` QoS. In cases where a node runs out of non-shareable resources, a Kubernetes `kubelet` default out-of-resource eviction policy evicts running pods with the `best-effort` QoS first.\n+\n+- **Burstable QoS** (medium priority): If you configure both resource requests and limits for a pod, and set the requests to be less than their respective limits, then the pod will be given a `burstable` QoS. Similarly, if you only configure resource requests (without limits) for a pod, then the pod QoS is also `burstable`. If a node runs out of non-shareable resources, the node's `kubelet` will evict `burstable` pods only when there are no more running `best-effort` pods.\n+\n+- **Guaranteed QoS** (highest priority): If you set a pod's requests and the limits to equal values, then the pod will have a `guaranteed` QoS. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, then a Kubernetes node's `kubelet` will evict `best-effort` and `burstable` QoS pods before terminating `guaranteed` QoS pods.\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring WebLogic pods with memory and CPU requests and limits, and furthermore setting requests equal to their respective limits in order to ensure a `guaranteed` QoS.\n+{{% /notice %}}\n+\n+In newer version of Kubernetes, it is possible to fine tune scheduling and eviction policies using [Pod Priority Preemption](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/). TBD Ryan - is it possible to change the priority class of a pod?\n+\n+### Java heap size and memory resource considerations\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring Java heap sizes for WebLogic pods instead of relying on defaults.\n+{{% /notice %}}\n+\n+#### Overview\n+\n+It's extremely important to set correct heap sizes, memory requests, and memory limits for WebLogic JVMs and Pods. \n+\n+A WebLogic JVM heap must be sufficiently sized to run its applications and services, but should not be sized too large so as not to waste memory resources.\n+\n+A pod memory limit must be sufficiently sized to accommodate the configured heap (and off-heap) requirements, but  not too big to waste memory resources. If a JVM's memory usage (sum of heap and native memory) exceeds its pod's limit, then the JVM process will be abruptly killed due to an out-of-memory error and the WebLogic container will consequently automatically restart due to a liveness probe failure.  \n+\n+{{% notice warning %}}\n+If resource requests and resource limits are set too high, then your pods may not be scheduled due to lack of node resources, will unnecessarily use up CPU shared resources that could be used by other pods, or may prevent other pods from running.\n+{{% /notice %}}\n+\n+#### Default heap sizes\n+\n+With the latest Java versions, Java 8 update 191 and onwards or Java 11, then if you don't configure a heap size (no '-Xms' or '-Xms') the default heap size is dynamically determined:\n+- If you configure the memory limit for a container, then the JVM default maximum heap size will be 25% (1/4th) of container memory limit and the default minimum heap size will be 1.56% (1/64th) of the limit value. \n+\n+  The default JVM heap settings in this case are often too conservative because the WebLogic JVM is the only major process running in the container.\n+\n+- If no memory limit is configured, then the JVM default maximum heap size will be  25% (1/4th) of the its node's machine RAM and the default minimum heap size will be 1.56% (1/64th) of the RAM.\n+\n+  The default JVM heap settings in this case can have undesirable behavior, including using unnecessary amounts of memory to the point where it might affect other pods that run on the same node.\n+\n+#### Configuring heap size\n+\n+If you specify pod memory limits, Oracle recommends configuring WebLogic Server heap sizes as a percentage. The JVM will interpret the percentage as a fraction of the limit. This is done using the JVM `-XX:MinRAMPercentage` and `-XX:MaxRAMPercentage` options in the `USER_MEM_ARGS` [domain resource environment variable]({{< relref \"/userguide/managing-domains/domain-resource#jvm-memory-and-java-option-environment-variables\" >}}).  For example:\n+\n+```\n+  spec:\n+    resources:\n+      env:\n+      - name: USER_MEM_ARGS\n+        value: \"--XX:MinRAMPercentage=25.0 --XX:MaxRAMPercentage=50.0 -Djava.security.egd=file:/dev/./urandom\"\n+```\n+\n+Additionally there's also a node-manager process that's running in the same container as the WebLogic Server which has its own heap and off-heap requirements. Its heap is tuned by using `-Xms` and `-Xmx` in the `NODEMGR_MEM_ARGS` environment variable. Oracle recommends setting the node manager heap memory to fixed sizes, instead of percentages, where [the default tuning]({{< relref \"/userguide/managing-domains/domain-resource#jvm-memory-and-java-option-environment-variables\" >}}) is usually sufficient.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAyOTY4Mg=="}, "originalCommit": {"oid": "0477ff166bc9e02dd33e555ab283d981f0eafeef"}, "originalPosition": 109}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MjQ1MDYxOnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQyMzoxOTo1NlrOGrRq-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxOToxMzo1MlrOGryQbw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAzMTQ4Mg==", "bodyText": "Why is it only \"usually\" recommended?", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r448031482", "createdAt": "2020-06-30T23:19:56Z", "author": {"login": "rjeberhard"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,162 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Overview](#overview)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. It's important that the container has enough resources in order for WebLogic to run efficiently.\n+\n+If a pod is scheduled on a node with limited resources, then it's possible for node to run out of memory or CPU resources, and for the pod's applications to stop working properly or have degraded performance. It's also possible for a rogue application to use all of a node's available memory and/or CPU, which makes other containers running on the same node unresponsive. The same problem can happen if an application has a memory leak.\n+\n+You can solve these problems by configuring resource requests and limits for your pods. A resource limit prevents a pod from using more than its share of a resource. Thus, it improves reliability, stability, and helps with hardware capacity planning. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: \n+\n+```\n+  spec:\n+    serverPod:\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+      limits:\n+        cpu: \"2\"\n+        memory: \"2Gi\"\n+```\n+\n+Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An `m` suffix in a CPU attribute indicates 'milli-CPU', so `250m` is 25% of a CPU. \n+\n+Memory can be expressed in various units, where one `Mi` is one IEC unit mega-byte (1024^2), and one `Gi` is one IEC unit giga-byte (1024^3).\n+\n+See also [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/) in the Kubernetes documentation.\n+\n+### Determining pod Quality Of Service\n+\n+A pod's Quality of Service (QoS) is based on whether it's configured with resource requests and limits:\n+\n+- **Best Effort QoS** (lowest priority): If you don't configure requests and limits for a pod, then the pod is given a `best-effort` QoS. In cases where a node runs out of non-shareable resources, a Kubernetes `kubelet` default out-of-resource eviction policy evicts running pods with the `best-effort` QoS first.\n+\n+- **Burstable QoS** (medium priority): If you configure both resource requests and limits for a pod, and set the requests to be less than their respective limits, then the pod will be given a `burstable` QoS. Similarly, if you only configure resource requests (without limits) for a pod, then the pod QoS is also `burstable`. If a node runs out of non-shareable resources, the node's `kubelet` will evict `burstable` pods only when there are no more running `best-effort` pods.\n+\n+- **Guaranteed QoS** (highest priority): If you set a pod's requests and the limits to equal values, then the pod will have a `guaranteed` QoS. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, then a Kubernetes node's `kubelet` will evict `best-effort` and `burstable` QoS pods before terminating `guaranteed` QoS pods.\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring WebLogic pods with memory and CPU requests and limits, and furthermore setting requests equal to their respective limits in order to ensure a `guaranteed` QoS.\n+{{% /notice %}}\n+\n+In newer version of Kubernetes, it is possible to fine tune scheduling and eviction policies using [Pod Priority Preemption](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/). TBD Ryan - is it possible to change the priority class of a pod?\n+\n+### Java heap size and memory resource considerations\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring Java heap sizes for WebLogic pods instead of relying on defaults.\n+{{% /notice %}}\n+\n+#### Overview\n+\n+It's extremely important to set correct heap sizes, memory requests, and memory limits for WebLogic JVMs and Pods. \n+\n+A WebLogic JVM heap must be sufficiently sized to run its applications and services, but should not be sized too large so as not to waste memory resources.\n+\n+A pod memory limit must be sufficiently sized to accommodate the configured heap (and off-heap) requirements, but  not too big to waste memory resources. If a JVM's memory usage (sum of heap and native memory) exceeds its pod's limit, then the JVM process will be abruptly killed due to an out-of-memory error and the WebLogic container will consequently automatically restart due to a liveness probe failure.  \n+\n+{{% notice warning %}}\n+If resource requests and resource limits are set too high, then your pods may not be scheduled due to lack of node resources, will unnecessarily use up CPU shared resources that could be used by other pods, or may prevent other pods from running.\n+{{% /notice %}}\n+\n+#### Default heap sizes\n+\n+With the latest Java versions, Java 8 update 191 and onwards or Java 11, then if you don't configure a heap size (no '-Xms' or '-Xms') the default heap size is dynamically determined:\n+- If you configure the memory limit for a container, then the JVM default maximum heap size will be 25% (1/4th) of container memory limit and the default minimum heap size will be 1.56% (1/64th) of the limit value. \n+\n+  The default JVM heap settings in this case are often too conservative because the WebLogic JVM is the only major process running in the container.\n+\n+- If no memory limit is configured, then the JVM default maximum heap size will be  25% (1/4th) of the its node's machine RAM and the default minimum heap size will be 1.56% (1/64th) of the RAM.\n+\n+  The default JVM heap settings in this case can have undesirable behavior, including using unnecessary amounts of memory to the point where it might affect other pods that run on the same node.\n+\n+#### Configuring heap size\n+\n+If you specify pod memory limits, Oracle recommends configuring WebLogic Server heap sizes as a percentage. The JVM will interpret the percentage as a fraction of the limit. This is done using the JVM `-XX:MinRAMPercentage` and `-XX:MaxRAMPercentage` options in the `USER_MEM_ARGS` [domain resource environment variable]({{< relref \"/userguide/managing-domains/domain-resource#jvm-memory-and-java-option-environment-variables\" >}}).  For example:\n+\n+```\n+  spec:\n+    resources:\n+      env:\n+      - name: USER_MEM_ARGS\n+        value: \"--XX:MinRAMPercentage=25.0 --XX:MaxRAMPercentage=50.0 -Djava.security.egd=file:/dev/./urandom\"\n+```\n+\n+Additionally there's also a node-manager process that's running in the same container as the WebLogic Server which has its own heap and off-heap requirements. Its heap is tuned by using `-Xms` and `-Xmx` in the `NODEMGR_MEM_ARGS` environment variable. Oracle recommends setting the node manager heap memory to fixed sizes, instead of percentages, where [the default tuning]({{< relref \"/userguide/managing-domains/domain-resource#jvm-memory-and-java-option-environment-variables\" >}}) is usually sufficient.\n+\n+{{% notice warning %}}\n+If you set `USER_MEM_ARGS` or `NODEMGR_MEM_ARGS` in your domain resource, then it is usually recommended to include `-Djava.security.egd=file:/dev/./urandom` in order to speedup boot times on systems with low entropy. This setting is included in the respective defaults for these two environment variables.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0477ff166bc9e02dd33e555ab283d981f0eafeef"}, "originalPosition": 112}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU2NTM1OQ==", "bodyText": "This was changed to match with other parts of Operator documentation.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r448565359", "createdAt": "2020-07-01T19:13:52Z", "author": {"login": "ankedia"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,162 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Overview](#overview)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. It's important that the container has enough resources in order for WebLogic to run efficiently.\n+\n+If a pod is scheduled on a node with limited resources, then it's possible for node to run out of memory or CPU resources, and for the pod's applications to stop working properly or have degraded performance. It's also possible for a rogue application to use all of a node's available memory and/or CPU, which makes other containers running on the same node unresponsive. The same problem can happen if an application has a memory leak.\n+\n+You can solve these problems by configuring resource requests and limits for your pods. A resource limit prevents a pod from using more than its share of a resource. Thus, it improves reliability, stability, and helps with hardware capacity planning. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: \n+\n+```\n+  spec:\n+    serverPod:\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+      limits:\n+        cpu: \"2\"\n+        memory: \"2Gi\"\n+```\n+\n+Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An `m` suffix in a CPU attribute indicates 'milli-CPU', so `250m` is 25% of a CPU. \n+\n+Memory can be expressed in various units, where one `Mi` is one IEC unit mega-byte (1024^2), and one `Gi` is one IEC unit giga-byte (1024^3).\n+\n+See also [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/) in the Kubernetes documentation.\n+\n+### Determining pod Quality Of Service\n+\n+A pod's Quality of Service (QoS) is based on whether it's configured with resource requests and limits:\n+\n+- **Best Effort QoS** (lowest priority): If you don't configure requests and limits for a pod, then the pod is given a `best-effort` QoS. In cases where a node runs out of non-shareable resources, a Kubernetes `kubelet` default out-of-resource eviction policy evicts running pods with the `best-effort` QoS first.\n+\n+- **Burstable QoS** (medium priority): If you configure both resource requests and limits for a pod, and set the requests to be less than their respective limits, then the pod will be given a `burstable` QoS. Similarly, if you only configure resource requests (without limits) for a pod, then the pod QoS is also `burstable`. If a node runs out of non-shareable resources, the node's `kubelet` will evict `burstable` pods only when there are no more running `best-effort` pods.\n+\n+- **Guaranteed QoS** (highest priority): If you set a pod's requests and the limits to equal values, then the pod will have a `guaranteed` QoS. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, then a Kubernetes node's `kubelet` will evict `best-effort` and `burstable` QoS pods before terminating `guaranteed` QoS pods.\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring WebLogic pods with memory and CPU requests and limits, and furthermore setting requests equal to their respective limits in order to ensure a `guaranteed` QoS.\n+{{% /notice %}}\n+\n+In newer version of Kubernetes, it is possible to fine tune scheduling and eviction policies using [Pod Priority Preemption](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/). TBD Ryan - is it possible to change the priority class of a pod?\n+\n+### Java heap size and memory resource considerations\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring Java heap sizes for WebLogic pods instead of relying on defaults.\n+{{% /notice %}}\n+\n+#### Overview\n+\n+It's extremely important to set correct heap sizes, memory requests, and memory limits for WebLogic JVMs and Pods. \n+\n+A WebLogic JVM heap must be sufficiently sized to run its applications and services, but should not be sized too large so as not to waste memory resources.\n+\n+A pod memory limit must be sufficiently sized to accommodate the configured heap (and off-heap) requirements, but  not too big to waste memory resources. If a JVM's memory usage (sum of heap and native memory) exceeds its pod's limit, then the JVM process will be abruptly killed due to an out-of-memory error and the WebLogic container will consequently automatically restart due to a liveness probe failure.  \n+\n+{{% notice warning %}}\n+If resource requests and resource limits are set too high, then your pods may not be scheduled due to lack of node resources, will unnecessarily use up CPU shared resources that could be used by other pods, or may prevent other pods from running.\n+{{% /notice %}}\n+\n+#### Default heap sizes\n+\n+With the latest Java versions, Java 8 update 191 and onwards or Java 11, then if you don't configure a heap size (no '-Xms' or '-Xms') the default heap size is dynamically determined:\n+- If you configure the memory limit for a container, then the JVM default maximum heap size will be 25% (1/4th) of container memory limit and the default minimum heap size will be 1.56% (1/64th) of the limit value. \n+\n+  The default JVM heap settings in this case are often too conservative because the WebLogic JVM is the only major process running in the container.\n+\n+- If no memory limit is configured, then the JVM default maximum heap size will be  25% (1/4th) of the its node's machine RAM and the default minimum heap size will be 1.56% (1/64th) of the RAM.\n+\n+  The default JVM heap settings in this case can have undesirable behavior, including using unnecessary amounts of memory to the point where it might affect other pods that run on the same node.\n+\n+#### Configuring heap size\n+\n+If you specify pod memory limits, Oracle recommends configuring WebLogic Server heap sizes as a percentage. The JVM will interpret the percentage as a fraction of the limit. This is done using the JVM `-XX:MinRAMPercentage` and `-XX:MaxRAMPercentage` options in the `USER_MEM_ARGS` [domain resource environment variable]({{< relref \"/userguide/managing-domains/domain-resource#jvm-memory-and-java-option-environment-variables\" >}}).  For example:\n+\n+```\n+  spec:\n+    resources:\n+      env:\n+      - name: USER_MEM_ARGS\n+        value: \"--XX:MinRAMPercentage=25.0 --XX:MaxRAMPercentage=50.0 -Djava.security.egd=file:/dev/./urandom\"\n+```\n+\n+Additionally there's also a node-manager process that's running in the same container as the WebLogic Server which has its own heap and off-heap requirements. Its heap is tuned by using `-Xms` and `-Xmx` in the `NODEMGR_MEM_ARGS` environment variable. Oracle recommends setting the node manager heap memory to fixed sizes, instead of percentages, where [the default tuning]({{< relref \"/userguide/managing-domains/domain-resource#jvm-memory-and-java-option-environment-variables\" >}}) is usually sufficient.\n+\n+{{% notice warning %}}\n+If you set `USER_MEM_ARGS` or `NODEMGR_MEM_ARGS` in your domain resource, then it is usually recommended to include `-Djava.security.egd=file:/dev/./urandom` in order to speedup boot times on systems with low entropy. This setting is included in the respective defaults for these two environment variables.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAzMTQ4Mg=="}, "originalCommit": {"oid": "0477ff166bc9e02dd33e555ab283d981f0eafeef"}, "originalPosition": 112}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MjQ1MzkwOnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQyMzoyMTozMlrOGrRs1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxOToxNDoxOFrOGryROA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAzMTk1Nw==", "bodyText": "It would be more helpful to show an example of the error when a Pod is never scheduled (it will be stuck in \"Pending\").", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r448031957", "createdAt": "2020-06-30T23:21:32Z", "author": {"login": "rjeberhard"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,162 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Overview](#overview)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. It's important that the container has enough resources in order for WebLogic to run efficiently.\n+\n+If a pod is scheduled on a node with limited resources, then it's possible for node to run out of memory or CPU resources, and for the pod's applications to stop working properly or have degraded performance. It's also possible for a rogue application to use all of a node's available memory and/or CPU, which makes other containers running on the same node unresponsive. The same problem can happen if an application has a memory leak.\n+\n+You can solve these problems by configuring resource requests and limits for your pods. A resource limit prevents a pod from using more than its share of a resource. Thus, it improves reliability, stability, and helps with hardware capacity planning. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: \n+\n+```\n+  spec:\n+    serverPod:\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+      limits:\n+        cpu: \"2\"\n+        memory: \"2Gi\"\n+```\n+\n+Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An `m` suffix in a CPU attribute indicates 'milli-CPU', so `250m` is 25% of a CPU. \n+\n+Memory can be expressed in various units, where one `Mi` is one IEC unit mega-byte (1024^2), and one `Gi` is one IEC unit giga-byte (1024^3).\n+\n+See also [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/) in the Kubernetes documentation.\n+\n+### Determining pod Quality Of Service\n+\n+A pod's Quality of Service (QoS) is based on whether it's configured with resource requests and limits:\n+\n+- **Best Effort QoS** (lowest priority): If you don't configure requests and limits for a pod, then the pod is given a `best-effort` QoS. In cases where a node runs out of non-shareable resources, a Kubernetes `kubelet` default out-of-resource eviction policy evicts running pods with the `best-effort` QoS first.\n+\n+- **Burstable QoS** (medium priority): If you configure both resource requests and limits for a pod, and set the requests to be less than their respective limits, then the pod will be given a `burstable` QoS. Similarly, if you only configure resource requests (without limits) for a pod, then the pod QoS is also `burstable`. If a node runs out of non-shareable resources, the node's `kubelet` will evict `burstable` pods only when there are no more running `best-effort` pods.\n+\n+- **Guaranteed QoS** (highest priority): If you set a pod's requests and the limits to equal values, then the pod will have a `guaranteed` QoS. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, then a Kubernetes node's `kubelet` will evict `best-effort` and `burstable` QoS pods before terminating `guaranteed` QoS pods.\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring WebLogic pods with memory and CPU requests and limits, and furthermore setting requests equal to their respective limits in order to ensure a `guaranteed` QoS.\n+{{% /notice %}}\n+\n+In newer version of Kubernetes, it is possible to fine tune scheduling and eviction policies using [Pod Priority Preemption](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/). TBD Ryan - is it possible to change the priority class of a pod?\n+\n+### Java heap size and memory resource considerations\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring Java heap sizes for WebLogic pods instead of relying on defaults.\n+{{% /notice %}}\n+\n+#### Overview\n+\n+It's extremely important to set correct heap sizes, memory requests, and memory limits for WebLogic JVMs and Pods. \n+\n+A WebLogic JVM heap must be sufficiently sized to run its applications and services, but should not be sized too large so as not to waste memory resources.\n+\n+A pod memory limit must be sufficiently sized to accommodate the configured heap (and off-heap) requirements, but  not too big to waste memory resources. If a JVM's memory usage (sum of heap and native memory) exceeds its pod's limit, then the JVM process will be abruptly killed due to an out-of-memory error and the WebLogic container will consequently automatically restart due to a liveness probe failure.  \n+\n+{{% notice warning %}}\n+If resource requests and resource limits are set too high, then your pods may not be scheduled due to lack of node resources, will unnecessarily use up CPU shared resources that could be used by other pods, or may prevent other pods from running.\n+{{% /notice %}}\n+\n+#### Default heap sizes\n+\n+With the latest Java versions, Java 8 update 191 and onwards or Java 11, then if you don't configure a heap size (no '-Xms' or '-Xms') the default heap size is dynamically determined:\n+- If you configure the memory limit for a container, then the JVM default maximum heap size will be 25% (1/4th) of container memory limit and the default minimum heap size will be 1.56% (1/64th) of the limit value. \n+\n+  The default JVM heap settings in this case are often too conservative because the WebLogic JVM is the only major process running in the container.\n+\n+- If no memory limit is configured, then the JVM default maximum heap size will be  25% (1/4th) of the its node's machine RAM and the default minimum heap size will be 1.56% (1/64th) of the RAM.\n+\n+  The default JVM heap settings in this case can have undesirable behavior, including using unnecessary amounts of memory to the point where it might affect other pods that run on the same node.\n+\n+#### Configuring heap size\n+\n+If you specify pod memory limits, Oracle recommends configuring WebLogic Server heap sizes as a percentage. The JVM will interpret the percentage as a fraction of the limit. This is done using the JVM `-XX:MinRAMPercentage` and `-XX:MaxRAMPercentage` options in the `USER_MEM_ARGS` [domain resource environment variable]({{< relref \"/userguide/managing-domains/domain-resource#jvm-memory-and-java-option-environment-variables\" >}}).  For example:\n+\n+```\n+  spec:\n+    resources:\n+      env:\n+      - name: USER_MEM_ARGS\n+        value: \"--XX:MinRAMPercentage=25.0 --XX:MaxRAMPercentage=50.0 -Djava.security.egd=file:/dev/./urandom\"\n+```\n+\n+Additionally there's also a node-manager process that's running in the same container as the WebLogic Server which has its own heap and off-heap requirements. Its heap is tuned by using `-Xms` and `-Xmx` in the `NODEMGR_MEM_ARGS` environment variable. Oracle recommends setting the node manager heap memory to fixed sizes, instead of percentages, where [the default tuning]({{< relref \"/userguide/managing-domains/domain-resource#jvm-memory-and-java-option-environment-variables\" >}}) is usually sufficient.\n+\n+{{% notice warning %}}\n+If you set `USER_MEM_ARGS` or `NODEMGR_MEM_ARGS` in your domain resource, then it is usually recommended to include `-Djava.security.egd=file:/dev/./urandom` in order to speedup boot times on systems with low entropy. This setting is included in the respective defaults for these two environment variables.\n+q\n+{{% /notice %}}\n+\n+In some cases, you might only want to configure memory resource requests but not configure memory resource limits. In such scenarios, you can use the traditional fixed heap size settings (`-Xms` and `-Xmx`) in your WebLogic Server `USER_MEM_ARGS` instead of the percentage settings (`-XX:MinRAMPercentage` and `-XX:MaxRAMPercentage`).\n+\n+### CPU resource considerations\n+\n+It's important to set both a CPU request and a limit for WebLogic Server pods. This ensures that all WebLogic server pods have enough CPU resources, and, as discussed earlier, if the request and limit are set to the same value, then they get a `guaranteed` QoS. A `guaranteed` QoS ensures the pods are handled with a higher priority during scheduling and so are the least likely to be evicted.\n+\n+If a CPU request and limit are _not_ configured for a WebLogic Server pod:\n+- The pod can end up using all CPU resources available on its node and starve other containers from using shareable CPU cycles. \n+\n+- The WebLogic server JVM may choose an unsuitable garbage collection (GC) strategy.\n+\n+- A WebLogic Server self-tuning work-manager may incorrectly optimize the number of threads it allocates for the default thread pool. \n+\n+It's also important to keep in mind that if you set a value of CPU core count that's larger than core count of your biggest node, then the pod will never be scheduled. Let's say you have a pod that needs 4 cores but you have a kubernetes cluster that's comprised of 2 core VMs. In this case, your pod will never be scheduled.  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0477ff166bc9e02dd33e555ab283d981f0eafeef"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU2NTU2MA==", "bodyText": "Added an example showing pod in Pending state.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r448565560", "createdAt": "2020-07-01T19:14:18Z", "author": {"login": "ankedia"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,162 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Overview](#overview)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. It's important that the container has enough resources in order for WebLogic to run efficiently.\n+\n+If a pod is scheduled on a node with limited resources, then it's possible for node to run out of memory or CPU resources, and for the pod's applications to stop working properly or have degraded performance. It's also possible for a rogue application to use all of a node's available memory and/or CPU, which makes other containers running on the same node unresponsive. The same problem can happen if an application has a memory leak.\n+\n+You can solve these problems by configuring resource requests and limits for your pods. A resource limit prevents a pod from using more than its share of a resource. Thus, it improves reliability, stability, and helps with hardware capacity planning. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: \n+\n+```\n+  spec:\n+    serverPod:\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+      limits:\n+        cpu: \"2\"\n+        memory: \"2Gi\"\n+```\n+\n+Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An `m` suffix in a CPU attribute indicates 'milli-CPU', so `250m` is 25% of a CPU. \n+\n+Memory can be expressed in various units, where one `Mi` is one IEC unit mega-byte (1024^2), and one `Gi` is one IEC unit giga-byte (1024^3).\n+\n+See also [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/) in the Kubernetes documentation.\n+\n+### Determining pod Quality Of Service\n+\n+A pod's Quality of Service (QoS) is based on whether it's configured with resource requests and limits:\n+\n+- **Best Effort QoS** (lowest priority): If you don't configure requests and limits for a pod, then the pod is given a `best-effort` QoS. In cases where a node runs out of non-shareable resources, a Kubernetes `kubelet` default out-of-resource eviction policy evicts running pods with the `best-effort` QoS first.\n+\n+- **Burstable QoS** (medium priority): If you configure both resource requests and limits for a pod, and set the requests to be less than their respective limits, then the pod will be given a `burstable` QoS. Similarly, if you only configure resource requests (without limits) for a pod, then the pod QoS is also `burstable`. If a node runs out of non-shareable resources, the node's `kubelet` will evict `burstable` pods only when there are no more running `best-effort` pods.\n+\n+- **Guaranteed QoS** (highest priority): If you set a pod's requests and the limits to equal values, then the pod will have a `guaranteed` QoS. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, then a Kubernetes node's `kubelet` will evict `best-effort` and `burstable` QoS pods before terminating `guaranteed` QoS pods.\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring WebLogic pods with memory and CPU requests and limits, and furthermore setting requests equal to their respective limits in order to ensure a `guaranteed` QoS.\n+{{% /notice %}}\n+\n+In newer version of Kubernetes, it is possible to fine tune scheduling and eviction policies using [Pod Priority Preemption](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/). TBD Ryan - is it possible to change the priority class of a pod?\n+\n+### Java heap size and memory resource considerations\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring Java heap sizes for WebLogic pods instead of relying on defaults.\n+{{% /notice %}}\n+\n+#### Overview\n+\n+It's extremely important to set correct heap sizes, memory requests, and memory limits for WebLogic JVMs and Pods. \n+\n+A WebLogic JVM heap must be sufficiently sized to run its applications and services, but should not be sized too large so as not to waste memory resources.\n+\n+A pod memory limit must be sufficiently sized to accommodate the configured heap (and off-heap) requirements, but  not too big to waste memory resources. If a JVM's memory usage (sum of heap and native memory) exceeds its pod's limit, then the JVM process will be abruptly killed due to an out-of-memory error and the WebLogic container will consequently automatically restart due to a liveness probe failure.  \n+\n+{{% notice warning %}}\n+If resource requests and resource limits are set too high, then your pods may not be scheduled due to lack of node resources, will unnecessarily use up CPU shared resources that could be used by other pods, or may prevent other pods from running.\n+{{% /notice %}}\n+\n+#### Default heap sizes\n+\n+With the latest Java versions, Java 8 update 191 and onwards or Java 11, then if you don't configure a heap size (no '-Xms' or '-Xms') the default heap size is dynamically determined:\n+- If you configure the memory limit for a container, then the JVM default maximum heap size will be 25% (1/4th) of container memory limit and the default minimum heap size will be 1.56% (1/64th) of the limit value. \n+\n+  The default JVM heap settings in this case are often too conservative because the WebLogic JVM is the only major process running in the container.\n+\n+- If no memory limit is configured, then the JVM default maximum heap size will be  25% (1/4th) of the its node's machine RAM and the default minimum heap size will be 1.56% (1/64th) of the RAM.\n+\n+  The default JVM heap settings in this case can have undesirable behavior, including using unnecessary amounts of memory to the point where it might affect other pods that run on the same node.\n+\n+#### Configuring heap size\n+\n+If you specify pod memory limits, Oracle recommends configuring WebLogic Server heap sizes as a percentage. The JVM will interpret the percentage as a fraction of the limit. This is done using the JVM `-XX:MinRAMPercentage` and `-XX:MaxRAMPercentage` options in the `USER_MEM_ARGS` [domain resource environment variable]({{< relref \"/userguide/managing-domains/domain-resource#jvm-memory-and-java-option-environment-variables\" >}}).  For example:\n+\n+```\n+  spec:\n+    resources:\n+      env:\n+      - name: USER_MEM_ARGS\n+        value: \"--XX:MinRAMPercentage=25.0 --XX:MaxRAMPercentage=50.0 -Djava.security.egd=file:/dev/./urandom\"\n+```\n+\n+Additionally there's also a node-manager process that's running in the same container as the WebLogic Server which has its own heap and off-heap requirements. Its heap is tuned by using `-Xms` and `-Xmx` in the `NODEMGR_MEM_ARGS` environment variable. Oracle recommends setting the node manager heap memory to fixed sizes, instead of percentages, where [the default tuning]({{< relref \"/userguide/managing-domains/domain-resource#jvm-memory-and-java-option-environment-variables\" >}}) is usually sufficient.\n+\n+{{% notice warning %}}\n+If you set `USER_MEM_ARGS` or `NODEMGR_MEM_ARGS` in your domain resource, then it is usually recommended to include `-Djava.security.egd=file:/dev/./urandom` in order to speedup boot times on systems with low entropy. This setting is included in the respective defaults for these two environment variables.\n+q\n+{{% /notice %}}\n+\n+In some cases, you might only want to configure memory resource requests but not configure memory resource limits. In such scenarios, you can use the traditional fixed heap size settings (`-Xms` and `-Xmx`) in your WebLogic Server `USER_MEM_ARGS` instead of the percentage settings (`-XX:MinRAMPercentage` and `-XX:MaxRAMPercentage`).\n+\n+### CPU resource considerations\n+\n+It's important to set both a CPU request and a limit for WebLogic Server pods. This ensures that all WebLogic server pods have enough CPU resources, and, as discussed earlier, if the request and limit are set to the same value, then they get a `guaranteed` QoS. A `guaranteed` QoS ensures the pods are handled with a higher priority during scheduling and so are the least likely to be evicted.\n+\n+If a CPU request and limit are _not_ configured for a WebLogic Server pod:\n+- The pod can end up using all CPU resources available on its node and starve other containers from using shareable CPU cycles. \n+\n+- The WebLogic server JVM may choose an unsuitable garbage collection (GC) strategy.\n+\n+- A WebLogic Server self-tuning work-manager may incorrectly optimize the number of threads it allocates for the default thread pool. \n+\n+It's also important to keep in mind that if you set a value of CPU core count that's larger than core count of your biggest node, then the pod will never be scheduled. Let's say you have a pod that needs 4 cores but you have a kubernetes cluster that's comprised of 2 core VMs. In this case, your pod will never be scheduled.  ", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAzMTk1Nw=="}, "originalCommit": {"oid": "0477ff166bc9e02dd33e555ab283d981f0eafeef"}, "originalPosition": 129}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MjQ1NTYxOnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQyMzoyMjozMVrOGrRt9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQxOToyNjoyNlrOGrynDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAzMjI0Ng==", "bodyText": "This is a good idea... You can also link to OKE monitoring.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r448032246", "createdAt": "2020-06-30T23:22:31Z", "author": {"login": "rjeberhard"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,162 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Overview](#overview)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. It's important that the container has enough resources in order for WebLogic to run efficiently.\n+\n+If a pod is scheduled on a node with limited resources, then it's possible for node to run out of memory or CPU resources, and for the pod's applications to stop working properly or have degraded performance. It's also possible for a rogue application to use all of a node's available memory and/or CPU, which makes other containers running on the same node unresponsive. The same problem can happen if an application has a memory leak.\n+\n+You can solve these problems by configuring resource requests and limits for your pods. A resource limit prevents a pod from using more than its share of a resource. Thus, it improves reliability, stability, and helps with hardware capacity planning. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: \n+\n+```\n+  spec:\n+    serverPod:\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+      limits:\n+        cpu: \"2\"\n+        memory: \"2Gi\"\n+```\n+\n+Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An `m` suffix in a CPU attribute indicates 'milli-CPU', so `250m` is 25% of a CPU. \n+\n+Memory can be expressed in various units, where one `Mi` is one IEC unit mega-byte (1024^2), and one `Gi` is one IEC unit giga-byte (1024^3).\n+\n+See also [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/) in the Kubernetes documentation.\n+\n+### Determining pod Quality Of Service\n+\n+A pod's Quality of Service (QoS) is based on whether it's configured with resource requests and limits:\n+\n+- **Best Effort QoS** (lowest priority): If you don't configure requests and limits for a pod, then the pod is given a `best-effort` QoS. In cases where a node runs out of non-shareable resources, a Kubernetes `kubelet` default out-of-resource eviction policy evicts running pods with the `best-effort` QoS first.\n+\n+- **Burstable QoS** (medium priority): If you configure both resource requests and limits for a pod, and set the requests to be less than their respective limits, then the pod will be given a `burstable` QoS. Similarly, if you only configure resource requests (without limits) for a pod, then the pod QoS is also `burstable`. If a node runs out of non-shareable resources, the node's `kubelet` will evict `burstable` pods only when there are no more running `best-effort` pods.\n+\n+- **Guaranteed QoS** (highest priority): If you set a pod's requests and the limits to equal values, then the pod will have a `guaranteed` QoS. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, then a Kubernetes node's `kubelet` will evict `best-effort` and `burstable` QoS pods before terminating `guaranteed` QoS pods.\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring WebLogic pods with memory and CPU requests and limits, and furthermore setting requests equal to their respective limits in order to ensure a `guaranteed` QoS.\n+{{% /notice %}}\n+\n+In newer version of Kubernetes, it is possible to fine tune scheduling and eviction policies using [Pod Priority Preemption](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/). TBD Ryan - is it possible to change the priority class of a pod?\n+\n+### Java heap size and memory resource considerations\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring Java heap sizes for WebLogic pods instead of relying on defaults.\n+{{% /notice %}}\n+\n+#### Overview\n+\n+It's extremely important to set correct heap sizes, memory requests, and memory limits for WebLogic JVMs and Pods. \n+\n+A WebLogic JVM heap must be sufficiently sized to run its applications and services, but should not be sized too large so as not to waste memory resources.\n+\n+A pod memory limit must be sufficiently sized to accommodate the configured heap (and off-heap) requirements, but  not too big to waste memory resources. If a JVM's memory usage (sum of heap and native memory) exceeds its pod's limit, then the JVM process will be abruptly killed due to an out-of-memory error and the WebLogic container will consequently automatically restart due to a liveness probe failure.  \n+\n+{{% notice warning %}}\n+If resource requests and resource limits are set too high, then your pods may not be scheduled due to lack of node resources, will unnecessarily use up CPU shared resources that could be used by other pods, or may prevent other pods from running.\n+{{% /notice %}}\n+\n+#### Default heap sizes\n+\n+With the latest Java versions, Java 8 update 191 and onwards or Java 11, then if you don't configure a heap size (no '-Xms' or '-Xms') the default heap size is dynamically determined:\n+- If you configure the memory limit for a container, then the JVM default maximum heap size will be 25% (1/4th) of container memory limit and the default minimum heap size will be 1.56% (1/64th) of the limit value. \n+\n+  The default JVM heap settings in this case are often too conservative because the WebLogic JVM is the only major process running in the container.\n+\n+- If no memory limit is configured, then the JVM default maximum heap size will be  25% (1/4th) of the its node's machine RAM and the default minimum heap size will be 1.56% (1/64th) of the RAM.\n+\n+  The default JVM heap settings in this case can have undesirable behavior, including using unnecessary amounts of memory to the point where it might affect other pods that run on the same node.\n+\n+#### Configuring heap size\n+\n+If you specify pod memory limits, Oracle recommends configuring WebLogic Server heap sizes as a percentage. The JVM will interpret the percentage as a fraction of the limit. This is done using the JVM `-XX:MinRAMPercentage` and `-XX:MaxRAMPercentage` options in the `USER_MEM_ARGS` [domain resource environment variable]({{< relref \"/userguide/managing-domains/domain-resource#jvm-memory-and-java-option-environment-variables\" >}}).  For example:\n+\n+```\n+  spec:\n+    resources:\n+      env:\n+      - name: USER_MEM_ARGS\n+        value: \"--XX:MinRAMPercentage=25.0 --XX:MaxRAMPercentage=50.0 -Djava.security.egd=file:/dev/./urandom\"\n+```\n+\n+Additionally there's also a node-manager process that's running in the same container as the WebLogic Server which has its own heap and off-heap requirements. Its heap is tuned by using `-Xms` and `-Xmx` in the `NODEMGR_MEM_ARGS` environment variable. Oracle recommends setting the node manager heap memory to fixed sizes, instead of percentages, where [the default tuning]({{< relref \"/userguide/managing-domains/domain-resource#jvm-memory-and-java-option-environment-variables\" >}}) is usually sufficient.\n+\n+{{% notice warning %}}\n+If you set `USER_MEM_ARGS` or `NODEMGR_MEM_ARGS` in your domain resource, then it is usually recommended to include `-Djava.security.egd=file:/dev/./urandom` in order to speedup boot times on systems with low entropy. This setting is included in the respective defaults for these two environment variables.\n+q\n+{{% /notice %}}\n+\n+In some cases, you might only want to configure memory resource requests but not configure memory resource limits. In such scenarios, you can use the traditional fixed heap size settings (`-Xms` and `-Xmx`) in your WebLogic Server `USER_MEM_ARGS` instead of the percentage settings (`-XX:MinRAMPercentage` and `-XX:MaxRAMPercentage`).\n+\n+### CPU resource considerations\n+\n+It's important to set both a CPU request and a limit for WebLogic Server pods. This ensures that all WebLogic server pods have enough CPU resources, and, as discussed earlier, if the request and limit are set to the same value, then they get a `guaranteed` QoS. A `guaranteed` QoS ensures the pods are handled with a higher priority during scheduling and so are the least likely to be evicted.\n+\n+If a CPU request and limit are _not_ configured for a WebLogic Server pod:\n+- The pod can end up using all CPU resources available on its node and starve other containers from using shareable CPU cycles. \n+\n+- The WebLogic server JVM may choose an unsuitable garbage collection (GC) strategy.\n+\n+- A WebLogic Server self-tuning work-manager may incorrectly optimize the number of threads it allocates for the default thread pool. \n+\n+It's also important to keep in mind that if you set a value of CPU core count that's larger than core count of your biggest node, then the pod will never be scheduled. Let's say you have a pod that needs 4 cores but you have a kubernetes cluster that's comprised of 2 core VMs. In this case, your pod will never be scheduled.  \n+\n+### Operator sample heap and resource configuration\n+\n+The operator samples configure non-default minimum and maximum heap sizes for WebLogic server JVMs of at least 256MB and 512MB respectively. You can edit a sample's template or domain resource `resources.env` `USER_MEM_ARGS` to have different values. See [Configuring heap size](#configuring-heap-size).\n+\n+Similarly, the operator samples configure CPU and memory resource requests to at least `250m` and `768Mi` respectively.\n+\n+There's no memory or CPU limit configured by default in samples and so the default QoS for sample WebLogic server pod's is `Burstable`. \n+\n+If you wish to set resource requests or limits differently on a sample domain resource or domain resource template, see [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource). Or for samples that generate their domain resource using an 'inputs' file, see the `serverPodMemoryRequest`, `serverPodMemoryLimit`, `serverPodCpuRequest`, and `serverPodCpuLimit` parameters in the sample's `create-domain.sh` input file.\n+\n+### Configuring CPU affinity\n+\n+A Kubernetes hosted WebLogic server may exhibit high lock contention in comparison to an on-premise deployment. This lock contention may be due to lack of CPU cache affinity and/or scheduling latency when workloads move between different CPU cores.  \n+\n+In an on-premise deployment, CPU cache affinity, and therefore reduced lock contention, can be achieved by binding WLS java process to particular CPU core(s) (using the `taskset` command).\n+\n+In a Kubernetes deployment, similar cache affinity can be achieved by doing the following:\n+- Ensuring a pod's CPU resource request and limit are set and equal (to ensure a `guaranteed` QoS).\n+- Configuring the `kubelet` CPU manager policy to be `static` (the default is `none`). See [Control CPU Management Policies on the Node](https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies). \n+Note that some Kubernetes environments may not allow changing the CPU management policy.\n+\n+### Measuring JVM heap, pod CPU, and pod memory\n+\n+TBD Discuss/link to Grafana/Prometheus.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0477ff166bc9e02dd33e555ab283d981f0eafeef"}, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU2NjQ1OQ==", "bodyText": "We added links to \"Monitoring a SOA domain\" which provides steps for setting up Prometheus and Grafana in order to monitor pod memory and CPU resources and setting up WebLogic monitoring exporter for JVM heap monitoring.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r448566459", "createdAt": "2020-07-01T19:16:14Z", "author": {"login": "ankedia"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,162 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Overview](#overview)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. It's important that the container has enough resources in order for WebLogic to run efficiently.\n+\n+If a pod is scheduled on a node with limited resources, then it's possible for node to run out of memory or CPU resources, and for the pod's applications to stop working properly or have degraded performance. It's also possible for a rogue application to use all of a node's available memory and/or CPU, which makes other containers running on the same node unresponsive. The same problem can happen if an application has a memory leak.\n+\n+You can solve these problems by configuring resource requests and limits for your pods. A resource limit prevents a pod from using more than its share of a resource. Thus, it improves reliability, stability, and helps with hardware capacity planning. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: \n+\n+```\n+  spec:\n+    serverPod:\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+      limits:\n+        cpu: \"2\"\n+        memory: \"2Gi\"\n+```\n+\n+Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An `m` suffix in a CPU attribute indicates 'milli-CPU', so `250m` is 25% of a CPU. \n+\n+Memory can be expressed in various units, where one `Mi` is one IEC unit mega-byte (1024^2), and one `Gi` is one IEC unit giga-byte (1024^3).\n+\n+See also [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/) in the Kubernetes documentation.\n+\n+### Determining pod Quality Of Service\n+\n+A pod's Quality of Service (QoS) is based on whether it's configured with resource requests and limits:\n+\n+- **Best Effort QoS** (lowest priority): If you don't configure requests and limits for a pod, then the pod is given a `best-effort` QoS. In cases where a node runs out of non-shareable resources, a Kubernetes `kubelet` default out-of-resource eviction policy evicts running pods with the `best-effort` QoS first.\n+\n+- **Burstable QoS** (medium priority): If you configure both resource requests and limits for a pod, and set the requests to be less than their respective limits, then the pod will be given a `burstable` QoS. Similarly, if you only configure resource requests (without limits) for a pod, then the pod QoS is also `burstable`. If a node runs out of non-shareable resources, the node's `kubelet` will evict `burstable` pods only when there are no more running `best-effort` pods.\n+\n+- **Guaranteed QoS** (highest priority): If you set a pod's requests and the limits to equal values, then the pod will have a `guaranteed` QoS. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, then a Kubernetes node's `kubelet` will evict `best-effort` and `burstable` QoS pods before terminating `guaranteed` QoS pods.\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring WebLogic pods with memory and CPU requests and limits, and furthermore setting requests equal to their respective limits in order to ensure a `guaranteed` QoS.\n+{{% /notice %}}\n+\n+In newer version of Kubernetes, it is possible to fine tune scheduling and eviction policies using [Pod Priority Preemption](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/). TBD Ryan - is it possible to change the priority class of a pod?\n+\n+### Java heap size and memory resource considerations\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring Java heap sizes for WebLogic pods instead of relying on defaults.\n+{{% /notice %}}\n+\n+#### Overview\n+\n+It's extremely important to set correct heap sizes, memory requests, and memory limits for WebLogic JVMs and Pods. \n+\n+A WebLogic JVM heap must be sufficiently sized to run its applications and services, but should not be sized too large so as not to waste memory resources.\n+\n+A pod memory limit must be sufficiently sized to accommodate the configured heap (and off-heap) requirements, but  not too big to waste memory resources. If a JVM's memory usage (sum of heap and native memory) exceeds its pod's limit, then the JVM process will be abruptly killed due to an out-of-memory error and the WebLogic container will consequently automatically restart due to a liveness probe failure.  \n+\n+{{% notice warning %}}\n+If resource requests and resource limits are set too high, then your pods may not be scheduled due to lack of node resources, will unnecessarily use up CPU shared resources that could be used by other pods, or may prevent other pods from running.\n+{{% /notice %}}\n+\n+#### Default heap sizes\n+\n+With the latest Java versions, Java 8 update 191 and onwards or Java 11, then if you don't configure a heap size (no '-Xms' or '-Xms') the default heap size is dynamically determined:\n+- If you configure the memory limit for a container, then the JVM default maximum heap size will be 25% (1/4th) of container memory limit and the default minimum heap size will be 1.56% (1/64th) of the limit value. \n+\n+  The default JVM heap settings in this case are often too conservative because the WebLogic JVM is the only major process running in the container.\n+\n+- If no memory limit is configured, then the JVM default maximum heap size will be  25% (1/4th) of the its node's machine RAM and the default minimum heap size will be 1.56% (1/64th) of the RAM.\n+\n+  The default JVM heap settings in this case can have undesirable behavior, including using unnecessary amounts of memory to the point where it might affect other pods that run on the same node.\n+\n+#### Configuring heap size\n+\n+If you specify pod memory limits, Oracle recommends configuring WebLogic Server heap sizes as a percentage. The JVM will interpret the percentage as a fraction of the limit. This is done using the JVM `-XX:MinRAMPercentage` and `-XX:MaxRAMPercentage` options in the `USER_MEM_ARGS` [domain resource environment variable]({{< relref \"/userguide/managing-domains/domain-resource#jvm-memory-and-java-option-environment-variables\" >}}).  For example:\n+\n+```\n+  spec:\n+    resources:\n+      env:\n+      - name: USER_MEM_ARGS\n+        value: \"--XX:MinRAMPercentage=25.0 --XX:MaxRAMPercentage=50.0 -Djava.security.egd=file:/dev/./urandom\"\n+```\n+\n+Additionally there's also a node-manager process that's running in the same container as the WebLogic Server which has its own heap and off-heap requirements. Its heap is tuned by using `-Xms` and `-Xmx` in the `NODEMGR_MEM_ARGS` environment variable. Oracle recommends setting the node manager heap memory to fixed sizes, instead of percentages, where [the default tuning]({{< relref \"/userguide/managing-domains/domain-resource#jvm-memory-and-java-option-environment-variables\" >}}) is usually sufficient.\n+\n+{{% notice warning %}}\n+If you set `USER_MEM_ARGS` or `NODEMGR_MEM_ARGS` in your domain resource, then it is usually recommended to include `-Djava.security.egd=file:/dev/./urandom` in order to speedup boot times on systems with low entropy. This setting is included in the respective defaults for these two environment variables.\n+q\n+{{% /notice %}}\n+\n+In some cases, you might only want to configure memory resource requests but not configure memory resource limits. In such scenarios, you can use the traditional fixed heap size settings (`-Xms` and `-Xmx`) in your WebLogic Server `USER_MEM_ARGS` instead of the percentage settings (`-XX:MinRAMPercentage` and `-XX:MaxRAMPercentage`).\n+\n+### CPU resource considerations\n+\n+It's important to set both a CPU request and a limit for WebLogic Server pods. This ensures that all WebLogic server pods have enough CPU resources, and, as discussed earlier, if the request and limit are set to the same value, then they get a `guaranteed` QoS. A `guaranteed` QoS ensures the pods are handled with a higher priority during scheduling and so are the least likely to be evicted.\n+\n+If a CPU request and limit are _not_ configured for a WebLogic Server pod:\n+- The pod can end up using all CPU resources available on its node and starve other containers from using shareable CPU cycles. \n+\n+- The WebLogic server JVM may choose an unsuitable garbage collection (GC) strategy.\n+\n+- A WebLogic Server self-tuning work-manager may incorrectly optimize the number of threads it allocates for the default thread pool. \n+\n+It's also important to keep in mind that if you set a value of CPU core count that's larger than core count of your biggest node, then the pod will never be scheduled. Let's say you have a pod that needs 4 cores but you have a kubernetes cluster that's comprised of 2 core VMs. In this case, your pod will never be scheduled.  \n+\n+### Operator sample heap and resource configuration\n+\n+The operator samples configure non-default minimum and maximum heap sizes for WebLogic server JVMs of at least 256MB and 512MB respectively. You can edit a sample's template or domain resource `resources.env` `USER_MEM_ARGS` to have different values. See [Configuring heap size](#configuring-heap-size).\n+\n+Similarly, the operator samples configure CPU and memory resource requests to at least `250m` and `768Mi` respectively.\n+\n+There's no memory or CPU limit configured by default in samples and so the default QoS for sample WebLogic server pod's is `Burstable`. \n+\n+If you wish to set resource requests or limits differently on a sample domain resource or domain resource template, see [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource). Or for samples that generate their domain resource using an 'inputs' file, see the `serverPodMemoryRequest`, `serverPodMemoryLimit`, `serverPodCpuRequest`, and `serverPodCpuLimit` parameters in the sample's `create-domain.sh` input file.\n+\n+### Configuring CPU affinity\n+\n+A Kubernetes hosted WebLogic server may exhibit high lock contention in comparison to an on-premise deployment. This lock contention may be due to lack of CPU cache affinity and/or scheduling latency when workloads move between different CPU cores.  \n+\n+In an on-premise deployment, CPU cache affinity, and therefore reduced lock contention, can be achieved by binding WLS java process to particular CPU core(s) (using the `taskset` command).\n+\n+In a Kubernetes deployment, similar cache affinity can be achieved by doing the following:\n+- Ensuring a pod's CPU resource request and limit are set and equal (to ensure a `guaranteed` QoS).\n+- Configuring the `kubelet` CPU manager policy to be `static` (the default is `none`). See [Control CPU Management Policies on the Node](https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies). \n+Note that some Kubernetes environments may not allow changing the CPU management policy.\n+\n+### Measuring JVM heap, pod CPU, and pod memory\n+\n+TBD Discuss/link to Grafana/Prometheus.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAzMjI0Ng=="}, "originalCommit": {"oid": "0477ff166bc9e02dd33e555ab283d981f0eafeef"}, "originalPosition": 154}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODU3MTE0OA==", "bodyText": "Also, added links to \"Tools for Monitoring Resources\" in the Kubernetes documentation. We didn't find an option to monitor pod level resources in OKE monitoring. There's a section about monitoring OKE cluster and section about node level monitoring in OCI IaaS documentation.  Please let me know if we need to link a particular section in OKE documentation. Thanks.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r448571148", "createdAt": "2020-07-01T19:26:26Z", "author": {"login": "ankedia"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,162 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Overview](#overview)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. It's important that the container has enough resources in order for WebLogic to run efficiently.\n+\n+If a pod is scheduled on a node with limited resources, then it's possible for node to run out of memory or CPU resources, and for the pod's applications to stop working properly or have degraded performance. It's also possible for a rogue application to use all of a node's available memory and/or CPU, which makes other containers running on the same node unresponsive. The same problem can happen if an application has a memory leak.\n+\n+You can solve these problems by configuring resource requests and limits for your pods. A resource limit prevents a pod from using more than its share of a resource. Thus, it improves reliability, stability, and helps with hardware capacity planning. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: \n+\n+```\n+  spec:\n+    serverPod:\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+      limits:\n+        cpu: \"2\"\n+        memory: \"2Gi\"\n+```\n+\n+Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An `m` suffix in a CPU attribute indicates 'milli-CPU', so `250m` is 25% of a CPU. \n+\n+Memory can be expressed in various units, where one `Mi` is one IEC unit mega-byte (1024^2), and one `Gi` is one IEC unit giga-byte (1024^3).\n+\n+See also [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/) in the Kubernetes documentation.\n+\n+### Determining pod Quality Of Service\n+\n+A pod's Quality of Service (QoS) is based on whether it's configured with resource requests and limits:\n+\n+- **Best Effort QoS** (lowest priority): If you don't configure requests and limits for a pod, then the pod is given a `best-effort` QoS. In cases where a node runs out of non-shareable resources, a Kubernetes `kubelet` default out-of-resource eviction policy evicts running pods with the `best-effort` QoS first.\n+\n+- **Burstable QoS** (medium priority): If you configure both resource requests and limits for a pod, and set the requests to be less than their respective limits, then the pod will be given a `burstable` QoS. Similarly, if you only configure resource requests (without limits) for a pod, then the pod QoS is also `burstable`. If a node runs out of non-shareable resources, the node's `kubelet` will evict `burstable` pods only when there are no more running `best-effort` pods.\n+\n+- **Guaranteed QoS** (highest priority): If you set a pod's requests and the limits to equal values, then the pod will have a `guaranteed` QoS. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, then a Kubernetes node's `kubelet` will evict `best-effort` and `burstable` QoS pods before terminating `guaranteed` QoS pods.\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring WebLogic pods with memory and CPU requests and limits, and furthermore setting requests equal to their respective limits in order to ensure a `guaranteed` QoS.\n+{{% /notice %}}\n+\n+In newer version of Kubernetes, it is possible to fine tune scheduling and eviction policies using [Pod Priority Preemption](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/). TBD Ryan - is it possible to change the priority class of a pod?\n+\n+### Java heap size and memory resource considerations\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring Java heap sizes for WebLogic pods instead of relying on defaults.\n+{{% /notice %}}\n+\n+#### Overview\n+\n+It's extremely important to set correct heap sizes, memory requests, and memory limits for WebLogic JVMs and Pods. \n+\n+A WebLogic JVM heap must be sufficiently sized to run its applications and services, but should not be sized too large so as not to waste memory resources.\n+\n+A pod memory limit must be sufficiently sized to accommodate the configured heap (and off-heap) requirements, but  not too big to waste memory resources. If a JVM's memory usage (sum of heap and native memory) exceeds its pod's limit, then the JVM process will be abruptly killed due to an out-of-memory error and the WebLogic container will consequently automatically restart due to a liveness probe failure.  \n+\n+{{% notice warning %}}\n+If resource requests and resource limits are set too high, then your pods may not be scheduled due to lack of node resources, will unnecessarily use up CPU shared resources that could be used by other pods, or may prevent other pods from running.\n+{{% /notice %}}\n+\n+#### Default heap sizes\n+\n+With the latest Java versions, Java 8 update 191 and onwards or Java 11, then if you don't configure a heap size (no '-Xms' or '-Xms') the default heap size is dynamically determined:\n+- If you configure the memory limit for a container, then the JVM default maximum heap size will be 25% (1/4th) of container memory limit and the default minimum heap size will be 1.56% (1/64th) of the limit value. \n+\n+  The default JVM heap settings in this case are often too conservative because the WebLogic JVM is the only major process running in the container.\n+\n+- If no memory limit is configured, then the JVM default maximum heap size will be  25% (1/4th) of the its node's machine RAM and the default minimum heap size will be 1.56% (1/64th) of the RAM.\n+\n+  The default JVM heap settings in this case can have undesirable behavior, including using unnecessary amounts of memory to the point where it might affect other pods that run on the same node.\n+\n+#### Configuring heap size\n+\n+If you specify pod memory limits, Oracle recommends configuring WebLogic Server heap sizes as a percentage. The JVM will interpret the percentage as a fraction of the limit. This is done using the JVM `-XX:MinRAMPercentage` and `-XX:MaxRAMPercentage` options in the `USER_MEM_ARGS` [domain resource environment variable]({{< relref \"/userguide/managing-domains/domain-resource#jvm-memory-and-java-option-environment-variables\" >}}).  For example:\n+\n+```\n+  spec:\n+    resources:\n+      env:\n+      - name: USER_MEM_ARGS\n+        value: \"--XX:MinRAMPercentage=25.0 --XX:MaxRAMPercentage=50.0 -Djava.security.egd=file:/dev/./urandom\"\n+```\n+\n+Additionally there's also a node-manager process that's running in the same container as the WebLogic Server which has its own heap and off-heap requirements. Its heap is tuned by using `-Xms` and `-Xmx` in the `NODEMGR_MEM_ARGS` environment variable. Oracle recommends setting the node manager heap memory to fixed sizes, instead of percentages, where [the default tuning]({{< relref \"/userguide/managing-domains/domain-resource#jvm-memory-and-java-option-environment-variables\" >}}) is usually sufficient.\n+\n+{{% notice warning %}}\n+If you set `USER_MEM_ARGS` or `NODEMGR_MEM_ARGS` in your domain resource, then it is usually recommended to include `-Djava.security.egd=file:/dev/./urandom` in order to speedup boot times on systems with low entropy. This setting is included in the respective defaults for these two environment variables.\n+q\n+{{% /notice %}}\n+\n+In some cases, you might only want to configure memory resource requests but not configure memory resource limits. In such scenarios, you can use the traditional fixed heap size settings (`-Xms` and `-Xmx`) in your WebLogic Server `USER_MEM_ARGS` instead of the percentage settings (`-XX:MinRAMPercentage` and `-XX:MaxRAMPercentage`).\n+\n+### CPU resource considerations\n+\n+It's important to set both a CPU request and a limit for WebLogic Server pods. This ensures that all WebLogic server pods have enough CPU resources, and, as discussed earlier, if the request and limit are set to the same value, then they get a `guaranteed` QoS. A `guaranteed` QoS ensures the pods are handled with a higher priority during scheduling and so are the least likely to be evicted.\n+\n+If a CPU request and limit are _not_ configured for a WebLogic Server pod:\n+- The pod can end up using all CPU resources available on its node and starve other containers from using shareable CPU cycles. \n+\n+- The WebLogic server JVM may choose an unsuitable garbage collection (GC) strategy.\n+\n+- A WebLogic Server self-tuning work-manager may incorrectly optimize the number of threads it allocates for the default thread pool. \n+\n+It's also important to keep in mind that if you set a value of CPU core count that's larger than core count of your biggest node, then the pod will never be scheduled. Let's say you have a pod that needs 4 cores but you have a kubernetes cluster that's comprised of 2 core VMs. In this case, your pod will never be scheduled.  \n+\n+### Operator sample heap and resource configuration\n+\n+The operator samples configure non-default minimum and maximum heap sizes for WebLogic server JVMs of at least 256MB and 512MB respectively. You can edit a sample's template or domain resource `resources.env` `USER_MEM_ARGS` to have different values. See [Configuring heap size](#configuring-heap-size).\n+\n+Similarly, the operator samples configure CPU and memory resource requests to at least `250m` and `768Mi` respectively.\n+\n+There's no memory or CPU limit configured by default in samples and so the default QoS for sample WebLogic server pod's is `Burstable`. \n+\n+If you wish to set resource requests or limits differently on a sample domain resource or domain resource template, see [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource). Or for samples that generate their domain resource using an 'inputs' file, see the `serverPodMemoryRequest`, `serverPodMemoryLimit`, `serverPodCpuRequest`, and `serverPodCpuLimit` parameters in the sample's `create-domain.sh` input file.\n+\n+### Configuring CPU affinity\n+\n+A Kubernetes hosted WebLogic server may exhibit high lock contention in comparison to an on-premise deployment. This lock contention may be due to lack of CPU cache affinity and/or scheduling latency when workloads move between different CPU cores.  \n+\n+In an on-premise deployment, CPU cache affinity, and therefore reduced lock contention, can be achieved by binding WLS java process to particular CPU core(s) (using the `taskset` command).\n+\n+In a Kubernetes deployment, similar cache affinity can be achieved by doing the following:\n+- Ensuring a pod's CPU resource request and limit are set and equal (to ensure a `guaranteed` QoS).\n+- Configuring the `kubelet` CPU manager policy to be `static` (the default is `none`). See [Control CPU Management Policies on the Node](https://kubernetes.io/docs/tasks/administer-cluster/cpu-management-policies). \n+Note that some Kubernetes environments may not allow changing the CPU management policy.\n+\n+### Measuring JVM heap, pod CPU, and pod memory\n+\n+TBD Discuss/link to Grafana/Prometheus.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAzMjI0Ng=="}, "originalCommit": {"oid": "0477ff166bc9e02dd33e555ab283d981f0eafeef"}, "originalPosition": 154}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MjQ1NjkzOnYy", "diffSide": "RIGHT", "path": "kubernetes/samples/scripts/common/domain-template.yaml", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQyMzoyMzoxNlrOGrRuyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwMzoxNjo1OFrOGrVY4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAzMjQ1Ng==", "bodyText": "What about resource requests/limits?", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r448032456", "createdAt": "2020-06-30T23:23:16Z", "author": {"login": "rjeberhard"}, "path": "kubernetes/samples/scripts/common/domain-template.yaml", "diffHunk": "@@ -63,7 +63,7 @@ spec:\n     - name: JAVA_OPTIONS\n       value: \"%JAVA_OPTIONS%\"\n     - name: USER_MEM_ARGS\n-      value: \"-Djava.security.egd=file:/dev/./urandom \"\n+      value: \"-Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx512m \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0477ff166bc9e02dd33e555ab283d981f0eafeef"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAzMjg4NQ==", "bodyText": "Oh, I see below... we already had those flags. You can ignore this.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r448032885", "createdAt": "2020-06-30T23:24:42Z", "author": {"login": "rjeberhard"}, "path": "kubernetes/samples/scripts/common/domain-template.yaml", "diffHunk": "@@ -63,7 +63,7 @@ spec:\n     - name: JAVA_OPTIONS\n       value: \"%JAVA_OPTIONS%\"\n     - name: USER_MEM_ARGS\n-      value: \"-Djava.security.egd=file:/dev/./urandom \"\n+      value: \"-Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx512m \"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAzMjQ1Ng=="}, "originalCommit": {"oid": "0477ff166bc9e02dd33e555ab283d981f0eafeef"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA5MjM4NQ==", "bodyText": "My understanding is that this template is used by domain-home-in-image and domain-on-pv samples and resource requests/limits section in domain resource is generated based on how below parameters are configured in create-domain-inputs.yaml file.\nserverPodMemoryRequest\nserverPodCpuRequest\nserverPodMemoryLimit\nserverPodCpuLimit", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r448092385", "createdAt": "2020-07-01T03:16:58Z", "author": {"login": "ankedia"}, "path": "kubernetes/samples/scripts/common/domain-template.yaml", "diffHunk": "@@ -63,7 +63,7 @@ spec:\n     - name: JAVA_OPTIONS\n       value: \"%JAVA_OPTIONS%\"\n     - name: USER_MEM_ARGS\n-      value: \"-Djava.security.egd=file:/dev/./urandom \"\n+      value: \"-Djava.security.egd=file:/dev/./urandom -Xms256m -Xmx512m \"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAzMjQ1Ng=="}, "originalCommit": {"oid": "0477ff166bc9e02dd33e555ab283d981f0eafeef"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MjQ1Nzg3OnYy", "diffSide": "RIGHT", "path": "kubernetes/samples/scripts/common/jrf-domain-template.yaml", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQyMzoyMzozN1rOGrRvRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwMzoyMjowM1rOGrVdIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAzMjU4Mw==", "bodyText": "Remove extra space", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r448032583", "createdAt": "2020-06-30T23:23:37Z", "author": {"login": "rjeberhard"}, "path": "kubernetes/samples/scripts/common/jrf-domain-template.yaml", "diffHunk": "@@ -0,0 +1,124 @@\n+# Copyright (c) 2017, 2020, Oracle Corporation and/or its affiliates.\n+# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.\n+#\n+# This is an example of how to define a Domain resource.\n+#\n+apiVersion: \"weblogic.oracle/v8\"\n+kind: Domain\n+metadata:\n+  name: %DOMAIN_UID%\n+  namespace: %NAMESPACE%\n+  labels:\n+    weblogic.domainUID: %DOMAIN_UID%\n+spec:\n+  # The WebLogic Domain Home\n+  domainHome: %DOMAIN_HOME%\n+\n+  # The domain home source type\n+  # Set to PersistentVolume for domain-in-pv, Image for domain-in-image, or FromModel for model-in-image\n+  domainHomeSourceType: %DOMAIN_HOME_SOURCE_TYPE%\n+\n+  # The WebLogic Server Docker image that the Operator uses to start the domain\n+  image: \"%WEBLOGIC_IMAGE%\"\n+\n+  # imagePullPolicy defaults to \"Always\" if image version is :latest\n+  imagePullPolicy: \"%WEBLOGIC_IMAGE_PULL_POLICY%\"\n+\n+  # Identify which Secret contains the credentials for pulling an image\n+  %WEBLOGIC_IMAGE_PULL_SECRET_PREFIX%imagePullSecrets:\n+  %WEBLOGIC_IMAGE_PULL_SECRET_PREFIX%- name: %WEBLOGIC_IMAGE_PULL_SECRET_NAME%\n+\n+  # Identify which Secret contains the WebLogic Admin credentials (note that there is an example of\n+  # how to create that Secret at the end of this file)\n+  webLogicCredentialsSecret: \n+    name: %WEBLOGIC_CREDENTIALS_SECRET_NAME%\n+\n+  # Whether to include the server out file into the pod's stdout, default is true\n+  includeServerOutInPodLog: %INCLUDE_SERVER_OUT_IN_POD_LOG%\n+\n+  # Whether to enable log home\n+  %LOG_HOME_ON_PV_PREFIX%logHomeEnabled: %LOG_HOME_ENABLED%\n+\n+  # Whether to write HTTP access log file to log home\n+  %LOG_HOME_ON_PV_PREFIX%httpAccessLogInLogHome: %HTTP_ACCESS_LOG_IN_LOG_HOME%\n+\n+  # The in-pod location for domain log, server logs, server out, and Node Manager log files\n+  %LOG_HOME_ON_PV_PREFIX%logHome: %LOG_HOME%\n+  # An (optional) in-pod location for data storage of default and custom file stores.\n+  # If not specified or the value is either not set or empty (e.g. dataHome: \"\") then the\n+  # data storage directories are determined from the WebLogic domain home configuration.\n+  dataHome: \"%DATA_HOME%\"\n+\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0477ff166bc9e02dd33e555ab283d981f0eafeef"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA5MzQ3NQ==", "bodyText": "Removed extra space in commit 686e19e. Thanks", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r448093475", "createdAt": "2020-07-01T03:22:03Z", "author": {"login": "ankedia"}, "path": "kubernetes/samples/scripts/common/jrf-domain-template.yaml", "diffHunk": "@@ -0,0 +1,124 @@\n+# Copyright (c) 2017, 2020, Oracle Corporation and/or its affiliates.\n+# Licensed under the Universal Permissive License v 1.0 as shown at https://oss.oracle.com/licenses/upl.\n+#\n+# This is an example of how to define a Domain resource.\n+#\n+apiVersion: \"weblogic.oracle/v8\"\n+kind: Domain\n+metadata:\n+  name: %DOMAIN_UID%\n+  namespace: %NAMESPACE%\n+  labels:\n+    weblogic.domainUID: %DOMAIN_UID%\n+spec:\n+  # The WebLogic Domain Home\n+  domainHome: %DOMAIN_HOME%\n+\n+  # The domain home source type\n+  # Set to PersistentVolume for domain-in-pv, Image for domain-in-image, or FromModel for model-in-image\n+  domainHomeSourceType: %DOMAIN_HOME_SOURCE_TYPE%\n+\n+  # The WebLogic Server Docker image that the Operator uses to start the domain\n+  image: \"%WEBLOGIC_IMAGE%\"\n+\n+  # imagePullPolicy defaults to \"Always\" if image version is :latest\n+  imagePullPolicy: \"%WEBLOGIC_IMAGE_PULL_POLICY%\"\n+\n+  # Identify which Secret contains the credentials for pulling an image\n+  %WEBLOGIC_IMAGE_PULL_SECRET_PREFIX%imagePullSecrets:\n+  %WEBLOGIC_IMAGE_PULL_SECRET_PREFIX%- name: %WEBLOGIC_IMAGE_PULL_SECRET_NAME%\n+\n+  # Identify which Secret contains the WebLogic Admin credentials (note that there is an example of\n+  # how to create that Secret at the end of this file)\n+  webLogicCredentialsSecret: \n+    name: %WEBLOGIC_CREDENTIALS_SECRET_NAME%\n+\n+  # Whether to include the server out file into the pod's stdout, default is true\n+  includeServerOutInPodLog: %INCLUDE_SERVER_OUT_IN_POD_LOG%\n+\n+  # Whether to enable log home\n+  %LOG_HOME_ON_PV_PREFIX%logHomeEnabled: %LOG_HOME_ENABLED%\n+\n+  # Whether to write HTTP access log file to log home\n+  %LOG_HOME_ON_PV_PREFIX%httpAccessLogInLogHome: %HTTP_ACCESS_LOG_IN_LOG_HOME%\n+\n+  # The in-pod location for domain log, server logs, server out, and Node Manager log files\n+  %LOG_HOME_ON_PV_PREFIX%logHome: %LOG_HOME%\n+  # An (optional) in-pod location for data storage of default and custom file stores.\n+  # If not specified or the value is either not set or empty (e.g. dataHome: \"\") then the\n+  # data storage directories are determined from the WebLogic domain home configuration.\n+  dataHome: \"%DATA_HOME%\"\n+\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAzMjU4Mw=="}, "originalCommit": {"oid": "0477ff166bc9e02dd33e555ab283d981f0eafeef"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5MjQ1ODcyOnYy", "diffSide": "RIGHT", "path": "kubernetes/samples/scripts/common/jrf-domain-template.yaml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0zMFQyMzoyNDowN1rOGrRvzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQwMzowOTozOFrOGrVRwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAzMjcxNw==", "bodyText": "Why does this look like a brand-new file?", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r448032717", "createdAt": "2020-06-30T23:24:07Z", "author": {"login": "rjeberhard"}, "path": "kubernetes/samples/scripts/common/jrf-domain-template.yaml", "diffHunk": "@@ -0,0 +1,124 @@\n+# Copyright (c) 2017, 2020, Oracle Corporation and/or its affiliates.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0477ff166bc9e02dd33e555ab283d981f0eafeef"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODA5MDU2MA==", "bodyText": "Previously same domain template was used for WLS and JRF samples for domain-home-in-image and domain-on-pv samples. Since we need to set different USER_MEM_ARGS (different heap sizes) for JRF domains, I created a new template which is used in JRF domain-home-in-image and domain-on-pv samples.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r448090560", "createdAt": "2020-07-01T03:09:38Z", "author": {"login": "ankedia"}, "path": "kubernetes/samples/scripts/common/jrf-domain-template.yaml", "diffHunk": "@@ -0,0 +1,124 @@\n+# Copyright (c) 2017, 2020, Oracle Corporation and/or its affiliates.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODAzMjcxNw=="}, "originalCommit": {"oid": "0477ff166bc9e02dd33e555ab283d981f0eafeef"}, "originalPosition": 1}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5ODk1MTgyOnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNDo0NzoxMlrOGsQNIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNDo1OTo1MVrOGsQvLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA1NjAzNA==", "bodyText": "Placeholder request for @rosemarymarano to please suggest description and weight that fits in with the updates you did for other FAQ's.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r449056034", "createdAt": "2020-07-02T14:47:12Z", "author": {"login": "rjeberhard"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,177 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "748fcce5ba5dfe90929d4fbc69c032e8bd398bb9"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA2NDc0OA==", "bodyText": "Yes. I'll provide that input on my final review of the FAQ.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r449064748", "createdAt": "2020-07-02T14:59:51Z", "author": {"login": "rosemarymarano"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,177 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA1NjAzNA=="}, "originalCommit": {"oid": "748fcce5ba5dfe90929d4fbc69c032e8bd398bb9"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5ODk4MjExOnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNDo1NDoyNFrOGsQgRg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNzo0Njo0OVrOGsXm2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA2MDkzNA==", "bodyText": "\"An operator\" -> \"The operator\"\nI think I would say it like this, \"The operator creates a container in its own Pod for each WebLogic Server instance.\"\n\"pod container memory\" -> \"container memory\"\n\"and/or\" -> \"and\"\n\"maximum amount of resource\" -> \"maximum amount of a resource\"\nI don't think \"Quality of Service\" should be capitalized here.\nAlso, we've been matching the style in the Kubernetes doc (on their site) and capitalized the names of all resources when specifically referenced as a resource, so all \"pod\" -> \"Pod\" and \"domain resource\" -> \"Domain resource\".", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r449060934", "createdAt": "2020-07-02T14:54:24Z", "author": {"login": "rjeberhard"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,177 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Importance of setting heap size and memory resources](#importance-of-setting-heap-size-and-memory-resources)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. You can tune pod container memory and/or CPU usage by configuring Kubernetes resource requests and limits, and you can tune a WebLogic JVM heap usage using the `USER_MEM_ARGS` environment variable in your domain resource. A resource request sets the minimum amount of a resource that a container requires. A resource limit is the maximum amount of resource a container is given and prevents a container from using more than its share of a resource. Additionally, resource requests and limits determine a pod's Quality of Service.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "748fcce5ba5dfe90929d4fbc69c032e8bd398bb9"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTE3NzMwNw==", "bodyText": "I have made changes based on above review comments. Thanks.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r449177307", "createdAt": "2020-07-02T17:46:49Z", "author": {"login": "ankedia"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,177 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Importance of setting heap size and memory resources](#importance-of-setting-heap-size-and-memory-resources)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. You can tune pod container memory and/or CPU usage by configuring Kubernetes resource requests and limits, and you can tune a WebLogic JVM heap usage using the `USER_MEM_ARGS` environment variable in your domain resource. A resource request sets the minimum amount of a resource that a container requires. A resource limit is the maximum amount of resource a container is given and prevents a container from using more than its share of a resource. Additionally, resource requests and limits determine a pod's Quality of Service.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA2MDkzNA=="}, "originalCommit": {"oid": "748fcce5ba5dfe90929d4fbc69c032e8bd398bb9"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5ODk4Mzc1OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNDo1NDo0N1rOGsQhYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNDo1NDo0N1rOGsQhYw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA2MTIxOQ==", "bodyText": "WebLogic Server instances", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r449061219", "createdAt": "2020-07-02T14:54:47Z", "author": {"login": "rjeberhard"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,177 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Importance of setting heap size and memory resources](#importance-of-setting-heap-size-and-memory-resources)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. You can tune pod container memory and/or CPU usage by configuring Kubernetes resource requests and limits, and you can tune a WebLogic JVM heap usage using the `USER_MEM_ARGS` environment variable in your domain resource. A resource request sets the minimum amount of a resource that a container requires. A resource limit is the maximum amount of resource a container is given and prevents a container from using more than its share of a resource. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+This FAQ discusses tuning these parameters so WebLogic servers can run efficiently.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "748fcce5ba5dfe90929d4fbc69c032e8bd398bb9"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5ODk5MjQ1OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNDo1Njo0N1rOGsQm2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNDo1Njo0N1rOGsQm2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA2MjYxNw==", "bodyText": "\"in the Kubernetes documentation.\"", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r449062617", "createdAt": "2020-07-02T14:56:47Z", "author": {"login": "rjeberhard"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,177 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Importance of setting heap size and memory resources](#importance-of-setting-heap-size-and-memory-resources)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. You can tune pod container memory and/or CPU usage by configuring Kubernetes resource requests and limits, and you can tune a WebLogic JVM heap usage using the `USER_MEM_ARGS` environment variable in your domain resource. A resource request sets the minimum amount of a resource that a container requires. A resource limit is the maximum amount of resource a container is given and prevents a container from using more than its share of a resource. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+This FAQ discusses tuning these parameters so WebLogic servers can run efficiently.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: \n+\n+```\n+  spec:\n+    serverPod:\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+      limits:\n+        cpu: \"2\"\n+        memory: \"2Gi\"\n+```\n+\n+Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An `m` suffix in a CPU attribute indicates 'milli-CPU', so `250m` is 25% of a CPU. \n+\n+Memory can be expressed in various units, where one `Mi` is one IEC unit mega-byte (1024^2), and one `Gi` is one IEC unit giga-byte (1024^3).\n+\n+See also [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/), [Assign Memory Resources to Containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource) and [Assign CPU Resources to Containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource) in Kubernetes documentation.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "748fcce5ba5dfe90929d4fbc69c032e8bd398bb9"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5ODk5NjE2OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNDo1Nzo0NVrOGsQpXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxODo0ODoxN1rOGsZbXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA2MzI2Mw==", "bodyText": "\"a Kubernetes kubelet default out-of-resource\" -> \"the default\"", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r449063263", "createdAt": "2020-07-02T14:57:45Z", "author": {"login": "rjeberhard"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,177 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Importance of setting heap size and memory resources](#importance-of-setting-heap-size-and-memory-resources)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. You can tune pod container memory and/or CPU usage by configuring Kubernetes resource requests and limits, and you can tune a WebLogic JVM heap usage using the `USER_MEM_ARGS` environment variable in your domain resource. A resource request sets the minimum amount of a resource that a container requires. A resource limit is the maximum amount of resource a container is given and prevents a container from using more than its share of a resource. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+This FAQ discusses tuning these parameters so WebLogic servers can run efficiently.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: \n+\n+```\n+  spec:\n+    serverPod:\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+      limits:\n+        cpu: \"2\"\n+        memory: \"2Gi\"\n+```\n+\n+Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An `m` suffix in a CPU attribute indicates 'milli-CPU', so `250m` is 25% of a CPU. \n+\n+Memory can be expressed in various units, where one `Mi` is one IEC unit mega-byte (1024^2), and one `Gi` is one IEC unit giga-byte (1024^3).\n+\n+See also [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/), [Assign Memory Resources to Containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource) and [Assign CPU Resources to Containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource) in Kubernetes documentation.\n+\n+### Determining pod Quality Of Service\n+\n+A pod's Quality of Service (QoS) is based on whether it's configured with resource requests and limits:\n+\n+- **Best Effort QoS** (lowest priority): If you don't configure requests and limits for a pod, then the pod is given a `best-effort` QoS. In cases where a node runs out of non-shareable resources, a Kubernetes `kubelet` default out-of-resource eviction policy evicts running pods with the `best-effort` QoS first.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "748fcce5ba5dfe90929d4fbc69c032e8bd398bb9"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTIwNzEzMg==", "bodyText": "Changed the wording to \"the default\".", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r449207132", "createdAt": "2020-07-02T18:48:17Z", "author": {"login": "ankedia"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,177 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Importance of setting heap size and memory resources](#importance-of-setting-heap-size-and-memory-resources)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. You can tune pod container memory and/or CPU usage by configuring Kubernetes resource requests and limits, and you can tune a WebLogic JVM heap usage using the `USER_MEM_ARGS` environment variable in your domain resource. A resource request sets the minimum amount of a resource that a container requires. A resource limit is the maximum amount of resource a container is given and prevents a container from using more than its share of a resource. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+This FAQ discusses tuning these parameters so WebLogic servers can run efficiently.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: \n+\n+```\n+  spec:\n+    serverPod:\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+      limits:\n+        cpu: \"2\"\n+        memory: \"2Gi\"\n+```\n+\n+Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An `m` suffix in a CPU attribute indicates 'milli-CPU', so `250m` is 25% of a CPU. \n+\n+Memory can be expressed in various units, where one `Mi` is one IEC unit mega-byte (1024^2), and one `Gi` is one IEC unit giga-byte (1024^3).\n+\n+See also [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/), [Assign Memory Resources to Containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource) and [Assign CPU Resources to Containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource) in Kubernetes documentation.\n+\n+### Determining pod Quality Of Service\n+\n+A pod's Quality of Service (QoS) is based on whether it's configured with resource requests and limits:\n+\n+- **Best Effort QoS** (lowest priority): If you don't configure requests and limits for a pod, then the pod is given a `best-effort` QoS. In cases where a node runs out of non-shareable resources, a Kubernetes `kubelet` default out-of-resource eviction policy evicts running pods with the `best-effort` QoS first.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA2MzI2Mw=="}, "originalCommit": {"oid": "748fcce5ba5dfe90929d4fbc69c032e8bd398bb9"}, "originalPosition": 54}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5OTAwNTgzOnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNDo1OTo1OFrOGsQvgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNTowMzoxM1rOGsQ4Sw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA2NDgzNQ==", "bodyText": "Neither of these priority classes sound like they apply to WebLogic Server instances. Are we recommending that customers choose either of these priority classes? If not, then we don't need to mention them.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r449064835", "createdAt": "2020-07-02T14:59:58Z", "author": {"login": "rjeberhard"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,177 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Importance of setting heap size and memory resources](#importance-of-setting-heap-size-and-memory-resources)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. You can tune pod container memory and/or CPU usage by configuring Kubernetes resource requests and limits, and you can tune a WebLogic JVM heap usage using the `USER_MEM_ARGS` environment variable in your domain resource. A resource request sets the minimum amount of a resource that a container requires. A resource limit is the maximum amount of resource a container is given and prevents a container from using more than its share of a resource. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+This FAQ discusses tuning these parameters so WebLogic servers can run efficiently.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: \n+\n+```\n+  spec:\n+    serverPod:\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+      limits:\n+        cpu: \"2\"\n+        memory: \"2Gi\"\n+```\n+\n+Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An `m` suffix in a CPU attribute indicates 'milli-CPU', so `250m` is 25% of a CPU. \n+\n+Memory can be expressed in various units, where one `Mi` is one IEC unit mega-byte (1024^2), and one `Gi` is one IEC unit giga-byte (1024^3).\n+\n+See also [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/), [Assign Memory Resources to Containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource) and [Assign CPU Resources to Containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource) in Kubernetes documentation.\n+\n+### Determining pod Quality Of Service\n+\n+A pod's Quality of Service (QoS) is based on whether it's configured with resource requests and limits:\n+\n+- **Best Effort QoS** (lowest priority): If you don't configure requests and limits for a pod, then the pod is given a `best-effort` QoS. In cases where a node runs out of non-shareable resources, a Kubernetes `kubelet` default out-of-resource eviction policy evicts running pods with the `best-effort` QoS first.\n+\n+- **Burstable QoS** (medium priority): If you configure both resource requests and limits for a pod, and set the requests to be less than their respective limits, then the pod will be given a `burstable` QoS. Similarly, if you only configure resource requests (without limits) for a pod, then the pod QoS is also `burstable`. If a node runs out of non-shareable resources, the node's `kubelet` will evict `burstable` pods only when there are no more running `best-effort` pods.\n+\n+- **Guaranteed QoS** (highest priority): If you set a pod's requests and the limits to equal values, then the pod will have a `guaranteed` QoS. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, then a Kubernetes node's `kubelet` will evict `best-effort` and `burstable` QoS pods before terminating `guaranteed` QoS pods.\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring WebLogic pods with memory and CPU requests and limits, and furthermore setting requests equal to their respective limits in order to ensure a `guaranteed` QoS.\n+{{% /notice %}}\n+\n+{{% notice note %}} \n+In newer version of Kubernetes, it is possible to fine tune scheduling and eviction policies using [Pod Priority Preemption](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/) in combination with the `serverPod.priorityClassName` domain resource attribute. Note that Kubernetes already ships with two PriorityClasses: `system-cluster-critical` and `system-node-critical`. These are common classes and are used to [ensure that critical components are always scheduled first](https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "748fcce5ba5dfe90929d4fbc69c032e8bd398bb9"}, "originalPosition": 65}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA2NzA4Mw==", "bodyText": "This text is borrowed from kubectl explain domain.spec.serverPod.priorityClassName.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r449067083", "createdAt": "2020-07-02T15:03:13Z", "author": {"login": "tbarnes-us"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,177 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Importance of setting heap size and memory resources](#importance-of-setting-heap-size-and-memory-resources)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. You can tune pod container memory and/or CPU usage by configuring Kubernetes resource requests and limits, and you can tune a WebLogic JVM heap usage using the `USER_MEM_ARGS` environment variable in your domain resource. A resource request sets the minimum amount of a resource that a container requires. A resource limit is the maximum amount of resource a container is given and prevents a container from using more than its share of a resource. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+This FAQ discusses tuning these parameters so WebLogic servers can run efficiently.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: \n+\n+```\n+  spec:\n+    serverPod:\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+      limits:\n+        cpu: \"2\"\n+        memory: \"2Gi\"\n+```\n+\n+Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An `m` suffix in a CPU attribute indicates 'milli-CPU', so `250m` is 25% of a CPU. \n+\n+Memory can be expressed in various units, where one `Mi` is one IEC unit mega-byte (1024^2), and one `Gi` is one IEC unit giga-byte (1024^3).\n+\n+See also [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/), [Assign Memory Resources to Containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource) and [Assign CPU Resources to Containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource) in Kubernetes documentation.\n+\n+### Determining pod Quality Of Service\n+\n+A pod's Quality of Service (QoS) is based on whether it's configured with resource requests and limits:\n+\n+- **Best Effort QoS** (lowest priority): If you don't configure requests and limits for a pod, then the pod is given a `best-effort` QoS. In cases where a node runs out of non-shareable resources, a Kubernetes `kubelet` default out-of-resource eviction policy evicts running pods with the `best-effort` QoS first.\n+\n+- **Burstable QoS** (medium priority): If you configure both resource requests and limits for a pod, and set the requests to be less than their respective limits, then the pod will be given a `burstable` QoS. Similarly, if you only configure resource requests (without limits) for a pod, then the pod QoS is also `burstable`. If a node runs out of non-shareable resources, the node's `kubelet` will evict `burstable` pods only when there are no more running `best-effort` pods.\n+\n+- **Guaranteed QoS** (highest priority): If you set a pod's requests and the limits to equal values, then the pod will have a `guaranteed` QoS. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, then a Kubernetes node's `kubelet` will evict `best-effort` and `burstable` QoS pods before terminating `guaranteed` QoS pods.\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring WebLogic pods with memory and CPU requests and limits, and furthermore setting requests equal to their respective limits in order to ensure a `guaranteed` QoS.\n+{{% /notice %}}\n+\n+{{% notice note %}} \n+In newer version of Kubernetes, it is possible to fine tune scheduling and eviction policies using [Pod Priority Preemption](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/) in combination with the `serverPod.priorityClassName` domain resource attribute. Note that Kubernetes already ships with two PriorityClasses: `system-cluster-critical` and `system-node-critical`. These are common classes and are used to [ensure that critical components are always scheduled first](https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA2NDgzNQ=="}, "originalCommit": {"oid": "748fcce5ba5dfe90929d4fbc69c032e8bd398bb9"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5OTAwOTE5OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNTowMDo0MVrOGsQxqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNzo1MDowNVrOGsXtGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA2NTM4NA==", "bodyText": "Does a use case exist where we wouldn't recommend this? If not, you can remove \"For most use cases,\"", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r449065384", "createdAt": "2020-07-02T15:00:41Z", "author": {"login": "rjeberhard"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,177 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Importance of setting heap size and memory resources](#importance-of-setting-heap-size-and-memory-resources)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. You can tune pod container memory and/or CPU usage by configuring Kubernetes resource requests and limits, and you can tune a WebLogic JVM heap usage using the `USER_MEM_ARGS` environment variable in your domain resource. A resource request sets the minimum amount of a resource that a container requires. A resource limit is the maximum amount of resource a container is given and prevents a container from using more than its share of a resource. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+This FAQ discusses tuning these parameters so WebLogic servers can run efficiently.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: \n+\n+```\n+  spec:\n+    serverPod:\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+      limits:\n+        cpu: \"2\"\n+        memory: \"2Gi\"\n+```\n+\n+Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An `m` suffix in a CPU attribute indicates 'milli-CPU', so `250m` is 25% of a CPU. \n+\n+Memory can be expressed in various units, where one `Mi` is one IEC unit mega-byte (1024^2), and one `Gi` is one IEC unit giga-byte (1024^3).\n+\n+See also [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/), [Assign Memory Resources to Containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource) and [Assign CPU Resources to Containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource) in Kubernetes documentation.\n+\n+### Determining pod Quality Of Service\n+\n+A pod's Quality of Service (QoS) is based on whether it's configured with resource requests and limits:\n+\n+- **Best Effort QoS** (lowest priority): If you don't configure requests and limits for a pod, then the pod is given a `best-effort` QoS. In cases where a node runs out of non-shareable resources, a Kubernetes `kubelet` default out-of-resource eviction policy evicts running pods with the `best-effort` QoS first.\n+\n+- **Burstable QoS** (medium priority): If you configure both resource requests and limits for a pod, and set the requests to be less than their respective limits, then the pod will be given a `burstable` QoS. Similarly, if you only configure resource requests (without limits) for a pod, then the pod QoS is also `burstable`. If a node runs out of non-shareable resources, the node's `kubelet` will evict `burstable` pods only when there are no more running `best-effort` pods.\n+\n+- **Guaranteed QoS** (highest priority): If you set a pod's requests and the limits to equal values, then the pod will have a `guaranteed` QoS. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, then a Kubernetes node's `kubelet` will evict `best-effort` and `burstable` QoS pods before terminating `guaranteed` QoS pods.\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring WebLogic pods with memory and CPU requests and limits, and furthermore setting requests equal to their respective limits in order to ensure a `guaranteed` QoS.\n+{{% /notice %}}\n+\n+{{% notice note %}} \n+In newer version of Kubernetes, it is possible to fine tune scheduling and eviction policies using [Pod Priority Preemption](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/) in combination with the `serverPod.priorityClassName` domain resource attribute. Note that Kubernetes already ships with two PriorityClasses: `system-cluster-critical` and `system-node-critical`. These are common classes and are used to [ensure that critical components are always scheduled first](https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/).\n+{{% /notice %}}\n+\n+### Java heap size and memory resource considerations\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring Java heap sizes for WebLogic pods instead of relying on defaults.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "748fcce5ba5dfe90929d4fbc69c032e8bd398bb9"}, "originalPosition": 71}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTE3ODkwNA==", "bodyText": "Removed \"For most use cases,\"", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r449178904", "createdAt": "2020-07-02T17:50:05Z", "author": {"login": "ankedia"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,177 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Importance of setting heap size and memory resources](#importance-of-setting-heap-size-and-memory-resources)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. You can tune pod container memory and/or CPU usage by configuring Kubernetes resource requests and limits, and you can tune a WebLogic JVM heap usage using the `USER_MEM_ARGS` environment variable in your domain resource. A resource request sets the minimum amount of a resource that a container requires. A resource limit is the maximum amount of resource a container is given and prevents a container from using more than its share of a resource. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+This FAQ discusses tuning these parameters so WebLogic servers can run efficiently.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: \n+\n+```\n+  spec:\n+    serverPod:\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+      limits:\n+        cpu: \"2\"\n+        memory: \"2Gi\"\n+```\n+\n+Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An `m` suffix in a CPU attribute indicates 'milli-CPU', so `250m` is 25% of a CPU. \n+\n+Memory can be expressed in various units, where one `Mi` is one IEC unit mega-byte (1024^2), and one `Gi` is one IEC unit giga-byte (1024^3).\n+\n+See also [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/), [Assign Memory Resources to Containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource) and [Assign CPU Resources to Containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource) in Kubernetes documentation.\n+\n+### Determining pod Quality Of Service\n+\n+A pod's Quality of Service (QoS) is based on whether it's configured with resource requests and limits:\n+\n+- **Best Effort QoS** (lowest priority): If you don't configure requests and limits for a pod, then the pod is given a `best-effort` QoS. In cases where a node runs out of non-shareable resources, a Kubernetes `kubelet` default out-of-resource eviction policy evicts running pods with the `best-effort` QoS first.\n+\n+- **Burstable QoS** (medium priority): If you configure both resource requests and limits for a pod, and set the requests to be less than their respective limits, then the pod will be given a `burstable` QoS. Similarly, if you only configure resource requests (without limits) for a pod, then the pod QoS is also `burstable`. If a node runs out of non-shareable resources, the node's `kubelet` will evict `burstable` pods only when there are no more running `best-effort` pods.\n+\n+- **Guaranteed QoS** (highest priority): If you set a pod's requests and the limits to equal values, then the pod will have a `guaranteed` QoS. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, then a Kubernetes node's `kubelet` will evict `best-effort` and `burstable` QoS pods before terminating `guaranteed` QoS pods.\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring WebLogic pods with memory and CPU requests and limits, and furthermore setting requests equal to their respective limits in order to ensure a `guaranteed` QoS.\n+{{% /notice %}}\n+\n+{{% notice note %}} \n+In newer version of Kubernetes, it is possible to fine tune scheduling and eviction policies using [Pod Priority Preemption](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/) in combination with the `serverPod.priorityClassName` domain resource attribute. Note that Kubernetes already ships with two PriorityClasses: `system-cluster-critical` and `system-node-critical`. These are common classes and are used to [ensure that critical components are always scheduled first](https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/).\n+{{% /notice %}}\n+\n+### Java heap size and memory resource considerations\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring Java heap sizes for WebLogic pods instead of relying on defaults.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA2NTM4NA=="}, "originalCommit": {"oid": "748fcce5ba5dfe90929d4fbc69c032e8bd398bb9"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5OTAxMDkxOnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNTowMTowOFrOGsQyyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNTowMTowOFrOGsQyyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA2NTY3Mg==", "bodyText": "extra space after \"but\"", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r449065672", "createdAt": "2020-07-02T15:01:08Z", "author": {"login": "rjeberhard"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,177 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Importance of setting heap size and memory resources](#importance-of-setting-heap-size-and-memory-resources)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. You can tune pod container memory and/or CPU usage by configuring Kubernetes resource requests and limits, and you can tune a WebLogic JVM heap usage using the `USER_MEM_ARGS` environment variable in your domain resource. A resource request sets the minimum amount of a resource that a container requires. A resource limit is the maximum amount of resource a container is given and prevents a container from using more than its share of a resource. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+This FAQ discusses tuning these parameters so WebLogic servers can run efficiently.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: \n+\n+```\n+  spec:\n+    serverPod:\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+      limits:\n+        cpu: \"2\"\n+        memory: \"2Gi\"\n+```\n+\n+Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An `m` suffix in a CPU attribute indicates 'milli-CPU', so `250m` is 25% of a CPU. \n+\n+Memory can be expressed in various units, where one `Mi` is one IEC unit mega-byte (1024^2), and one `Gi` is one IEC unit giga-byte (1024^3).\n+\n+See also [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/), [Assign Memory Resources to Containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource) and [Assign CPU Resources to Containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource) in Kubernetes documentation.\n+\n+### Determining pod Quality Of Service\n+\n+A pod's Quality of Service (QoS) is based on whether it's configured with resource requests and limits:\n+\n+- **Best Effort QoS** (lowest priority): If you don't configure requests and limits for a pod, then the pod is given a `best-effort` QoS. In cases where a node runs out of non-shareable resources, a Kubernetes `kubelet` default out-of-resource eviction policy evicts running pods with the `best-effort` QoS first.\n+\n+- **Burstable QoS** (medium priority): If you configure both resource requests and limits for a pod, and set the requests to be less than their respective limits, then the pod will be given a `burstable` QoS. Similarly, if you only configure resource requests (without limits) for a pod, then the pod QoS is also `burstable`. If a node runs out of non-shareable resources, the node's `kubelet` will evict `burstable` pods only when there are no more running `best-effort` pods.\n+\n+- **Guaranteed QoS** (highest priority): If you set a pod's requests and the limits to equal values, then the pod will have a `guaranteed` QoS. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, then a Kubernetes node's `kubelet` will evict `best-effort` and `burstable` QoS pods before terminating `guaranteed` QoS pods.\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring WebLogic pods with memory and CPU requests and limits, and furthermore setting requests equal to their respective limits in order to ensure a `guaranteed` QoS.\n+{{% /notice %}}\n+\n+{{% notice note %}} \n+In newer version of Kubernetes, it is possible to fine tune scheduling and eviction policies using [Pod Priority Preemption](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/) in combination with the `serverPod.priorityClassName` domain resource attribute. Note that Kubernetes already ships with two PriorityClasses: `system-cluster-critical` and `system-node-critical`. These are common classes and are used to [ensure that critical components are always scheduled first](https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/).\n+{{% /notice %}}\n+\n+### Java heap size and memory resource considerations\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring Java heap sizes for WebLogic pods instead of relying on defaults.\n+{{% /notice %}}\n+\n+#### Importance of setting heap size and memory resources\n+\n+It's extremely important to set correct heap sizes, memory requests, and memory limits for WebLogic JVMs and Pods. \n+\n+A WebLogic JVM heap must be sufficiently sized to run its applications and services, but should not be sized too large so as not to waste memory resources.\n+\n+A pod memory limit must be sufficiently sized to accommodate the configured heap and native memory requirements, but  not too big to waste memory resources. If a JVM's memory usage (sum of heap and native memory) exceeds its pod's limit, then the JVM process will be abruptly killed due to an out-of-memory error and the WebLogic container will consequently automatically restart due to a liveness probe failure.  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "748fcce5ba5dfe90929d4fbc69c032e8bd398bb9"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5OTAxOTk3OnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNTowMzoxMFrOGsQ4LQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNzo1MjowNFrOGsXxGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA2NzA1Mw==", "bodyText": "Aren't they guaranteed to not be evicted?", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r449067053", "createdAt": "2020-07-02T15:03:10Z", "author": {"login": "rjeberhard"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,177 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Importance of setting heap size and memory resources](#importance-of-setting-heap-size-and-memory-resources)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. You can tune pod container memory and/or CPU usage by configuring Kubernetes resource requests and limits, and you can tune a WebLogic JVM heap usage using the `USER_MEM_ARGS` environment variable in your domain resource. A resource request sets the minimum amount of a resource that a container requires. A resource limit is the maximum amount of resource a container is given and prevents a container from using more than its share of a resource. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+This FAQ discusses tuning these parameters so WebLogic servers can run efficiently.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: \n+\n+```\n+  spec:\n+    serverPod:\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+      limits:\n+        cpu: \"2\"\n+        memory: \"2Gi\"\n+```\n+\n+Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An `m` suffix in a CPU attribute indicates 'milli-CPU', so `250m` is 25% of a CPU. \n+\n+Memory can be expressed in various units, where one `Mi` is one IEC unit mega-byte (1024^2), and one `Gi` is one IEC unit giga-byte (1024^3).\n+\n+See also [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/), [Assign Memory Resources to Containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource) and [Assign CPU Resources to Containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource) in Kubernetes documentation.\n+\n+### Determining pod Quality Of Service\n+\n+A pod's Quality of Service (QoS) is based on whether it's configured with resource requests and limits:\n+\n+- **Best Effort QoS** (lowest priority): If you don't configure requests and limits for a pod, then the pod is given a `best-effort` QoS. In cases where a node runs out of non-shareable resources, a Kubernetes `kubelet` default out-of-resource eviction policy evicts running pods with the `best-effort` QoS first.\n+\n+- **Burstable QoS** (medium priority): If you configure both resource requests and limits for a pod, and set the requests to be less than their respective limits, then the pod will be given a `burstable` QoS. Similarly, if you only configure resource requests (without limits) for a pod, then the pod QoS is also `burstable`. If a node runs out of non-shareable resources, the node's `kubelet` will evict `burstable` pods only when there are no more running `best-effort` pods.\n+\n+- **Guaranteed QoS** (highest priority): If you set a pod's requests and the limits to equal values, then the pod will have a `guaranteed` QoS. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, then a Kubernetes node's `kubelet` will evict `best-effort` and `burstable` QoS pods before terminating `guaranteed` QoS pods.\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring WebLogic pods with memory and CPU requests and limits, and furthermore setting requests equal to their respective limits in order to ensure a `guaranteed` QoS.\n+{{% /notice %}}\n+\n+{{% notice note %}} \n+In newer version of Kubernetes, it is possible to fine tune scheduling and eviction policies using [Pod Priority Preemption](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/) in combination with the `serverPod.priorityClassName` domain resource attribute. Note that Kubernetes already ships with two PriorityClasses: `system-cluster-critical` and `system-node-critical`. These are common classes and are used to [ensure that critical components are always scheduled first](https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/).\n+{{% /notice %}}\n+\n+### Java heap size and memory resource considerations\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring Java heap sizes for WebLogic pods instead of relying on defaults.\n+{{% /notice %}}\n+\n+#### Importance of setting heap size and memory resources\n+\n+It's extremely important to set correct heap sizes, memory requests, and memory limits for WebLogic JVMs and Pods. \n+\n+A WebLogic JVM heap must be sufficiently sized to run its applications and services, but should not be sized too large so as not to waste memory resources.\n+\n+A pod memory limit must be sufficiently sized to accommodate the configured heap and native memory requirements, but  not too big to waste memory resources. If a JVM's memory usage (sum of heap and native memory) exceeds its pod's limit, then the JVM process will be abruptly killed due to an out-of-memory error and the WebLogic container will consequently automatically restart due to a liveness probe failure.  \n+\n+Oracle recommends setting minimum and maximum heap (or heap percentages) and at least a container memory request.\n+\n+{{% notice warning %}}\n+If resource requests and resource limits are set too high, then your pods may not be scheduled due to lack of node resources, will unnecessarily use up CPU shared resources that could be used by other pods, or may prevent other pods from running.\n+{{% /notice %}}\n+\n+#### Default heap sizes\n+\n+With the latest Java versions, Java 8 update 191 and onwards or Java 11, then if you don't configure a heap size (no '-Xms' or '-Xms') the default heap size is dynamically determined:\n+- If you configure the memory limit for a container, then the JVM default maximum heap size will be 25% (1/4th) of container memory limit and the default minimum heap size will be 1.56% (1/64th) of the limit value. \n+\n+  The default JVM heap settings in this case are often too conservative because the WebLogic JVM is the only major process running in the container.\n+\n+- If no memory limit is configured, then the JVM default maximum heap size will be  25% (1/4th) of the its node's machine RAM and the default minimum heap size will be 1.56% (1/64th) of the RAM.\n+\n+  The default JVM heap settings in this case can have undesirable behavior, including using unnecessary amounts of memory to the point where it might affect other pods that run on the same node.\n+\n+#### Configuring heap size\n+\n+If you specify pod memory limits, Oracle recommends configuring WebLogic Server heap sizes as a percentage. The JVM will interpret the percentage as a fraction of the limit. This is done using the JVM `-XX:MinRAMPercentage` and `-XX:MaxRAMPercentage` options in the `USER_MEM_ARGS` [domain resource environment variable]({{< relref \"/userguide/managing-domains/domain-resource#jvm-memory-and-java-option-environment-variables\" >}}).  For example:\n+\n+```\n+  spec:\n+    resources:\n+      env:\n+      - name: USER_MEM_ARGS\n+        value: \"--XX:MinRAMPercentage=25.0 --XX:MaxRAMPercentage=50.0 -Djava.security.egd=file:/dev/./urandom\"\n+```\n+\n+Additionally there's also a node-manager process that's running in the same container as the WebLogic Server which has its own heap and native memory requirements. Its heap is tuned by using `-Xms` and `-Xmx` in the `NODEMGR_MEM_ARGS` environment variable. Oracle recommends setting the node manager heap memory to fixed sizes, instead of percentages, where [the default tuning]({{< relref \"/userguide/managing-domains/domain-resource#jvm-memory-and-java-option-environment-variables\" >}}) is usually sufficient.\n+\n+{{% notice note %}}\n+Notice that the `NODEMGR_MEM_ARGS` and `USER_MEM_ARGS` environment variables both set `-Djava.security.egd=file:/dev/./urandom` by default so we have also included them in the above example for specifying a `USER_MEM_ARGS` value. This helps speed up Node Manager and WebLogic Server startup on systems with low entropy. \n+{{% /notice %}}\n+\n+In some cases, you might only want to configure memory resource requests but not configure memory resource limits. In such scenarios, you can use the traditional fixed heap size settings (`-Xms` and `-Xmx`) in your WebLogic Server `USER_MEM_ARGS` instead of the percentage settings (`-XX:MinRAMPercentage` and `-XX:MaxRAMPercentage`).\n+\n+### CPU resource considerations\n+\n+It's important to set both a CPU request and a limit for WebLogic Server pods. This ensures that all WebLogic server pods have enough CPU resources, and, as discussed earlier, if the request and limit are set to the same value, then they get a `guaranteed` QoS. A `guaranteed` QoS ensures the pods are handled with a higher priority during scheduling and so are the least likely to be evicted.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "748fcce5ba5dfe90929d4fbc69c032e8bd398bb9"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTE3OTkyOQ==", "bodyText": "It's possible for Guaranteed Pod to be evicted in some cases.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r449179929", "createdAt": "2020-07-02T17:52:04Z", "author": {"login": "ankedia"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,177 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Importance of setting heap size and memory resources](#importance-of-setting-heap-size-and-memory-resources)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. You can tune pod container memory and/or CPU usage by configuring Kubernetes resource requests and limits, and you can tune a WebLogic JVM heap usage using the `USER_MEM_ARGS` environment variable in your domain resource. A resource request sets the minimum amount of a resource that a container requires. A resource limit is the maximum amount of resource a container is given and prevents a container from using more than its share of a resource. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+This FAQ discusses tuning these parameters so WebLogic servers can run efficiently.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: \n+\n+```\n+  spec:\n+    serverPod:\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+      limits:\n+        cpu: \"2\"\n+        memory: \"2Gi\"\n+```\n+\n+Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An `m` suffix in a CPU attribute indicates 'milli-CPU', so `250m` is 25% of a CPU. \n+\n+Memory can be expressed in various units, where one `Mi` is one IEC unit mega-byte (1024^2), and one `Gi` is one IEC unit giga-byte (1024^3).\n+\n+See also [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/), [Assign Memory Resources to Containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource) and [Assign CPU Resources to Containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource) in Kubernetes documentation.\n+\n+### Determining pod Quality Of Service\n+\n+A pod's Quality of Service (QoS) is based on whether it's configured with resource requests and limits:\n+\n+- **Best Effort QoS** (lowest priority): If you don't configure requests and limits for a pod, then the pod is given a `best-effort` QoS. In cases where a node runs out of non-shareable resources, a Kubernetes `kubelet` default out-of-resource eviction policy evicts running pods with the `best-effort` QoS first.\n+\n+- **Burstable QoS** (medium priority): If you configure both resource requests and limits for a pod, and set the requests to be less than their respective limits, then the pod will be given a `burstable` QoS. Similarly, if you only configure resource requests (without limits) for a pod, then the pod QoS is also `burstable`. If a node runs out of non-shareable resources, the node's `kubelet` will evict `burstable` pods only when there are no more running `best-effort` pods.\n+\n+- **Guaranteed QoS** (highest priority): If you set a pod's requests and the limits to equal values, then the pod will have a `guaranteed` QoS. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, then a Kubernetes node's `kubelet` will evict `best-effort` and `burstable` QoS pods before terminating `guaranteed` QoS pods.\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring WebLogic pods with memory and CPU requests and limits, and furthermore setting requests equal to their respective limits in order to ensure a `guaranteed` QoS.\n+{{% /notice %}}\n+\n+{{% notice note %}} \n+In newer version of Kubernetes, it is possible to fine tune scheduling and eviction policies using [Pod Priority Preemption](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/) in combination with the `serverPod.priorityClassName` domain resource attribute. Note that Kubernetes already ships with two PriorityClasses: `system-cluster-critical` and `system-node-critical`. These are common classes and are used to [ensure that critical components are always scheduled first](https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/).\n+{{% /notice %}}\n+\n+### Java heap size and memory resource considerations\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring Java heap sizes for WebLogic pods instead of relying on defaults.\n+{{% /notice %}}\n+\n+#### Importance of setting heap size and memory resources\n+\n+It's extremely important to set correct heap sizes, memory requests, and memory limits for WebLogic JVMs and Pods. \n+\n+A WebLogic JVM heap must be sufficiently sized to run its applications and services, but should not be sized too large so as not to waste memory resources.\n+\n+A pod memory limit must be sufficiently sized to accommodate the configured heap and native memory requirements, but  not too big to waste memory resources. If a JVM's memory usage (sum of heap and native memory) exceeds its pod's limit, then the JVM process will be abruptly killed due to an out-of-memory error and the WebLogic container will consequently automatically restart due to a liveness probe failure.  \n+\n+Oracle recommends setting minimum and maximum heap (or heap percentages) and at least a container memory request.\n+\n+{{% notice warning %}}\n+If resource requests and resource limits are set too high, then your pods may not be scheduled due to lack of node resources, will unnecessarily use up CPU shared resources that could be used by other pods, or may prevent other pods from running.\n+{{% /notice %}}\n+\n+#### Default heap sizes\n+\n+With the latest Java versions, Java 8 update 191 and onwards or Java 11, then if you don't configure a heap size (no '-Xms' or '-Xms') the default heap size is dynamically determined:\n+- If you configure the memory limit for a container, then the JVM default maximum heap size will be 25% (1/4th) of container memory limit and the default minimum heap size will be 1.56% (1/64th) of the limit value. \n+\n+  The default JVM heap settings in this case are often too conservative because the WebLogic JVM is the only major process running in the container.\n+\n+- If no memory limit is configured, then the JVM default maximum heap size will be  25% (1/4th) of the its node's machine RAM and the default minimum heap size will be 1.56% (1/64th) of the RAM.\n+\n+  The default JVM heap settings in this case can have undesirable behavior, including using unnecessary amounts of memory to the point where it might affect other pods that run on the same node.\n+\n+#### Configuring heap size\n+\n+If you specify pod memory limits, Oracle recommends configuring WebLogic Server heap sizes as a percentage. The JVM will interpret the percentage as a fraction of the limit. This is done using the JVM `-XX:MinRAMPercentage` and `-XX:MaxRAMPercentage` options in the `USER_MEM_ARGS` [domain resource environment variable]({{< relref \"/userguide/managing-domains/domain-resource#jvm-memory-and-java-option-environment-variables\" >}}).  For example:\n+\n+```\n+  spec:\n+    resources:\n+      env:\n+      - name: USER_MEM_ARGS\n+        value: \"--XX:MinRAMPercentage=25.0 --XX:MaxRAMPercentage=50.0 -Djava.security.egd=file:/dev/./urandom\"\n+```\n+\n+Additionally there's also a node-manager process that's running in the same container as the WebLogic Server which has its own heap and native memory requirements. Its heap is tuned by using `-Xms` and `-Xmx` in the `NODEMGR_MEM_ARGS` environment variable. Oracle recommends setting the node manager heap memory to fixed sizes, instead of percentages, where [the default tuning]({{< relref \"/userguide/managing-domains/domain-resource#jvm-memory-and-java-option-environment-variables\" >}}) is usually sufficient.\n+\n+{{% notice note %}}\n+Notice that the `NODEMGR_MEM_ARGS` and `USER_MEM_ARGS` environment variables both set `-Djava.security.egd=file:/dev/./urandom` by default so we have also included them in the above example for specifying a `USER_MEM_ARGS` value. This helps speed up Node Manager and WebLogic Server startup on systems with low entropy. \n+{{% /notice %}}\n+\n+In some cases, you might only want to configure memory resource requests but not configure memory resource limits. In such scenarios, you can use the traditional fixed heap size settings (`-Xms` and `-Xmx`) in your WebLogic Server `USER_MEM_ARGS` instead of the percentage settings (`-XX:MinRAMPercentage` and `-XX:MaxRAMPercentage`).\n+\n+### CPU resource considerations\n+\n+It's important to set both a CPU request and a limit for WebLogic Server pods. This ensures that all WebLogic server pods have enough CPU resources, and, as discussed earlier, if the request and limit are set to the same value, then they get a `guaranteed` QoS. A `guaranteed` QoS ensures the pods are handled with a higher priority during scheduling and so are the least likely to be evicted.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA2NzA1Mw=="}, "originalCommit": {"oid": "748fcce5ba5dfe90929d4fbc69c032e8bd398bb9"}, "originalPosition": 121}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5OTAyNDUxOnYy", "diffSide": "RIGHT", "path": "docs-source/content/faq/resource-settings.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNTowNDoyMFrOGsQ7AA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNTowNDoyMFrOGsQ7AA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA2Nzc3Ng==", "bodyText": "\"and/or\" -> \"or\"", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r449067776", "createdAt": "2020-07-02T15:04:20Z", "author": {"login": "rjeberhard"}, "path": "docs-source/content/faq/resource-settings.md", "diffHunk": "@@ -0,0 +1,177 @@\n+---\n+title: \"Pod Memory and CPU Resources\"\n+date: 2020-06-30T08:55:00-05:00\n+draft: false\n+weight: 25\n+---\n+\n+### Contents\n+\n+ - [Introduction](#introduction)\n+ - [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource)\n+ - [Determining pod Quality Of Service](#determining-pod-quality-of-service)\n+ - [Java heap size and memory resource considerations](#java-heap-size-and-memory-resource-considerations)\n+   - [Importance of setting heap size and memory resources](#importance-of-setting-heap-size-and-memory-resources)\n+   - [Default heap sizes](#default-heap-sizes)\n+   - [Configuring heap size](#configuring-heap-size)\n+ - [CPU resource considerations](#cpu-resource-considerations)\n+ - [Operator sample heap and resource configuration](#operator-sample-heap-and-resource-configuration)\n+ - [Configuring CPU affinity](#configuring-cpu-affinity)\n+ - [Measuring JVM heap, pod CPU, and pod memory](#measuring-jvm-heap-pod-cpu-and-pod-memory)\n+ - [References](#references)\n+\n+### Introduction\n+\n+An operator creates a pod for each WebLogic Server instance and each pod will have a container. You can tune pod container memory and/or CPU usage by configuring Kubernetes resource requests and limits, and you can tune a WebLogic JVM heap usage using the `USER_MEM_ARGS` environment variable in your domain resource. A resource request sets the minimum amount of a resource that a container requires. A resource limit is the maximum amount of resource a container is given and prevents a container from using more than its share of a resource. Additionally, resource requests and limits determine a pod's Quality of Service.\n+\n+This FAQ discusses tuning these parameters so WebLogic servers can run efficiently.\n+\n+### Setting resource requests and limits in a domain resource\n+\n+You can set Kubernetes memory and CPU requests and limits in a [domain resource]({{< relref \"/userguide/managing-domains/domain-resource\" >}}) using its `spec.serverPod.resources` stanza, and you can override the setting for individual WebLogic servers or clusters using the `serverPod.resources` element in `spec.adminServer`, `spec.clusters`, and/or `spec.managedServers`. For example: \n+\n+```\n+  spec:\n+    serverPod:\n+      requests:\n+        cpu: \"250m\"\n+        memory: \"768Mi\"\n+      limits:\n+        cpu: \"2\"\n+        memory: \"2Gi\"\n+```\n+\n+Limits and requests for CPU resources are measured in cpu units. One cpu, in Kubernetes, is equivalent to 1 vCPU/Core for cloud providers and 1 hyperthread on bare-metal Intel processors. An `m` suffix in a CPU attribute indicates 'milli-CPU', so `250m` is 25% of a CPU. \n+\n+Memory can be expressed in various units, where one `Mi` is one IEC unit mega-byte (1024^2), and one `Gi` is one IEC unit giga-byte (1024^3).\n+\n+See also [Managing Resources for Containers](https://kubernetes.io/docs/concepts/configuration/manage-resources-containers/), [Assign Memory Resources to Containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource) and [Assign CPU Resources to Containers and Pods](https://kubernetes.io/docs/tasks/configure-pod-container/assign-cpu-resource) in Kubernetes documentation.\n+\n+### Determining pod Quality Of Service\n+\n+A pod's Quality of Service (QoS) is based on whether it's configured with resource requests and limits:\n+\n+- **Best Effort QoS** (lowest priority): If you don't configure requests and limits for a pod, then the pod is given a `best-effort` QoS. In cases where a node runs out of non-shareable resources, a Kubernetes `kubelet` default out-of-resource eviction policy evicts running pods with the `best-effort` QoS first.\n+\n+- **Burstable QoS** (medium priority): If you configure both resource requests and limits for a pod, and set the requests to be less than their respective limits, then the pod will be given a `burstable` QoS. Similarly, if you only configure resource requests (without limits) for a pod, then the pod QoS is also `burstable`. If a node runs out of non-shareable resources, the node's `kubelet` will evict `burstable` pods only when there are no more running `best-effort` pods.\n+\n+- **Guaranteed QoS** (highest priority): If you set a pod's requests and the limits to equal values, then the pod will have a `guaranteed` QoS. These settings indicates that your pod will consume a fixed amount of memory and CPU. With this configuration, if a node runs out of shareable resources, then a Kubernetes node's `kubelet` will evict `best-effort` and `burstable` QoS pods before terminating `guaranteed` QoS pods.\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring WebLogic pods with memory and CPU requests and limits, and furthermore setting requests equal to their respective limits in order to ensure a `guaranteed` QoS.\n+{{% /notice %}}\n+\n+{{% notice note %}} \n+In newer version of Kubernetes, it is possible to fine tune scheduling and eviction policies using [Pod Priority Preemption](https://kubernetes.io/docs/concepts/configuration/pod-priority-preemption/) in combination with the `serverPod.priorityClassName` domain resource attribute. Note that Kubernetes already ships with two PriorityClasses: `system-cluster-critical` and `system-node-critical`. These are common classes and are used to [ensure that critical components are always scheduled first](https://kubernetes.io/docs/tasks/administer-cluster/guaranteed-scheduling-critical-addon-pods/).\n+{{% /notice %}}\n+\n+### Java heap size and memory resource considerations\n+\n+{{% notice note %}} \n+For most use cases, Oracle recommends configuring Java heap sizes for WebLogic pods instead of relying on defaults.\n+{{% /notice %}}\n+\n+#### Importance of setting heap size and memory resources\n+\n+It's extremely important to set correct heap sizes, memory requests, and memory limits for WebLogic JVMs and Pods. \n+\n+A WebLogic JVM heap must be sufficiently sized to run its applications and services, but should not be sized too large so as not to waste memory resources.\n+\n+A pod memory limit must be sufficiently sized to accommodate the configured heap and native memory requirements, but  not too big to waste memory resources. If a JVM's memory usage (sum of heap and native memory) exceeds its pod's limit, then the JVM process will be abruptly killed due to an out-of-memory error and the WebLogic container will consequently automatically restart due to a liveness probe failure.  \n+\n+Oracle recommends setting minimum and maximum heap (or heap percentages) and at least a container memory request.\n+\n+{{% notice warning %}}\n+If resource requests and resource limits are set too high, then your pods may not be scheduled due to lack of node resources, will unnecessarily use up CPU shared resources that could be used by other pods, or may prevent other pods from running.\n+{{% /notice %}}\n+\n+#### Default heap sizes\n+\n+With the latest Java versions, Java 8 update 191 and onwards or Java 11, then if you don't configure a heap size (no '-Xms' or '-Xms') the default heap size is dynamically determined:\n+- If you configure the memory limit for a container, then the JVM default maximum heap size will be 25% (1/4th) of container memory limit and the default minimum heap size will be 1.56% (1/64th) of the limit value. \n+\n+  The default JVM heap settings in this case are often too conservative because the WebLogic JVM is the only major process running in the container.\n+\n+- If no memory limit is configured, then the JVM default maximum heap size will be  25% (1/4th) of the its node's machine RAM and the default minimum heap size will be 1.56% (1/64th) of the RAM.\n+\n+  The default JVM heap settings in this case can have undesirable behavior, including using unnecessary amounts of memory to the point where it might affect other pods that run on the same node.\n+\n+#### Configuring heap size\n+\n+If you specify pod memory limits, Oracle recommends configuring WebLogic Server heap sizes as a percentage. The JVM will interpret the percentage as a fraction of the limit. This is done using the JVM `-XX:MinRAMPercentage` and `-XX:MaxRAMPercentage` options in the `USER_MEM_ARGS` [domain resource environment variable]({{< relref \"/userguide/managing-domains/domain-resource#jvm-memory-and-java-option-environment-variables\" >}}).  For example:\n+\n+```\n+  spec:\n+    resources:\n+      env:\n+      - name: USER_MEM_ARGS\n+        value: \"--XX:MinRAMPercentage=25.0 --XX:MaxRAMPercentage=50.0 -Djava.security.egd=file:/dev/./urandom\"\n+```\n+\n+Additionally there's also a node-manager process that's running in the same container as the WebLogic Server which has its own heap and native memory requirements. Its heap is tuned by using `-Xms` and `-Xmx` in the `NODEMGR_MEM_ARGS` environment variable. Oracle recommends setting the node manager heap memory to fixed sizes, instead of percentages, where [the default tuning]({{< relref \"/userguide/managing-domains/domain-resource#jvm-memory-and-java-option-environment-variables\" >}}) is usually sufficient.\n+\n+{{% notice note %}}\n+Notice that the `NODEMGR_MEM_ARGS` and `USER_MEM_ARGS` environment variables both set `-Djava.security.egd=file:/dev/./urandom` by default so we have also included them in the above example for specifying a `USER_MEM_ARGS` value. This helps speed up Node Manager and WebLogic Server startup on systems with low entropy. \n+{{% /notice %}}\n+\n+In some cases, you might only want to configure memory resource requests but not configure memory resource limits. In such scenarios, you can use the traditional fixed heap size settings (`-Xms` and `-Xmx`) in your WebLogic Server `USER_MEM_ARGS` instead of the percentage settings (`-XX:MinRAMPercentage` and `-XX:MaxRAMPercentage`).\n+\n+### CPU resource considerations\n+\n+It's important to set both a CPU request and a limit for WebLogic Server pods. This ensures that all WebLogic server pods have enough CPU resources, and, as discussed earlier, if the request and limit are set to the same value, then they get a `guaranteed` QoS. A `guaranteed` QoS ensures the pods are handled with a higher priority during scheduling and so are the least likely to be evicted.\n+\n+If a CPU request and limit are _not_ configured for a WebLogic Server pod:\n+- The pod can end up using all CPU resources available on its node and starve other containers from using shareable CPU cycles. \n+\n+- The WebLogic server JVM may choose an unsuitable garbage collection (GC) strategy.\n+\n+- A WebLogic Server self-tuning work-manager may incorrectly optimize the number of threads it allocates for the default thread pool. \n+\n+It's also important to keep in mind that if you set a value of CPU core count that's larger than core count of your biggest node, then the pod will never be scheduled. Let's say you have a pod that needs 4 cores but you have a kubernetes cluster that's comprised of 2 core VMs. In this case, your pod will never be scheduled and will have `Pending` status. For example:\n+\n+```\n+$ kubectl get pod sample-domain1-managed-server1 -n sample-domain1-ns\n+NAME                              READY   STATUS    RESTARTS   AGE\n+sample-domain1-managed-server1    0/1     Pending   0          65s\n+\n+$ kubectl describe pod sample-domain1-managed-server1 -n sample-domain1-ns\n+Events:\n+  Type     Reason            Age                From               Message\n+  ----     ------            ----               ----               -------\n+  Warning  FailedScheduling  16s (x3 over 26s)  default-scheduler  0/2 nodes are available: 2 Insufficient cpu.\n+```\n+\n+### Operator sample heap and resource configuration\n+\n+The operator samples configure non-default minimum and maximum heap sizes for WebLogic server JVMs of at least 256MB and 512MB respectively. You can edit a sample's template or domain resource `resources.env` `USER_MEM_ARGS` to have different values. See [Configuring heap size](#configuring-heap-size).\n+\n+Similarly, the operator samples configure CPU and memory resource requests to at least `250m` and `768Mi` respectively.\n+\n+There's no memory or CPU limit configured by default in samples and so the default QoS for sample WebLogic server pod's is `Burstable`. \n+\n+If you wish to set resource requests or limits differently on a sample domain resource or domain resource template, see [Setting resource requests and limits in a domain resource](#setting-resource-requests-and-limits-in-a-domain-resource). Or for samples that generate their domain resource using an 'inputs' file, see the `serverPodMemoryRequest`, `serverPodMemoryLimit`, `serverPodCpuRequest`, and `serverPodCpuLimit` parameters in the sample's `create-domain.sh` input file.\n+\n+### Configuring CPU affinity\n+\n+A Kubernetes hosted WebLogic server may exhibit high lock contention in comparison to an on-premise deployment. This lock contention may be due to lack of CPU cache affinity and/or scheduling latency when workloads move between different CPU cores.  ", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "748fcce5ba5dfe90929d4fbc69c032e8bd398bb9"}, "originalPosition": 156}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5OTA4NTU1OnYy", "diffSide": "RIGHT", "path": "kubernetes/samples/scripts/create-weblogic-domain/manually-create-domain/domain.yaml", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNToxOToyNlrOGsRh2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNzo1MjozN1rOGsXyKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA3NzcyMA==", "bodyText": "It looks like USER_MEM_ARGS here should have the urandom.  A long-standing error - can you please add it?", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r449077720", "createdAt": "2020-07-02T15:19:26Z", "author": {"login": "tbarnes-us"}, "path": "kubernetes/samples/scripts/create-weblogic-domain/manually-create-domain/domain.yaml", "diffHunk": "@@ -65,7 +65,10 @@ spec:\n     - name: JAVA_OPTIONS\n       value: \"-Dweblogic.StdoutDebugEnabled=false\"\n     - name: USER_MEM_ARGS\n-      value: \"-Xms64m -Xmx256m \"\n+      value: \"-Xms256m -Xmx512m \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "748fcce5ba5dfe90929d4fbc69c032e8bd398bb9"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTE4MDIwMw==", "bodyText": "Added urandom in USER_MEM_ARGS.", "url": "https://github.com/oracle/weblogic-kubernetes-operator/pull/1775#discussion_r449180203", "createdAt": "2020-07-02T17:52:37Z", "author": {"login": "ankedia"}, "path": "kubernetes/samples/scripts/create-weblogic-domain/manually-create-domain/domain.yaml", "diffHunk": "@@ -65,7 +65,10 @@ spec:\n     - name: JAVA_OPTIONS\n       value: \"-Dweblogic.StdoutDebugEnabled=false\"\n     - name: USER_MEM_ARGS\n-      value: \"-Xms64m -Xmx256m \"\n+      value: \"-Xms256m -Xmx512m \"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA3NzcyMA=="}, "originalCommit": {"oid": "748fcce5ba5dfe90929d4fbc69c032e8bd398bb9"}, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4424, "cost": 1, "resetAt": "2021-11-12T12:57:47Z"}}}