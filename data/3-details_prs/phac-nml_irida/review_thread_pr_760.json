{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDUzOTA3NzA3", "number": 760, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxNTo0NTozNFrOE6oTAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMDo1NjowMFrOE6wxhg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5OTEzMDg5OnYy", "diffSide": "RIGHT", "path": "src/main/java/ca/corefacility/bioinformatics/irida/repositories/filesystem/IridaFileStorageAwsUtilityImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQxNTo0NTozNFrOH10KkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMToxMTozOVrOH2CUhQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjE5MTI0OQ==", "bodyText": "You can also add this line to the try-with-resources block above, and it'll automatically close both the S3Object and the S3ObjectInputStream.  Just separate them with a ;", "url": "https://github.com/phac-nml/irida/pull/760#discussion_r526191249", "createdAt": "2020-11-18T15:45:34Z", "author": {"login": "tom114"}, "path": "src/main/java/ca/corefacility/bioinformatics/irida/repositories/filesystem/IridaFileStorageAwsUtilityImpl.java", "diffHunk": "@@ -0,0 +1,287 @@\n+package ca.corefacility.bioinformatics.irida.repositories.filesystem;\n+\n+import java.io.*;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.zip.GZIPInputStream;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.stereotype.Component;\n+\n+import ca.corefacility.bioinformatics.irida.exceptions.StorageException;\n+import ca.corefacility.bioinformatics.irida.model.sequenceFile.SequenceFile;\n+import ca.corefacility.bioinformatics.irida.model.sequenceFile.SequencingObject;\n+import ca.corefacility.bioinformatics.irida.util.FileUtils;\n+\n+import com.amazonaws.AmazonServiceException;\n+import com.amazonaws.auth.AWSStaticCredentialsProvider;\n+import com.amazonaws.auth.BasicAWSCredentials;\n+import com.amazonaws.regions.Regions;\n+import com.amazonaws.services.s3.AmazonS3;\n+import com.amazonaws.services.s3.AmazonS3ClientBuilder;\n+import com.amazonaws.services.s3.model.S3Object;\n+import com.amazonaws.services.s3.model.S3ObjectInputStream;\n+\n+/**\n+ * Component implementation of file utitlities for aws storage\n+ */\n+@Component\n+public class IridaFileStorageAwsUtilityImpl implements IridaFileStorageUtility{\n+\tprivate static final Logger logger = LoggerFactory.getLogger(IridaFileStorageAwsUtilityImpl.class);\n+\n+\tprivate String bucketName;\n+\tprivate BasicAWSCredentials awsCreds;\n+\tprivate AmazonS3 s3;\n+\n+\t@Autowired\n+\tpublic IridaFileStorageAwsUtilityImpl(String bucketName, String bucketRegion, String accessKey, String secretKey){\n+\t\tthis.awsCreds = new BasicAWSCredentials(accessKey, secretKey);\n+\t\tthis.s3 = AmazonS3ClientBuilder.standard().withRegion(Regions.fromName(bucketRegion))\n+\t\t\t\t.withCredentials(new AWSStaticCredentialsProvider(awsCreds)).build();\n+\t\tthis.bucketName = bucketName;\n+\t}\n+\n+\t/**\n+\t * {@inheritDoc}\n+\t */\n+\t@Override\n+\tpublic IridaTemporaryFile getTemporaryFile(Path file) {\n+\t\ttry {\n+\t\t\tlogger.trace(\"Getting file from aws s3 [\" + file.toString() + \"]\");\n+\t\t\tPath tempDirectory = Files.createTempDirectory(\"aws-tmp-\");\n+\t\t\tPath tempFile = tempDirectory.resolve(file.getFileName().toString());\n+\n+\t\t\ttry(S3Object s3Object = s3.getObject(bucketName, getAwsFileAbsolutePath(file))) {\n+\t\t\t\tS3ObjectInputStream s3ObjectInputStream = s3Object.getObjectContent();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "206263d4fbd866238220742e7646a8fd816e6cd5"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQyMzE3Mw==", "bodyText": "Updated in 2ff8d49. Didn't realize I could add multiple things to the try-with-resources block. Thanks", "url": "https://github.com/phac-nml/irida/pull/760#discussion_r526423173", "createdAt": "2020-11-18T21:11:39Z", "author": {"login": "deepsidhu85"}, "path": "src/main/java/ca/corefacility/bioinformatics/irida/repositories/filesystem/IridaFileStorageAwsUtilityImpl.java", "diffHunk": "@@ -0,0 +1,287 @@\n+package ca.corefacility.bioinformatics.irida.repositories.filesystem;\n+\n+import java.io.*;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.zip.GZIPInputStream;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.stereotype.Component;\n+\n+import ca.corefacility.bioinformatics.irida.exceptions.StorageException;\n+import ca.corefacility.bioinformatics.irida.model.sequenceFile.SequenceFile;\n+import ca.corefacility.bioinformatics.irida.model.sequenceFile.SequencingObject;\n+import ca.corefacility.bioinformatics.irida.util.FileUtils;\n+\n+import com.amazonaws.AmazonServiceException;\n+import com.amazonaws.auth.AWSStaticCredentialsProvider;\n+import com.amazonaws.auth.BasicAWSCredentials;\n+import com.amazonaws.regions.Regions;\n+import com.amazonaws.services.s3.AmazonS3;\n+import com.amazonaws.services.s3.AmazonS3ClientBuilder;\n+import com.amazonaws.services.s3.model.S3Object;\n+import com.amazonaws.services.s3.model.S3ObjectInputStream;\n+\n+/**\n+ * Component implementation of file utitlities for aws storage\n+ */\n+@Component\n+public class IridaFileStorageAwsUtilityImpl implements IridaFileStorageUtility{\n+\tprivate static final Logger logger = LoggerFactory.getLogger(IridaFileStorageAwsUtilityImpl.class);\n+\n+\tprivate String bucketName;\n+\tprivate BasicAWSCredentials awsCreds;\n+\tprivate AmazonS3 s3;\n+\n+\t@Autowired\n+\tpublic IridaFileStorageAwsUtilityImpl(String bucketName, String bucketRegion, String accessKey, String secretKey){\n+\t\tthis.awsCreds = new BasicAWSCredentials(accessKey, secretKey);\n+\t\tthis.s3 = AmazonS3ClientBuilder.standard().withRegion(Regions.fromName(bucketRegion))\n+\t\t\t\t.withCredentials(new AWSStaticCredentialsProvider(awsCreds)).build();\n+\t\tthis.bucketName = bucketName;\n+\t}\n+\n+\t/**\n+\t * {@inheritDoc}\n+\t */\n+\t@Override\n+\tpublic IridaTemporaryFile getTemporaryFile(Path file) {\n+\t\ttry {\n+\t\t\tlogger.trace(\"Getting file from aws s3 [\" + file.toString() + \"]\");\n+\t\t\tPath tempDirectory = Files.createTempDirectory(\"aws-tmp-\");\n+\t\t\tPath tempFile = tempDirectory.resolve(file.getFileName().toString());\n+\n+\t\t\ttry(S3Object s3Object = s3.getObject(bucketName, getAwsFileAbsolutePath(file))) {\n+\t\t\t\tS3ObjectInputStream s3ObjectInputStream = s3Object.getObjectContent();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjE5MTI0OQ=="}, "originalCommit": {"oid": "206263d4fbd866238220742e7646a8fd816e6cd5"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwMDUxOTc0OnYy", "diffSide": "RIGHT", "path": "src/main/java/ca/corefacility/bioinformatics/irida/repositories/filesystem/IridaFileStorageAwsUtilityImpl.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMDo1NjowMFrOH2BzYw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMToxMTo0NVrOH2CUtw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQxNDY5MQ==", "bodyText": "In all these throw new StorageException lines can you add the throwable cause to the exception?  That way the  caused by lines in the stack trace will actually show us where the exception came from.\nSo like throw new StorageException(\"some message\", e)", "url": "https://github.com/phac-nml/irida/pull/760#discussion_r526414691", "createdAt": "2020-11-18T20:56:00Z", "author": {"login": "tom114"}, "path": "src/main/java/ca/corefacility/bioinformatics/irida/repositories/filesystem/IridaFileStorageAwsUtilityImpl.java", "diffHunk": "@@ -0,0 +1,287 @@\n+package ca.corefacility.bioinformatics.irida.repositories.filesystem;\n+\n+import java.io.*;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.zip.GZIPInputStream;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.stereotype.Component;\n+\n+import ca.corefacility.bioinformatics.irida.exceptions.StorageException;\n+import ca.corefacility.bioinformatics.irida.model.sequenceFile.SequenceFile;\n+import ca.corefacility.bioinformatics.irida.model.sequenceFile.SequencingObject;\n+import ca.corefacility.bioinformatics.irida.util.FileUtils;\n+\n+import com.amazonaws.AmazonServiceException;\n+import com.amazonaws.auth.AWSStaticCredentialsProvider;\n+import com.amazonaws.auth.BasicAWSCredentials;\n+import com.amazonaws.regions.Regions;\n+import com.amazonaws.services.s3.AmazonS3;\n+import com.amazonaws.services.s3.AmazonS3ClientBuilder;\n+import com.amazonaws.services.s3.model.S3Object;\n+import com.amazonaws.services.s3.model.S3ObjectInputStream;\n+\n+/**\n+ * Component implementation of file utitlities for aws storage\n+ */\n+@Component\n+public class IridaFileStorageAwsUtilityImpl implements IridaFileStorageUtility{\n+\tprivate static final Logger logger = LoggerFactory.getLogger(IridaFileStorageAwsUtilityImpl.class);\n+\n+\tprivate String bucketName;\n+\tprivate BasicAWSCredentials awsCreds;\n+\tprivate AmazonS3 s3;\n+\n+\t@Autowired\n+\tpublic IridaFileStorageAwsUtilityImpl(String bucketName, String bucketRegion, String accessKey, String secretKey){\n+\t\tthis.awsCreds = new BasicAWSCredentials(accessKey, secretKey);\n+\t\tthis.s3 = AmazonS3ClientBuilder.standard().withRegion(Regions.fromName(bucketRegion))\n+\t\t\t\t.withCredentials(new AWSStaticCredentialsProvider(awsCreds)).build();\n+\t\tthis.bucketName = bucketName;\n+\t}\n+\n+\t/**\n+\t * {@inheritDoc}\n+\t */\n+\t@Override\n+\tpublic IridaTemporaryFile getTemporaryFile(Path file) {\n+\t\ttry {\n+\t\t\tlogger.trace(\"Getting file from aws s3 [\" + file.toString() + \"]\");\n+\t\t\tPath tempDirectory = Files.createTempDirectory(\"aws-tmp-\");\n+\t\t\tPath tempFile = tempDirectory.resolve(file.getFileName().toString());\n+\n+\t\t\ttry(S3Object s3Object = s3.getObject(bucketName, getAwsFileAbsolutePath(file))) {\n+\t\t\t\tS3ObjectInputStream s3ObjectInputStream = s3Object.getObjectContent();\n+\t\t\t\torg.apache.commons.io.FileUtils.copyInputStreamToFile(s3ObjectInputStream, tempFile.toFile());\n+\t\t\t\ts3ObjectInputStream.close();\n+\t\t\t} catch (AmazonServiceException e) {\n+\t\t\t\tlogger.error(e.getErrorMessage());\n+\t\t\t\tthrow new StorageException(e.getMessage());\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tlogger.error(\"Unable to get object from aws S3: \" + e);\n+\t\t\t\tthrow new StorageException(e.getMessage());\n+\t\t\t}\n+\n+\t\t\treturn new IridaTemporaryFile(tempFile, tempDirectory);\n+\t\t} catch (FileNotFoundException e) {\n+\t\t\tlogger.error(e.getMessage());\n+\t\t\tthrow new StorageException(e.getMessage());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "206263d4fbd866238220742e7646a8fd816e6cd5"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQyMzIyMw==", "bodyText": "Updated in 2ff8d49", "url": "https://github.com/phac-nml/irida/pull/760#discussion_r526423223", "createdAt": "2020-11-18T21:11:45Z", "author": {"login": "deepsidhu85"}, "path": "src/main/java/ca/corefacility/bioinformatics/irida/repositories/filesystem/IridaFileStorageAwsUtilityImpl.java", "diffHunk": "@@ -0,0 +1,287 @@\n+package ca.corefacility.bioinformatics.irida.repositories.filesystem;\n+\n+import java.io.*;\n+import java.nio.channels.FileChannel;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.StandardOpenOption;\n+import java.util.List;\n+import java.util.Optional;\n+import java.util.zip.GZIPInputStream;\n+\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import org.springframework.beans.factory.annotation.Autowired;\n+import org.springframework.stereotype.Component;\n+\n+import ca.corefacility.bioinformatics.irida.exceptions.StorageException;\n+import ca.corefacility.bioinformatics.irida.model.sequenceFile.SequenceFile;\n+import ca.corefacility.bioinformatics.irida.model.sequenceFile.SequencingObject;\n+import ca.corefacility.bioinformatics.irida.util.FileUtils;\n+\n+import com.amazonaws.AmazonServiceException;\n+import com.amazonaws.auth.AWSStaticCredentialsProvider;\n+import com.amazonaws.auth.BasicAWSCredentials;\n+import com.amazonaws.regions.Regions;\n+import com.amazonaws.services.s3.AmazonS3;\n+import com.amazonaws.services.s3.AmazonS3ClientBuilder;\n+import com.amazonaws.services.s3.model.S3Object;\n+import com.amazonaws.services.s3.model.S3ObjectInputStream;\n+\n+/**\n+ * Component implementation of file utitlities for aws storage\n+ */\n+@Component\n+public class IridaFileStorageAwsUtilityImpl implements IridaFileStorageUtility{\n+\tprivate static final Logger logger = LoggerFactory.getLogger(IridaFileStorageAwsUtilityImpl.class);\n+\n+\tprivate String bucketName;\n+\tprivate BasicAWSCredentials awsCreds;\n+\tprivate AmazonS3 s3;\n+\n+\t@Autowired\n+\tpublic IridaFileStorageAwsUtilityImpl(String bucketName, String bucketRegion, String accessKey, String secretKey){\n+\t\tthis.awsCreds = new BasicAWSCredentials(accessKey, secretKey);\n+\t\tthis.s3 = AmazonS3ClientBuilder.standard().withRegion(Regions.fromName(bucketRegion))\n+\t\t\t\t.withCredentials(new AWSStaticCredentialsProvider(awsCreds)).build();\n+\t\tthis.bucketName = bucketName;\n+\t}\n+\n+\t/**\n+\t * {@inheritDoc}\n+\t */\n+\t@Override\n+\tpublic IridaTemporaryFile getTemporaryFile(Path file) {\n+\t\ttry {\n+\t\t\tlogger.trace(\"Getting file from aws s3 [\" + file.toString() + \"]\");\n+\t\t\tPath tempDirectory = Files.createTempDirectory(\"aws-tmp-\");\n+\t\t\tPath tempFile = tempDirectory.resolve(file.getFileName().toString());\n+\n+\t\t\ttry(S3Object s3Object = s3.getObject(bucketName, getAwsFileAbsolutePath(file))) {\n+\t\t\t\tS3ObjectInputStream s3ObjectInputStream = s3Object.getObjectContent();\n+\t\t\t\torg.apache.commons.io.FileUtils.copyInputStreamToFile(s3ObjectInputStream, tempFile.toFile());\n+\t\t\t\ts3ObjectInputStream.close();\n+\t\t\t} catch (AmazonServiceException e) {\n+\t\t\t\tlogger.error(e.getErrorMessage());\n+\t\t\t\tthrow new StorageException(e.getMessage());\n+\t\t\t} catch (Exception e) {\n+\t\t\t\tlogger.error(\"Unable to get object from aws S3: \" + e);\n+\t\t\t\tthrow new StorageException(e.getMessage());\n+\t\t\t}\n+\n+\t\t\treturn new IridaTemporaryFile(tempFile, tempDirectory);\n+\t\t} catch (FileNotFoundException e) {\n+\t\t\tlogger.error(e.getMessage());\n+\t\t\tthrow new StorageException(e.getMessage());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQxNDY5MQ=="}, "originalCommit": {"oid": "206263d4fbd866238220742e7646a8fd816e6cd5"}, "originalPosition": 75}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 680, "cost": 1, "resetAt": "2021-11-12T12:57:47Z"}}}