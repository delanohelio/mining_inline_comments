{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQ2MDA3NzYz", "number": 13619, "title": "[BEAM-11527] Allow user defined Hadoop ReadSupport flags for ParquetReader", "bodyText": "ParquetIO.Sink supports user provided Configuration flags. With increasing number of flags in AvroParquetReader options.\nMaking the configuration flags accessible to the user makes it more usable.\nFor example, enable reading INT96 data.\nR:@iemejia can you help review.\nThank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:\n\n Choose reviewer(s) and mention them in a comment (R: @username).\n Format the pull request title like [BEAM-XXX] Fixes bug in ApproximateQuantiles, where you replace BEAM-XXX with the appropriate JIRA issue, if applicable. This will automatically link the pull request to the issue.\n\nPost-Commit Tests Status (on master branch)\n\n\n\nLang\nSDK\nDataflow\nFlink\nSamza\nSpark\nTwister2\n\n\n\n\nGo\n\n---\n\n---\n\n---\n\n\nJava\n\n\n\n\n\n\n\n\nPython\n\n\n\n---\n\n---\n\n\nXLang\n\n\n\n---\n\n---\n\n\n\nPre-Commit Tests Status (on master branch)\n\n\n\n---\nJava\nPython\nGo\nWebsite\nWhitespace\nTypescript\n\n\n\n\nNon-portable\n\n \n\n\n\n\n\n\nPortable\n---\n\n---\n---\n---\n---\n\n\n\nSee .test-infra/jenkins/README for trigger phrase, status and link of all Jenkins jobs.\nGitHub Actions Tests Status (on master branch)\n\n\n\nSee CI.md for more information about GitHub Actions CI.", "createdAt": "2020-12-28T09:12:37Z", "url": "https://github.com/apache/beam/pull/13619", "merged": true, "mergeCommit": {"oid": "cf7431368f932e496afcb3a3063b5cd60c152903"}, "closed": true, "closedAt": "2020-12-31T13:59:24Z", "author": {"login": "anantdamle"}, "timelineItems": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdqiaEXAH2gAyNTQ2MDA3NzYzOmE3MzAxYjc1MTc5MTMwODk2NTcxNWRmMjQ3NjU3YTg3NTVlNzViMTM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdrkYCagFqTU2MDM1NTc1Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "a7301b751791308965715df247657a8755e75b13", "author": {"user": null}, "url": "https://github.com/apache/beam/commit/a7301b751791308965715df247657a8755e75b13", "committedDate": "2020-12-28T09:07:18Z", "message": "[BEAM-11527] Add builder parameter to allow user defined Hadoop ReadSupport flags in Hadoop Configuration."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTU5MTQxNTE0", "url": "https://github.com/apache/beam/pull/13619#pullrequestreview-559141514", "createdAt": "2020-12-28T13:18:40Z", "commit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzoxODo0MFrOIL5Xag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzo1NzoyMVrOIL6Fag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0NTEzMA==", "bodyText": "Can you please remove this method and replace its uses with setConfiguration(makeHadoopConfiguration(...))", "url": "https://github.com/apache/beam/pull/13619#discussion_r549345130", "createdAt": "2020-12-28T13:18:40Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -311,6 +313,12 @@ public static ReadFiles readFiles(Schema schema) {\n \n       abstract Builder setAvroDataModel(GenericData model);\n \n+      abstract Builder setConfiguration(SerializableConfiguration configuration);\n+\n+      Builder setHadoopConfigurationFlags(Map<String, String> flags) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0NTg4OA==", "bodyText": "Please remove all definitions of this method and replace its uses with setConfiguration(makeHadoopConfiguration(...)) in all classes where it appears", "url": "https://github.com/apache/beam/pull/13619#discussion_r549345888", "createdAt": "2020-12-28T13:21:12Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -388,6 +402,12 @@ public void populateDisplayData(DisplayData.Builder builder) {\n \n       abstract Builder<T> setParseFn(SerializableFunction<GenericRecord, T> parseFn);\n \n+      abstract Builder<T> setConfiguration(SerializableConfiguration configuration);\n+\n+      Builder<T> setHadoopConfigurationFlags(Map<String, String> flags) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0NjcxOA==", "bodyText": "rename to configuration", "url": "https://github.com/apache/beam/pull/13619#discussion_r549346718", "createdAt": "2020-12-28T13:24:07Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -564,14 +623,20 @@ public ReadFiles withSplit() {\n       // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n       private static final long SPLIT_LIMIT = 64000000;\n \n+      private final SerializableConfiguration hadoopBaseConfig;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 177}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0Njc2Mg==", "bodyText": "rename to configuration", "url": "https://github.com/apache/beam/pull/13619#discussion_r549346762", "createdAt": "2020-12-28T13:24:18Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -564,14 +623,20 @@ public ReadFiles withSplit() {\n       // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n       private static final long SPLIT_LIMIT = 64000000;\n \n+      private final SerializableConfiguration hadoopBaseConfig;\n+\n       private final SerializableFunction<GenericRecord, T> parseFn;\n \n       SplitReadFn(\n-          GenericData model, Schema requestSchema, SerializableFunction<GenericRecord, T> parseFn) {\n+          GenericData model,\n+          Schema requestSchema,\n+          SerializableFunction<GenericRecord, T> parseFn,\n+          SerializableConfiguration hadoopBaseConfig) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0NzA1MQ==", "bodyText": "rename to configuration", "url": "https://github.com/apache/beam/pull/13619#discussion_r549347051", "createdAt": "2020-12-28T13:25:13Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -819,9 +884,15 @@ public Progress getProgress() {\n \n       private final SerializableFunction<GenericRecord, T> parseFn;\n \n-      ReadFn(GenericData model, SerializableFunction<GenericRecord, T> parseFn) {\n+      private final SerializableConfiguration hadoopBaseConfig;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 209}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0NzUwNQ==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/apache/beam/pull/13619#discussion_r549347505", "createdAt": "2020-12-28T13:26:33Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -920,13 +996,7 @@ public Sink withCompressionCodec(CompressionCodecName compressionCodecName) {\n \n     /** Specifies configuration to be passed into the sink's writer. */\n     public Sink withConfiguration(Map<String, String> configuration) {\n-      Configuration hadoopConfiguration = new Configuration();\n-      for (Map.Entry<String, String> entry : configuration.entrySet()) {\n-        hadoopConfiguration.set(entry.getKey(), entry.getValue());\n-      }\n-      return toBuilder()\n-          .setConfiguration(new SerializableConfiguration(hadoopConfiguration))\n-          .build();\n+      return toBuilder().setConfiguration(makeHadoopConfigurationUsingFlags(configuration)).build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 253}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0OTUwOQ==", "bodyText": "Can we move this method into the SerializableConfiguration class and make it public static SerializableConfiguration fromMap(Map<String, string> entries) {", "url": "https://github.com/apache/beam/pull/13619#discussion_r549349509", "createdAt": "2020-12-28T13:33:18Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -1058,6 +1128,18 @@ public GenericRecord apply(GenericRecord input) {\n     private GenericRecordPassthroughFn() {}\n   }\n \n+  /** Returns a new Hadoop {@link Configuration} instance with provided flags. */\n+  private static SerializableConfiguration makeHadoopConfigurationUsingFlags(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 262}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0OTY1NQ==", "bodyText": "test or remove", "url": "https://github.com/apache/beam/pull/13619#discussion_r549349655", "createdAt": "2020-12-28T13:33:48Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java", "diffHunk": "@@ -416,6 +416,9 @@ public void testWriteAndReadwithSplitUsingReflectDataSchemaWithDataModel() {\n     readPipeline.run().waitUntilFinish();\n   }\n \n+  @Test\n+  public void testConfigurationReadFile() {}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1MDE0Ng==", "bodyText": "can you please name the argument of the withConfiguration methods consistently everywhere as configuration instead of flags or hadoopConfigFlags", "url": "https://github.com/apache/beam/pull/13619#discussion_r549350146", "createdAt": "2020-12-28T13:35:23Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -332,6 +340,10 @@ public Read withProjection(Schema projectionSchema, Schema encoderSchema) {\n           .build();\n     }\n \n+    public Read withConfiguration(Map<String, String> flags) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1MTE2MA==", "bodyText": "Rename to withConfiguration to be consistent with the other methods + s/configurationFlags/configuration", "url": "https://github.com/apache/beam/pull/13619#discussion_r549351160", "createdAt": "2020-12-28T13:38:39Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -532,6 +581,12 @@ public ReadFiles withProjection(Schema projectionSchema, Schema encoderSchema) {\n           .setSplittable(true)\n           .build();\n     }\n+\n+    /** Specify Hadoop configuration for ParquetReader. */\n+    public ReadFiles withHadoopConfiguration(Map<String, String> configurationFlags) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 148}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1MjY3OQ==", "bodyText": "We should probably define a default value inside of the builders (read, readFiles, parseGenericRecords, parseFilesGenericRecords)  .setConfiguration(...) and since we define a default value we won't need this if", "url": "https://github.com/apache/beam/pull/13619#discussion_r549352679", "createdAt": "2020-12-28T13:43:46Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -835,13 +906,18 @@ public void processElement(ProcessContext processContext) throws Exception {\n \n         SeekableByteChannel seekableByteChannel = file.openSeekable();\n \n-        AvroParquetReader.Builder builder =\n-            AvroParquetReader.<GenericRecord>builder(new BeamParquetInputFile(seekableByteChannel));\n+        AvroParquetReader.Builder<GenericRecord> builder =\n+            AvroParquetReader.builder(new BeamParquetInputFile(seekableByteChannel));\n         if (modelClass != null) {\n           // all GenericData implementations have a static get method\n           builder = builder.withDataModel((GenericData) modelClass.getMethod(\"get\").invoke(null));\n         }\n \n+        if (hadoopBaseConfig != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 234}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1MjkwOA==", "bodyText": "s/Hadoop {@link Configuration}/{@link SerializableConfiguration}", "url": "https://github.com/apache/beam/pull/13619#discussion_r549352908", "createdAt": "2020-12-28T13:44:33Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -1058,6 +1128,18 @@ public GenericRecord apply(GenericRecord input) {\n     private GenericRecordPassthroughFn() {}\n   }\n \n+  /** Returns a new Hadoop {@link Configuration} instance with provided flags. */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 261}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1NjAzMw==", "bodyText": "Test with new Configuration(), this should not be nullable", "url": "https://github.com/apache/beam/pull/13619#discussion_r549356033", "createdAt": "2020-12-28T13:54:44Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java", "diffHunk": "@@ -147,7 +147,7 @@ public void testBlockTracker() throws Exception {\n   public void testSplitBlockWithLimit() {\n     ParquetIO.ReadFiles.SplitReadFn<GenericRecord> testFn =\n         new ParquetIO.ReadFiles.SplitReadFn<>(\n-            null, null, ParquetIO.GenericRecordPassthroughFn.create());\n+            null, null, ParquetIO.GenericRecordPassthroughFn.create(), null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1NjkwNg==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/apache/beam/pull/13619#discussion_r549356906", "createdAt": "2020-12-28T13:57:21Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -682,7 +747,7 @@ public void processElement(\n       }\n \n       public Configuration getConfWithModelClass() throws Exception {\n-        Configuration conf = new Configuration();\n+        Configuration conf = SerializableConfiguration.newConfiguration(hadoopBaseConfig);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 200}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ff5a094be93d41e14391600b071bdcb1369391bb", "author": {"user": null}, "url": "https://github.com/apache/beam/commit/ff5a094be93d41e14391600b071bdcb1369391bb", "committedDate": "2020-12-29T04:08:25Z", "message": "Consistency improvements and other fixes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTYwMzU1NzUz", "url": "https://github.com/apache/beam/pull/13619#pullrequestreview-560355753", "createdAt": "2020-12-31T13:58:22Z", "commit": {"oid": "ff5a094be93d41e14391600b071bdcb1369391bb"}, "state": "APPROVED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMVQxMzo1ODoyMlrOIM_KgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMVQxMzo1ODoyMlrOIM_KgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDQ4ODcwNA==", "bodyText": "I would prefer it to not be Nullable but since this is internal I suppose we can adjust this later, on the other hand if someday Parquet finally gets rid of its Hadoop dependencies probably the null value would align better.", "url": "https://github.com/apache/beam/pull/13619#discussion_r550488704", "createdAt": "2020-12-31T13:58:22Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java", "diffHunk": "@@ -147,7 +147,7 @@ public void testBlockTracker() throws Exception {\n   public void testSplitBlockWithLimit() {\n     ParquetIO.ReadFiles.SplitReadFn<GenericRecord> testFn =\n         new ParquetIO.ReadFiles.SplitReadFn<>(\n-            null, null, ParquetIO.GenericRecordPassthroughFn.create());\n+            null, null, ParquetIO.GenericRecordPassthroughFn.create(), null);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1NjAzMw=="}, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4268, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}