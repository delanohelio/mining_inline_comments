{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDI0MzE3MjQ1", "number": 11847, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxOTo1MjowNFrOEAptsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMDowNzoxOFrOEAqHiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MTE4ODk3OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/io/external/xlang_kafkaio_it_test.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxOTo1MjowNFrOGcERPQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMTo0Nzo0NlrOGcH6kA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA4MzI2MQ==", "bodyText": "Is this only for internal testing ?\nExternally, kafkaio.py should automatically startup an expansion service as long as we are in a release or a Beam repo clone.", "url": "https://github.com/apache/beam/pull/11847#discussion_r432083261", "createdAt": "2020-05-28T19:52:04Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/io/external/xlang_kafkaio_it_test.py", "diffHunk": "@@ -0,0 +1,145 @@\n+\"\"\"Integration test for Python cross-language pipelines for Java KafkaIO.\"\"\"\n+\n+from __future__ import absolute_import\n+\n+import contextlib\n+import logging\n+import os\n+import socket\n+import subprocess\n+import time\n+import typing\n+import unittest\n+\n+import grpc\n+\n+import apache_beam as beam\n+from apache_beam.io.external.kafka import ReadFromKafka\n+from apache_beam.io.external.kafka import WriteToKafka\n+from apache_beam.metrics import Metrics\n+from apache_beam.options.pipeline_options import PipelineOptions\n+from apache_beam.testing.test_pipeline import TestPipeline\n+\n+\n+class CrossLanguageKafkaIO(object):\n+  def __init__(self, bootstrap_servers, topic, expansion_service=None):\n+    self.bootstrap_servers = bootstrap_servers\n+    self.topic = topic\n+    self.expansion_service = expansion_service or (\n+        'localhost:%s' % os.environ.get('EXPANSION_PORT'))\n+    self.sum_counter = Metrics.counter('source', 'elements_sum')\n+\n+  def build_write_pipeline(self, pipeline):\n+    _ = (\n+        pipeline\n+        | 'Impulse' >> beam.Impulse()\n+        | 'Generate' >> beam.FlatMap(lambda x: range(1000)) # pylint: disable=range-builtin-not-iterating\n+        | 'Reshuffle' >> beam.Reshuffle()\n+        | 'MakeKV' >> beam.Map(lambda x:\n+                               (b'', str(x).encode())).with_output_types(\n+                                   typing.Tuple[bytes, bytes])\n+        | 'WriteToKafka' >> WriteToKafka(\n+            producer_config={'bootstrap.servers': self.bootstrap_servers},\n+            topic=self.topic,\n+            expansion_service=self.expansion_service))\n+\n+  def build_read_pipeline(self, pipeline):\n+    _ = (\n+        pipeline\n+        | 'ReadFromKafka' >> ReadFromKafka(\n+            consumer_config={\n+                'bootstrap.servers': self.bootstrap_servers,\n+                'auto.offset.reset': 'earliest'\n+            },\n+            topics=[self.topic],\n+            expansion_service=self.expansion_service)\n+        | 'Windowing' >> beam.WindowInto(\n+            beam.window.FixedWindows(300),\n+            trigger=beam.transforms.trigger.AfterProcessingTime(60),\n+            accumulation_mode=beam.transforms.trigger.AccumulationMode.\n+            DISCARDING)\n+        | 'DecodingValue' >> beam.Map(lambda elem: int(elem[1].decode()))\n+        | 'CombineGlobally' >> beam.CombineGlobally(sum).without_defaults()\n+        | 'SetSumCounter' >> beam.Map(self.sum_counter.inc))\n+\n+  def run_xlang_kafkaio(self, pipeline):\n+    self.build_write_pipeline(pipeline)\n+    self.build_read_pipeline(pipeline)\n+    pipeline.run(False)\n+\n+\n+@unittest.skipUnless(\n+    os.environ.get('LOCAL_KAFKA_JAR'),\n+    \"LOCAL_KAFKA_JAR environment var is not provided.\")\n+@unittest.skipUnless(\n+    os.environ.get('EXPANSION_JAR'),\n+    \"EXPANSION_JAR environment var is not provided.\")\n+class CrossLanguageKafkaIOTest(unittest.TestCase):\n+  def get_open_port(self):\n+    s = None\n+    try:\n+      s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n+    except:  # pylint: disable=bare-except\n+      # Above call will fail for nodes that only support IPv6.\n+      s = socket.socket(socket.AF_INET6, socket.SOCK_STREAM)\n+    s.bind(('localhost', 0))\n+    s.listen(1)\n+    port = s.getsockname()[1]\n+    s.close()\n+    return port\n+\n+  @contextlib.contextmanager\n+  def local_services(self, expansion_service_jar_file, local_kafka_jar_file):\n+    expansion_service_port = str(self.get_open_port())\n+    kafka_port = str(self.get_open_port())\n+    zookeeper_port = str(self.get_open_port())\n+\n+    expansion_server = None\n+    kafka_server = None\n+    try:\n+      expansion_server = subprocess.Popen(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "492adc0fadf110d087d63590e2b655b53543be8c"}, "originalPosition": 100}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE0Mjk5Mg==", "bodyText": "updated.", "url": "https://github.com/apache/beam/pull/11847#discussion_r432142992", "createdAt": "2020-05-28T21:47:46Z", "author": {"login": "ihji"}, "path": "sdks/python/apache_beam/io/external/xlang_kafkaio_it_test.py", "diffHunk": "@@ -0,0 +1,145 @@\n+\"\"\"Integration test for Python cross-language pipelines for Java KafkaIO.\"\"\"\n+\n+from __future__ import absolute_import\n+\n+import contextlib\n+import logging\n+import os\n+import socket\n+import subprocess\n+import time\n+import typing\n+import unittest\n+\n+import grpc\n+\n+import apache_beam as beam\n+from apache_beam.io.external.kafka import ReadFromKafka\n+from apache_beam.io.external.kafka import WriteToKafka\n+from apache_beam.metrics import Metrics\n+from apache_beam.options.pipeline_options import PipelineOptions\n+from apache_beam.testing.test_pipeline import TestPipeline\n+\n+\n+class CrossLanguageKafkaIO(object):\n+  def __init__(self, bootstrap_servers, topic, expansion_service=None):\n+    self.bootstrap_servers = bootstrap_servers\n+    self.topic = topic\n+    self.expansion_service = expansion_service or (\n+        'localhost:%s' % os.environ.get('EXPANSION_PORT'))\n+    self.sum_counter = Metrics.counter('source', 'elements_sum')\n+\n+  def build_write_pipeline(self, pipeline):\n+    _ = (\n+        pipeline\n+        | 'Impulse' >> beam.Impulse()\n+        | 'Generate' >> beam.FlatMap(lambda x: range(1000)) # pylint: disable=range-builtin-not-iterating\n+        | 'Reshuffle' >> beam.Reshuffle()\n+        | 'MakeKV' >> beam.Map(lambda x:\n+                               (b'', str(x).encode())).with_output_types(\n+                                   typing.Tuple[bytes, bytes])\n+        | 'WriteToKafka' >> WriteToKafka(\n+            producer_config={'bootstrap.servers': self.bootstrap_servers},\n+            topic=self.topic,\n+            expansion_service=self.expansion_service))\n+\n+  def build_read_pipeline(self, pipeline):\n+    _ = (\n+        pipeline\n+        | 'ReadFromKafka' >> ReadFromKafka(\n+            consumer_config={\n+                'bootstrap.servers': self.bootstrap_servers,\n+                'auto.offset.reset': 'earliest'\n+            },\n+            topics=[self.topic],\n+            expansion_service=self.expansion_service)\n+        | 'Windowing' >> beam.WindowInto(\n+            beam.window.FixedWindows(300),\n+            trigger=beam.transforms.trigger.AfterProcessingTime(60),\n+            accumulation_mode=beam.transforms.trigger.AccumulationMode.\n+            DISCARDING)\n+        | 'DecodingValue' >> beam.Map(lambda elem: int(elem[1].decode()))\n+        | 'CombineGlobally' >> beam.CombineGlobally(sum).without_defaults()\n+        | 'SetSumCounter' >> beam.Map(self.sum_counter.inc))\n+\n+  def run_xlang_kafkaio(self, pipeline):\n+    self.build_write_pipeline(pipeline)\n+    self.build_read_pipeline(pipeline)\n+    pipeline.run(False)\n+\n+\n+@unittest.skipUnless(\n+    os.environ.get('LOCAL_KAFKA_JAR'),\n+    \"LOCAL_KAFKA_JAR environment var is not provided.\")\n+@unittest.skipUnless(\n+    os.environ.get('EXPANSION_JAR'),\n+    \"EXPANSION_JAR environment var is not provided.\")\n+class CrossLanguageKafkaIOTest(unittest.TestCase):\n+  def get_open_port(self):\n+    s = None\n+    try:\n+      s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n+    except:  # pylint: disable=bare-except\n+      # Above call will fail for nodes that only support IPv6.\n+      s = socket.socket(socket.AF_INET6, socket.SOCK_STREAM)\n+    s.bind(('localhost', 0))\n+    s.listen(1)\n+    port = s.getsockname()[1]\n+    s.close()\n+    return port\n+\n+  @contextlib.contextmanager\n+  def local_services(self, expansion_service_jar_file, local_kafka_jar_file):\n+    expansion_service_port = str(self.get_open_port())\n+    kafka_port = str(self.get_open_port())\n+    zookeeper_port = str(self.get_open_port())\n+\n+    expansion_server = None\n+    kafka_server = None\n+    try:\n+      expansion_server = subprocess.Popen(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA4MzI2MQ=="}, "originalCommit": {"oid": "492adc0fadf110d087d63590e2b655b53543be8c"}, "originalPosition": 100}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MTIwNDgzOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/io/external/xlang_kafkaio_it_test.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQxOTo1NToxMVrOGcEa9A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMTo0NzoyM1rOGcH53g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA4NTc0OA==", "bodyText": "Can we update this test to not startup an expansion service and use the default expansion service setup for external tests ?", "url": "https://github.com/apache/beam/pull/11847#discussion_r432085748", "createdAt": "2020-05-28T19:55:11Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/io/external/xlang_kafkaio_it_test.py", "diffHunk": "@@ -0,0 +1,145 @@\n+\"\"\"Integration test for Python cross-language pipelines for Java KafkaIO.\"\"\"\n+\n+from __future__ import absolute_import\n+\n+import contextlib\n+import logging\n+import os\n+import socket\n+import subprocess\n+import time\n+import typing\n+import unittest\n+\n+import grpc\n+\n+import apache_beam as beam\n+from apache_beam.io.external.kafka import ReadFromKafka\n+from apache_beam.io.external.kafka import WriteToKafka\n+from apache_beam.metrics import Metrics\n+from apache_beam.options.pipeline_options import PipelineOptions\n+from apache_beam.testing.test_pipeline import TestPipeline\n+\n+\n+class CrossLanguageKafkaIO(object):\n+  def __init__(self, bootstrap_servers, topic, expansion_service=None):\n+    self.bootstrap_servers = bootstrap_servers\n+    self.topic = topic\n+    self.expansion_service = expansion_service or (\n+        'localhost:%s' % os.environ.get('EXPANSION_PORT'))\n+    self.sum_counter = Metrics.counter('source', 'elements_sum')\n+\n+  def build_write_pipeline(self, pipeline):\n+    _ = (\n+        pipeline\n+        | 'Impulse' >> beam.Impulse()\n+        | 'Generate' >> beam.FlatMap(lambda x: range(1000)) # pylint: disable=range-builtin-not-iterating\n+        | 'Reshuffle' >> beam.Reshuffle()\n+        | 'MakeKV' >> beam.Map(lambda x:\n+                               (b'', str(x).encode())).with_output_types(\n+                                   typing.Tuple[bytes, bytes])\n+        | 'WriteToKafka' >> WriteToKafka(\n+            producer_config={'bootstrap.servers': self.bootstrap_servers},\n+            topic=self.topic,\n+            expansion_service=self.expansion_service))\n+\n+  def build_read_pipeline(self, pipeline):\n+    _ = (\n+        pipeline\n+        | 'ReadFromKafka' >> ReadFromKafka(\n+            consumer_config={\n+                'bootstrap.servers': self.bootstrap_servers,\n+                'auto.offset.reset': 'earliest'\n+            },\n+            topics=[self.topic],\n+            expansion_service=self.expansion_service)\n+        | 'Windowing' >> beam.WindowInto(\n+            beam.window.FixedWindows(300),\n+            trigger=beam.transforms.trigger.AfterProcessingTime(60),\n+            accumulation_mode=beam.transforms.trigger.AccumulationMode.\n+            DISCARDING)\n+        | 'DecodingValue' >> beam.Map(lambda elem: int(elem[1].decode()))\n+        | 'CombineGlobally' >> beam.CombineGlobally(sum).without_defaults()\n+        | 'SetSumCounter' >> beam.Map(self.sum_counter.inc))\n+\n+  def run_xlang_kafkaio(self, pipeline):\n+    self.build_write_pipeline(pipeline)\n+    self.build_read_pipeline(pipeline)\n+    pipeline.run(False)\n+\n+\n+@unittest.skipUnless(\n+    os.environ.get('LOCAL_KAFKA_JAR'),\n+    \"LOCAL_KAFKA_JAR environment var is not provided.\")\n+@unittest.skipUnless(\n+    os.environ.get('EXPANSION_JAR'),\n+    \"EXPANSION_JAR environment var is not provided.\")\n+class CrossLanguageKafkaIOTest(unittest.TestCase):\n+  def get_open_port(self):\n+    s = None\n+    try:\n+      s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n+    except:  # pylint: disable=bare-except\n+      # Above call will fail for nodes that only support IPv6.\n+      s = socket.socket(socket.AF_INET6, socket.SOCK_STREAM)\n+    s.bind(('localhost', 0))\n+    s.listen(1)\n+    port = s.getsockname()[1]\n+    s.close()\n+    return port\n+\n+  @contextlib.contextmanager\n+  def local_services(self, expansion_service_jar_file, local_kafka_jar_file):\n+    expansion_service_port = str(self.get_open_port())\n+    kafka_port = str(self.get_open_port())\n+    zookeeper_port = str(self.get_open_port())\n+\n+    expansion_server = None\n+    kafka_server = None\n+    try:\n+      expansion_server = subprocess.Popen(\n+          ['java', '-jar', expansion_service_jar_file, expansion_service_port])\n+      kafka_server = subprocess.Popen(\n+          ['java', '-jar', local_kafka_jar_file, kafka_port, zookeeper_port])\n+      time.sleep(3)\n+      channel_creds = grpc.local_channel_credentials()\n+      with grpc.secure_channel('localhost:%s' % expansion_service_port,\n+                               channel_creds) as channel:\n+        grpc.channel_ready_future(channel).result()\n+\n+      yield expansion_service_port, kafka_port\n+    finally:\n+      if expansion_server:\n+        expansion_server.kill()\n+      if kafka_server:\n+        kafka_server.kill()\n+\n+  def get_options(self):\n+    options = PipelineOptions([\n+        '--runner',\n+        'FlinkRunner',\n+        '--parallelism',\n+        '2',\n+        '--experiment',\n+        'beam_fn_api'\n+    ])\n+    return options\n+\n+  def test_kafkaio_write(self):\n+    local_kafka_jar = os.environ.get('LOCAL_KAFKA_JAR')\n+    expansion_service_jar = os.environ.get('EXPANSION_JAR')\n+    with self.local_services(expansion_service_jar,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "492adc0fadf110d087d63590e2b655b53543be8c"}, "originalPosition": 131}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE0MjgxNA==", "bodyText": "updated.", "url": "https://github.com/apache/beam/pull/11847#discussion_r432142814", "createdAt": "2020-05-28T21:47:23Z", "author": {"login": "ihji"}, "path": "sdks/python/apache_beam/io/external/xlang_kafkaio_it_test.py", "diffHunk": "@@ -0,0 +1,145 @@\n+\"\"\"Integration test for Python cross-language pipelines for Java KafkaIO.\"\"\"\n+\n+from __future__ import absolute_import\n+\n+import contextlib\n+import logging\n+import os\n+import socket\n+import subprocess\n+import time\n+import typing\n+import unittest\n+\n+import grpc\n+\n+import apache_beam as beam\n+from apache_beam.io.external.kafka import ReadFromKafka\n+from apache_beam.io.external.kafka import WriteToKafka\n+from apache_beam.metrics import Metrics\n+from apache_beam.options.pipeline_options import PipelineOptions\n+from apache_beam.testing.test_pipeline import TestPipeline\n+\n+\n+class CrossLanguageKafkaIO(object):\n+  def __init__(self, bootstrap_servers, topic, expansion_service=None):\n+    self.bootstrap_servers = bootstrap_servers\n+    self.topic = topic\n+    self.expansion_service = expansion_service or (\n+        'localhost:%s' % os.environ.get('EXPANSION_PORT'))\n+    self.sum_counter = Metrics.counter('source', 'elements_sum')\n+\n+  def build_write_pipeline(self, pipeline):\n+    _ = (\n+        pipeline\n+        | 'Impulse' >> beam.Impulse()\n+        | 'Generate' >> beam.FlatMap(lambda x: range(1000)) # pylint: disable=range-builtin-not-iterating\n+        | 'Reshuffle' >> beam.Reshuffle()\n+        | 'MakeKV' >> beam.Map(lambda x:\n+                               (b'', str(x).encode())).with_output_types(\n+                                   typing.Tuple[bytes, bytes])\n+        | 'WriteToKafka' >> WriteToKafka(\n+            producer_config={'bootstrap.servers': self.bootstrap_servers},\n+            topic=self.topic,\n+            expansion_service=self.expansion_service))\n+\n+  def build_read_pipeline(self, pipeline):\n+    _ = (\n+        pipeline\n+        | 'ReadFromKafka' >> ReadFromKafka(\n+            consumer_config={\n+                'bootstrap.servers': self.bootstrap_servers,\n+                'auto.offset.reset': 'earliest'\n+            },\n+            topics=[self.topic],\n+            expansion_service=self.expansion_service)\n+        | 'Windowing' >> beam.WindowInto(\n+            beam.window.FixedWindows(300),\n+            trigger=beam.transforms.trigger.AfterProcessingTime(60),\n+            accumulation_mode=beam.transforms.trigger.AccumulationMode.\n+            DISCARDING)\n+        | 'DecodingValue' >> beam.Map(lambda elem: int(elem[1].decode()))\n+        | 'CombineGlobally' >> beam.CombineGlobally(sum).without_defaults()\n+        | 'SetSumCounter' >> beam.Map(self.sum_counter.inc))\n+\n+  def run_xlang_kafkaio(self, pipeline):\n+    self.build_write_pipeline(pipeline)\n+    self.build_read_pipeline(pipeline)\n+    pipeline.run(False)\n+\n+\n+@unittest.skipUnless(\n+    os.environ.get('LOCAL_KAFKA_JAR'),\n+    \"LOCAL_KAFKA_JAR environment var is not provided.\")\n+@unittest.skipUnless(\n+    os.environ.get('EXPANSION_JAR'),\n+    \"EXPANSION_JAR environment var is not provided.\")\n+class CrossLanguageKafkaIOTest(unittest.TestCase):\n+  def get_open_port(self):\n+    s = None\n+    try:\n+      s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n+    except:  # pylint: disable=bare-except\n+      # Above call will fail for nodes that only support IPv6.\n+      s = socket.socket(socket.AF_INET6, socket.SOCK_STREAM)\n+    s.bind(('localhost', 0))\n+    s.listen(1)\n+    port = s.getsockname()[1]\n+    s.close()\n+    return port\n+\n+  @contextlib.contextmanager\n+  def local_services(self, expansion_service_jar_file, local_kafka_jar_file):\n+    expansion_service_port = str(self.get_open_port())\n+    kafka_port = str(self.get_open_port())\n+    zookeeper_port = str(self.get_open_port())\n+\n+    expansion_server = None\n+    kafka_server = None\n+    try:\n+      expansion_server = subprocess.Popen(\n+          ['java', '-jar', expansion_service_jar_file, expansion_service_port])\n+      kafka_server = subprocess.Popen(\n+          ['java', '-jar', local_kafka_jar_file, kafka_port, zookeeper_port])\n+      time.sleep(3)\n+      channel_creds = grpc.local_channel_credentials()\n+      with grpc.secure_channel('localhost:%s' % expansion_service_port,\n+                               channel_creds) as channel:\n+        grpc.channel_ready_future(channel).result()\n+\n+      yield expansion_service_port, kafka_port\n+    finally:\n+      if expansion_server:\n+        expansion_server.kill()\n+      if kafka_server:\n+        kafka_server.kill()\n+\n+  def get_options(self):\n+    options = PipelineOptions([\n+        '--runner',\n+        'FlinkRunner',\n+        '--parallelism',\n+        '2',\n+        '--experiment',\n+        'beam_fn_api'\n+    ])\n+    return options\n+\n+  def test_kafkaio_write(self):\n+    local_kafka_jar = os.environ.get('LOCAL_KAFKA_JAR')\n+    expansion_service_jar = os.environ.get('EXPANSION_JAR')\n+    with self.local_services(expansion_service_jar,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA4NTc0OA=="}, "originalCommit": {"oid": "492adc0fadf110d087d63590e2b655b53543be8c"}, "originalPosition": 131}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjY5MTI1NTEzOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/io/external/xlang_kafkaio_it_test.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMDowNzoxOFrOGcE7CQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yOFQyMTo0NzowOFrOGcH5Zw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA5Mzk2MQ==", "bodyText": "Is this for external testing (given that internally we have a setup for starting Kafka cluster) ? Or is this required for this test for some reason ?", "url": "https://github.com/apache/beam/pull/11847#discussion_r432093961", "createdAt": "2020-05-28T20:07:18Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/io/external/xlang_kafkaio_it_test.py", "diffHunk": "@@ -0,0 +1,145 @@\n+\"\"\"Integration test for Python cross-language pipelines for Java KafkaIO.\"\"\"\n+\n+from __future__ import absolute_import\n+\n+import contextlib\n+import logging\n+import os\n+import socket\n+import subprocess\n+import time\n+import typing\n+import unittest\n+\n+import grpc\n+\n+import apache_beam as beam\n+from apache_beam.io.external.kafka import ReadFromKafka\n+from apache_beam.io.external.kafka import WriteToKafka\n+from apache_beam.metrics import Metrics\n+from apache_beam.options.pipeline_options import PipelineOptions\n+from apache_beam.testing.test_pipeline import TestPipeline\n+\n+\n+class CrossLanguageKafkaIO(object):\n+  def __init__(self, bootstrap_servers, topic, expansion_service=None):\n+    self.bootstrap_servers = bootstrap_servers\n+    self.topic = topic\n+    self.expansion_service = expansion_service or (\n+        'localhost:%s' % os.environ.get('EXPANSION_PORT'))\n+    self.sum_counter = Metrics.counter('source', 'elements_sum')\n+\n+  def build_write_pipeline(self, pipeline):\n+    _ = (\n+        pipeline\n+        | 'Impulse' >> beam.Impulse()\n+        | 'Generate' >> beam.FlatMap(lambda x: range(1000)) # pylint: disable=range-builtin-not-iterating\n+        | 'Reshuffle' >> beam.Reshuffle()\n+        | 'MakeKV' >> beam.Map(lambda x:\n+                               (b'', str(x).encode())).with_output_types(\n+                                   typing.Tuple[bytes, bytes])\n+        | 'WriteToKafka' >> WriteToKafka(\n+            producer_config={'bootstrap.servers': self.bootstrap_servers},\n+            topic=self.topic,\n+            expansion_service=self.expansion_service))\n+\n+  def build_read_pipeline(self, pipeline):\n+    _ = (\n+        pipeline\n+        | 'ReadFromKafka' >> ReadFromKafka(\n+            consumer_config={\n+                'bootstrap.servers': self.bootstrap_servers,\n+                'auto.offset.reset': 'earliest'\n+            },\n+            topics=[self.topic],\n+            expansion_service=self.expansion_service)\n+        | 'Windowing' >> beam.WindowInto(\n+            beam.window.FixedWindows(300),\n+            trigger=beam.transforms.trigger.AfterProcessingTime(60),\n+            accumulation_mode=beam.transforms.trigger.AccumulationMode.\n+            DISCARDING)\n+        | 'DecodingValue' >> beam.Map(lambda elem: int(elem[1].decode()))\n+        | 'CombineGlobally' >> beam.CombineGlobally(sum).without_defaults()\n+        | 'SetSumCounter' >> beam.Map(self.sum_counter.inc))\n+\n+  def run_xlang_kafkaio(self, pipeline):\n+    self.build_write_pipeline(pipeline)\n+    self.build_read_pipeline(pipeline)\n+    pipeline.run(False)\n+\n+\n+@unittest.skipUnless(\n+    os.environ.get('LOCAL_KAFKA_JAR'),\n+    \"LOCAL_KAFKA_JAR environment var is not provided.\")\n+@unittest.skipUnless(\n+    os.environ.get('EXPANSION_JAR'),\n+    \"EXPANSION_JAR environment var is not provided.\")\n+class CrossLanguageKafkaIOTest(unittest.TestCase):\n+  def get_open_port(self):\n+    s = None\n+    try:\n+      s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n+    except:  # pylint: disable=bare-except\n+      # Above call will fail for nodes that only support IPv6.\n+      s = socket.socket(socket.AF_INET6, socket.SOCK_STREAM)\n+    s.bind(('localhost', 0))\n+    s.listen(1)\n+    port = s.getsockname()[1]\n+    s.close()\n+    return port\n+\n+  @contextlib.contextmanager\n+  def local_services(self, expansion_service_jar_file, local_kafka_jar_file):\n+    expansion_service_port = str(self.get_open_port())\n+    kafka_port = str(self.get_open_port())\n+    zookeeper_port = str(self.get_open_port())\n+\n+    expansion_server = None\n+    kafka_server = None\n+    try:\n+      expansion_server = subprocess.Popen(\n+          ['java', '-jar', expansion_service_jar_file, expansion_service_port])\n+      kafka_server = subprocess.Popen(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "492adc0fadf110d087d63590e2b655b53543be8c"}, "originalPosition": 102}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjE0MjY5NQ==", "bodyText": "Yes, this is for external testing (probably only for small scale correctness tests, we may still need kubernetes cluster for large scale performance tests).", "url": "https://github.com/apache/beam/pull/11847#discussion_r432142695", "createdAt": "2020-05-28T21:47:08Z", "author": {"login": "ihji"}, "path": "sdks/python/apache_beam/io/external/xlang_kafkaio_it_test.py", "diffHunk": "@@ -0,0 +1,145 @@\n+\"\"\"Integration test for Python cross-language pipelines for Java KafkaIO.\"\"\"\n+\n+from __future__ import absolute_import\n+\n+import contextlib\n+import logging\n+import os\n+import socket\n+import subprocess\n+import time\n+import typing\n+import unittest\n+\n+import grpc\n+\n+import apache_beam as beam\n+from apache_beam.io.external.kafka import ReadFromKafka\n+from apache_beam.io.external.kafka import WriteToKafka\n+from apache_beam.metrics import Metrics\n+from apache_beam.options.pipeline_options import PipelineOptions\n+from apache_beam.testing.test_pipeline import TestPipeline\n+\n+\n+class CrossLanguageKafkaIO(object):\n+  def __init__(self, bootstrap_servers, topic, expansion_service=None):\n+    self.bootstrap_servers = bootstrap_servers\n+    self.topic = topic\n+    self.expansion_service = expansion_service or (\n+        'localhost:%s' % os.environ.get('EXPANSION_PORT'))\n+    self.sum_counter = Metrics.counter('source', 'elements_sum')\n+\n+  def build_write_pipeline(self, pipeline):\n+    _ = (\n+        pipeline\n+        | 'Impulse' >> beam.Impulse()\n+        | 'Generate' >> beam.FlatMap(lambda x: range(1000)) # pylint: disable=range-builtin-not-iterating\n+        | 'Reshuffle' >> beam.Reshuffle()\n+        | 'MakeKV' >> beam.Map(lambda x:\n+                               (b'', str(x).encode())).with_output_types(\n+                                   typing.Tuple[bytes, bytes])\n+        | 'WriteToKafka' >> WriteToKafka(\n+            producer_config={'bootstrap.servers': self.bootstrap_servers},\n+            topic=self.topic,\n+            expansion_service=self.expansion_service))\n+\n+  def build_read_pipeline(self, pipeline):\n+    _ = (\n+        pipeline\n+        | 'ReadFromKafka' >> ReadFromKafka(\n+            consumer_config={\n+                'bootstrap.servers': self.bootstrap_servers,\n+                'auto.offset.reset': 'earliest'\n+            },\n+            topics=[self.topic],\n+            expansion_service=self.expansion_service)\n+        | 'Windowing' >> beam.WindowInto(\n+            beam.window.FixedWindows(300),\n+            trigger=beam.transforms.trigger.AfterProcessingTime(60),\n+            accumulation_mode=beam.transforms.trigger.AccumulationMode.\n+            DISCARDING)\n+        | 'DecodingValue' >> beam.Map(lambda elem: int(elem[1].decode()))\n+        | 'CombineGlobally' >> beam.CombineGlobally(sum).without_defaults()\n+        | 'SetSumCounter' >> beam.Map(self.sum_counter.inc))\n+\n+  def run_xlang_kafkaio(self, pipeline):\n+    self.build_write_pipeline(pipeline)\n+    self.build_read_pipeline(pipeline)\n+    pipeline.run(False)\n+\n+\n+@unittest.skipUnless(\n+    os.environ.get('LOCAL_KAFKA_JAR'),\n+    \"LOCAL_KAFKA_JAR environment var is not provided.\")\n+@unittest.skipUnless(\n+    os.environ.get('EXPANSION_JAR'),\n+    \"EXPANSION_JAR environment var is not provided.\")\n+class CrossLanguageKafkaIOTest(unittest.TestCase):\n+  def get_open_port(self):\n+    s = None\n+    try:\n+      s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n+    except:  # pylint: disable=bare-except\n+      # Above call will fail for nodes that only support IPv6.\n+      s = socket.socket(socket.AF_INET6, socket.SOCK_STREAM)\n+    s.bind(('localhost', 0))\n+    s.listen(1)\n+    port = s.getsockname()[1]\n+    s.close()\n+    return port\n+\n+  @contextlib.contextmanager\n+  def local_services(self, expansion_service_jar_file, local_kafka_jar_file):\n+    expansion_service_port = str(self.get_open_port())\n+    kafka_port = str(self.get_open_port())\n+    zookeeper_port = str(self.get_open_port())\n+\n+    expansion_server = None\n+    kafka_server = None\n+    try:\n+      expansion_server = subprocess.Popen(\n+          ['java', '-jar', expansion_service_jar_file, expansion_service_port])\n+      kafka_server = subprocess.Popen(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzMjA5Mzk2MQ=="}, "originalCommit": {"oid": "492adc0fadf110d087d63590e2b655b53543be8c"}, "originalPosition": 102}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3723, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}