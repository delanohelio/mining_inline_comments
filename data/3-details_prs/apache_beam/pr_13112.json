{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAzNDU2MzAx", "number": 13112, "title": "[BEAM-11065] Apache Beam pipeline example to ingest from Apache Kafka to Google Pub/Sub ", "bodyText": "[Proposal] Apache Beam pipeline example to ingest data from Apache Kafka to Google Cloud Pub/Sub. It can be used as a Dataflow Flex template in the Google Cloud Platform.\n\nThank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:\n\n Choose reviewer(s) and mention them in a comment (R: @username).\n Format the pull request title like [BEAM-XXX] Fixes bug in ApproximateQuantiles, where you replace BEAM-XXX with the appropriate JIRA issue, if applicable. This will automatically link the pull request to the issue.\n Update CHANGES.md with noteworthy changes.\n If this contribution is large, please file an Apache Individual Contributor License Agreement.\n\nSee the Contributor Guide for more tips on how to make review process smoother.\nPost-Commit Tests Status (on master branch)\n\n\n\nLang\nSDK\nDataflow\nFlink\nSamza\nSpark\nTwister2\n\n\n\n\nGo\n\n---\n\n---\n\n---\n\n\nJava\n\n\n\n\n\n\n\n\nPython\n\n\n\n---\n\n---\n\n\nXLang\n\n---\n\n---\n\n---\n\n\n\nPre-Commit Tests Status (on master branch)\n\n\n\n---\nJava\nPython\nGo\nWebsite\nWhitespace\nTypescript\n\n\n\n\nNon-portable\n\n \n\n\n\n\n\n\nPortable\n---\n\n---\n---\n---\n---\n\n\n\nSee .test-infra/jenkins/README for trigger phrase, status and link of all Jenkins jobs.\nGitHub Actions Tests Status (on master branch)\n\n\n\nSee CI.md for more information about GitHub Actions CI.", "createdAt": "2020-10-14T15:25:05Z", "url": "https://github.com/apache/beam/pull/13112", "merged": true, "mergeCommit": {"oid": "34ae21b108b354ce588d4094419a1dc8aa36f2ac"}, "closed": true, "closedAt": "2020-12-14T22:40:30Z", "author": {"login": "ilya-kozyrev"}, "timelineItems": {"totalCount": 86, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdMVSwOgH2gAyNTAzNDU2MzAxOjQ1ZWYwMDkxZGYyNDg1OTVmNmRhOTQ0YzM5OTg1N2VkNzFjMjdhNTY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdmNpEhgFqTU1MTk1NDE0Mw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "45ef0091df248595f6da944c399857ed71c27a56", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/45ef0091df248595f6da944c399857ed71c27a56", "committedDate": "2020-09-25T12:52:49Z", "message": "add initial template and dependencies"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "321c7158f61765c8d515f20e607dbb7ef07473a4", "author": {"user": null}, "url": "https://github.com/apache/beam/commit/321c7158f61765c8d515f20e607dbb7ef07473a4", "committedDate": "2020-10-02T14:21:36Z", "message": "Added flex template creation with metadata support and instructions"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e88fd31914a79223164181f151ea7ec14965ec8f", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/e88fd31914a79223164181f151ea7ec14965ec8f", "committedDate": "2020-10-02T14:40:42Z", "message": "added new gradle modules for templates"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ed9be584996f3d210e55c72d7c2b7ea737c01a2e", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/ed9be584996f3d210e55c72d7c2b7ea737c01a2e", "committedDate": "2020-10-02T14:40:42Z", "message": "moved metadata to template location, reverted examples build.gradle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e2a8317b55d3969ead0b58fd8d11f68bec1db40e", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/e2a8317b55d3969ead0b58fd8d11f68bec1db40e", "committedDate": "2020-10-02T14:40:42Z", "message": "Moved KafkaToPubsub to template, implemented options in separate package"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4ab8e58a8b96ac3d5ce4ee8189b2268a103829ce", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/4ab8e58a8b96ac3d5ce4ee8189b2268a103829ce", "committedDate": "2020-10-02T14:40:42Z", "message": "Added package-info.java to new packages"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aeb64114430f20a22abd671a698400d1b012f6e9", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/aeb64114430f20a22abd671a698400d1b012f6e9", "committedDate": "2020-10-02T15:02:53Z", "message": "Reverted build.gradle to master branch state"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "61bf9921454afa9923c438d68437eeb1af6949e9", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/61bf9921454afa9923c438d68437eeb1af6949e9", "committedDate": "2020-10-02T15:11:46Z", "message": "fixed JAVADOC and metadata"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "015e6e6913656d3f95a423bdee49d3731dc40826", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/015e6e6913656d3f95a423bdee49d3731dc40826", "committedDate": "2020-10-05T13:55:53Z", "message": "Added the Read Me section with a step-by-step guide"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "129f4666adf36c453ff56f69b60ae8f13456b38e", "author": {"user": {"login": "AKosolapov", "name": "Alex Kosolapov"}}, "url": "https://github.com/apache/beam/commit/129f4666adf36c453ff56f69b60ae8f13456b38e", "committedDate": "2020-10-05T16:52:02Z", "message": "Update README.md"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6db17adc37596b202332e38ddc2f8b61df482335", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/6db17adc37596b202332e38ddc2f8b61df482335", "committedDate": "2020-10-07T08:32:33Z", "message": "Readme fixes regarding comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "01e4987c5957272409dc4f4f86f359b3aac77aa5", "author": {"user": {"login": "AKosolapov", "name": "Alex Kosolapov"}}, "url": "https://github.com/apache/beam/commit/01e4987c5957272409dc4f4f86f359b3aac77aa5", "committedDate": "2020-10-07T13:56:24Z", "message": "Update README.md"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b6decbd1d617770f5e74e048306d6eb94c01ebd5", "author": {"user": {"login": "AKosolapov", "name": "Alex Kosolapov"}}, "url": "https://github.com/apache/beam/commit/b6decbd1d617770f5e74e048306d6eb94c01ebd5", "committedDate": "2020-10-07T14:13:02Z", "message": "Update README.md"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8e41d136f1cc94eb2442608fd7117b8d7972fd2c", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/8e41d136f1cc94eb2442608fd7117b8d7972fd2c", "committedDate": "2020-10-07T14:24:01Z", "message": "Fixed typos in README.md"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "369a2058ac462da9bb2658bc756a85b66b9d1f28", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/369a2058ac462da9bb2658bc756a85b66b9d1f28", "committedDate": "2020-10-07T15:32:31Z", "message": "refactored README.md added case to run template locally"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "01db016e4bafb1aa1e525b02e44e2923d82488f6", "author": {"user": {"login": "AKosolapov", "name": "Alex Kosolapov"}}, "url": "https://github.com/apache/beam/commit/01db016e4bafb1aa1e525b02e44e2923d82488f6", "committedDate": "2020-10-07T15:56:03Z", "message": "Update README.md"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5821bdff66137df7a9978419d384e3b18079b6a6", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/5821bdff66137df7a9978419d384e3b18079b6a6", "committedDate": "2020-10-08T12:46:52Z", "message": "fix build script for dataflow in README.md"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "98de91f880cbf01585a24de12c6c459af1ee7380", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/98de91f880cbf01585a24de12c6c459af1ee7380", "committedDate": "2020-10-09T08:19:40Z", "message": "Added unit test and fixed metadata file"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fc1ed9adb283ad69441c74cb1858dcc7e81a5668", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/fc1ed9adb283ad69441c74cb1858dcc7e81a5668", "committedDate": "2020-10-09T09:21:25Z", "message": "Added Licenses and style fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6e159c7551d135223ef2653992936e5515b88d66", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/6e159c7551d135223ef2653992936e5515b88d66", "committedDate": "2020-10-12T15:29:01Z", "message": "Added support for retrieving Kafka credentials from HashiCorp Vault secret storage with url and token"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "523f796b5bd42eca177c3e4a170cb6c5b26bf2e2", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/523f796b5bd42eca177c3e4a170cb6c5b26bf2e2", "committedDate": "2020-10-13T14:04:52Z", "message": "Updated README.md and metadata with parameters for Vault access; refactored Kafka configuration"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e78c02cf851ee4fa634eee4b838b6752e6645d20", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/e78c02cf851ee4fa634eee4b838b6752e6645d20", "committedDate": "2020-10-13T14:09:26Z", "message": "Style fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1055ec2c6e12f11fae67c6885721ba3f5e86d58f", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/1055ec2c6e12f11fae67c6885721ba3f5e86d58f", "committedDate": "2020-10-14T14:54:35Z", "message": "Added description for Vault parameters in metadata"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "77defc11c7707acd417b45a0779be634dcf8953f", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/77defc11c7707acd417b45a0779be634dcf8953f", "committedDate": "2020-10-15T14:49:40Z", "message": "FIX trailing whitespaces in README.md"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d6ab0f6b13652260cb50b159575c253a7036b97f", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/d6ab0f6b13652260cb50b159575c253a7036b97f", "committedDate": "2020-10-15T14:54:03Z", "message": "FIX. Blank line contains whitespace README.md"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8881ff34a381cb3985aa0e2621c7a54fda8396e8", "author": {"user": {"login": "AKosolapov", "name": "Alex Kosolapov"}}, "url": "https://github.com/apache/beam/commit/8881ff34a381cb3985aa0e2621c7a54fda8396e8", "committedDate": "2020-10-15T17:43:09Z", "message": "Update README.md"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a821560f90e6a02928f30d132251dec9a22baa21", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/a821560f90e6a02928f30d132251dec9a22baa21", "committedDate": "2020-10-16T12:16:21Z", "message": "Refactored to examples folder"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c22f110872aa19ba60e8b593556acabcd08381eb", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/c22f110872aa19ba60e8b593556acabcd08381eb", "committedDate": "2020-10-16T15:58:14Z", "message": "Added conversion from JSON into PubsubMessage and extracted all transformations from the pipeline class into the separate class"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b56ec7bd557ad965b7ce8c0fdcaa5331a8858245", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/b56ec7bd557ad965b7ce8c0fdcaa5331a8858245", "committedDate": "2020-10-16T16:37:37Z", "message": "Whitespacelint fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "71308e573cd3f9b94913534020ddd76d67eba748", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/71308e573cd3f9b94913534020ddd76d67eba748", "committedDate": "2020-10-21T15:09:13Z", "message": "Updated README.md and output formats"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3c218dfc039276c684fe2218bf3a326b76bcd791", "author": {"user": {"login": "AKosolapov", "name": "Alex Kosolapov"}}, "url": "https://github.com/apache/beam/commit/3c218dfc039276c684fe2218bf3a326b76bcd791", "committedDate": "2020-10-22T07:11:43Z", "message": "Update README.md"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8bdfc3413ae39c7f3451a675b15ae41a56d4689a", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/8bdfc3413ae39c7f3451a675b15ae41a56d4689a", "committedDate": "2020-10-22T14:49:44Z", "message": "Update README.md"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "72c38e08a85445f627696c14e775010fe70d2d71", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/72c38e08a85445f627696c14e775010fe70d2d71", "committedDate": "2020-10-22T14:50:18Z", "message": "Merge pull request #2 from akvelon/Readme\n\nUpdate README.md"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1d94fcdd79cd485b1d8193c443f72dcd88da6df6", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/1d94fcdd79cd485b1d8193c443f72dcd88da6df6", "committedDate": "2020-10-22T16:13:13Z", "message": "Added support for SSL and removed outputFormat option"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a194d54ffc9a54d7a76607203bbb1a3a5dd963f4", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/a194d54ffc9a54d7a76607203bbb1a3a5dd963f4", "committedDate": "2020-10-22T20:31:05Z", "message": "Added avro usage example"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "12c553fbb925bcc3c6240bfc89feb6f0fb9ae03a", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/12c553fbb925bcc3c6240bfc89feb6f0fb9ae03a", "committedDate": "2020-10-22T20:41:11Z", "message": "Merge branch 'KafkaToPubsubTemplate' of github.com:akvelon/beam into KafkaToPubsubTemplate\n\n\u0001 Conflicts:\n\u0001\texamples/templates/java/kafka-to-pubsub/src/main/java/org/apache/beam/templates/KafkaToPubsub.java\n\u0001\texamples/templates/java/kafka-to-pubsub/src/main/java/org/apache/beam/templates/transforms/FormatTransform.java"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f754aa81f0a307ab78c73266d576256da0ac8ecc", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/f754aa81f0a307ab78c73266d576256da0ac8ecc", "committedDate": "2020-10-22T20:45:56Z", "message": "Added ssl to AVRO reader"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "695467aea548fa0a4c57cc0cac8db757d886a680", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/695467aea548fa0a4c57cc0cac8db757d886a680", "committedDate": "2020-10-22T20:51:57Z", "message": "FIX whitespaces."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0212b90f22159e083f67fe595284fa4b76ce43f1", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/0212b90f22159e083f67fe595284fa4b76ce43f1", "committedDate": "2020-10-23T14:52:51Z", "message": "added readme/docs regarding of Avro"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0d0c8249e1143b5198509539b3cb313cea103c7a", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/0d0c8249e1143b5198509539b3cb313cea103c7a", "committedDate": "2020-10-26T09:56:25Z", "message": "README.md and javadoc fixes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a4fea29c76c33321271f21b699e379e3b66a2edc", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/a4fea29c76c33321271f21b699e379e3b66a2edc", "committedDate": "2020-10-26T14:52:07Z", "message": "Added Vault's response JSON schema description"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "17957b8a9f5468883de469dd9999fbb051301beb", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/17957b8a9f5468883de469dd9999fbb051301beb", "committedDate": "2020-10-28T13:10:59Z", "message": "Style fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/a22c6966012dfbfed51aa0e23c8e8ca2e74b2245", "committedDate": "2020-10-28T15:20:38Z", "message": "Merge branch 'master' into KafkaToPubsubTemplate"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMwOTUxOTIw", "url": "https://github.com/apache/beam/pull/13112#pullrequestreview-530951920", "createdAt": "2020-11-16T06:55:15Z", "commit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwNjo1NToxNlrOHzqD8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwNzowODoyOFrOHzqUuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkyODU2MQ==", "bodyText": "Can we write a positive test as well ?", "url": "https://github.com/apache/beam/pull/13112#discussion_r523928561", "createdAt": "2020-11-16T06:55:16Z", "author": {"login": "manavgarg"}, "path": "examples/templates/java/kafka-to-pubsub/src/test/java/org/apache/beam/templates/KafkaToPubsubTest.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.templates.transforms.FormatTransform;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+/** Test of KafkaToPubsub. */\n+@RunWith(JUnit4.class)\n+public class KafkaToPubsubTest {\n+\n+  @Rule public final transient TestPipeline pipeline = TestPipeline.create();\n+\n+  @Rule public ExpectedException thrown = ExpectedException.none();\n+\n+  @Test\n+  public void testKafkaReadingFailsWrongBootstrapServer() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkzMjg1Nw==", "bodyText": "Remove default value.", "url": "https://github.com/apache/beam/pull/13112#discussion_r523932857", "createdAt": "2020-11-16T07:08:28Z", "author": {"login": "manavgarg"}, "path": "examples/templates/java/kafka-to-pubsub/src/main/java/org/apache/beam/templates/options/KafkaToPubsubOptions.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates.options;\n+\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.Validation;\n+import org.apache.beam.templates.transforms.FormatTransform;\n+\n+public interface KafkaToPubsubOptions extends PipelineOptions {\n+  @Description(\"Kafka Bootstrap Servers\")\n+  @Validation.Required\n+  String getBootstrapServers();\n+\n+  void setBootstrapServers(String value);\n+\n+  @Description(\"Kafka topics to read the input from\")\n+  @Validation.Required\n+  String getInputTopics();\n+\n+  void setInputTopics(String value);\n+\n+  @Description(\n+      \"The Cloud Pub/Sub topic to publish to. \"\n+          + \"The name should be in the format of \"\n+          + \"projects/<project-id>/topics/<topic-name>.\")\n+  @Validation.Required\n+  String getOutputTopic();\n+\n+  void setOutputTopic(String outputTopic);\n+\n+  @Description(\"\")\n+  @Validation.Required\n+  @Default.Enum(\"PUBSUB\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 50}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMyOTE4OTA1", "url": "https://github.com/apache/beam/pull/13112#pullrequestreview-532918905", "createdAt": "2020-11-18T00:19:44Z", "commit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "state": "COMMENTED", "comments": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQwMDoxOTo0NFrOH1Qz2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQwMDo1MzowNFrOH1RhOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYxMTk5NQ==", "bodyText": "Does this work with a local path? Wouldn't the keystore need to get staged for use on a distributed runner?\nFWIW I think that configuring KafkaIO to use SSL is much more difficult than it should be. Here's an SO question that describes how you can create a custom ConsumerFactoryFn that downloads a keystore from GCS at execution time: https://stackoverflow.com/questions/42726011/truststore-and-google-cloud-dataflow\nI think it could be worthwhile to make KafkaIO do this by default", "url": "https://github.com/apache/beam/pull/13112#discussion_r525611995", "createdAt": "2020-11-18T00:19:44Z", "author": {"login": "TheNeuralBit"}, "path": "examples/templates/java/README.md", "diffHunk": "@@ -0,0 +1,252 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam Template to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) Template that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Pub/Sub](https://cloud.google.com/pubsub).\n+\n+Supported data formats:\n+- Serializable plaintext formats, such as JSON\n+- [PubSubMessage](https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage).\n+\n+Supported input source configurations:\n+- Single or multiple Apache Kafka bootstrap servers\n+- Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection\n+- Secrets vault service [HashiCorp Vault](https://www.vaultproject.io/).\n+\n+Supported destination configuration:\n+- Single Google Pub/Sub topic.\n+\n+In a simple scenario, the template will create an Apache Beam pipeline that will read messages from a source Kafka server with a source topic, and stream the text messages into specified Pub/Sub destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed over plain text or SSL encrypted connection. The template supports using a single Kafka user account to authenticate in the provided source Kafka servers and topics. To support SASL authenticaton over SSL the template will need an SSL certificate location and access to a secrets vault service with Kafka username and password, currently supporting HashiCorp Vault.\n+\n+## Requirements\n+\n+- Java 11\n+- Kafka Bootstrap Server(s) up and running\n+- Existing source Kafka topic(s)\n+- An existing Pub/Sub destination output topic\n+- (Optional) An existing HashiCorp Vault\n+- (Optional) A configured secure SSL connection for Kafka\n+\n+## Getting Started\n+\n+This section describes what is needed to get the template up and running.\n+- Assembling the Uber-JAR\n+- Local execution\n+- Google Dataflow Template\n+  - Set up the environment\n+  - Creating the Dataflow Flex Template\n+  - Create a Dataflow job to ingest data using the template.\n+- Avro format transferring.\n+\n+## Assembling the Uber-JAR\n+\n+To run this template the template Java project should be built into\n+an Uber JAR file.\n+\n+Navigate to the Beam folder:\n+\n+```\n+cd /path/to/beam\n+```\n+\n+In order to create Uber JAR with Gradle, [Shadow plugin](https://github.com/johnrengelman/shadow)\n+is used. It creates the `shadowJar` task that builds the Uber JAR:\n+\n+```\n+./gradlew -p examples/templates/java/kafka-to-pubsub clean shadowJar\n+```\n+\n+\u2139\ufe0f An **Uber JAR** - also known as **fat JAR** - is a single JAR file that contains\n+both target package *and* all its dependencies.\n+\n+The result of the `shadowJar` task execution is a `.jar` file that is generated\n+under the `build/libs/` folder in kafka-to-pubsub directory.\n+\n+## Local execution\n+To execute this pipeline locally, specify the parameters:\n+- Kafka Bootstrap servers\n+- Kafka input topics\n+- Pub/Sub output topic\n+in the following format:\n+```bash\n+--bootstrapServers=host:port \\\n+--inputTopics=your-input-topic \\\n+--outputTopic=projects/your-project-id/topics/your-topic-pame\n+```\n+Optionally, to retrieve Kafka credentials for SASL/SCRAM,\n+specify a URL to the credentials in HashiCorp Vault and the vault access token:\n+```bash\n+--secretStoreUrl=http(s)://host:port/path/to/credentials\n+--vaultToken=your-token\n+```\n+Optionally, to configure secure SSL connection between the Beam pipeline and Kafka,\n+specify the parameters:\n+- A local path to a truststore file\n+- A local path to a keystore file", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYxOTExNQ==", "bodyText": "I'm not sure I understand the purpose of this, won't this just end up re-serializing to the same byte array? And in that case couldn't we just forward the value byte array directly instead?\nMaybe I'm missing something, could you clarify?", "url": "https://github.com/apache/beam/pull/13112#discussion_r525619115", "createdAt": "2020-11-18T00:39:59Z", "author": {"login": "TheNeuralBit"}, "path": "examples/templates/java/README.md", "diffHunk": "@@ -0,0 +1,252 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam Template to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) Template that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Pub/Sub](https://cloud.google.com/pubsub).\n+\n+Supported data formats:\n+- Serializable plaintext formats, such as JSON\n+- [PubSubMessage](https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage).\n+\n+Supported input source configurations:\n+- Single or multiple Apache Kafka bootstrap servers\n+- Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection\n+- Secrets vault service [HashiCorp Vault](https://www.vaultproject.io/).\n+\n+Supported destination configuration:\n+- Single Google Pub/Sub topic.\n+\n+In a simple scenario, the template will create an Apache Beam pipeline that will read messages from a source Kafka server with a source topic, and stream the text messages into specified Pub/Sub destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed over plain text or SSL encrypted connection. The template supports using a single Kafka user account to authenticate in the provided source Kafka servers and topics. To support SASL authenticaton over SSL the template will need an SSL certificate location and access to a secrets vault service with Kafka username and password, currently supporting HashiCorp Vault.\n+\n+## Requirements\n+\n+- Java 11\n+- Kafka Bootstrap Server(s) up and running\n+- Existing source Kafka topic(s)\n+- An existing Pub/Sub destination output topic\n+- (Optional) An existing HashiCorp Vault\n+- (Optional) A configured secure SSL connection for Kafka\n+\n+## Getting Started\n+\n+This section describes what is needed to get the template up and running.\n+- Assembling the Uber-JAR\n+- Local execution\n+- Google Dataflow Template\n+  - Set up the environment\n+  - Creating the Dataflow Flex Template\n+  - Create a Dataflow job to ingest data using the template.\n+- Avro format transferring.\n+\n+## Assembling the Uber-JAR\n+\n+To run this template the template Java project should be built into\n+an Uber JAR file.\n+\n+Navigate to the Beam folder:\n+\n+```\n+cd /path/to/beam\n+```\n+\n+In order to create Uber JAR with Gradle, [Shadow plugin](https://github.com/johnrengelman/shadow)\n+is used. It creates the `shadowJar` task that builds the Uber JAR:\n+\n+```\n+./gradlew -p examples/templates/java/kafka-to-pubsub clean shadowJar\n+```\n+\n+\u2139\ufe0f An **Uber JAR** - also known as **fat JAR** - is a single JAR file that contains\n+both target package *and* all its dependencies.\n+\n+The result of the `shadowJar` task execution is a `.jar` file that is generated\n+under the `build/libs/` folder in kafka-to-pubsub directory.\n+\n+## Local execution\n+To execute this pipeline locally, specify the parameters:\n+- Kafka Bootstrap servers\n+- Kafka input topics\n+- Pub/Sub output topic\n+in the following format:\n+```bash\n+--bootstrapServers=host:port \\\n+--inputTopics=your-input-topic \\\n+--outputTopic=projects/your-project-id/topics/your-topic-pame\n+```\n+Optionally, to retrieve Kafka credentials for SASL/SCRAM,\n+specify a URL to the credentials in HashiCorp Vault and the vault access token:\n+```bash\n+--secretStoreUrl=http(s)://host:port/path/to/credentials\n+--vaultToken=your-token\n+```\n+Optionally, to configure secure SSL connection between the Beam pipeline and Kafka,\n+specify the parameters:\n+- A local path to a truststore file\n+- A local path to a keystore file\n+- Truststore password\n+- Keystore password\n+- Key password\n+```bash\n+--truststorePath=path/to/kafka.truststore.jks\n+--keystorePath=path/to/kafka.keystore.jks\n+--truststorePassword=your-truststore-password\n+--keystorePassword=your-keystore-password\n+--keyPassword=your-key-password\n+```\n+To change the runner, specify:\n+```bash\n+--runner=YOUR_SELECTED_RUNNER\n+```\n+See examples/java/README.md for steps and examples to configure different runners.\n+\n+## Google Dataflow Template\n+\n+### Setting Up Project Environment\n+\n+#### Pipeline variables:\n+\n+```\n+PROJECT=id-of-my-project\n+BUCKET_NAME=my-bucket\n+REGION=my-region\n+```\n+\n+#### Template Metadata Storage Bucket Creation\n+\n+The Dataflow Flex template has to store its metadata in a bucket in\n+[Google Cloud Storage](https://cloud.google.com/storage), so it can be executed from the Google Cloud Platform.\n+Create the bucket in Google Cloud Storage if it doesn't exist yet:\n+\n+```\n+gsutil mb gs://${BUCKET_NAME}\n+```\n+\n+#### Containerization variables:\n+\n+```\n+IMAGE_NAME=my-image-name\n+TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+BASE_CONTAINER_IMAGE=my-base-container-image\n+TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+```\n+\n+### Creating the Dataflow Flex Template\n+\n+Dataflow Flex Templates package the pipeline as a Docker image and stage these images\n+on your project's [Container Registry](https://cloud.google.com/container-registry).\n+\n+To execute the template you need to create the template spec file containing all\n+the necessary information to run the job. This template already has the following\n+[metadata file](kafka-to-pubsub/src/main/resources/kafka_to_pubsub_metadata.json) in resources.\n+\n+Navigate to the template folder:\n+\n+```\n+cd /path/to/beam/examples/templates/java/kafka-to-pubsub\n+```\n+\n+Build the Dataflow Flex Template:\n+\n+```\n+gcloud dataflow flex-template build ${TEMPLATE_PATH} \\\n+       --image-gcr-path ${TARGET_GCR_IMAGE} \\\n+       --sdk-language \"JAVA\" \\\n+       --flex-template-base-image ${BASE_CONTAINER_IMAGE} \\\n+       --metadata-file \"src/main/resources/kafka_to_pubsub_metadata.json\" \\\n+       --jar \"build/libs/beam-examples-templates-java-kafka-to-pubsub-2.25.0-SNAPSHOT-all.jar\" \\\n+       --env FLEX_TEMPLATE_JAVA_MAIN_CLASS=\"org.apache.beam.templates.KafkaToPubsub\"\n+```\n+\n+### Create Dataflow Job Using the Apache Kafka to Google Pub/Sub Dataflow Flex Template\n+\n+To deploy the pipeline, you should refer to the template file and pass the\n+[parameters](https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-dataflow-pipeline-options)\n+required by the pipeline.\n+\n+You can do this in 3 different ways:\n+1. Using [Dataflow Google Cloud Console](https://console.cloud.google.com/dataflow/jobs)\n+\n+2. Using `gcloud` CLI tool\n+    ```\n+    gcloud dataflow flex-template run \"kafka-to-pubsub-`date +%Y%m%d-%H%M%S`\" \\\n+        --template-file-gcs-location \"${TEMPLATE_PATH}\" \\\n+        --parameters bootstrapServers=\"broker_1:9092,broker_2:9092\" \\\n+        --parameters inputTopics=\"topic1,topic2\" \\\n+        --parameters outputTopic=\"projects/${PROJECT}/topics/your-topic-name\" \\\n+        --parameters outputFormat=\"PLAINTEXT\" \\\n+        --parameters secretStoreUrl=\"http(s)://host:port/path/to/credentials\" \\\n+        --parameters vaultToken=\"your-token\" \\\n+        --region \"${REGION}\"\n+    ```\n+3. With a REST API request\n+    ```\n+    API_ROOT_URL=\"https://dataflow.googleapis.com\"\n+    TEMPLATES_LAUNCH_API=\"${API_ROOT_URL}/v1b3/projects/${PROJECT}/locations/${REGION}/flexTemplates:launch\"\n+    JOB_NAME=\"kafka-to-pubsub-`date +%Y%m%d-%H%M%S-%N`\"\n+\n+    time curl -X POST -H \"Content-Type: application/json\" \\\n+        -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n+        -d '\n+         {\n+             \"launch_parameter\": {\n+                 \"jobName\": \"'$JOB_NAME'\",\n+                 \"containerSpecGcsPath\": \"'$TEMPLATE_PATH'\",\n+                 \"parameters\": {\n+                     \"bootstrapServers\": \"broker_1:9091, broker_2:9092\",\n+                     \"inputTopics\": \"topic1, topic2\",\n+                     \"outputTopic\": \"projects/'$PROJECT'/topics/your-topic-name\",\n+                     \"outputFormat\": \"PLAINTEXT\",\n+                     \"secretStoreUrl\": \"http(s)://host:port/path/to/credentials\",\n+                     \"vaultToken\": \"your-token\"\n+                 }\n+             }\n+         }\n+        '\n+        \"${TEMPLATES_LAUNCH_API}\"\n+    ```\n+\n+## AVRO format transferring.\n+This template contains an example Class to deserialize AVRO from Kafka and serialize it to AVRO in Pub/Sub.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 229}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYxOTg0Nw==", "bodyText": "nit: this shouldn't specify a specific version\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                   --jar \"build/libs/beam-examples-templates-java-kafka-to-pubsub-2.25.0-SNAPSHOT-all.jar\" \\\n          \n          \n            \n                   --jar \"build/libs/beam-examples-templates-java-kafka-to-pubsub-<version>-all.jar\" \\", "url": "https://github.com/apache/beam/pull/13112#discussion_r525619847", "createdAt": "2020-11-18T00:42:00Z", "author": {"login": "TheNeuralBit"}, "path": "examples/templates/java/README.md", "diffHunk": "@@ -0,0 +1,252 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam Template to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) Template that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Pub/Sub](https://cloud.google.com/pubsub).\n+\n+Supported data formats:\n+- Serializable plaintext formats, such as JSON\n+- [PubSubMessage](https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage).\n+\n+Supported input source configurations:\n+- Single or multiple Apache Kafka bootstrap servers\n+- Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection\n+- Secrets vault service [HashiCorp Vault](https://www.vaultproject.io/).\n+\n+Supported destination configuration:\n+- Single Google Pub/Sub topic.\n+\n+In a simple scenario, the template will create an Apache Beam pipeline that will read messages from a source Kafka server with a source topic, and stream the text messages into specified Pub/Sub destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed over plain text or SSL encrypted connection. The template supports using a single Kafka user account to authenticate in the provided source Kafka servers and topics. To support SASL authenticaton over SSL the template will need an SSL certificate location and access to a secrets vault service with Kafka username and password, currently supporting HashiCorp Vault.\n+\n+## Requirements\n+\n+- Java 11\n+- Kafka Bootstrap Server(s) up and running\n+- Existing source Kafka topic(s)\n+- An existing Pub/Sub destination output topic\n+- (Optional) An existing HashiCorp Vault\n+- (Optional) A configured secure SSL connection for Kafka\n+\n+## Getting Started\n+\n+This section describes what is needed to get the template up and running.\n+- Assembling the Uber-JAR\n+- Local execution\n+- Google Dataflow Template\n+  - Set up the environment\n+  - Creating the Dataflow Flex Template\n+  - Create a Dataflow job to ingest data using the template.\n+- Avro format transferring.\n+\n+## Assembling the Uber-JAR\n+\n+To run this template the template Java project should be built into\n+an Uber JAR file.\n+\n+Navigate to the Beam folder:\n+\n+```\n+cd /path/to/beam\n+```\n+\n+In order to create Uber JAR with Gradle, [Shadow plugin](https://github.com/johnrengelman/shadow)\n+is used. It creates the `shadowJar` task that builds the Uber JAR:\n+\n+```\n+./gradlew -p examples/templates/java/kafka-to-pubsub clean shadowJar\n+```\n+\n+\u2139\ufe0f An **Uber JAR** - also known as **fat JAR** - is a single JAR file that contains\n+both target package *and* all its dependencies.\n+\n+The result of the `shadowJar` task execution is a `.jar` file that is generated\n+under the `build/libs/` folder in kafka-to-pubsub directory.\n+\n+## Local execution\n+To execute this pipeline locally, specify the parameters:\n+- Kafka Bootstrap servers\n+- Kafka input topics\n+- Pub/Sub output topic\n+in the following format:\n+```bash\n+--bootstrapServers=host:port \\\n+--inputTopics=your-input-topic \\\n+--outputTopic=projects/your-project-id/topics/your-topic-pame\n+```\n+Optionally, to retrieve Kafka credentials for SASL/SCRAM,\n+specify a URL to the credentials in HashiCorp Vault and the vault access token:\n+```bash\n+--secretStoreUrl=http(s)://host:port/path/to/credentials\n+--vaultToken=your-token\n+```\n+Optionally, to configure secure SSL connection between the Beam pipeline and Kafka,\n+specify the parameters:\n+- A local path to a truststore file\n+- A local path to a keystore file\n+- Truststore password\n+- Keystore password\n+- Key password\n+```bash\n+--truststorePath=path/to/kafka.truststore.jks\n+--keystorePath=path/to/kafka.keystore.jks\n+--truststorePassword=your-truststore-password\n+--keystorePassword=your-keystore-password\n+--keyPassword=your-key-password\n+```\n+To change the runner, specify:\n+```bash\n+--runner=YOUR_SELECTED_RUNNER\n+```\n+See examples/java/README.md for steps and examples to configure different runners.\n+\n+## Google Dataflow Template\n+\n+### Setting Up Project Environment\n+\n+#### Pipeline variables:\n+\n+```\n+PROJECT=id-of-my-project\n+BUCKET_NAME=my-bucket\n+REGION=my-region\n+```\n+\n+#### Template Metadata Storage Bucket Creation\n+\n+The Dataflow Flex template has to store its metadata in a bucket in\n+[Google Cloud Storage](https://cloud.google.com/storage), so it can be executed from the Google Cloud Platform.\n+Create the bucket in Google Cloud Storage if it doesn't exist yet:\n+\n+```\n+gsutil mb gs://${BUCKET_NAME}\n+```\n+\n+#### Containerization variables:\n+\n+```\n+IMAGE_NAME=my-image-name\n+TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+BASE_CONTAINER_IMAGE=my-base-container-image\n+TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+```\n+\n+### Creating the Dataflow Flex Template\n+\n+Dataflow Flex Templates package the pipeline as a Docker image and stage these images\n+on your project's [Container Registry](https://cloud.google.com/container-registry).\n+\n+To execute the template you need to create the template spec file containing all\n+the necessary information to run the job. This template already has the following\n+[metadata file](kafka-to-pubsub/src/main/resources/kafka_to_pubsub_metadata.json) in resources.\n+\n+Navigate to the template folder:\n+\n+```\n+cd /path/to/beam/examples/templates/java/kafka-to-pubsub\n+```\n+\n+Build the Dataflow Flex Template:\n+\n+```\n+gcloud dataflow flex-template build ${TEMPLATE_PATH} \\\n+       --image-gcr-path ${TARGET_GCR_IMAGE} \\\n+       --sdk-language \"JAVA\" \\\n+       --flex-template-base-image ${BASE_CONTAINER_IMAGE} \\\n+       --metadata-file \"src/main/resources/kafka_to_pubsub_metadata.json\" \\\n+       --jar \"build/libs/beam-examples-templates-java-kafka-to-pubsub-2.25.0-SNAPSHOT-all.jar\" \\", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 176}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMDk2OQ==", "bodyText": "nit: Google Pub/Sub -> Google Cloud Pub/Sub\nThere's a couple places where cloud products are referenced as Google X, they should be Google Cloud X, or just X", "url": "https://github.com/apache/beam/pull/13112#discussion_r525620969", "createdAt": "2020-11-18T00:45:21Z", "author": {"login": "TheNeuralBit"}, "path": "examples/templates/java/README.md", "diffHunk": "@@ -0,0 +1,252 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam Template to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) Template that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Pub/Sub](https://cloud.google.com/pubsub).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMTg3Mw==", "bodyText": "Maybe this could point to the GCP docs with more details on launching a flex tempalte from cloud console (if such a page exists)", "url": "https://github.com/apache/beam/pull/13112#discussion_r525621873", "createdAt": "2020-11-18T00:48:01Z", "author": {"login": "TheNeuralBit"}, "path": "examples/templates/java/README.md", "diffHunk": "@@ -0,0 +1,252 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam Template to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) Template that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Pub/Sub](https://cloud.google.com/pubsub).\n+\n+Supported data formats:\n+- Serializable plaintext formats, such as JSON\n+- [PubSubMessage](https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage).\n+\n+Supported input source configurations:\n+- Single or multiple Apache Kafka bootstrap servers\n+- Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection\n+- Secrets vault service [HashiCorp Vault](https://www.vaultproject.io/).\n+\n+Supported destination configuration:\n+- Single Google Pub/Sub topic.\n+\n+In a simple scenario, the template will create an Apache Beam pipeline that will read messages from a source Kafka server with a source topic, and stream the text messages into specified Pub/Sub destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed over plain text or SSL encrypted connection. The template supports using a single Kafka user account to authenticate in the provided source Kafka servers and topics. To support SASL authenticaton over SSL the template will need an SSL certificate location and access to a secrets vault service with Kafka username and password, currently supporting HashiCorp Vault.\n+\n+## Requirements\n+\n+- Java 11\n+- Kafka Bootstrap Server(s) up and running\n+- Existing source Kafka topic(s)\n+- An existing Pub/Sub destination output topic\n+- (Optional) An existing HashiCorp Vault\n+- (Optional) A configured secure SSL connection for Kafka\n+\n+## Getting Started\n+\n+This section describes what is needed to get the template up and running.\n+- Assembling the Uber-JAR\n+- Local execution\n+- Google Dataflow Template\n+  - Set up the environment\n+  - Creating the Dataflow Flex Template\n+  - Create a Dataflow job to ingest data using the template.\n+- Avro format transferring.\n+\n+## Assembling the Uber-JAR\n+\n+To run this template the template Java project should be built into\n+an Uber JAR file.\n+\n+Navigate to the Beam folder:\n+\n+```\n+cd /path/to/beam\n+```\n+\n+In order to create Uber JAR with Gradle, [Shadow plugin](https://github.com/johnrengelman/shadow)\n+is used. It creates the `shadowJar` task that builds the Uber JAR:\n+\n+```\n+./gradlew -p examples/templates/java/kafka-to-pubsub clean shadowJar\n+```\n+\n+\u2139\ufe0f An **Uber JAR** - also known as **fat JAR** - is a single JAR file that contains\n+both target package *and* all its dependencies.\n+\n+The result of the `shadowJar` task execution is a `.jar` file that is generated\n+under the `build/libs/` folder in kafka-to-pubsub directory.\n+\n+## Local execution\n+To execute this pipeline locally, specify the parameters:\n+- Kafka Bootstrap servers\n+- Kafka input topics\n+- Pub/Sub output topic\n+in the following format:\n+```bash\n+--bootstrapServers=host:port \\\n+--inputTopics=your-input-topic \\\n+--outputTopic=projects/your-project-id/topics/your-topic-pame\n+```\n+Optionally, to retrieve Kafka credentials for SASL/SCRAM,\n+specify a URL to the credentials in HashiCorp Vault and the vault access token:\n+```bash\n+--secretStoreUrl=http(s)://host:port/path/to/credentials\n+--vaultToken=your-token\n+```\n+Optionally, to configure secure SSL connection between the Beam pipeline and Kafka,\n+specify the parameters:\n+- A local path to a truststore file\n+- A local path to a keystore file\n+- Truststore password\n+- Keystore password\n+- Key password\n+```bash\n+--truststorePath=path/to/kafka.truststore.jks\n+--keystorePath=path/to/kafka.keystore.jks\n+--truststorePassword=your-truststore-password\n+--keystorePassword=your-keystore-password\n+--keyPassword=your-key-password\n+```\n+To change the runner, specify:\n+```bash\n+--runner=YOUR_SELECTED_RUNNER\n+```\n+See examples/java/README.md for steps and examples to configure different runners.\n+\n+## Google Dataflow Template\n+\n+### Setting Up Project Environment\n+\n+#### Pipeline variables:\n+\n+```\n+PROJECT=id-of-my-project\n+BUCKET_NAME=my-bucket\n+REGION=my-region\n+```\n+\n+#### Template Metadata Storage Bucket Creation\n+\n+The Dataflow Flex template has to store its metadata in a bucket in\n+[Google Cloud Storage](https://cloud.google.com/storage), so it can be executed from the Google Cloud Platform.\n+Create the bucket in Google Cloud Storage if it doesn't exist yet:\n+\n+```\n+gsutil mb gs://${BUCKET_NAME}\n+```\n+\n+#### Containerization variables:\n+\n+```\n+IMAGE_NAME=my-image-name\n+TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+BASE_CONTAINER_IMAGE=my-base-container-image\n+TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+```\n+\n+### Creating the Dataflow Flex Template\n+\n+Dataflow Flex Templates package the pipeline as a Docker image and stage these images\n+on your project's [Container Registry](https://cloud.google.com/container-registry).\n+\n+To execute the template you need to create the template spec file containing all\n+the necessary information to run the job. This template already has the following\n+[metadata file](kafka-to-pubsub/src/main/resources/kafka_to_pubsub_metadata.json) in resources.\n+\n+Navigate to the template folder:\n+\n+```\n+cd /path/to/beam/examples/templates/java/kafka-to-pubsub\n+```\n+\n+Build the Dataflow Flex Template:\n+\n+```\n+gcloud dataflow flex-template build ${TEMPLATE_PATH} \\\n+       --image-gcr-path ${TARGET_GCR_IMAGE} \\\n+       --sdk-language \"JAVA\" \\\n+       --flex-template-base-image ${BASE_CONTAINER_IMAGE} \\\n+       --metadata-file \"src/main/resources/kafka_to_pubsub_metadata.json\" \\\n+       --jar \"build/libs/beam-examples-templates-java-kafka-to-pubsub-2.25.0-SNAPSHOT-all.jar\" \\\n+       --env FLEX_TEMPLATE_JAVA_MAIN_CLASS=\"org.apache.beam.templates.KafkaToPubsub\"\n+```\n+\n+### Create Dataflow Job Using the Apache Kafka to Google Pub/Sub Dataflow Flex Template\n+\n+To deploy the pipeline, you should refer to the template file and pass the\n+[parameters](https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-dataflow-pipeline-options)\n+required by the pipeline.\n+\n+You can do this in 3 different ways:\n+1. Using [Dataflow Google Cloud Console](https://console.cloud.google.com/dataflow/jobs)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 187}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMjkxMg==", "bodyText": "Please enable the checker and try to fix any nullness issues it detects. If there are confusing/tricky issues you can supress these warnings at the class or function level with @SuppressWarnings(\"nullness\")", "url": "https://github.com/apache/beam/pull/13112#discussion_r525622912", "createdAt": "2020-11-18T00:50:59Z", "author": {"login": "TheNeuralBit"}, "path": "examples/templates/java/kafka-to-pubsub/build.gradle", "diffHunk": "@@ -0,0 +1,53 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * License); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an AS IS BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+plugins {\n+    id 'java'\n+    id 'org.apache.beam.module'\n+    id 'com.github.johnrengelman.shadow'\n+}\n+applyJavaNature(\n+        exportJavadoc: false,\n+        enableChecker: false,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMzA2MA==", "bodyText": "Is this file necessary?", "url": "https://github.com/apache/beam/pull/13112#discussion_r525623060", "createdAt": "2020-11-18T00:51:27Z", "author": {"login": "TheNeuralBit"}, "path": "examples/templates/java/build.gradle", "diffHunk": "@@ -0,0 +1,31 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * License); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an AS IS BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+plugins {\n+    id 'java'\n+}\n+\n+version '2.25.0-SNAPSHOT'\n+\n+repositories {\n+    mavenCentral()\n+}\n+\n+dependencies {\n+    testCompile group: 'junit', name: 'junit', version: '4.12'\n+}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMzYwOQ==", "bodyText": "This looks like a dupe of the README, could the javadoc just refer to that instead?", "url": "https://github.com/apache/beam/pull/13112#discussion_r525623609", "createdAt": "2020-11-18T00:53:04Z", "author": {"login": "TheNeuralBit"}, "path": "examples/templates/java/kafka-to-pubsub/src/main/java/org/apache/beam/templates/KafkaToPubsub.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.gson.JsonObject;\n+import com.google.gson.JsonParser;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult;\n+import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.Values;\n+import org.apache.beam.templates.avro.TaxiRide;\n+import org.apache.beam.templates.options.KafkaToPubsubOptions;\n+import org.apache.beam.templates.transforms.FormatTransform;\n+import org.apache.http.HttpResponse;\n+import org.apache.http.client.HttpClient;\n+import org.apache.http.client.methods.HttpGet;\n+import org.apache.http.impl.client.HttpClientBuilder;\n+import org.apache.http.util.EntityUtils;\n+import org.apache.kafka.common.config.SaslConfigs;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.common.security.scram.ScramMechanism;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The {@link KafkaToPubsub} pipeline is a streaming pipeline which ingests data in JSON format from\n+ * Kafka, and outputs the resulting records to PubSub. Input topics, output topic, Bootstrap servers\n+ * are specified by the user as template parameters. <br>\n+ * Kafka may be configured with SASL/SCRAM security mechanism, in this case a Vault secret storage\n+ * with credentials should be provided. URL to credentials and Vault token are specified by the user\n+ * as template parameters.\n+ *\n+ * <p><b>Pipeline Requirements</b>\n+ *\n+ * <ul>\n+ *   <li>Kafka Bootstrap Server(s).\n+ *   <li>Kafka Topic(s) exists.\n+ *   <li>The PubSub output topic exists.\n+ *   <li>(Optional) An existing HashiCorp Vault secret storage\n+ * </ul>\n+ *\n+ * <p><b>Example Usage</b>\n+ *\n+ * <pre>\n+ * # Set the pipeline vars\n+ * PROJECT=id-of-my-project\n+ * BUCKET_NAME=my-bucket\n+ *\n+ * # Set containerization vars\n+ * IMAGE_NAME=my-image-name\n+ * TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+ * BASE_CONTAINER_IMAGE=my-base-container-image\n+ * TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+ *\n+ * # Create bucket in the cloud storage\n+ * gsutil mb gs://${BUCKET_NAME}\n+ *\n+ * # Go to the beam folder\n+ * cd /path/to/beam\n+ *\n+ * <b>FLEX TEMPLATE</b>\n+ * # Assemble uber-jar\n+ * ./gradlew -p templates/kafka-to-pubsub clean shadowJar\n+ *\n+ * # Go to the template folder\n+ * cd /path/to/beam/templates/kafka-to-pubsub\n+ *\n+ * # Build the flex template\n+ * gcloud dataflow flex-template build ${TEMPLATE_PATH} \\\n+ *       --image-gcr-path \"${TARGET_GCR_IMAGE}\" \\\n+ *       --sdk-language \"JAVA\" \\\n+ *       --flex-template-base-image ${BASE_CONTAINER_IMAGE} \\\n+ *       --metadata-file \"src/main/resources/kafka_to_pubsub_metadata.json\" \\\n+ *       --jar \"build/libs/beam-templates-kafka-to-pubsub-2.25.0-SNAPSHOT-all.jar\" \\\n+ *       --env FLEX_TEMPLATE_JAVA_MAIN_CLASS=\"org.apache.beam.templates.KafkaToPubsub\"\n+ *\n+ * # Execute template:\n+ *    API_ROOT_URL=\"https://dataflow.googleapis.com\"\n+ *    TEMPLATES_LAUNCH_API=\"${API_ROOT_URL}/v1b3/projects/${PROJECT}/locations/${REGION}/flexTemplates:launch\"\n+ *    JOB_NAME=\"kafka-to-pubsub-`date +%Y%m%d-%H%M%S-%N`\"\n+ *\n+ *    time curl -X POST -H \"Content-Type: application/json\" \\\n+ *            -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n+ *            -d '\n+ *             {\n+ *                 \"launch_parameter\": {\n+ *                     \"jobName\": \"'$JOB_NAME'\",\n+ *                     \"containerSpecGcsPath\": \"'$TEMPLATE_PATH'\",\n+ *                     \"parameters\": {\n+ *                         \"bootstrapServers\": \"broker_1:9091, broker_2:9092\",\n+ *                         \"inputTopics\": \"topic1, topic2\",\n+ *                         \"outputTopic\": \"projects/'$PROJECT'/topics/your-topic-name\",\n+ *                         \"secretStoreUrl\": \"http(s)://host:port/path/to/credentials\",\n+ *                         \"vaultToken\": \"your-token\"\n+ *                     }\n+ *                 }\n+ *             }\n+ *            '\n+ *            \"${TEMPLATES_LAUNCH_API}\"\n+ * </pre>\n+ *\n+ * <p><b>Example Avro usage</b>\n+ *\n+ * <pre>\n+ * This template contains an example Class to deserialize AVRO from Kafka and serialize it to AVRO in Pub/Sub.\n+ *\n+ * To use this example in the specific case, follow the few steps:\n+ * <ul>\n+ * <li> Create your own class to describe AVRO schema. As an example use {@link TaxiRide}. Just define necessary fields.\n+ * <li> Create your own Avro Deserializer class. As an example use {@link org.apache.beam.templates.avro.TaxiRidesKafkaAvroDeserializer}. Just rename it, and put your own Schema class as the necessary types.\n+ * <li> Modify the {@link FormatTransform}. Put your Schema class and Deserializer to the related parameter.\n+ * <li> Modify write step in the {@link KafkaToPubsub} by put your Schema class to \"writeAvrosToPubSub\" step.\n+ * </ul>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 138}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b4e7081334f1f21e7e28e522c884099ecc34605e", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/b4e7081334f1f21e7e28e522c884099ecc34605e", "committedDate": "2020-11-19T16:10:01Z", "message": "Refactoring."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aa857d7b314ade00e5e387cbf7cc8dca7b8ef649", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/aa857d7b314ade00e5e387cbf7cc8dca7b8ef649", "committedDate": "2020-11-19T16:53:58Z", "message": "Fixed ssl parameters"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "25efe95508dedcacd9374fffd67e9f0d40e7b51e", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/25efe95508dedcacd9374fffd67e9f0d40e7b51e", "committedDate": "2020-11-19T16:57:19Z", "message": "Fixed style"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7fd7ea62ef7cc7fb788685d596a9866a0eda1da2", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/7fd7ea62ef7cc7fb788685d596a9866a0eda1da2", "committedDate": "2020-11-20T15:07:22Z", "message": "Merge branch 'master' into KafkaToPubsubTemplate"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f300de51ca3b07d575002b3e9a14970911aa5ce0", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/f300de51ca3b07d575002b3e9a14970911aa5ce0", "committedDate": "2020-11-20T15:12:39Z", "message": "optimize build.gradle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c0e6ad0d5943795839635b977cf1a875680ef2d9", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/c0e6ad0d5943795839635b977cf1a875680ef2d9", "committedDate": "2020-11-20T15:31:43Z", "message": "Resolve conversations"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "998f4c0f5d1631e157cb61fca02908e7bdad2a33", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/998f4c0f5d1631e157cb61fca02908e7bdad2a33", "committedDate": "2020-11-23T10:07:24Z", "message": "Updated regarding comments and added unit tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6a6aa46a96838ad3a989fee61adba8902f3023d3", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/6a6aa46a96838ad3a989fee61adba8902f3023d3", "committedDate": "2020-11-23T11:53:41Z", "message": "README.md update"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0858e4717809d44ddb5ca4fa1924fa55b9f85ce2", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/0858e4717809d44ddb5ca4fa1924fa55b9f85ce2", "committedDate": "2020-11-23T13:37:37Z", "message": "made Avro class more abstract"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8a3d85d19e45613b28ad5bc0042921a0f961598c", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/8a3d85d19e45613b28ad5bc0042921a0f961598c", "committedDate": "2020-11-23T13:54:07Z", "message": "fix style"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM2NzY2ODAz", "url": "https://github.com/apache/beam/pull/13112#pullrequestreview-536766803", "createdAt": "2020-11-23T19:07:06Z", "commit": {"oid": "8a3d85d19e45613b28ad5bc0042921a0f961598c"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QxOTowNzowNlrOH4bgnw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QxOTowNzowNlrOH4bgnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMzAyMw==", "bodyText": "Maybe I am missing something here but these tests only validate the config.\nIs it possible to have a more e2e test where we create & execute a pipeline like what we had before ?", "url": "https://github.com/apache/beam/pull/13112#discussion_r528933023", "createdAt": "2020-11-23T19:07:06Z", "author": {"login": "manavgarg"}, "path": "examples/templates/java/kafka-to-pubsub/src/test/java/org/apache/beam/templates/KafkaToPubsubTest.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates;\n+\n+import static org.apache.beam.templates.KafkaPubsubConstants.PASSWORD;\n+import static org.apache.beam.templates.KafkaPubsubConstants.USERNAME;\n+import static org.apache.beam.templates.kafka.consumer.Utils.getKafkaCredentialsFromVault;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.beam.templates.kafka.consumer.Utils;\n+import org.apache.kafka.common.config.SaslConfigs;\n+import org.apache.kafka.common.security.scram.ScramMechanism;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+/** Test of KafkaToPubsub. */\n+@RunWith(JUnit4.class)\n+public class KafkaToPubsubTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a3d85d19e45613b28ad5bc0042921a0f961598c"}, "originalPosition": 36}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM4MDc5MzAw", "url": "https://github.com/apache/beam/pull/13112#pullrequestreview-538079300", "createdAt": "2020-11-25T00:54:26Z", "commit": {"oid": "8a3d85d19e45613b28ad5bc0042921a0f961598c"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNVQwMDo1NDoyNlrOH5fBLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNVQwMToyNTo0M1rOH5fmoA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDAzOTA4NQ==", "bodyText": "Since your implementation for getGcsFileAsLocal relies on FileSystems.matchSingleFileSpec I think it will actually work for the else path here as well (It could also pull a truststore from AWS s3 if you include the dependency).\nIt's worth noting that the local file option will fail at execution time for a distributed runner, we may want to catch that and raise a more helpful error - e.g. suggest that they stage the file on cloud storage", "url": "https://github.com/apache/beam/pull/13112#discussion_r530039085", "createdAt": "2020-11-25T00:54:26Z", "author": {"login": "TheNeuralBit"}, "path": "examples/templates/java/kafka-to-pubsub/src/main/java/org/apache/beam/templates/kafka/consumer/SslConsumerFactoryFn.java", "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates.kafka.consumer;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** Class to create Kafka Consumer with configured SSL. */\n+public class SslConsumerFactoryFn\n+    implements SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> {\n+  private final Map<String, String> sslConfig;\n+  private static final String TRUSTSTORE_LOCAL_PATH = \"/tmp/kafka.truststore.jks\";\n+  private static final String KEYSTORE_LOCAL_PATH = \"/tmp/kafka.keystore.jks\";\n+\n+  /* Logger for class.*/\n+  private static final Logger LOG = LoggerFactory.getLogger(SslConsumerFactoryFn.class);\n+\n+  public SslConsumerFactoryFn(Map<String, String> sslConfig) {\n+    this.sslConfig = sslConfig;\n+  }\n+\n+  @SuppressWarnings(\"nullness\")\n+  @Override\n+  public Consumer<byte[], byte[]> apply(Map<String, Object> config) {\n+    try {\n+      String truststoreLocation = sslConfig.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG);\n+      if (truststoreLocation.startsWith(\"gs://\")) {\n+        getGcsFileAsLocal(truststoreLocation, TRUSTSTORE_LOCAL_PATH);\n+      } else {\n+        checkFileExists(truststoreLocation);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a3d85d19e45613b28ad5bc0042921a0f961598c"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDA0MDAzNw==", "bodyText": "This will never be triggered its the same condition as the if", "url": "https://github.com/apache/beam/pull/13112#discussion_r530040037", "createdAt": "2020-11-25T00:57:28Z", "author": {"login": "TheNeuralBit"}, "path": "examples/templates/java/kafka-to-pubsub/src/main/java/org/apache/beam/templates/KafkaToPubsub.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates;\n+\n+import static org.apache.beam.templates.kafka.consumer.Utils.configureKafka;\n+import static org.apache.beam.templates.kafka.consumer.Utils.configureSsl;\n+import static org.apache.beam.templates.kafka.consumer.Utils.getKafkaCredentialsFromVault;\n+import static org.apache.beam.templates.kafka.consumer.Utils.isSslSpecified;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult;\n+import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.Values;\n+import org.apache.beam.templates.avro.AvroDataClass;\n+import org.apache.beam.templates.avro.AvroDataClassKafkaAvroDeserializer;\n+import org.apache.beam.templates.options.KafkaToPubsubOptions;\n+import org.apache.beam.templates.transforms.FormatTransform;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The {@link KafkaToPubsub} pipeline is a streaming pipeline which ingests data in JSON format from\n+ * Kafka, and outputs the resulting records to PubSub. Input topics, output topic, Bootstrap servers\n+ * are specified by the user as template parameters. <br>\n+ * Kafka may be configured with SASL/SCRAM security mechanism, in this case a Vault secret storage\n+ * with credentials should be provided. URL to credentials and Vault token are specified by the user\n+ * as template parameters.\n+ *\n+ * <p><b>Pipeline Requirements</b>\n+ *\n+ * <ul>\n+ *   <li>Kafka Bootstrap Server(s).\n+ *   <li>Kafka Topic(s) exists.\n+ *   <li>The PubSub output topic exists.\n+ *   <li>(Optional) An existing HashiCorp Vault secret storage\n+ * </ul>\n+ *\n+ * <p><b>Example Usage</b>\n+ *\n+ * <pre>\n+ * # Set the pipeline vars\n+ * PROJECT=id-of-my-project\n+ * BUCKET_NAME=my-bucket\n+ *\n+ * # Set containerization vars\n+ * IMAGE_NAME=my-image-name\n+ * TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+ * BASE_CONTAINER_IMAGE=my-base-container-image\n+ * TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+ *\n+ * # Create bucket in the cloud storage\n+ * gsutil mb gs://${BUCKET_NAME}\n+ *\n+ * # Go to the beam folder\n+ * cd /path/to/beam\n+ *\n+ * <b>FLEX TEMPLATE</b>\n+ * # Assemble uber-jar\n+ * ./gradlew -p templates/kafka-to-pubsub clean shadowJar\n+ *\n+ * # Go to the template folder\n+ * cd /path/to/beam/templates/kafka-to-pubsub\n+ *\n+ * # Build the flex template\n+ * gcloud dataflow flex-template build ${TEMPLATE_PATH} \\\n+ *       --image-gcr-path \"${TARGET_GCR_IMAGE}\" \\\n+ *       --sdk-language \"JAVA\" \\\n+ *       --flex-template-base-image ${BASE_CONTAINER_IMAGE} \\\n+ *       --metadata-file \"src/main/resources/kafka_to_pubsub_metadata.json\" \\\n+ *       --jar \"build/libs/beam-templates-kafka-to-pubsub-<version>-all.jar\" \\\n+ *       --env FLEX_TEMPLATE_JAVA_MAIN_CLASS=\"org.apache.beam.templates.KafkaToPubsub\"\n+ *\n+ * # Execute template:\n+ *    API_ROOT_URL=\"https://dataflow.googleapis.com\"\n+ *    TEMPLATES_LAUNCH_API=\"${API_ROOT_URL}/v1b3/projects/${PROJECT}/locations/${REGION}/flexTemplates:launch\"\n+ *    JOB_NAME=\"kafka-to-pubsub-`date +%Y%m%d-%H%M%S-%N`\"\n+ *\n+ *    time curl -X POST -H \"Content-Type: application/json\" \\\n+ *            -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n+ *            -d '\n+ *             {\n+ *                 \"launch_parameter\": {\n+ *                     \"jobName\": \"'$JOB_NAME'\",\n+ *                     \"containerSpecGcsPath\": \"'$TEMPLATE_PATH'\",\n+ *                     \"parameters\": {\n+ *                         \"bootstrapServers\": \"broker_1:9091, broker_2:9092\",\n+ *                         \"inputTopics\": \"topic1, topic2\",\n+ *                         \"outputTopic\": \"projects/'$PROJECT'/topics/your-topic-name\",\n+ *                         \"secretStoreUrl\": \"http(s)://host:port/path/to/credentials\",\n+ *                         \"vaultToken\": \"your-token\"\n+ *                     }\n+ *                 }\n+ *             }\n+ *            '\n+ *            \"${TEMPLATES_LAUNCH_API}\"\n+ * </pre>\n+ *\n+ * <p><b>Example Avro usage</b>\n+ *\n+ * <pre>\n+ * This template contains an example Class to deserialize AVRO from Kafka and serialize it to AVRO in Pub/Sub.\n+ *\n+ * To use this example in the specific case, follow the few steps:\n+ * <ul>\n+ * <li> Create your own class to describe AVRO schema. As an example use {@link AvroDataClass}. Just define necessary fields.\n+ * <li> Create your own Avro Deserializer class. As an example use {@link AvroDataClassKafkaAvroDeserializer}. Just rename it, and put your own Schema class as the necessary types.\n+ * <li> Modify the {@link FormatTransform}. Put your Schema class and Deserializer to the related parameter.\n+ * <li> Modify write step in the {@link KafkaToPubsub} by put your Schema class to \"writeAvrosToPubSub\" step.\n+ * </ul>\n+ * </pre>\n+ */\n+public class KafkaToPubsub {\n+\n+  /* Logger for class.*/\n+  private static final Logger LOG = LoggerFactory.getLogger(KafkaToPubsub.class);\n+\n+  /**\n+   * Main entry point for pipeline execution.\n+   *\n+   * @param args Command line arguments to the pipeline.\n+   */\n+  public static void main(String[] args) {\n+    KafkaToPubsubOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(KafkaToPubsubOptions.class);\n+\n+    run(options);\n+  }\n+\n+  /**\n+   * Runs a pipeline which reads message from Kafka and writes it to GCS.\n+   *\n+   * @param options arguments to the pipeline\n+   */\n+  public static PipelineResult run(KafkaToPubsubOptions options) {\n+    // Configure Kafka consumer properties\n+    Map<String, Object> kafkaConfig = new HashMap<>();\n+    if (options.getSecretStoreUrl() != null && options.getVaultToken() != null) {\n+      Map<String, Map<String, String>> credentials =\n+          getKafkaCredentialsFromVault(options.getSecretStoreUrl(), options.getVaultToken());\n+      kafkaConfig = configureKafka(credentials.get(KafkaPubsubConstants.KAFKA_CREDENTIALS));\n+    } else {\n+      LOG.warn(\n+          \"No information to retrieve Kafka credentials was provided. \"\n+              + \"Trying to initiate an unauthorized connection.\");\n+    }\n+\n+    Map<String, String> sslConfig = new HashMap<>();\n+    if (isSslSpecified(options)) {\n+      sslConfig.putAll(configureSsl(options));\n+    } else {\n+      LOG.info(\n+          \"No information to retrieve SSL certificate was provided. \"\n+              + \"Trying to initiate a plain text connection.\");\n+    }\n+\n+    List<String> topicsList = new ArrayList<>(Arrays.asList(options.getInputTopics().split(\",\")));\n+\n+    checkArgument(\n+        topicsList.size() > 0 && topicsList.get(0).length() > 0,\n+        \"inputTopics cannot be an empty string.\");\n+\n+    List<String> bootstrapServersList =\n+        new ArrayList<>(Arrays.asList(options.getBootstrapServers().split(\",\")));\n+\n+    checkArgument(\n+        bootstrapServersList.size() > 0 && topicsList.get(0).length() > 0,\n+        \"bootstrapServers cannot be an empty string.\");\n+\n+    // Create the pipeline\n+    Pipeline pipeline = Pipeline.create(options);\n+    LOG.info(\n+        \"Starting Kafka-To-PubSub pipeline with parameters bootstrap servers:\"\n+            + options.getBootstrapServers()\n+            + \" input topics: \"\n+            + options.getInputTopics()\n+            + \" output pubsub topic: \"\n+            + options.getOutputTopic());\n+\n+    /*\n+     * Steps:\n+     *  1) Read messages in from Kafka\n+     *  2) Extract values only\n+     *  3) Write successful records to PubSub\n+     */\n+\n+    if (options.getOutputFormat() == FormatTransform.FORMAT.AVRO) {\n+      pipeline\n+          .apply(\n+              \"readAvrosFromKafka\",\n+              FormatTransform.readAvrosFromKafka(\n+                  options.getBootstrapServers(), topicsList, kafkaConfig, sslConfig))\n+          .apply(\"createValues\", Values.create())\n+          .apply(\"writeAvrosToPubSub\", PubsubIO.writeAvros(AvroDataClass.class));\n+\n+    } else if (options.getOutputFormat() == FormatTransform.FORMAT.AVRO) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a3d85d19e45613b28ad5bc0042921a0f961598c"}, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDA0ODE2OQ==", "bodyText": "I'm not sure it makes sense to discuss Dataflow Flex templates here, we should leave that to GoogleCloudPlatform/DataflowTemplates#176\nI think this README should instead discuss how to run directly on Dataflow and/or how to run on some other Beam runners", "url": "https://github.com/apache/beam/pull/13112#discussion_r530048169", "createdAt": "2020-11-25T01:24:06Z", "author": {"login": "TheNeuralBit"}, "path": "examples/templates/java/README.md", "diffHunk": "@@ -0,0 +1,254 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam Template to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) Template that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Cloud Pub/Sub](https://cloud.google.com/pubsub).\n+\n+Supported data formats:\n+- Serializable plaintext formats, such as JSON\n+- [PubSubMessage](https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage).\n+\n+Supported input source configurations:\n+- Single or multiple Apache Kafka bootstrap servers\n+- Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection\n+- Secrets vault service [HashiCorp Vault](https://www.vaultproject.io/).\n+\n+Supported destination configuration:\n+- Single Google Cloud Pub/Sub topic.\n+\n+In a simple scenario, the template will create an Apache Beam pipeline that will read messages from a source Kafka server with a source topic, and stream the text messages into specified Pub/Sub destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed over plain text or SSL encrypted connection. The template supports using a single Kafka user account to authenticate in the provided source Kafka servers and topics. To support SASL authenticaton over SSL the template will need an SSL certificate location and access to a secrets vault service with Kafka username and password, currently supporting HashiCorp Vault.\n+\n+## Requirements\n+\n+- Java 8\n+- Kafka Bootstrap Server(s) up and running\n+- Existing source Kafka topic(s)\n+- An existing Pub/Sub destination output topic\n+- (Optional) An existing HashiCorp Vault\n+- (Optional) A configured secure SSL connection for Kafka\n+\n+## Getting Started\n+\n+This section describes what is needed to get the template up and running.\n+- Assembling the Uber-JAR\n+- Local execution\n+- Google Dataflow Template\n+  - Set up the environment\n+  - Creating the Dataflow Flex Template\n+  - Create a Dataflow job to ingest data using the template.\n+- Avro format transferring.\n+\n+## Assembling the Uber-JAR\n+\n+To run this template the template Java project should be built into\n+an Uber JAR file.\n+\n+Navigate to the Beam folder:\n+\n+```\n+cd /path/to/beam\n+```\n+\n+In order to create Uber JAR with Gradle, [Shadow plugin](https://github.com/johnrengelman/shadow)\n+is used. It creates the `shadowJar` task that builds the Uber JAR:\n+\n+```\n+./gradlew -p examples/templates/java/kafka-to-pubsub clean shadowJar\n+```\n+\n+\u2139\ufe0f An **Uber JAR** - also known as **fat JAR** - is a single JAR file that contains\n+both target package *and* all its dependencies.\n+\n+The result of the `shadowJar` task execution is a `.jar` file that is generated\n+under the `build/libs/` folder in kafka-to-pubsub directory.\n+\n+## Local execution\n+To execute this pipeline locally, specify the parameters:\n+- Kafka Bootstrap servers\n+- Kafka input topics\n+- Pub/Sub output topic\n+in the following format:\n+```bash\n+--bootstrapServers=host:port \\\n+--inputTopics=your-input-topic \\\n+--outputTopic=projects/your-project-id/topics/your-topic-pame\n+```\n+Optionally, to retrieve Kafka credentials for SASL/SCRAM,\n+specify a URL to the credentials in HashiCorp Vault and the vault access token:\n+```bash\n+--secretStoreUrl=http(s)://host:port/path/to/credentials\n+--vaultToken=your-token\n+```\n+Optionally, to configure secure SSL connection between the Beam pipeline and Kafka,\n+specify the parameters:\n+- A path to a truststore file (it can be a local path or a GCS path, which should start with `gs://`)\n+- A path to a keystore file (it can be a local path or a GCS path, which should start with `gs://`)\n+- Truststore password\n+- Keystore password\n+- Key password\n+```bash\n+--truststorePath=path/to/kafka.truststore.jks\n+--keystorePath=path/to/kafka.keystore.jks\n+--truststorePassword=your-truststore-password\n+--keystorePassword=your-keystore-password\n+--keyPassword=your-key-password\n+```\n+To change the runner, specify:\n+```bash\n+--runner=YOUR_SELECTED_RUNNER\n+```\n+See examples/java/README.md for steps and examples to configure different runners.\n+\n+## Google Dataflow Template\n+\n+### Setting Up Project Environment\n+\n+#### Pipeline variables:\n+\n+```\n+PROJECT=id-of-my-project\n+BUCKET_NAME=my-bucket\n+REGION=my-region\n+```\n+\n+#### Template Metadata Storage Bucket Creation\n+\n+The Dataflow Flex template has to store its metadata in a bucket in", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a3d85d19e45613b28ad5bc0042921a0f961598c"}, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDA0ODY3Mg==", "bodyText": "But even with the updated language, the user would just deserialize the byte array to an instance MyAvroClass and then reserialize it back to the same byte array to send to PubSub. If we want to show an example of deserializing and serializing, it should at least do some processing, or convert the message to another format. Otherwise I feel this could just confuse users.", "url": "https://github.com/apache/beam/pull/13112#discussion_r530048672", "createdAt": "2020-11-25T01:25:43Z", "author": {"login": "TheNeuralBit"}, "path": "examples/templates/java/README.md", "diffHunk": "@@ -0,0 +1,252 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam Template to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) Template that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Pub/Sub](https://cloud.google.com/pubsub).\n+\n+Supported data formats:\n+- Serializable plaintext formats, such as JSON\n+- [PubSubMessage](https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage).\n+\n+Supported input source configurations:\n+- Single or multiple Apache Kafka bootstrap servers\n+- Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection\n+- Secrets vault service [HashiCorp Vault](https://www.vaultproject.io/).\n+\n+Supported destination configuration:\n+- Single Google Pub/Sub topic.\n+\n+In a simple scenario, the template will create an Apache Beam pipeline that will read messages from a source Kafka server with a source topic, and stream the text messages into specified Pub/Sub destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed over plain text or SSL encrypted connection. The template supports using a single Kafka user account to authenticate in the provided source Kafka servers and topics. To support SASL authenticaton over SSL the template will need an SSL certificate location and access to a secrets vault service with Kafka username and password, currently supporting HashiCorp Vault.\n+\n+## Requirements\n+\n+- Java 11\n+- Kafka Bootstrap Server(s) up and running\n+- Existing source Kafka topic(s)\n+- An existing Pub/Sub destination output topic\n+- (Optional) An existing HashiCorp Vault\n+- (Optional) A configured secure SSL connection for Kafka\n+\n+## Getting Started\n+\n+This section describes what is needed to get the template up and running.\n+- Assembling the Uber-JAR\n+- Local execution\n+- Google Dataflow Template\n+  - Set up the environment\n+  - Creating the Dataflow Flex Template\n+  - Create a Dataflow job to ingest data using the template.\n+- Avro format transferring.\n+\n+## Assembling the Uber-JAR\n+\n+To run this template the template Java project should be built into\n+an Uber JAR file.\n+\n+Navigate to the Beam folder:\n+\n+```\n+cd /path/to/beam\n+```\n+\n+In order to create Uber JAR with Gradle, [Shadow plugin](https://github.com/johnrengelman/shadow)\n+is used. It creates the `shadowJar` task that builds the Uber JAR:\n+\n+```\n+./gradlew -p examples/templates/java/kafka-to-pubsub clean shadowJar\n+```\n+\n+\u2139\ufe0f An **Uber JAR** - also known as **fat JAR** - is a single JAR file that contains\n+both target package *and* all its dependencies.\n+\n+The result of the `shadowJar` task execution is a `.jar` file that is generated\n+under the `build/libs/` folder in kafka-to-pubsub directory.\n+\n+## Local execution\n+To execute this pipeline locally, specify the parameters:\n+- Kafka Bootstrap servers\n+- Kafka input topics\n+- Pub/Sub output topic\n+in the following format:\n+```bash\n+--bootstrapServers=host:port \\\n+--inputTopics=your-input-topic \\\n+--outputTopic=projects/your-project-id/topics/your-topic-pame\n+```\n+Optionally, to retrieve Kafka credentials for SASL/SCRAM,\n+specify a URL to the credentials in HashiCorp Vault and the vault access token:\n+```bash\n+--secretStoreUrl=http(s)://host:port/path/to/credentials\n+--vaultToken=your-token\n+```\n+Optionally, to configure secure SSL connection between the Beam pipeline and Kafka,\n+specify the parameters:\n+- A local path to a truststore file\n+- A local path to a keystore file\n+- Truststore password\n+- Keystore password\n+- Key password\n+```bash\n+--truststorePath=path/to/kafka.truststore.jks\n+--keystorePath=path/to/kafka.keystore.jks\n+--truststorePassword=your-truststore-password\n+--keystorePassword=your-keystore-password\n+--keyPassword=your-key-password\n+```\n+To change the runner, specify:\n+```bash\n+--runner=YOUR_SELECTED_RUNNER\n+```\n+See examples/java/README.md for steps and examples to configure different runners.\n+\n+## Google Dataflow Template\n+\n+### Setting Up Project Environment\n+\n+#### Pipeline variables:\n+\n+```\n+PROJECT=id-of-my-project\n+BUCKET_NAME=my-bucket\n+REGION=my-region\n+```\n+\n+#### Template Metadata Storage Bucket Creation\n+\n+The Dataflow Flex template has to store its metadata in a bucket in\n+[Google Cloud Storage](https://cloud.google.com/storage), so it can be executed from the Google Cloud Platform.\n+Create the bucket in Google Cloud Storage if it doesn't exist yet:\n+\n+```\n+gsutil mb gs://${BUCKET_NAME}\n+```\n+\n+#### Containerization variables:\n+\n+```\n+IMAGE_NAME=my-image-name\n+TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+BASE_CONTAINER_IMAGE=my-base-container-image\n+TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+```\n+\n+### Creating the Dataflow Flex Template\n+\n+Dataflow Flex Templates package the pipeline as a Docker image and stage these images\n+on your project's [Container Registry](https://cloud.google.com/container-registry).\n+\n+To execute the template you need to create the template spec file containing all\n+the necessary information to run the job. This template already has the following\n+[metadata file](kafka-to-pubsub/src/main/resources/kafka_to_pubsub_metadata.json) in resources.\n+\n+Navigate to the template folder:\n+\n+```\n+cd /path/to/beam/examples/templates/java/kafka-to-pubsub\n+```\n+\n+Build the Dataflow Flex Template:\n+\n+```\n+gcloud dataflow flex-template build ${TEMPLATE_PATH} \\\n+       --image-gcr-path ${TARGET_GCR_IMAGE} \\\n+       --sdk-language \"JAVA\" \\\n+       --flex-template-base-image ${BASE_CONTAINER_IMAGE} \\\n+       --metadata-file \"src/main/resources/kafka_to_pubsub_metadata.json\" \\\n+       --jar \"build/libs/beam-examples-templates-java-kafka-to-pubsub-2.25.0-SNAPSHOT-all.jar\" \\\n+       --env FLEX_TEMPLATE_JAVA_MAIN_CLASS=\"org.apache.beam.templates.KafkaToPubsub\"\n+```\n+\n+### Create Dataflow Job Using the Apache Kafka to Google Pub/Sub Dataflow Flex Template\n+\n+To deploy the pipeline, you should refer to the template file and pass the\n+[parameters](https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-dataflow-pipeline-options)\n+required by the pipeline.\n+\n+You can do this in 3 different ways:\n+1. Using [Dataflow Google Cloud Console](https://console.cloud.google.com/dataflow/jobs)\n+\n+2. Using `gcloud` CLI tool\n+    ```\n+    gcloud dataflow flex-template run \"kafka-to-pubsub-`date +%Y%m%d-%H%M%S`\" \\\n+        --template-file-gcs-location \"${TEMPLATE_PATH}\" \\\n+        --parameters bootstrapServers=\"broker_1:9092,broker_2:9092\" \\\n+        --parameters inputTopics=\"topic1,topic2\" \\\n+        --parameters outputTopic=\"projects/${PROJECT}/topics/your-topic-name\" \\\n+        --parameters outputFormat=\"PLAINTEXT\" \\\n+        --parameters secretStoreUrl=\"http(s)://host:port/path/to/credentials\" \\\n+        --parameters vaultToken=\"your-token\" \\\n+        --region \"${REGION}\"\n+    ```\n+3. With a REST API request\n+    ```\n+    API_ROOT_URL=\"https://dataflow.googleapis.com\"\n+    TEMPLATES_LAUNCH_API=\"${API_ROOT_URL}/v1b3/projects/${PROJECT}/locations/${REGION}/flexTemplates:launch\"\n+    JOB_NAME=\"kafka-to-pubsub-`date +%Y%m%d-%H%M%S-%N`\"\n+\n+    time curl -X POST -H \"Content-Type: application/json\" \\\n+        -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n+        -d '\n+         {\n+             \"launch_parameter\": {\n+                 \"jobName\": \"'$JOB_NAME'\",\n+                 \"containerSpecGcsPath\": \"'$TEMPLATE_PATH'\",\n+                 \"parameters\": {\n+                     \"bootstrapServers\": \"broker_1:9091, broker_2:9092\",\n+                     \"inputTopics\": \"topic1, topic2\",\n+                     \"outputTopic\": \"projects/'$PROJECT'/topics/your-topic-name\",\n+                     \"outputFormat\": \"PLAINTEXT\",\n+                     \"secretStoreUrl\": \"http(s)://host:port/path/to/credentials\",\n+                     \"vaultToken\": \"your-token\"\n+                 }\n+             }\n+         }\n+        '\n+        \"${TEMPLATES_LAUNCH_API}\"\n+    ```\n+\n+## AVRO format transferring.\n+This template contains an example Class to deserialize AVRO from Kafka and serialize it to AVRO in Pub/Sub.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYxOTExNQ=="}, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 229}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4172c96f2b7fe9119a5e05e38732c89fe5e868ff", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/4172c96f2b7fe9119a5e05e38732c89fe5e868ff", "committedDate": "2020-11-26T23:49:02Z", "message": "fixed review conversation items"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3c63298179b3f7a485885e2de59994f6cf05897d", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/3c63298179b3f7a485885e2de59994f6cf05897d", "committedDate": "2020-11-30T14:02:38Z", "message": "fix getting ssl credentials from Vault"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "477df3e234f351e43f91105eef79ecf68eebbf56", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/477df3e234f351e43f91105eef79ecf68eebbf56", "committedDate": "2020-11-30T14:10:19Z", "message": "FIX add empty && null map validation to sslConfig"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "83ff7b78aa84a9f7b79cb47ff896b10a41016d37", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/83ff7b78aa84a9f7b79cb47ff896b10a41016d37", "committedDate": "2020-11-30T15:06:02Z", "message": "FIX. remove vault ssl certs parameters"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b1ba8e123a2000ff14a6dd7b4118cd296654df3b", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/b1ba8e123a2000ff14a6dd7b4118cd296654df3b", "committedDate": "2020-11-30T15:27:18Z", "message": "metadata fix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "08bc3fefca612d43d1abab890c7d96fc8aa8bf15", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/08bc3fefca612d43d1abab890c7d96fc8aa8bf15", "committedDate": "2020-11-30T15:38:37Z", "message": "Local paths fix for SSL from GCS"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "9fb43b63430253f4a2b3cd57de7b9886ab4b3f67", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/9fb43b63430253f4a2b3cd57de7b9886ab4b3f67", "committedDate": "2020-12-01T15:43:49Z", "message": "add new log message to avoid wrong local files usage"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e99a7d31210f202f80fab645ce4f787c849160f9", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/e99a7d31210f202f80fab645ce4f787c849160f9", "committedDate": "2020-12-01T15:48:59Z", "message": "fix style"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQyMTIzNDU3", "url": "https://github.com/apache/beam/pull/13112#pullrequestreview-542123457", "createdAt": "2020-12-01T17:16:30Z", "commit": {"oid": "e99a7d31210f202f80fab645ce4f787c849160f9"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2ae7d024a5576823415a5448a29bff62c9b19893", "author": {"user": {"login": "ramazan-yapparov", "name": null}}, "url": "https://github.com/apache/beam/commit/2ae7d024a5576823415a5448a29bff62c9b19893", "committedDate": "2020-12-04T16:16:12Z", "message": "Moved kafka-to-pubsub to examples/ directory and updated README.md (#6)"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3e558a39aeed7671d4d9ae1f630d22f41e354044", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/3e558a39aeed7671d4d9ae1f630d22f41e354044", "committedDate": "2020-12-04T18:07:01Z", "message": "Stylefix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "51ddeadef3daf09f1d2af569d5d747c185a5f529", "author": {"user": {"login": "ramazan-yapparov", "name": null}}, "url": "https://github.com/apache/beam/commit/51ddeadef3daf09f1d2af569d5d747c185a5f529", "committedDate": "2020-12-07T11:46:30Z", "message": "Removed unused file"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e8ca92d68646cec459d3ecd94b0efe7551699268", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/e8ca92d68646cec459d3ecd94b0efe7551699268", "committedDate": "2020-12-07T20:12:26Z", "message": "add tbd section for e-2-e tests"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "880f7e6d00ad6e582a0edce14c4bdaff4173313e", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/880f7e6d00ad6e582a0edce14c4bdaff4173313e", "committedDate": "2020-12-07T20:13:07Z", "message": "fix styles"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0eba7ba5c7661c1e8a60178103f698064686eda4", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/0eba7ba5c7661c1e8a60178103f698064686eda4", "committedDate": "2020-12-07T22:48:02Z", "message": "specifying kafka-clients version"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5919f9b15047c6308bac5d8b7134895023cc0a75", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/5919f9b15047c6308bac5d8b7134895023cc0a75", "committedDate": "2020-12-07T23:33:32Z", "message": "fix readme"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4372d6ff56bdef40e106f358adb1881cd3032e6f", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/4372d6ff56bdef40e106f358adb1881cd3032e6f", "committedDate": "2020-12-07T23:39:57Z", "message": "template -> exmples"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3NDcxNzIw", "url": "https://github.com/apache/beam/pull/13112#pullrequestreview-547471720", "createdAt": "2020-12-08T17:39:00Z", "commit": {"oid": "4372d6ff56bdef40e106f358adb1881cd3032e6f"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzozOTowMFrOIBsVoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzo0MToyNFrOIBsf8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY0NTkyMQ==", "bodyText": "\ud83d\udc4d I think this is a great way to make the connection to the related Dataflow template while still making this example useful for Beam users using other runners. Thank you!", "url": "https://github.com/apache/beam/pull/13112#discussion_r538645921", "createdAt": "2020-12-08T17:39:00Z", "author": {"login": "TheNeuralBit"}, "path": "examples/kafka-to-pubsub/README.md", "diffHunk": "@@ -0,0 +1,163 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam pipeline example to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) pipeline example that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Cloud Pub/Sub](https://cloud.google.com/pubsub).\n+\n+Supported data formats:\n+- Serializable plaintext formats, such as JSON\n+- [PubSubMessage](https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage).\n+\n+Supported input source configurations:\n+- Single or multiple Apache Kafka bootstrap servers\n+- Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection\n+- Secrets vault service [HashiCorp Vault](https://www.vaultproject.io/).\n+\n+Supported destination configuration:\n+- Single Google Cloud Pub/Sub topic.\n+\n+In a simple scenario, the example will create an Apache Beam pipeline that will read messages from a source Kafka server with a source topic, and stream the text messages into specified Pub/Sub destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed over plain text or SSL encrypted connection. The example supports using a single Kafka user account to authenticate in the provided source Kafka servers and topics. To support SASL authenticaton over SSL the example will need an SSL certificate location and access to a secrets vault service with Kafka username and password, currently supporting HashiCorp Vault.\n+\n+## Requirements\n+\n+- Java 8\n+- Kafka Bootstrap Server(s) up and running\n+- Existing source Kafka topic(s)\n+- An existing Pub/Sub destination output topic\n+- (Optional) An existing HashiCorp Vault\n+- (Optional) A configured secure SSL connection for Kafka\n+\n+## Getting Started\n+\n+This section describes what is needed to get the exaple up and running.\n+- Assembling the Uber-JAR\n+- Local execution\n+- Google Dataflow Template\n+  - Set up the environment\n+  - Creating the Dataflow Flex Template\n+  - Create a Dataflow job to ingest data using the template\n+- Avro format transferring.\n+- E2E tests (TBD)\n+\n+## Assembling the Uber-JAR\n+\n+To run this example the Java project should be built into\n+an Uber JAR file.\n+\n+Navigate to the Beam folder:\n+\n+```\n+cd /path/to/beam\n+```\n+\n+In order to create Uber JAR with Gradle, [Shadow plugin](https://github.com/johnrengelman/shadow)\n+is used. It creates the `shadowJar` task that builds the Uber JAR:\n+\n+```\n+./gradlew -p examples/kafka-to-pubsub clean shadowJar\n+```\n+\n+\u2139\ufe0f An **Uber JAR** - also known as **fat JAR** - is a single JAR file that contains\n+both target package *and* all its dependencies.\n+\n+The result of the `shadowJar` task execution is a `.jar` file that is generated\n+under the `build/libs/` folder in kafka-to-pubsub directory.\n+\n+## Local execution\n+To execute this pipeline locally, specify the parameters:\n+- Kafka Bootstrap servers\n+- Kafka input topics\n+- Pub/Sub output topic\n+- Output format\n+\n+in the following format:\n+```bash\n+--bootstrapServers=host:port \\\n+--inputTopics=your-input-topic \\\n+--outputTopic=projects/your-project-id/topics/your-topic-pame \\\n+--outputFormat=AVRO|PUBSUB\n+```\n+Optionally, to retrieve Kafka credentials for SASL/SCRAM,\n+specify a URL to the credentials in HashiCorp Vault and the vault access token:\n+```bash\n+--secretStoreUrl=http(s)://host:port/path/to/credentials\n+--vaultToken=your-token\n+```\n+Optionally, to configure secure SSL connection between the Beam pipeline and Kafka,\n+specify the parameters:\n+- A path to a truststore file (it can be a local path or a GCS path, which should start with `gs://`)\n+- A path to a keystore file (it can be a local path or a GCS path, which should start with `gs://`)\n+- Truststore password\n+- Keystore password\n+- Key password\n+```bash\n+--truststorePath=path/to/kafka.truststore.jks\n+--keystorePath=path/to/kafka.keystore.jks\n+--truststorePassword=your-truststore-password\n+--keystorePassword=your-keystore-password\n+--keyPassword=your-key-password\n+```\n+To change the runner, specify:\n+```bash\n+--runner=YOUR_SELECTED_RUNNER\n+```\n+See examples/java/README.md for steps and examples to configure different runners.\n+\n+## Google Dataflow Execution\n+\n+This example also exists as Google Dataflow Template, see its [README.md](https://github.com/GoogleCloudPlatform/DataflowTemplates/blob/master/v2/kafka-to-pubsub/README.md) for more information.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4372d6ff56bdef40e106f358adb1881cd3032e6f"}, "originalPosition": 128}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY0ODU2MQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            To change the runner, specify:\n          \n          \n            \n            By default this will run the pipeline locally with the DirectRunner. To change the runner, specify:", "url": "https://github.com/apache/beam/pull/13112#discussion_r538648561", "createdAt": "2020-12-08T17:41:24Z", "author": {"login": "TheNeuralBit"}, "path": "examples/kafka-to-pubsub/README.md", "diffHunk": "@@ -0,0 +1,163 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam pipeline example to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) pipeline example that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Cloud Pub/Sub](https://cloud.google.com/pubsub).\n+\n+Supported data formats:\n+- Serializable plaintext formats, such as JSON\n+- [PubSubMessage](https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage).\n+\n+Supported input source configurations:\n+- Single or multiple Apache Kafka bootstrap servers\n+- Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection\n+- Secrets vault service [HashiCorp Vault](https://www.vaultproject.io/).\n+\n+Supported destination configuration:\n+- Single Google Cloud Pub/Sub topic.\n+\n+In a simple scenario, the example will create an Apache Beam pipeline that will read messages from a source Kafka server with a source topic, and stream the text messages into specified Pub/Sub destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed over plain text or SSL encrypted connection. The example supports using a single Kafka user account to authenticate in the provided source Kafka servers and topics. To support SASL authenticaton over SSL the example will need an SSL certificate location and access to a secrets vault service with Kafka username and password, currently supporting HashiCorp Vault.\n+\n+## Requirements\n+\n+- Java 8\n+- Kafka Bootstrap Server(s) up and running\n+- Existing source Kafka topic(s)\n+- An existing Pub/Sub destination output topic\n+- (Optional) An existing HashiCorp Vault\n+- (Optional) A configured secure SSL connection for Kafka\n+\n+## Getting Started\n+\n+This section describes what is needed to get the exaple up and running.\n+- Assembling the Uber-JAR\n+- Local execution\n+- Google Dataflow Template\n+  - Set up the environment\n+  - Creating the Dataflow Flex Template\n+  - Create a Dataflow job to ingest data using the template\n+- Avro format transferring.\n+- E2E tests (TBD)\n+\n+## Assembling the Uber-JAR\n+\n+To run this example the Java project should be built into\n+an Uber JAR file.\n+\n+Navigate to the Beam folder:\n+\n+```\n+cd /path/to/beam\n+```\n+\n+In order to create Uber JAR with Gradle, [Shadow plugin](https://github.com/johnrengelman/shadow)\n+is used. It creates the `shadowJar` task that builds the Uber JAR:\n+\n+```\n+./gradlew -p examples/kafka-to-pubsub clean shadowJar\n+```\n+\n+\u2139\ufe0f An **Uber JAR** - also known as **fat JAR** - is a single JAR file that contains\n+both target package *and* all its dependencies.\n+\n+The result of the `shadowJar` task execution is a `.jar` file that is generated\n+under the `build/libs/` folder in kafka-to-pubsub directory.\n+\n+## Local execution\n+To execute this pipeline locally, specify the parameters:\n+- Kafka Bootstrap servers\n+- Kafka input topics\n+- Pub/Sub output topic\n+- Output format\n+\n+in the following format:\n+```bash\n+--bootstrapServers=host:port \\\n+--inputTopics=your-input-topic \\\n+--outputTopic=projects/your-project-id/topics/your-topic-pame \\\n+--outputFormat=AVRO|PUBSUB\n+```\n+Optionally, to retrieve Kafka credentials for SASL/SCRAM,\n+specify a URL to the credentials in HashiCorp Vault and the vault access token:\n+```bash\n+--secretStoreUrl=http(s)://host:port/path/to/credentials\n+--vaultToken=your-token\n+```\n+Optionally, to configure secure SSL connection between the Beam pipeline and Kafka,\n+specify the parameters:\n+- A path to a truststore file (it can be a local path or a GCS path, which should start with `gs://`)\n+- A path to a keystore file (it can be a local path or a GCS path, which should start with `gs://`)\n+- Truststore password\n+- Keystore password\n+- Key password\n+```bash\n+--truststorePath=path/to/kafka.truststore.jks\n+--keystorePath=path/to/kafka.keystore.jks\n+--truststorePassword=your-truststore-password\n+--keystorePassword=your-keystore-password\n+--keyPassword=your-key-password\n+```\n+To change the runner, specify:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4372d6ff56bdef40e106f358adb1881cd3032e6f"}, "originalPosition": 120}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTQ3NDc5MjAw", "url": "https://github.com/apache/beam/pull/13112#pullrequestreview-547479200", "createdAt": "2020-12-08T17:47:53Z", "commit": {"oid": "4372d6ff56bdef40e106f358adb1881cd3032e6f"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzo0Nzo1M1rOIBs66Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzo1ODo1MFrOIBtpAQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY1NTQ2NQ==", "bodyText": "Let's call this just \"Running the pipeline\", since it also describes how to run on other runners, not just locally.\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ## Local execution\n          \n          \n            \n            ## Running the pipeline\n          \n      \n    \n    \n  \n\nTo be clear, users would normally use this approach to run on Dataflow just like any other runner, but the way this is written it looks like you have to use the approach in the next section. In fact the next section is for running the pipeline as a Dataflow template. For that reason please rename the \"Google Dataflow Execution\" section to \"Running as a Dataflow Template\".", "url": "https://github.com/apache/beam/pull/13112#discussion_r538655465", "createdAt": "2020-12-08T17:47:53Z", "author": {"login": "TheNeuralBit"}, "path": "examples/kafka-to-pubsub/README.md", "diffHunk": "@@ -0,0 +1,163 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam pipeline example to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) pipeline example that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Cloud Pub/Sub](https://cloud.google.com/pubsub).\n+\n+Supported data formats:\n+- Serializable plaintext formats, such as JSON\n+- [PubSubMessage](https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage).\n+\n+Supported input source configurations:\n+- Single or multiple Apache Kafka bootstrap servers\n+- Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection\n+- Secrets vault service [HashiCorp Vault](https://www.vaultproject.io/).\n+\n+Supported destination configuration:\n+- Single Google Cloud Pub/Sub topic.\n+\n+In a simple scenario, the example will create an Apache Beam pipeline that will read messages from a source Kafka server with a source topic, and stream the text messages into specified Pub/Sub destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed over plain text or SSL encrypted connection. The example supports using a single Kafka user account to authenticate in the provided source Kafka servers and topics. To support SASL authenticaton over SSL the example will need an SSL certificate location and access to a secrets vault service with Kafka username and password, currently supporting HashiCorp Vault.\n+\n+## Requirements\n+\n+- Java 8\n+- Kafka Bootstrap Server(s) up and running\n+- Existing source Kafka topic(s)\n+- An existing Pub/Sub destination output topic\n+- (Optional) An existing HashiCorp Vault\n+- (Optional) A configured secure SSL connection for Kafka\n+\n+## Getting Started\n+\n+This section describes what is needed to get the exaple up and running.\n+- Assembling the Uber-JAR\n+- Local execution\n+- Google Dataflow Template\n+  - Set up the environment\n+  - Creating the Dataflow Flex Template\n+  - Create a Dataflow job to ingest data using the template\n+- Avro format transferring.\n+- E2E tests (TBD)\n+\n+## Assembling the Uber-JAR\n+\n+To run this example the Java project should be built into\n+an Uber JAR file.\n+\n+Navigate to the Beam folder:\n+\n+```\n+cd /path/to/beam\n+```\n+\n+In order to create Uber JAR with Gradle, [Shadow plugin](https://github.com/johnrengelman/shadow)\n+is used. It creates the `shadowJar` task that builds the Uber JAR:\n+\n+```\n+./gradlew -p examples/kafka-to-pubsub clean shadowJar\n+```\n+\n+\u2139\ufe0f An **Uber JAR** - also known as **fat JAR** - is a single JAR file that contains\n+both target package *and* all its dependencies.\n+\n+The result of the `shadowJar` task execution is a `.jar` file that is generated\n+under the `build/libs/` folder in kafka-to-pubsub directory.\n+\n+## Local execution", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4372d6ff56bdef40e106f358adb1881cd3032e6f"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY2NDc4OA==", "bodyText": "Please remove the Dataflow template specific parts from this javadoc", "url": "https://github.com/apache/beam/pull/13112#discussion_r538664788", "createdAt": "2020-12-08T17:56:35Z", "author": {"login": "TheNeuralBit"}, "path": "examples/kafka-to-pubsub/src/main/java/org/apache/beam/examples/KafkaToPubsub.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.examples;\n+\n+import static org.apache.beam.examples.kafka.consumer.Utils.configureKafka;\n+import static org.apache.beam.examples.kafka.consumer.Utils.configureSsl;\n+import static org.apache.beam.examples.kafka.consumer.Utils.getKafkaCredentialsFromVault;\n+import static org.apache.beam.examples.kafka.consumer.Utils.isSslSpecified;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.beam.examples.avro.AvroDataClass;\n+import org.apache.beam.examples.avro.AvroDataClassKafkaAvroDeserializer;\n+import org.apache.beam.examples.options.KafkaToPubsubOptions;\n+import org.apache.beam.examples.transforms.FormatTransform;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult;\n+import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.Values;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The {@link KafkaToPubsub} pipeline is a streaming pipeline which ingests data in JSON format from\n+ * Kafka, and outputs the resulting records to PubSub. Input topics, output topic, Bootstrap servers\n+ * are specified by the user as template parameters. <br>\n+ * Kafka may be configured with SASL/SCRAM security mechanism, in this case a Vault secret storage\n+ * with credentials should be provided. URL to credentials and Vault token are specified by the user\n+ * as template parameters.\n+ *\n+ * <p><b>Pipeline Requirements</b>\n+ *\n+ * <ul>\n+ *   <li>Kafka Bootstrap Server(s).\n+ *   <li>Kafka Topic(s) exists.\n+ *   <li>The PubSub output topic exists.\n+ *   <li>(Optional) An existing HashiCorp Vault secret storage\n+ * </ul>\n+ *\n+ * <p><b>Example Usage</b>\n+ *\n+ * <pre>\n+ * # Set the pipeline vars\n+ * PROJECT=id-of-my-project\n+ * BUCKET_NAME=my-bucket\n+ *\n+ * # Set containerization vars\n+ * IMAGE_NAME=my-image-name\n+ * TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+ * BASE_CONTAINER_IMAGE=my-base-container-image\n+ * TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+ *\n+ * # Create bucket in the cloud storage\n+ * gsutil mb gs://${BUCKET_NAME}\n+ *\n+ * # Go to the beam folder\n+ * cd /path/to/beam\n+ *\n+ * <b>FLEX TEMPLATE</b>\n+ * # Assemble uber-jar\n+ * ./gradlew -p templates/kafka-to-pubsub clean shadowJar\n+ *\n+ * # Go to the template folder\n+ * cd /path/to/beam/templates/kafka-to-pubsub\n+ *\n+ * # Build the flex template\n+ * gcloud dataflow flex-template build ${TEMPLATE_PATH} \\", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4372d6ff56bdef40e106f358adb1881cd3032e6f"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY2NzI2NQ==", "bodyText": "Is it worth having this PUBSUB path? The README and javadoc only discuss the AVRO path. I think we should just have that one and remove the enum", "url": "https://github.com/apache/beam/pull/13112#discussion_r538667265", "createdAt": "2020-12-08T17:58:50Z", "author": {"login": "TheNeuralBit"}, "path": "examples/kafka-to-pubsub/src/main/java/org/apache/beam/examples/KafkaToPubsub.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.examples;\n+\n+import static org.apache.beam.examples.kafka.consumer.Utils.configureKafka;\n+import static org.apache.beam.examples.kafka.consumer.Utils.configureSsl;\n+import static org.apache.beam.examples.kafka.consumer.Utils.getKafkaCredentialsFromVault;\n+import static org.apache.beam.examples.kafka.consumer.Utils.isSslSpecified;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.beam.examples.avro.AvroDataClass;\n+import org.apache.beam.examples.avro.AvroDataClassKafkaAvroDeserializer;\n+import org.apache.beam.examples.options.KafkaToPubsubOptions;\n+import org.apache.beam.examples.transforms.FormatTransform;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult;\n+import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.Values;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The {@link KafkaToPubsub} pipeline is a streaming pipeline which ingests data in JSON format from\n+ * Kafka, and outputs the resulting records to PubSub. Input topics, output topic, Bootstrap servers\n+ * are specified by the user as template parameters. <br>\n+ * Kafka may be configured with SASL/SCRAM security mechanism, in this case a Vault secret storage\n+ * with credentials should be provided. URL to credentials and Vault token are specified by the user\n+ * as template parameters.\n+ *\n+ * <p><b>Pipeline Requirements</b>\n+ *\n+ * <ul>\n+ *   <li>Kafka Bootstrap Server(s).\n+ *   <li>Kafka Topic(s) exists.\n+ *   <li>The PubSub output topic exists.\n+ *   <li>(Optional) An existing HashiCorp Vault secret storage\n+ * </ul>\n+ *\n+ * <p><b>Example Usage</b>\n+ *\n+ * <pre>\n+ * # Set the pipeline vars\n+ * PROJECT=id-of-my-project\n+ * BUCKET_NAME=my-bucket\n+ *\n+ * # Set containerization vars\n+ * IMAGE_NAME=my-image-name\n+ * TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+ * BASE_CONTAINER_IMAGE=my-base-container-image\n+ * TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+ *\n+ * # Create bucket in the cloud storage\n+ * gsutil mb gs://${BUCKET_NAME}\n+ *\n+ * # Go to the beam folder\n+ * cd /path/to/beam\n+ *\n+ * <b>FLEX TEMPLATE</b>\n+ * # Assemble uber-jar\n+ * ./gradlew -p templates/kafka-to-pubsub clean shadowJar\n+ *\n+ * # Go to the template folder\n+ * cd /path/to/beam/templates/kafka-to-pubsub\n+ *\n+ * # Build the flex template\n+ * gcloud dataflow flex-template build ${TEMPLATE_PATH} \\\n+ *       --image-gcr-path \"${TARGET_GCR_IMAGE}\" \\\n+ *       --sdk-language \"JAVA\" \\\n+ *       --flex-template-base-image ${BASE_CONTAINER_IMAGE} \\\n+ *       --metadata-file \"src/main/resources/kafka_to_pubsub_metadata.json\" \\\n+ *       --jar \"build/libs/beam-templates-kafka-to-pubsub-<version>-all.jar\" \\\n+ *       --env FLEX_TEMPLATE_JAVA_MAIN_CLASS=\"org.apache.beam.templates.KafkaToPubsub\"\n+ *\n+ * # Execute template:\n+ *    API_ROOT_URL=\"https://dataflow.googleapis.com\"\n+ *    TEMPLATES_LAUNCH_API=\"${API_ROOT_URL}/v1b3/projects/${PROJECT}/locations/${REGION}/flexTemplates:launch\"\n+ *    JOB_NAME=\"kafka-to-pubsub-`date +%Y%m%d-%H%M%S-%N`\"\n+ *\n+ *    time curl -X POST -H \"Content-Type: application/json\" \\\n+ *            -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n+ *            -d '\n+ *             {\n+ *                 \"launch_parameter\": {\n+ *                     \"jobName\": \"'$JOB_NAME'\",\n+ *                     \"containerSpecGcsPath\": \"'$TEMPLATE_PATH'\",\n+ *                     \"parameters\": {\n+ *                         \"bootstrapServers\": \"broker_1:9091, broker_2:9092\",\n+ *                         \"inputTopics\": \"topic1, topic2\",\n+ *                         \"outputTopic\": \"projects/'$PROJECT'/topics/your-topic-name\",\n+ *                         \"secretStoreUrl\": \"http(s)://host:port/path/to/credentials\",\n+ *                         \"vaultToken\": \"your-token\"\n+ *                     }\n+ *                 }\n+ *             }\n+ *            '\n+ *            \"${TEMPLATES_LAUNCH_API}\"\n+ * </pre>\n+ *\n+ * <p><b>Example Avro usage</b>\n+ *\n+ * <pre>\n+ * This template contains an example Class to deserialize AVRO from Kafka and serialize it to AVRO in Pub/Sub.\n+ *\n+ * To use this example in the specific case, follow the few steps:\n+ * <ul>\n+ * <li> Create your own class to describe AVRO schema. As an example use {@link AvroDataClass}. Just define necessary fields.\n+ * <li> Create your own Avro Deserializer class. As an example use {@link AvroDataClassKafkaAvroDeserializer}. Just rename it, and put your own Schema class as the necessary types.\n+ * <li> Modify the {@link FormatTransform}. Put your Schema class and Deserializer to the related parameter.\n+ * <li> Modify write step in the {@link KafkaToPubsub} by put your Schema class to \"writeAvrosToPubSub\" step.\n+ * </ul>\n+ * </pre>\n+ */\n+public class KafkaToPubsub {\n+\n+  /* Logger for class.*/\n+  private static final Logger LOG = LoggerFactory.getLogger(KafkaToPubsub.class);\n+\n+  /**\n+   * Main entry point for pipeline execution.\n+   *\n+   * @param args Command line arguments to the pipeline.\n+   */\n+  public static void main(String[] args) {\n+    KafkaToPubsubOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(KafkaToPubsubOptions.class);\n+\n+    run(options);\n+  }\n+\n+  /**\n+   * Runs a pipeline which reads message from Kafka and writes it to GCS.\n+   *\n+   * @param options arguments to the pipeline\n+   */\n+  public static PipelineResult run(KafkaToPubsubOptions options) {\n+    // Configure Kafka consumer properties\n+    Map<String, Object> kafkaConfig = new HashMap<>();\n+    Map<String, String> sslConfig = new HashMap<>();\n+    if (options.getSecretStoreUrl() != null && options.getVaultToken() != null) {\n+      Map<String, Map<String, String>> credentials =\n+          getKafkaCredentialsFromVault(options.getSecretStoreUrl(), options.getVaultToken());\n+      kafkaConfig = configureKafka(credentials.get(KafkaPubsubConstants.KAFKA_CREDENTIALS));\n+    } else {\n+      LOG.warn(\n+          \"No information to retrieve Kafka credentials was provided. \"\n+              + \"Trying to initiate an unauthorized connection.\");\n+    }\n+\n+    if (isSslSpecified(options)) {\n+      sslConfig.putAll(configureSsl(options));\n+    } else {\n+      LOG.info(\n+          \"No information to retrieve SSL certificate was provided by parameters.\"\n+              + \"Trying to initiate a plain text connection.\");\n+    }\n+\n+    List<String> topicsList = new ArrayList<>(Arrays.asList(options.getInputTopics().split(\",\")));\n+\n+    checkArgument(\n+        topicsList.size() > 0 && topicsList.get(0).length() > 0,\n+        \"inputTopics cannot be an empty string.\");\n+\n+    List<String> bootstrapServersList =\n+        new ArrayList<>(Arrays.asList(options.getBootstrapServers().split(\",\")));\n+\n+    checkArgument(\n+        bootstrapServersList.size() > 0 && topicsList.get(0).length() > 0,\n+        \"bootstrapServers cannot be an empty string.\");\n+\n+    // Create the pipeline\n+    Pipeline pipeline = Pipeline.create(options);\n+    LOG.info(\n+        \"Starting Kafka-To-PubSub pipeline with parameters bootstrap servers:\"\n+            + options.getBootstrapServers()\n+            + \" input topics: \"\n+            + options.getInputTopics()\n+            + \" output pubsub topic: \"\n+            + options.getOutputTopic());\n+\n+    /*\n+     * Steps:\n+     *  1) Read messages in from Kafka\n+     *  2) Extract values only\n+     *  3) Write successful records to PubSub\n+     */\n+\n+    if (options.getOutputFormat() == FormatTransform.FORMAT.AVRO) {\n+      pipeline\n+          .apply(\n+              \"readAvrosFromKafka\",\n+              FormatTransform.readAvrosFromKafka(\n+                  options.getBootstrapServers(), topicsList, kafkaConfig, sslConfig))\n+          .apply(\"createValues\", Values.create())\n+          .apply(\"writeAvrosToPubSub\", PubsubIO.writeAvros(AvroDataClass.class));\n+\n+    } else {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4372d6ff56bdef40e106f358adb1881cd3032e6f"}, "originalPosition": 217}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dd85c93e8d405bb3a936ae5a14eca14825f89eaf", "author": {"user": {"login": "ilya-kozyrev", "name": "Ilya"}}, "url": "https://github.com/apache/beam/commit/dd85c93e8d405bb3a936ae5a14eca14825f89eaf", "committedDate": "2020-12-08T19:46:41Z", "message": "Update examples/kafka-to-pubsub/README.md\n\nCo-authored-by: Brian Hulette <hulettbh@gmail.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "89adf0bdc98aef01092a62494f1bce2d96e7fc01", "author": {"user": {"login": "ramazan-yapparov", "name": null}}, "url": "https://github.com/apache/beam/commit/89adf0bdc98aef01092a62494f1bce2d96e7fc01", "committedDate": "2020-12-09T08:45:54Z", "message": "Merge branch 'master' into KafkaToPubsubTemplate"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "51182eb24d4d7f2c7f424259f5287aed26dd67af", "author": {"user": {"login": "ramazan-yapparov", "name": null}}, "url": "https://github.com/apache/beam/commit/51182eb24d4d7f2c7f424259f5287aed26dd67af", "committedDate": "2020-12-09T08:58:27Z", "message": "Fixed outdated import"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a5406a558b095ea2eded4536d1bcb9aed1c8e449", "author": {"user": {"login": "ramazan-yapparov", "name": null}}, "url": "https://github.com/apache/beam/commit/a5406a558b095ea2eded4536d1bcb9aed1c8e449", "committedDate": "2020-12-09T14:56:51Z", "message": "Moved template to examples/complete"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e7fc00e6df390e84d55cbc4e8e03c98b66f56625", "author": {"user": {"login": "ramazan-yapparov", "name": null}}, "url": "https://github.com/apache/beam/commit/e7fc00e6df390e84d55cbc4e8e03c98b66f56625", "committedDate": "2020-12-09T15:10:54Z", "message": "Updated paths in readme file"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ac76e738bd9d466669721c8511f8d8bf999521fe", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/ac76e738bd9d466669721c8511f8d8bf999521fe", "committedDate": "2020-12-09T16:54:10Z", "message": "Updated README.md and javadoc regarding comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b0c44289db70c7a4dc2b6f0f6275c62bc2214c86", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/b0c44289db70c7a4dc2b6f0f6275c62bc2214c86", "committedDate": "2020-12-09T17:16:07Z", "message": "README.md stylefix"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3931fba06323d0e238355e8c5538c61e6a26310a", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/3931fba06323d0e238355e8c5538c61e6a26310a", "committedDate": "2020-12-09T17:33:04Z", "message": "Added link to KafkaToPubsub example into complete/README.md"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "21d8b36652b9be13ba17be1c2ae8ef6e3c351472", "author": {"user": {"login": "KhaninArtur", "name": "Artur Khanin"}}, "url": "https://github.com/apache/beam/commit/21d8b36652b9be13ba17be1c2ae8ef6e3c351472", "committedDate": "2020-12-09T17:47:58Z", "message": "Stylefix"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUxOTU0MTQz", "url": "https://github.com/apache/beam/pull/13112#pullrequestreview-551954143", "createdAt": "2020-12-14T22:39:59Z", "commit": {"oid": "21d8b36652b9be13ba17be1c2ae8ef6e3c351472"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2030, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}