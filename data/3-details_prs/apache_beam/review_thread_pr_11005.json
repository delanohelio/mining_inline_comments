{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzgxNjIxNzU0", "number": 11005, "reviewThreads": {"totalCount": 27, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QxOTo1Njo1MFrODky3gA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxNzoyNjo0MlrODnLKtQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTA4NzM2OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/direct/direct_runner.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QxOTo1Njo1MFrOFxUZCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQxOTozNTozNVrOFx62mQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI1ODYzNQ==", "bodyText": "Avoid backslashes for continuation. If needed, use ()'s (or just let yapf do it). (Creating the stub here seems a violation of encapsulation though, best to do it where it's used.)", "url": "https://github.com/apache/beam/pull/11005#discussion_r387258635", "createdAt": "2020-03-03T19:56:50Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/direct_runner.py", "diffHunk": "@@ -399,6 +395,15 @@ def visit_transform(self, applied_ptransform):\n     self.consumer_tracking_visitor = ConsumerTrackingPipelineVisitor()\n     pipeline.visit(self.consumer_tracking_visitor)\n \n+    test_stream_service_endpoint = \\", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzg4ODc5Mw==", "bodyText": "Ack, using parentheses.", "url": "https://github.com/apache/beam/pull/11005#discussion_r387888793", "createdAt": "2020-03-04T19:35:35Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/direct/direct_runner.py", "diffHunk": "@@ -399,6 +395,15 @@ def visit_transform(self, applied_ptransform):\n     self.consumer_tracking_visitor = ConsumerTrackingPipelineVisitor()\n     pipeline.visit(self.consumer_tracking_visitor)\n \n+    test_stream_service_endpoint = \\", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI1ODYzNQ=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTA5MDc5OnYy", "diffSide": "LEFT", "path": "sdks/python/apache_beam/runners/direct/direct_runner.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QxOTo1Nzo0NlrOFxUbHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQxOToyNTowNlrOFx6fsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI1OTE2Nw==", "bodyText": "Why was this removed?", "url": "https://github.com/apache/beam/pull/11005#discussion_r387259167", "createdAt": "2020-03-03T19:57:46Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/direct_runner.py", "diffHunk": "@@ -114,13 +117,6 @@ def visit_transform(self, applied_ptransform):\n     # FnApiRunner, and the pipeline was not meant to be run as streaming.\n     use_fnapi_runner = (_FnApiRunnerSupportVisitor().accept(pipeline))\n \n-    # Also ensure grpc is available.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzg4MjkyOQ==", "bodyText": "Added back", "url": "https://github.com/apache/beam/pull/11005#discussion_r387882929", "createdAt": "2020-03-04T19:25:06Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/direct/direct_runner.py", "diffHunk": "@@ -114,13 +117,6 @@ def visit_transform(self, applied_ptransform):\n     # FnApiRunner, and the pipeline was not meant to be run as streaming.\n     use_fnapi_runner = (_FnApiRunnerSupportVisitor().accept(pipeline))\n \n-    # Also ensure grpc is available.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI1OTE2Nw=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTA5MzYwOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/options/pipeline_options.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QxOTo1ODozNVrOFxUc1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQxOToyNToxN1rOFx6gCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI1OTYwNQ==", "bodyText": "This shouldn't be a pipeline option, it should be a parameter in the TestStream proto itself.", "url": "https://github.com/apache/beam/pull/11005#discussion_r387259605", "createdAt": "2020-03-03T19:58:35Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/options/pipeline_options.py", "diffHunk": "@@ -1127,6 +1127,12 @@ def _add_argparse_args(cls, parser):\n         help='The time to wait (in milliseconds) for test pipeline to finish. '\n         'If it is set to None, it will wait indefinitely until the job '\n         'is finished.')\n+    parser.add_argument(\n+        '--test_stream_service_endpoint',", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzg4MzAxOQ==", "bodyText": "Ack, taken from proto", "url": "https://github.com/apache/beam/pull/11005#discussion_r387883019", "createdAt": "2020-03-04T19:25:17Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/options/pipeline_options.py", "diffHunk": "@@ -1127,6 +1127,12 @@ def _add_argparse_args(cls, parser):\n         help='The time to wait (in milliseconds) for test pipeline to finish. '\n         'If it is set to None, it will wait indefinitely until the job '\n         'is finished.')\n+    parser.add_argument(\n+        '--test_stream_service_endpoint',", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI1OTYwNQ=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTA5ODQyOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/direct/clock.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QxOTo1OTo1MlrOFxUfkw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQxOToyNjozMlrOFx6itg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI2MDMwNw==", "bodyText": "We shouldn't be changing the API of TestClock. If the caller needs a float, convert it there.", "url": "https://github.com/apache/beam/pull/11005#discussion_r387260307", "createdAt": "2020-03-03T19:59:52Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/clock.py", "diffHunk": "@@ -44,11 +46,11 @@ def time(self):\n \n class TestClock(object):\n   \"\"\"Clock used for Testing\"\"\"\n-  def __init__(self, current_time=0):\n-    self._current_time = current_time\n+  def __init__(self, current_time=None):\n+    self._current_time = current_time if current_time else Timestamp()\n \n   def time(self):\n-    return self._current_time\n+    return float(self._current_time)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzg4MzcwMg==", "bodyText": "Not changing it, fixing it. The TestClock has the same interface as the RealClock and should return the same thing. See the comment on the Clock interface.", "url": "https://github.com/apache/beam/pull/11005#discussion_r387883702", "createdAt": "2020-03-04T19:26:32Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/direct/clock.py", "diffHunk": "@@ -44,11 +46,11 @@ def time(self):\n \n class TestClock(object):\n   \"\"\"Clock used for Testing\"\"\"\n-  def __init__(self, current_time=0):\n-    self._current_time = current_time\n+  def __init__(self, current_time=None):\n+    self._current_time = current_time if current_time else Timestamp()\n \n   def time(self):\n-    return self._current_time\n+    return float(self._current_time)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI2MDMwNw=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTEwOTI4OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/direct/evaluation_context.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMDowMzowN1rOFxUmXA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQxOToyNzowM1rOFx6j0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI2MjA0NA==", "bodyText": "Similarly, the test_stream_event_channel doesn't seem a property of the evaluation context, rather a property of the test stream (evaluator) itself.", "url": "https://github.com/apache/beam/pull/11005#discussion_r387262044", "createdAt": "2020-03-03T20:03:07Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/evaluation_context.py", "diffHunk": "@@ -273,7 +274,8 @@ def __init__(self,\n     ]  # type: List[Tuple[TransformExecutor, Timestamp]]\n     self._counter_factory = counters.CounterFactory()\n     self._metrics = DirectMetrics()\n-\n+    self._test_stream_event_stub = test_stream_event_stub\n+    self._test_stream_event_channel = None", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzg4Mzk4Nw==", "bodyText": "Ack, I moved the event generation into the test_stream_impl module.", "url": "https://github.com/apache/beam/pull/11005#discussion_r387883987", "createdAt": "2020-03-04T19:27:03Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/direct/evaluation_context.py", "diffHunk": "@@ -273,7 +274,8 @@ def __init__(self,\n     ]  # type: List[Tuple[TransformExecutor, Timestamp]]\n     self._counter_factory = counters.CounterFactory()\n     self._metrics = DirectMetrics()\n-\n+    self._test_stream_event_stub = test_stream_event_stub\n+    self._test_stream_event_channel = None", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI2MjA0NA=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 16}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTExMjkyOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMDowNDoyMFrOFxUotw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQxOToyNzoyNFrOFx6kmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI2MjY0Nw==", "bodyText": "This logic feels odd, needs an explanation.", "url": "https://github.com/apache/beam/pull/11005#discussion_r387262647", "createdAt": "2020-03-03T20:04:20Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -212,6 +218,19 @@ class _TestStreamRootBundleProvider(RootBundleProvider):\n   \"\"\"\n   def get_root_bundles(self):\n     test_stream = self._applied_ptransform.transform\n+    if self._evaluation_context._test_stream_event_stub:\n+      stub = self._evaluation_context._test_stream_event_stub\n+\n+      if list(test_stream.output_tags) == [None]:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzg4NDE4Ng==", "bodyText": "Added a comment", "url": "https://github.com/apache/beam/pull/11005#discussion_r387884186", "createdAt": "2020-03-04T19:27:24Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -212,6 +218,19 @@ class _TestStreamRootBundleProvider(RootBundleProvider):\n   \"\"\"\n   def get_root_bundles(self):\n     test_stream = self._applied_ptransform.transform\n+    if self._evaluation_context._test_stream_event_stub:\n+      stub = self._evaluation_context._test_stream_event_stub\n+\n+      if list(test_stream.output_tags) == [None]:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI2MjY0Nw=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTE0MTQwOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "isResolved": true, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMDoxMjo1N1rOFxU58w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wN1QwMDozNTowNVrOFzLOpg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI2NzA1OQ==", "bodyText": "This is fragile--we should not be (attempting to) interpret values as dicts of windowed value parameters and creating globally windowed values if anything goes wrong. Instead, store the WindowedValue object in tv.value. (Alternatively, one could let events be full WindowedValues rather than TimestampedValues, but that might be a bigger change.)", "url": "https://github.com/apache/beam/pull/11005#discussion_r387267059", "createdAt": "2020-03-03T20:12:57Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -421,8 +440,12 @@ def process_element(self, element):\n       main_output = list(self._outputs)[0]\n       bundle = self._evaluation_context.create_bundle(main_output)\n       for tv in event.timestamped_values:\n-        bundle.output(\n-            GlobalWindows.windowed_value(tv.value, timestamp=tv.timestamp))\n+        # Unreify the value into the correct window.\n+        try:\n+          bundle.output(WindowedValue(**tv.value))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI2ODc0Mg==", "bodyText": "(For that matter, it seems odd to be mutating ElementEvents in an operator called _WatermarkControllerEvaluator.)", "url": "https://github.com/apache/beam/pull/11005#discussion_r387268742", "createdAt": "2020-03-03T20:16:12Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -421,8 +440,12 @@ def process_element(self, element):\n       main_output = list(self._outputs)[0]\n       bundle = self._evaluation_context.create_bundle(main_output)\n       for tv in event.timestamped_values:\n-        bundle.output(\n-            GlobalWindows.windowed_value(tv.value, timestamp=tv.timestamp))\n+        # Unreify the value into the correct window.\n+        try:\n+          bundle.output(WindowedValue(**tv.value))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI2NzA1OQ=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzg4NzM1NQ==", "bodyText": "Unfortunately the Python SDK interprets an element that's a WindowedValue as an element that should have its windowing information changed. This is used directly by the ReverseTestStream to produce WindowedValue args that keep the windowing information. But if these elements were WindowedValues themselves, then the Python SDK would reinterpret and unnest them at every step.", "url": "https://github.com/apache/beam/pull/11005#discussion_r387887355", "createdAt": "2020-03-04T19:32:54Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -421,8 +440,12 @@ def process_element(self, element):\n       main_output = list(self._outputs)[0]\n       bundle = self._evaluation_context.create_bundle(main_output)\n       for tv in event.timestamped_values:\n-        bundle.output(\n-            GlobalWindows.windowed_value(tv.value, timestamp=tv.timestamp))\n+        # Unreify the value into the correct window.\n+        try:\n+          bundle.output(WindowedValue(**tv.value))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI2NzA1OQ=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTExNzYxNg==", "bodyText": "In that case, it'd be cleaner to make a new class WindowedValueHolder, or use a 1-tuple of WindowedValue, or something explicit like that rather than trying to use every element as kwargs to WindowedValue constructor and falling back otherwise.", "url": "https://github.com/apache/beam/pull/11005#discussion_r389117616", "createdAt": "2020-03-06T20:04:25Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -421,8 +440,12 @@ def process_element(self, element):\n       main_output = list(self._outputs)[0]\n       bundle = self._evaluation_context.create_bundle(main_output)\n       for tv in event.timestamped_values:\n-        bundle.output(\n-            GlobalWindows.windowed_value(tv.value, timestamp=tv.timestamp))\n+        # Unreify the value into the correct window.\n+        try:\n+          bundle.output(WindowedValue(**tv.value))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI2NzA1OQ=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyMjk4NA==", "bodyText": "Alternatively, it might be cleaner to just pass the test stream event proto here rather than decoding it and then interpreting it. (Maybe this'd be a larger change.)", "url": "https://github.com/apache/beam/pull/11005#discussion_r389122984", "createdAt": "2020-03-06T20:16:49Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -421,8 +440,12 @@ def process_element(self, element):\n       main_output = list(self._outputs)[0]\n       bundle = self._evaluation_context.create_bundle(main_output)\n       for tv in event.timestamped_values:\n-        bundle.output(\n-            GlobalWindows.windowed_value(tv.value, timestamp=tv.timestamp))\n+        # Unreify the value into the correct window.\n+        try:\n+          bundle.output(WindowedValue(**tv.value))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI2NzA1OQ=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTIwNTY3MA==", "bodyText": "Ack, for now I changed it to a WindowedValueHolder.", "url": "https://github.com/apache/beam/pull/11005#discussion_r389205670", "createdAt": "2020-03-07T00:35:05Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -421,8 +440,12 @@ def process_element(self, element):\n       main_output = list(self._outputs)[0]\n       bundle = self._evaluation_context.create_bundle(main_output)\n       for tv in event.timestamped_values:\n-        bundle.output(\n-            GlobalWindows.windowed_value(tv.value, timestamp=tv.timestamp))\n+        # Unreify the value into the correct window.\n+        try:\n+          bundle.output(WindowedValue(**tv.value))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI2NzA1OQ=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTE1NjMwOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMDoxNzoyNVrOFxVC6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQxOTozMzoxM1rOFx6xxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI2OTM1Mw==", "bodyText": "Why is this being created here?", "url": "https://github.com/apache/beam/pull/11005#discussion_r387269353", "createdAt": "2020-03-03T20:17:25Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -212,6 +218,19 @@ class _TestStreamRootBundleProvider(RootBundleProvider):\n   \"\"\"\n   def get_root_bundles(self):\n     test_stream = self._applied_ptransform.transform\n+    if self._evaluation_context._test_stream_event_stub:\n+      stub = self._evaluation_context._test_stream_event_stub\n+\n+      if list(test_stream.output_tags) == [None]:\n+        event_request = beam_runner_api_pb2.EventsRequest()\n+      else:\n+        event_request = beam_runner_api_pb2.EventsRequest(\n+            keys=list(test_stream.output_tags))\n+\n+      test_stream_event_channel = stub.Events(event_request)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzg4NzU1Nw==", "bodyText": "Moved to the test_stream_impl module.", "url": "https://github.com/apache/beam/pull/11005#discussion_r387887557", "createdAt": "2020-03-04T19:33:13Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -212,6 +218,19 @@ class _TestStreamRootBundleProvider(RootBundleProvider):\n   \"\"\"\n   def get_root_bundles(self):\n     test_stream = self._applied_ptransform.transform\n+    if self._evaluation_context._test_stream_event_stub:\n+      stub = self._evaluation_context._test_stream_event_stub\n+\n+      if list(test_stream.output_tags) == [None]:\n+        event_request = beam_runner_api_pb2.EventsRequest()\n+      else:\n+        event_request = beam_runner_api_pb2.EventsRequest(\n+            keys=list(test_stream.output_tags))\n+\n+      test_stream_event_channel = stub.Events(event_request)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI2OTM1Mw=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTE2MTg3OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMDoxOTowNlrOFxVGQw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQxOTozNDowNlrOFx6zfA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI3MDIxMQ==", "bodyText": "Why does the set of keys even need to be sent here?", "url": "https://github.com/apache/beam/pull/11005#discussion_r387270211", "createdAt": "2020-03-03T20:19:06Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -212,6 +218,19 @@ class _TestStreamRootBundleProvider(RootBundleProvider):\n   \"\"\"\n   def get_root_bundles(self):\n     test_stream = self._applied_ptransform.transform\n+    if self._evaluation_context._test_stream_event_stub:\n+      stub = self._evaluation_context._test_stream_event_stub\n+\n+      if list(test_stream.output_tags) == [None]:\n+        event_request = beam_runner_api_pb2.EventsRequest()\n+      else:\n+        event_request = beam_runner_api_pb2.EventsRequest(\n+            keys=list(test_stream.output_tags))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzg4Nzk5Ng==", "bodyText": "To know which PCollections to read from. Added a comment.", "url": "https://github.com/apache/beam/pull/11005#discussion_r387887996", "createdAt": "2020-03-04T19:34:06Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -212,6 +218,19 @@ class _TestStreamRootBundleProvider(RootBundleProvider):\n   \"\"\"\n   def get_root_bundles(self):\n     test_stream = self._applied_ptransform.transform\n+    if self._evaluation_context._test_stream_event_stub:\n+      stub = self._evaluation_context._test_stream_event_stub\n+\n+      if list(test_stream.output_tags) == [None]:\n+        event_request = beam_runner_api_pb2.EventsRequest()\n+      else:\n+        event_request = beam_runner_api_pb2.EventsRequest(\n+            keys=list(test_stream.output_tags))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI3MDIxMQ=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTE2ODc1OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMDoyMToxMVrOFxVKWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQxOTozNDoxOFrOFx6z-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI3MTI1OQ==", "bodyText": "Don't mutate the input.", "url": "https://github.com/apache/beam/pull/11005#discussion_r387271259", "createdAt": "2020-03-03T20:21:11Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -432,6 +455,45 @@ def finish_bundle(self):\n         self, self.bundles, [], None, {None: self._watermark})\n \n \n+class PairWithTimingEvaluator(_TransformEvaluator):\n+  \"\"\"TransformEvaluator for the PairWithTiming transform.\n+\n+  This transform takes an element as an input and outputs\n+  KV(element, `TimingInfo`). Where the `TimingInfo` contains both the\n+  processing time timestamp and watermark.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      evaluation_context,\n+      applied_ptransform,\n+      input_committed_bundle,\n+      side_inputs):\n+    assert not side_inputs\n+    super(PairWithTimingEvaluator, self).__init__(\n+        evaluation_context,\n+        applied_ptransform,\n+        input_committed_bundle,\n+        side_inputs)\n+\n+  def start_bundle(self):\n+    main_output = list(self._outputs)[0]\n+    self.bundle = self._evaluation_context.create_bundle(main_output)\n+\n+  def process_element(self, element):\n+    watermark_manager = self._evaluation_context._watermark_manager\n+    watermarks = watermark_manager.get_watermarks(self._applied_ptransform)\n+\n+    output_watermark = watermarks.output_watermark\n+    now = Timestamp(seconds=watermark_manager._clock.time())\n+    timing_info = TimingInfo(now, output_watermark)\n+\n+    element.value = (element.value, timing_info)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzg4ODEyMA==", "bodyText": "Ack returning a new WindowedValue", "url": "https://github.com/apache/beam/pull/11005#discussion_r387888120", "createdAt": "2020-03-04T19:34:18Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -432,6 +455,45 @@ def finish_bundle(self):\n         self, self.bundles, [], None, {None: self._watermark})\n \n \n+class PairWithTimingEvaluator(_TransformEvaluator):\n+  \"\"\"TransformEvaluator for the PairWithTiming transform.\n+\n+  This transform takes an element as an input and outputs\n+  KV(element, `TimingInfo`). Where the `TimingInfo` contains both the\n+  processing time timestamp and watermark.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      evaluation_context,\n+      applied_ptransform,\n+      input_committed_bundle,\n+      side_inputs):\n+    assert not side_inputs\n+    super(PairWithTimingEvaluator, self).__init__(\n+        evaluation_context,\n+        applied_ptransform,\n+        input_committed_bundle,\n+        side_inputs)\n+\n+  def start_bundle(self):\n+    main_output = list(self._outputs)[0]\n+    self.bundle = self._evaluation_context.create_bundle(main_output)\n+\n+  def process_element(self, element):\n+    watermark_manager = self._evaluation_context._watermark_manager\n+    watermarks = watermark_manager.get_watermarks(self._applied_ptransform)\n+\n+    output_watermark = watermarks.output_watermark\n+    now = Timestamp(seconds=watermark_manager._clock.time())\n+    timing_info = TimingInfo(now, output_watermark)\n+\n+    element.value = (element.value, timing_info)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI3MTI1OQ=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTIyNjk5OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMDo0MDo1N1rOFxVvTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQxOTozNTowMVrOFx61Yw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI4MDcxOQ==", "bodyText": "Don't we already have this translation logic elsewhere. Also, it seems we could unify events-from-memory vs. events-from-grpc more.", "url": "https://github.com/apache/beam/pull/11005#discussion_r387280719", "createdAt": "2020-03-03T20:40:57Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -471,7 +537,44 @@ def process_element(self, element):\n     # We can either have the _TestStream or the _WatermarkController to emit\n     # the elements. We chose to emit in the _WatermarkController so that the\n     # element is emitted at the correct watermark value.\n-    for event in self.test_stream.events(self.current_index):\n+    events = []\n+    if self.watermark == MIN_TIMESTAMP:\n+      for event in self.test_stream._set_up(self.test_stream.output_tags):\n+        events.append(event)\n+\n+    if self.test_stream_event_channel:\n+      try:\n+        event = next(self.test_stream_event_channel)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 150}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4Nzg4ODQ4Mw==", "bodyText": "Ack, unified it with the two methods in the test_stream_impl to return iterators. These will be stored in the global EvaluationContext state.", "url": "https://github.com/apache/beam/pull/11005#discussion_r387888483", "createdAt": "2020-03-04T19:35:01Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -471,7 +537,44 @@ def process_element(self, element):\n     # We can either have the _TestStream or the _WatermarkController to emit\n     # the elements. We chose to emit in the _WatermarkController so that the\n     # element is emitted at the correct watermark value.\n-    for event in self.test_stream.events(self.current_index):\n+    events = []\n+    if self.watermark == MIN_TIMESTAMP:\n+      for event in self.test_stream._set_up(self.test_stream.output_tags):\n+        events.append(event)\n+\n+    if self.test_stream_event_channel:\n+      try:\n+        event = next(self.test_stream_event_channel)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI4MDcxOQ=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 150}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTYwOTQ4OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/interactive/cache_manager.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMjo0ODoxN1rOFxZdgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQwMDoxMTo1NVrOFyCOaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM0MTY5OQ==", "bodyText": "Thanks .iter([]) is slightly more idiomatic, but this is fine.", "url": "https://github.com/apache/beam/pull/11005#discussion_r387341699", "createdAt": "2020-03-03T22:48:17Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/interactive/cache_manager.py", "diffHunk": "@@ -167,20 +177,35 @@ def load_pcoder(self, *labels):\n         self._saved_pcoders[self._path(*labels)])\n \n   def read(self, *labels):\n+    # Return an iterator to an empty list if it doesn't exist.\n     if not self.exists(*labels):\n-      return [], -1\n+      return [].__iter__(), -1", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAwOTU3OQ==", "bodyText": "Gotcha, didn't know about this. Changed to iter([])", "url": "https://github.com/apache/beam/pull/11005#discussion_r388009579", "createdAt": "2020-03-05T00:11:55Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/interactive/cache_manager.py", "diffHunk": "@@ -167,20 +177,35 @@ def load_pcoder(self, *labels):\n         self._saved_pcoders[self._path(*labels)])\n \n   def read(self, *labels):\n+    # Return an iterator to an empty list if it doesn't exist.\n     if not self.exists(*labels):\n-      return [], -1\n+      return [].__iter__(), -1", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM0MTY5OQ=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 49}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTYzNDIyOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/interactive/cache_manager.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMjo1NzowOFrOFxZsSQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQwMDoxMjoxNVrOFyCOyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM0NTQ4MQ==", "bodyText": "This is not part of the public API for sinks. Instead, do\n      writer = sink.open_writer(init_result, path)\n      for v in values:\n        writer.write(v)\n      write_results.append(writer.close())\n\nhttps://github.com/apache/beam/blob/release-2.18.0/sdks/python/apache_beam/io/iobase.py#L668", "url": "https://github.com/apache/beam/pull/11005#discussion_r387345481", "createdAt": "2020-03-03T22:57:08Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/interactive/cache_manager.py", "diffHunk": "@@ -167,20 +177,35 @@ def load_pcoder(self, *labels):\n         self._saved_pcoders[self._path(*labels)])\n \n   def read(self, *labels):\n+    # Return an iterator to an empty list if it doesn't exist.\n     if not self.exists(*labels):\n-      return [], -1\n+      return [].__iter__(), -1\n \n-    source = self.source(*labels)\n+    # Otherwise, return a generator to the cached PCollection.\n+    source = self._source(*labels)\n     range_tracker = source.get_range_tracker(None, None)\n-    result = list(source.read(range_tracker))\n+    reader = source.read(range_tracker)\n     version = self._latest_version(*labels)\n-    return result, version\n+    return reader, version\n+\n+  def write(self, values, *labels):\n+    sink = self._sink(*labels)\n+    path = self._path(*labels)\n+    with open(path, 'wb') as f:\n+      for v in values:\n+        sink.write_record(f, v)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAwOTY3Mw==", "bodyText": "Cool, thank you. Changed to the above code.", "url": "https://github.com/apache/beam/pull/11005#discussion_r388009673", "createdAt": "2020-03-05T00:12:15Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/interactive/cache_manager.py", "diffHunk": "@@ -167,20 +177,35 @@ def load_pcoder(self, *labels):\n         self._saved_pcoders[self._path(*labels)])\n \n   def read(self, *labels):\n+    # Return an iterator to an empty list if it doesn't exist.\n     if not self.exists(*labels):\n-      return [], -1\n+      return [].__iter__(), -1\n \n-    source = self.source(*labels)\n+    # Otherwise, return a generator to the cached PCollection.\n+    source = self._source(*labels)\n     range_tracker = source.get_range_tracker(None, None)\n-    result = list(source.read(range_tracker))\n+    reader = source.read(range_tracker)\n     version = self._latest_version(*labels)\n-    return result, version\n+    return reader, version\n+\n+  def write(self, values, *labels):\n+    sink = self._sink(*labels)\n+    path = self._path(*labels)\n+    with open(path, 'wb') as f:\n+      for v in values:\n+        sink.write_record(f, v)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM0NTQ4MQ=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTc3MjE5OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwMDowMDowNVrOFxa_4w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQwMDoxMjoyMlrOFyCO9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2Njg4Mw==", "bodyText": "Why is it best-effort?", "url": "https://github.com/apache/beam/pull/11005#discussion_r387366883", "createdAt": "2020-03-04T00:00:05Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "diffHunk": "@@ -19,15 +19,298 @@\n \n from __future__ import absolute_import\n \n+import itertools\n+import os\n+import shutil\n+import tempfile\n+import time\n+\n+import apache_beam as beam\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileHeader\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileRecord\n from apache_beam.portability.api.beam_runner_api_pb2 import TestStreamPayload\n+from apache_beam.runners.interactive.cache_manager import CacheManager\n+from apache_beam.runners.interactive.cache_manager import SafeFastPrimitivesCoder\n+from apache_beam.testing.test_stream import ReverseTestStream\n from apache_beam.utils import timestamp\n \n \n-class StreamingCache(object):\n+class StreamingCacheSink(beam.PTransform):\n+  \"\"\"A PTransform that writes TestStreamFile(Header|Records)s to file.\n+\n+  This transform takes in an arbitrary element stream and writes the best-effort", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAwOTcxNw==", "bodyText": "Clarified in the comment that it is best-effort replay not best-effort writing. The original comment was confusing.", "url": "https://github.com/apache/beam/pull/11005#discussion_r388009717", "createdAt": "2020-03-05T00:12:22Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "diffHunk": "@@ -19,15 +19,298 @@\n \n from __future__ import absolute_import\n \n+import itertools\n+import os\n+import shutil\n+import tempfile\n+import time\n+\n+import apache_beam as beam\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileHeader\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileRecord\n from apache_beam.portability.api.beam_runner_api_pb2 import TestStreamPayload\n+from apache_beam.runners.interactive.cache_manager import CacheManager\n+from apache_beam.runners.interactive.cache_manager import SafeFastPrimitivesCoder\n+from apache_beam.testing.test_stream import ReverseTestStream\n from apache_beam.utils import timestamp\n \n \n-class StreamingCache(object):\n+class StreamingCacheSink(beam.PTransform):\n+  \"\"\"A PTransform that writes TestStreamFile(Header|Records)s to file.\n+\n+  This transform takes in an arbitrary element stream and writes the best-effort", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2Njg4Mw=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTc3NjI5OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwMDowMjowNFrOFxbCXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQwMDo0Mjo1M1rOFyCxPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2NzUxNw==", "bodyText": "Can you please file a JIRA to make this more general? (I don't see anything yet that would preclude writing something that works for all runners.)", "url": "https://github.com/apache/beam/pull/11005#discussion_r387367517", "createdAt": "2020-03-04T00:02:04Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "diffHunk": "@@ -19,15 +19,298 @@\n \n from __future__ import absolute_import\n \n+import itertools\n+import os\n+import shutil\n+import tempfile\n+import time\n+\n+import apache_beam as beam\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileHeader\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileRecord\n from apache_beam.portability.api.beam_runner_api_pb2 import TestStreamPayload\n+from apache_beam.runners.interactive.cache_manager import CacheManager\n+from apache_beam.runners.interactive.cache_manager import SafeFastPrimitivesCoder\n+from apache_beam.testing.test_stream import ReverseTestStream\n from apache_beam.utils import timestamp\n \n \n-class StreamingCache(object):\n+class StreamingCacheSink(beam.PTransform):\n+  \"\"\"A PTransform that writes TestStreamFile(Header|Records)s to file.\n+\n+  This transform takes in an arbitrary element stream and writes the best-effort\n+  list of TestStream events (as TestStreamFileRecords) to file.\n+\n+  Note that this PTransform is assumed to be only run on a single machine where\n+  the following assumptions are correct: elements come in ordered, no two\n+  transforms are writing to the same file. This PTransform is assumed to only\n+  run correctly with the DirectRunner.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAxODQ5Mg==", "bodyText": "Done, made BEAM-9447", "url": "https://github.com/apache/beam/pull/11005#discussion_r388018492", "createdAt": "2020-03-05T00:42:53Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "diffHunk": "@@ -19,15 +19,298 @@\n \n from __future__ import absolute_import\n \n+import itertools\n+import os\n+import shutil\n+import tempfile\n+import time\n+\n+import apache_beam as beam\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileHeader\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileRecord\n from apache_beam.portability.api.beam_runner_api_pb2 import TestStreamPayload\n+from apache_beam.runners.interactive.cache_manager import CacheManager\n+from apache_beam.runners.interactive.cache_manager import SafeFastPrimitivesCoder\n+from apache_beam.testing.test_stream import ReverseTestStream\n from apache_beam.utils import timestamp\n \n \n-class StreamingCache(object):\n+class StreamingCacheSink(beam.PTransform):\n+  \"\"\"A PTransform that writes TestStreamFile(Header|Records)s to file.\n+\n+  This transform takes in an arbitrary element stream and writes the best-effort\n+  list of TestStream events (as TestStreamFileRecords) to file.\n+\n+  Note that this PTransform is assumed to be only run on a single machine where\n+  the following assumptions are correct: elements come in ordered, no two\n+  transforms are writing to the same file. This PTransform is assumed to only\n+  run correctly with the DirectRunner.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2NzUxNw=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTc4MjMyOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwMDowNToyMlrOFxbGGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQwMDoxMjo1OVrOFyCPvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2ODQ3Mw==", "bodyText": "Just using os.path.exists would be cleaner.\nstart = time.time()\nwhile not os.path.exists(path):\n  time.sleep(1)\n  if time.time() - start > timeout_timestamp_secs:\n    raise RuntimeError(...)\nreturn open(path)", "url": "https://github.com/apache/beam/pull/11005#discussion_r387368473", "createdAt": "2020-03-04T00:05:22Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "diffHunk": "@@ -19,15 +19,298 @@\n \n from __future__ import absolute_import\n \n+import itertools\n+import os\n+import shutil\n+import tempfile\n+import time\n+\n+import apache_beam as beam\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileHeader\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileRecord\n from apache_beam.portability.api.beam_runner_api_pb2 import TestStreamPayload\n+from apache_beam.runners.interactive.cache_manager import CacheManager\n+from apache_beam.runners.interactive.cache_manager import SafeFastPrimitivesCoder\n+from apache_beam.testing.test_stream import ReverseTestStream\n from apache_beam.utils import timestamp\n \n \n-class StreamingCache(object):\n+class StreamingCacheSink(beam.PTransform):\n+  \"\"\"A PTransform that writes TestStreamFile(Header|Records)s to file.\n+\n+  This transform takes in an arbitrary element stream and writes the best-effort\n+  list of TestStream events (as TestStreamFileRecords) to file.\n+\n+  Note that this PTransform is assumed to be only run on a single machine where\n+  the following assumptions are correct: elements come in ordered, no two\n+  transforms are writing to the same file. This PTransform is assumed to only\n+  run correctly with the DirectRunner.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      filename,\n+      sample_resolution_sec,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._filename = filename\n+    self._sample_resolution_sec = sample_resolution_sec\n+    self._coder = coder\n+    self._path = os.path.join(self._cache_dir, self._filename)\n+\n+  @property\n+  def path(self):\n+    \"\"\"Returns the path the sink leads to.\"\"\"\n+    return self._path\n+\n+  def expand(self, pcoll):\n+    class StreamingWriteToText(beam.DoFn):\n+      \"\"\"DoFn that performs the writing.\n+\n+      Note that the other file writing methods cannot be used in streaming\n+      contexts.\n+      \"\"\"\n+      def __init__(self, full_path, coder=SafeFastPrimitivesCoder()):\n+        self._full_path = full_path\n+        self._coder = coder\n+\n+        # Try and make the given path.\n+        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n+\n+      def start_bundle(self):\n+        # Open the file for 'append-mode' and writing 'bytes'.\n+        self._fh = open(self._full_path, 'ab')\n+\n+      def finish_bundle(self):\n+        self._fh.close()\n+\n+      def process(self, e):\n+        \"\"\"Appends the given element to the file.\n+        \"\"\"\n+        self._fh.write(self._coder.encode(e))\n+        self._fh.write(b'\\n')\n+\n+    return (\n+        pcoll\n+        | ReverseTestStream(\n+            output_tag=self._filename,\n+            sample_resolution_sec=self._sample_resolution_sec,\n+            output_format=ReverseTestStream.Format.\n+            SERIALIZED_TEST_STREAM_FILE_RECORDS,\n+            coder=self._coder)\n+        | beam.ParDo(\n+            StreamingWriteToText(full_path=self._path, coder=self._coder)))\n+\n+\n+class StreamingCacheSource:\n+  \"\"\"A class that reads and parses TestStreamFile(Header|Reader)s.\n+\n+  This source operates in the following way:\n+\n+    1. Wait for up to `timeout_secs` for the file to be available.\n+    2. Read, parse, and emit the entire contents of the file\n+    3. Wait for more events to come or until `is_cache_complete` returns True\n+    4. If there are more events, then go to 2\n+    5. Otherwise, stop emitting.\n+\n+  This class is used to read from file and send its to the TestStream via the\n+  StreamingCacheManager.Reader.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      labels,\n+      is_cache_complete=None,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._coder = coder\n+    self._labels = labels\n+    self._is_cache_complete = (\n+        is_cache_complete if is_cache_complete else lambda: True)\n+\n+  def _wait_until_file_exists(self, timeout_secs=30):\n+    \"\"\"Blocks until the file exists for a maximum of timeout_secs.\n+    \"\"\"\n+    f = None\n+    now_secs = time.time()\n+    timeout_timestamp_secs = now_secs + timeout_secs\n+\n+    # Wait for up to `timeout_secs` for the file to be available.\n+    while f is None and now_secs < timeout_timestamp_secs:\n+      now_secs = time.time()\n+      try:\n+        path = os.path.join(self._cache_dir, *self._labels)\n+        f = open(path, mode='r')\n+      except EnvironmentError as e:\n+        # For Python2 and Python3 compatibility, this checks the\n+        # EnvironmentError to see if the file exists.\n+        # TODO: Change this to a FileNotFoundError when Python3 migration is\n+        # complete.\n+        import errno\n+        if e.errno != errno.ENOENT:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAwOTkxNg==", "bodyText": "Done", "url": "https://github.com/apache/beam/pull/11005#discussion_r388009916", "createdAt": "2020-03-05T00:12:59Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "diffHunk": "@@ -19,15 +19,298 @@\n \n from __future__ import absolute_import\n \n+import itertools\n+import os\n+import shutil\n+import tempfile\n+import time\n+\n+import apache_beam as beam\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileHeader\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileRecord\n from apache_beam.portability.api.beam_runner_api_pb2 import TestStreamPayload\n+from apache_beam.runners.interactive.cache_manager import CacheManager\n+from apache_beam.runners.interactive.cache_manager import SafeFastPrimitivesCoder\n+from apache_beam.testing.test_stream import ReverseTestStream\n from apache_beam.utils import timestamp\n \n \n-class StreamingCache(object):\n+class StreamingCacheSink(beam.PTransform):\n+  \"\"\"A PTransform that writes TestStreamFile(Header|Records)s to file.\n+\n+  This transform takes in an arbitrary element stream and writes the best-effort\n+  list of TestStream events (as TestStreamFileRecords) to file.\n+\n+  Note that this PTransform is assumed to be only run on a single machine where\n+  the following assumptions are correct: elements come in ordered, no two\n+  transforms are writing to the same file. This PTransform is assumed to only\n+  run correctly with the DirectRunner.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      filename,\n+      sample_resolution_sec,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._filename = filename\n+    self._sample_resolution_sec = sample_resolution_sec\n+    self._coder = coder\n+    self._path = os.path.join(self._cache_dir, self._filename)\n+\n+  @property\n+  def path(self):\n+    \"\"\"Returns the path the sink leads to.\"\"\"\n+    return self._path\n+\n+  def expand(self, pcoll):\n+    class StreamingWriteToText(beam.DoFn):\n+      \"\"\"DoFn that performs the writing.\n+\n+      Note that the other file writing methods cannot be used in streaming\n+      contexts.\n+      \"\"\"\n+      def __init__(self, full_path, coder=SafeFastPrimitivesCoder()):\n+        self._full_path = full_path\n+        self._coder = coder\n+\n+        # Try and make the given path.\n+        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n+\n+      def start_bundle(self):\n+        # Open the file for 'append-mode' and writing 'bytes'.\n+        self._fh = open(self._full_path, 'ab')\n+\n+      def finish_bundle(self):\n+        self._fh.close()\n+\n+      def process(self, e):\n+        \"\"\"Appends the given element to the file.\n+        \"\"\"\n+        self._fh.write(self._coder.encode(e))\n+        self._fh.write(b'\\n')\n+\n+    return (\n+        pcoll\n+        | ReverseTestStream(\n+            output_tag=self._filename,\n+            sample_resolution_sec=self._sample_resolution_sec,\n+            output_format=ReverseTestStream.Format.\n+            SERIALIZED_TEST_STREAM_FILE_RECORDS,\n+            coder=self._coder)\n+        | beam.ParDo(\n+            StreamingWriteToText(full_path=self._path, coder=self._coder)))\n+\n+\n+class StreamingCacheSource:\n+  \"\"\"A class that reads and parses TestStreamFile(Header|Reader)s.\n+\n+  This source operates in the following way:\n+\n+    1. Wait for up to `timeout_secs` for the file to be available.\n+    2. Read, parse, and emit the entire contents of the file\n+    3. Wait for more events to come or until `is_cache_complete` returns True\n+    4. If there are more events, then go to 2\n+    5. Otherwise, stop emitting.\n+\n+  This class is used to read from file and send its to the TestStream via the\n+  StreamingCacheManager.Reader.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      labels,\n+      is_cache_complete=None,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._coder = coder\n+    self._labels = labels\n+    self._is_cache_complete = (\n+        is_cache_complete if is_cache_complete else lambda: True)\n+\n+  def _wait_until_file_exists(self, timeout_secs=30):\n+    \"\"\"Blocks until the file exists for a maximum of timeout_secs.\n+    \"\"\"\n+    f = None\n+    now_secs = time.time()\n+    timeout_timestamp_secs = now_secs + timeout_secs\n+\n+    # Wait for up to `timeout_secs` for the file to be available.\n+    while f is None and now_secs < timeout_timestamp_secs:\n+      now_secs = time.time()\n+      try:\n+        path = os.path.join(self._cache_dir, *self._labels)\n+        f = open(path, mode='r')\n+      except EnvironmentError as e:\n+        # For Python2 and Python3 compatibility, this checks the\n+        # EnvironmentError to see if the file exists.\n+        # TODO: Change this to a FileNotFoundError when Python3 migration is\n+        # complete.\n+        import errno\n+        if e.errno != errno.ENOENT:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2ODQ3Mw=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 133}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTgwMjQ3OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "isResolved": false, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwMDoxNTo0NVrOFxbSKQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxNzo1Mjo0NVrOF1CVRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM3MTU2MQ==", "bodyText": "Is it possible for this to read part of a line? Perhaps the check should be whether it ends in a newline, otherwise you have to go back and try to read more.", "url": "https://github.com/apache/beam/pull/11005#discussion_r387371561", "createdAt": "2020-03-04T00:15:45Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "diffHunk": "@@ -19,15 +19,298 @@\n \n from __future__ import absolute_import\n \n+import itertools\n+import os\n+import shutil\n+import tempfile\n+import time\n+\n+import apache_beam as beam\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileHeader\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileRecord\n from apache_beam.portability.api.beam_runner_api_pb2 import TestStreamPayload\n+from apache_beam.runners.interactive.cache_manager import CacheManager\n+from apache_beam.runners.interactive.cache_manager import SafeFastPrimitivesCoder\n+from apache_beam.testing.test_stream import ReverseTestStream\n from apache_beam.utils import timestamp\n \n \n-class StreamingCache(object):\n+class StreamingCacheSink(beam.PTransform):\n+  \"\"\"A PTransform that writes TestStreamFile(Header|Records)s to file.\n+\n+  This transform takes in an arbitrary element stream and writes the best-effort\n+  list of TestStream events (as TestStreamFileRecords) to file.\n+\n+  Note that this PTransform is assumed to be only run on a single machine where\n+  the following assumptions are correct: elements come in ordered, no two\n+  transforms are writing to the same file. This PTransform is assumed to only\n+  run correctly with the DirectRunner.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      filename,\n+      sample_resolution_sec,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._filename = filename\n+    self._sample_resolution_sec = sample_resolution_sec\n+    self._coder = coder\n+    self._path = os.path.join(self._cache_dir, self._filename)\n+\n+  @property\n+  def path(self):\n+    \"\"\"Returns the path the sink leads to.\"\"\"\n+    return self._path\n+\n+  def expand(self, pcoll):\n+    class StreamingWriteToText(beam.DoFn):\n+      \"\"\"DoFn that performs the writing.\n+\n+      Note that the other file writing methods cannot be used in streaming\n+      contexts.\n+      \"\"\"\n+      def __init__(self, full_path, coder=SafeFastPrimitivesCoder()):\n+        self._full_path = full_path\n+        self._coder = coder\n+\n+        # Try and make the given path.\n+        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n+\n+      def start_bundle(self):\n+        # Open the file for 'append-mode' and writing 'bytes'.\n+        self._fh = open(self._full_path, 'ab')\n+\n+      def finish_bundle(self):\n+        self._fh.close()\n+\n+      def process(self, e):\n+        \"\"\"Appends the given element to the file.\n+        \"\"\"\n+        self._fh.write(self._coder.encode(e))\n+        self._fh.write(b'\\n')\n+\n+    return (\n+        pcoll\n+        | ReverseTestStream(\n+            output_tag=self._filename,\n+            sample_resolution_sec=self._sample_resolution_sec,\n+            output_format=ReverseTestStream.Format.\n+            SERIALIZED_TEST_STREAM_FILE_RECORDS,\n+            coder=self._coder)\n+        | beam.ParDo(\n+            StreamingWriteToText(full_path=self._path, coder=self._coder)))\n+\n+\n+class StreamingCacheSource:\n+  \"\"\"A class that reads and parses TestStreamFile(Header|Reader)s.\n+\n+  This source operates in the following way:\n+\n+    1. Wait for up to `timeout_secs` for the file to be available.\n+    2. Read, parse, and emit the entire contents of the file\n+    3. Wait for more events to come or until `is_cache_complete` returns True\n+    4. If there are more events, then go to 2\n+    5. Otherwise, stop emitting.\n+\n+  This class is used to read from file and send its to the TestStream via the\n+  StreamingCacheManager.Reader.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      labels,\n+      is_cache_complete=None,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._coder = coder\n+    self._labels = labels\n+    self._is_cache_complete = (\n+        is_cache_complete if is_cache_complete else lambda: True)\n+\n+  def _wait_until_file_exists(self, timeout_secs=30):\n+    \"\"\"Blocks until the file exists for a maximum of timeout_secs.\n+    \"\"\"\n+    f = None\n+    now_secs = time.time()\n+    timeout_timestamp_secs = now_secs + timeout_secs\n+\n+    # Wait for up to `timeout_secs` for the file to be available.\n+    while f is None and now_secs < timeout_timestamp_secs:\n+      now_secs = time.time()\n+      try:\n+        path = os.path.join(self._cache_dir, *self._labels)\n+        f = open(path, mode='r')\n+      except EnvironmentError as e:\n+        # For Python2 and Python3 compatibility, this checks the\n+        # EnvironmentError to see if the file exists.\n+        # TODO: Change this to a FileNotFoundError when Python3 migration is\n+        # complete.\n+        import errno\n+        if e.errno != errno.ENOENT:\n+          # Raise the exception if it is not a FileNotFoundError.\n+          raise\n+        time.sleep(1)\n+    if now_secs >= timeout_timestamp_secs:\n+      raise RuntimeError(\n+          \"Timed out waiting for file '{}' to be available\".format(path))\n+    return f\n+\n+  def _emit_from_file(self, fh, tail):\n+    \"\"\"Emits the TestStreamFile(Header|Record)s from file.\n+\n+    This returns a generator to be able to read all lines from the given file.\n+    If `tail` is True, then it will wait until the cache is complete to exit.\n+    Otherwise, it will read the file only once.\n+    \"\"\"\n+    # Always read at least once to read the whole file.\n+    while True:\n+      pos = fh.tell()\n+      line = fh.readline()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAxODgxMg==", "bodyText": "The Python API for the readline() only allows for part of a line to be read if the optional parameter size is given and is non-negative. So this is okay and will read the whole line.", "url": "https://github.com/apache/beam/pull/11005#discussion_r388018812", "createdAt": "2020-03-05T00:43:57Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "diffHunk": "@@ -19,15 +19,298 @@\n \n from __future__ import absolute_import\n \n+import itertools\n+import os\n+import shutil\n+import tempfile\n+import time\n+\n+import apache_beam as beam\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileHeader\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileRecord\n from apache_beam.portability.api.beam_runner_api_pb2 import TestStreamPayload\n+from apache_beam.runners.interactive.cache_manager import CacheManager\n+from apache_beam.runners.interactive.cache_manager import SafeFastPrimitivesCoder\n+from apache_beam.testing.test_stream import ReverseTestStream\n from apache_beam.utils import timestamp\n \n \n-class StreamingCache(object):\n+class StreamingCacheSink(beam.PTransform):\n+  \"\"\"A PTransform that writes TestStreamFile(Header|Records)s to file.\n+\n+  This transform takes in an arbitrary element stream and writes the best-effort\n+  list of TestStream events (as TestStreamFileRecords) to file.\n+\n+  Note that this PTransform is assumed to be only run on a single machine where\n+  the following assumptions are correct: elements come in ordered, no two\n+  transforms are writing to the same file. This PTransform is assumed to only\n+  run correctly with the DirectRunner.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      filename,\n+      sample_resolution_sec,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._filename = filename\n+    self._sample_resolution_sec = sample_resolution_sec\n+    self._coder = coder\n+    self._path = os.path.join(self._cache_dir, self._filename)\n+\n+  @property\n+  def path(self):\n+    \"\"\"Returns the path the sink leads to.\"\"\"\n+    return self._path\n+\n+  def expand(self, pcoll):\n+    class StreamingWriteToText(beam.DoFn):\n+      \"\"\"DoFn that performs the writing.\n+\n+      Note that the other file writing methods cannot be used in streaming\n+      contexts.\n+      \"\"\"\n+      def __init__(self, full_path, coder=SafeFastPrimitivesCoder()):\n+        self._full_path = full_path\n+        self._coder = coder\n+\n+        # Try and make the given path.\n+        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n+\n+      def start_bundle(self):\n+        # Open the file for 'append-mode' and writing 'bytes'.\n+        self._fh = open(self._full_path, 'ab')\n+\n+      def finish_bundle(self):\n+        self._fh.close()\n+\n+      def process(self, e):\n+        \"\"\"Appends the given element to the file.\n+        \"\"\"\n+        self._fh.write(self._coder.encode(e))\n+        self._fh.write(b'\\n')\n+\n+    return (\n+        pcoll\n+        | ReverseTestStream(\n+            output_tag=self._filename,\n+            sample_resolution_sec=self._sample_resolution_sec,\n+            output_format=ReverseTestStream.Format.\n+            SERIALIZED_TEST_STREAM_FILE_RECORDS,\n+            coder=self._coder)\n+        | beam.ParDo(\n+            StreamingWriteToText(full_path=self._path, coder=self._coder)))\n+\n+\n+class StreamingCacheSource:\n+  \"\"\"A class that reads and parses TestStreamFile(Header|Reader)s.\n+\n+  This source operates in the following way:\n+\n+    1. Wait for up to `timeout_secs` for the file to be available.\n+    2. Read, parse, and emit the entire contents of the file\n+    3. Wait for more events to come or until `is_cache_complete` returns True\n+    4. If there are more events, then go to 2\n+    5. Otherwise, stop emitting.\n+\n+  This class is used to read from file and send its to the TestStream via the\n+  StreamingCacheManager.Reader.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      labels,\n+      is_cache_complete=None,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._coder = coder\n+    self._labels = labels\n+    self._is_cache_complete = (\n+        is_cache_complete if is_cache_complete else lambda: True)\n+\n+  def _wait_until_file_exists(self, timeout_secs=30):\n+    \"\"\"Blocks until the file exists for a maximum of timeout_secs.\n+    \"\"\"\n+    f = None\n+    now_secs = time.time()\n+    timeout_timestamp_secs = now_secs + timeout_secs\n+\n+    # Wait for up to `timeout_secs` for the file to be available.\n+    while f is None and now_secs < timeout_timestamp_secs:\n+      now_secs = time.time()\n+      try:\n+        path = os.path.join(self._cache_dir, *self._labels)\n+        f = open(path, mode='r')\n+      except EnvironmentError as e:\n+        # For Python2 and Python3 compatibility, this checks the\n+        # EnvironmentError to see if the file exists.\n+        # TODO: Change this to a FileNotFoundError when Python3 migration is\n+        # complete.\n+        import errno\n+        if e.errno != errno.ENOENT:\n+          # Raise the exception if it is not a FileNotFoundError.\n+          raise\n+        time.sleep(1)\n+    if now_secs >= timeout_timestamp_secs:\n+      raise RuntimeError(\n+          \"Timed out waiting for file '{}' to be available\".format(path))\n+    return f\n+\n+  def _emit_from_file(self, fh, tail):\n+    \"\"\"Emits the TestStreamFile(Header|Record)s from file.\n+\n+    This returns a generator to be able to read all lines from the given file.\n+    If `tail` is True, then it will wait until the cache is complete to exit.\n+    Otherwise, it will read the file only once.\n+    \"\"\"\n+    # Always read at least once to read the whole file.\n+    while True:\n+      pos = fh.tell()\n+      line = fh.readline()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM3MTU2MQ=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyNjQ1Mg==", "bodyText": "This only holds if the file is not being written to concurrently (just tried it out). Otherwise it may get to EOF half way through a line.", "url": "https://github.com/apache/beam/pull/11005#discussion_r389126452", "createdAt": "2020-03-06T20:25:01Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "diffHunk": "@@ -19,15 +19,298 @@\n \n from __future__ import absolute_import\n \n+import itertools\n+import os\n+import shutil\n+import tempfile\n+import time\n+\n+import apache_beam as beam\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileHeader\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileRecord\n from apache_beam.portability.api.beam_runner_api_pb2 import TestStreamPayload\n+from apache_beam.runners.interactive.cache_manager import CacheManager\n+from apache_beam.runners.interactive.cache_manager import SafeFastPrimitivesCoder\n+from apache_beam.testing.test_stream import ReverseTestStream\n from apache_beam.utils import timestamp\n \n \n-class StreamingCache(object):\n+class StreamingCacheSink(beam.PTransform):\n+  \"\"\"A PTransform that writes TestStreamFile(Header|Records)s to file.\n+\n+  This transform takes in an arbitrary element stream and writes the best-effort\n+  list of TestStream events (as TestStreamFileRecords) to file.\n+\n+  Note that this PTransform is assumed to be only run on a single machine where\n+  the following assumptions are correct: elements come in ordered, no two\n+  transforms are writing to the same file. This PTransform is assumed to only\n+  run correctly with the DirectRunner.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      filename,\n+      sample_resolution_sec,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._filename = filename\n+    self._sample_resolution_sec = sample_resolution_sec\n+    self._coder = coder\n+    self._path = os.path.join(self._cache_dir, self._filename)\n+\n+  @property\n+  def path(self):\n+    \"\"\"Returns the path the sink leads to.\"\"\"\n+    return self._path\n+\n+  def expand(self, pcoll):\n+    class StreamingWriteToText(beam.DoFn):\n+      \"\"\"DoFn that performs the writing.\n+\n+      Note that the other file writing methods cannot be used in streaming\n+      contexts.\n+      \"\"\"\n+      def __init__(self, full_path, coder=SafeFastPrimitivesCoder()):\n+        self._full_path = full_path\n+        self._coder = coder\n+\n+        # Try and make the given path.\n+        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n+\n+      def start_bundle(self):\n+        # Open the file for 'append-mode' and writing 'bytes'.\n+        self._fh = open(self._full_path, 'ab')\n+\n+      def finish_bundle(self):\n+        self._fh.close()\n+\n+      def process(self, e):\n+        \"\"\"Appends the given element to the file.\n+        \"\"\"\n+        self._fh.write(self._coder.encode(e))\n+        self._fh.write(b'\\n')\n+\n+    return (\n+        pcoll\n+        | ReverseTestStream(\n+            output_tag=self._filename,\n+            sample_resolution_sec=self._sample_resolution_sec,\n+            output_format=ReverseTestStream.Format.\n+            SERIALIZED_TEST_STREAM_FILE_RECORDS,\n+            coder=self._coder)\n+        | beam.ParDo(\n+            StreamingWriteToText(full_path=self._path, coder=self._coder)))\n+\n+\n+class StreamingCacheSource:\n+  \"\"\"A class that reads and parses TestStreamFile(Header|Reader)s.\n+\n+  This source operates in the following way:\n+\n+    1. Wait for up to `timeout_secs` for the file to be available.\n+    2. Read, parse, and emit the entire contents of the file\n+    3. Wait for more events to come or until `is_cache_complete` returns True\n+    4. If there are more events, then go to 2\n+    5. Otherwise, stop emitting.\n+\n+  This class is used to read from file and send its to the TestStream via the\n+  StreamingCacheManager.Reader.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      labels,\n+      is_cache_complete=None,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._coder = coder\n+    self._labels = labels\n+    self._is_cache_complete = (\n+        is_cache_complete if is_cache_complete else lambda: True)\n+\n+  def _wait_until_file_exists(self, timeout_secs=30):\n+    \"\"\"Blocks until the file exists for a maximum of timeout_secs.\n+    \"\"\"\n+    f = None\n+    now_secs = time.time()\n+    timeout_timestamp_secs = now_secs + timeout_secs\n+\n+    # Wait for up to `timeout_secs` for the file to be available.\n+    while f is None and now_secs < timeout_timestamp_secs:\n+      now_secs = time.time()\n+      try:\n+        path = os.path.join(self._cache_dir, *self._labels)\n+        f = open(path, mode='r')\n+      except EnvironmentError as e:\n+        # For Python2 and Python3 compatibility, this checks the\n+        # EnvironmentError to see if the file exists.\n+        # TODO: Change this to a FileNotFoundError when Python3 migration is\n+        # complete.\n+        import errno\n+        if e.errno != errno.ENOENT:\n+          # Raise the exception if it is not a FileNotFoundError.\n+          raise\n+        time.sleep(1)\n+    if now_secs >= timeout_timestamp_secs:\n+      raise RuntimeError(\n+          \"Timed out waiting for file '{}' to be available\".format(path))\n+    return f\n+\n+  def _emit_from_file(self, fh, tail):\n+    \"\"\"Emits the TestStreamFile(Header|Record)s from file.\n+\n+    This returns a generator to be able to read all lines from the given file.\n+    If `tail` is True, then it will wait until the cache is complete to exit.\n+    Otherwise, it will read the file only once.\n+    \"\"\"\n+    # Always read at least once to read the whole file.\n+    while True:\n+      pos = fh.tell()\n+      line = fh.readline()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM3MTU2MQ=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTIxMDY1OA==", "bodyText": "This was not fixed.", "url": "https://github.com/apache/beam/pull/11005#discussion_r389210658", "createdAt": "2020-03-07T01:07:27Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "diffHunk": "@@ -19,15 +19,298 @@\n \n from __future__ import absolute_import\n \n+import itertools\n+import os\n+import shutil\n+import tempfile\n+import time\n+\n+import apache_beam as beam\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileHeader\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileRecord\n from apache_beam.portability.api.beam_runner_api_pb2 import TestStreamPayload\n+from apache_beam.runners.interactive.cache_manager import CacheManager\n+from apache_beam.runners.interactive.cache_manager import SafeFastPrimitivesCoder\n+from apache_beam.testing.test_stream import ReverseTestStream\n from apache_beam.utils import timestamp\n \n \n-class StreamingCache(object):\n+class StreamingCacheSink(beam.PTransform):\n+  \"\"\"A PTransform that writes TestStreamFile(Header|Records)s to file.\n+\n+  This transform takes in an arbitrary element stream and writes the best-effort\n+  list of TestStream events (as TestStreamFileRecords) to file.\n+\n+  Note that this PTransform is assumed to be only run on a single machine where\n+  the following assumptions are correct: elements come in ordered, no two\n+  transforms are writing to the same file. This PTransform is assumed to only\n+  run correctly with the DirectRunner.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      filename,\n+      sample_resolution_sec,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._filename = filename\n+    self._sample_resolution_sec = sample_resolution_sec\n+    self._coder = coder\n+    self._path = os.path.join(self._cache_dir, self._filename)\n+\n+  @property\n+  def path(self):\n+    \"\"\"Returns the path the sink leads to.\"\"\"\n+    return self._path\n+\n+  def expand(self, pcoll):\n+    class StreamingWriteToText(beam.DoFn):\n+      \"\"\"DoFn that performs the writing.\n+\n+      Note that the other file writing methods cannot be used in streaming\n+      contexts.\n+      \"\"\"\n+      def __init__(self, full_path, coder=SafeFastPrimitivesCoder()):\n+        self._full_path = full_path\n+        self._coder = coder\n+\n+        # Try and make the given path.\n+        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n+\n+      def start_bundle(self):\n+        # Open the file for 'append-mode' and writing 'bytes'.\n+        self._fh = open(self._full_path, 'ab')\n+\n+      def finish_bundle(self):\n+        self._fh.close()\n+\n+      def process(self, e):\n+        \"\"\"Appends the given element to the file.\n+        \"\"\"\n+        self._fh.write(self._coder.encode(e))\n+        self._fh.write(b'\\n')\n+\n+    return (\n+        pcoll\n+        | ReverseTestStream(\n+            output_tag=self._filename,\n+            sample_resolution_sec=self._sample_resolution_sec,\n+            output_format=ReverseTestStream.Format.\n+            SERIALIZED_TEST_STREAM_FILE_RECORDS,\n+            coder=self._coder)\n+        | beam.ParDo(\n+            StreamingWriteToText(full_path=self._path, coder=self._coder)))\n+\n+\n+class StreamingCacheSource:\n+  \"\"\"A class that reads and parses TestStreamFile(Header|Reader)s.\n+\n+  This source operates in the following way:\n+\n+    1. Wait for up to `timeout_secs` for the file to be available.\n+    2. Read, parse, and emit the entire contents of the file\n+    3. Wait for more events to come or until `is_cache_complete` returns True\n+    4. If there are more events, then go to 2\n+    5. Otherwise, stop emitting.\n+\n+  This class is used to read from file and send its to the TestStream via the\n+  StreamingCacheManager.Reader.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      labels,\n+      is_cache_complete=None,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._coder = coder\n+    self._labels = labels\n+    self._is_cache_complete = (\n+        is_cache_complete if is_cache_complete else lambda: True)\n+\n+  def _wait_until_file_exists(self, timeout_secs=30):\n+    \"\"\"Blocks until the file exists for a maximum of timeout_secs.\n+    \"\"\"\n+    f = None\n+    now_secs = time.time()\n+    timeout_timestamp_secs = now_secs + timeout_secs\n+\n+    # Wait for up to `timeout_secs` for the file to be available.\n+    while f is None and now_secs < timeout_timestamp_secs:\n+      now_secs = time.time()\n+      try:\n+        path = os.path.join(self._cache_dir, *self._labels)\n+        f = open(path, mode='r')\n+      except EnvironmentError as e:\n+        # For Python2 and Python3 compatibility, this checks the\n+        # EnvironmentError to see if the file exists.\n+        # TODO: Change this to a FileNotFoundError when Python3 migration is\n+        # complete.\n+        import errno\n+        if e.errno != errno.ENOENT:\n+          # Raise the exception if it is not a FileNotFoundError.\n+          raise\n+        time.sleep(1)\n+    if now_secs >= timeout_timestamp_secs:\n+      raise RuntimeError(\n+          \"Timed out waiting for file '{}' to be available\".format(path))\n+    return f\n+\n+  def _emit_from_file(self, fh, tail):\n+    \"\"\"Emits the TestStreamFile(Header|Record)s from file.\n+\n+    This returns a generator to be able to read all lines from the given file.\n+    If `tail` is True, then it will wait until the cache is complete to exit.\n+    Otherwise, it will read the file only once.\n+    \"\"\"\n+    # Always read at least once to read the whole file.\n+    while True:\n+      pos = fh.tell()\n+      line = fh.readline()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM3MTU2MQ=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDAwODU0NA==", "bodyText": "I think this is okay though. It's a bug if the file is being concurrently written to.", "url": "https://github.com/apache/beam/pull/11005#discussion_r390008544", "createdAt": "2020-03-09T23:04:42Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "diffHunk": "@@ -19,15 +19,298 @@\n \n from __future__ import absolute_import\n \n+import itertools\n+import os\n+import shutil\n+import tempfile\n+import time\n+\n+import apache_beam as beam\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileHeader\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileRecord\n from apache_beam.portability.api.beam_runner_api_pb2 import TestStreamPayload\n+from apache_beam.runners.interactive.cache_manager import CacheManager\n+from apache_beam.runners.interactive.cache_manager import SafeFastPrimitivesCoder\n+from apache_beam.testing.test_stream import ReverseTestStream\n from apache_beam.utils import timestamp\n \n \n-class StreamingCache(object):\n+class StreamingCacheSink(beam.PTransform):\n+  \"\"\"A PTransform that writes TestStreamFile(Header|Records)s to file.\n+\n+  This transform takes in an arbitrary element stream and writes the best-effort\n+  list of TestStream events (as TestStreamFileRecords) to file.\n+\n+  Note that this PTransform is assumed to be only run on a single machine where\n+  the following assumptions are correct: elements come in ordered, no two\n+  transforms are writing to the same file. This PTransform is assumed to only\n+  run correctly with the DirectRunner.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      filename,\n+      sample_resolution_sec,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._filename = filename\n+    self._sample_resolution_sec = sample_resolution_sec\n+    self._coder = coder\n+    self._path = os.path.join(self._cache_dir, self._filename)\n+\n+  @property\n+  def path(self):\n+    \"\"\"Returns the path the sink leads to.\"\"\"\n+    return self._path\n+\n+  def expand(self, pcoll):\n+    class StreamingWriteToText(beam.DoFn):\n+      \"\"\"DoFn that performs the writing.\n+\n+      Note that the other file writing methods cannot be used in streaming\n+      contexts.\n+      \"\"\"\n+      def __init__(self, full_path, coder=SafeFastPrimitivesCoder()):\n+        self._full_path = full_path\n+        self._coder = coder\n+\n+        # Try and make the given path.\n+        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n+\n+      def start_bundle(self):\n+        # Open the file for 'append-mode' and writing 'bytes'.\n+        self._fh = open(self._full_path, 'ab')\n+\n+      def finish_bundle(self):\n+        self._fh.close()\n+\n+      def process(self, e):\n+        \"\"\"Appends the given element to the file.\n+        \"\"\"\n+        self._fh.write(self._coder.encode(e))\n+        self._fh.write(b'\\n')\n+\n+    return (\n+        pcoll\n+        | ReverseTestStream(\n+            output_tag=self._filename,\n+            sample_resolution_sec=self._sample_resolution_sec,\n+            output_format=ReverseTestStream.Format.\n+            SERIALIZED_TEST_STREAM_FILE_RECORDS,\n+            coder=self._coder)\n+        | beam.ParDo(\n+            StreamingWriteToText(full_path=self._path, coder=self._coder)))\n+\n+\n+class StreamingCacheSource:\n+  \"\"\"A class that reads and parses TestStreamFile(Header|Reader)s.\n+\n+  This source operates in the following way:\n+\n+    1. Wait for up to `timeout_secs` for the file to be available.\n+    2. Read, parse, and emit the entire contents of the file\n+    3. Wait for more events to come or until `is_cache_complete` returns True\n+    4. If there are more events, then go to 2\n+    5. Otherwise, stop emitting.\n+\n+  This class is used to read from file and send its to the TestStream via the\n+  StreamingCacheManager.Reader.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      labels,\n+      is_cache_complete=None,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._coder = coder\n+    self._labels = labels\n+    self._is_cache_complete = (\n+        is_cache_complete if is_cache_complete else lambda: True)\n+\n+  def _wait_until_file_exists(self, timeout_secs=30):\n+    \"\"\"Blocks until the file exists for a maximum of timeout_secs.\n+    \"\"\"\n+    f = None\n+    now_secs = time.time()\n+    timeout_timestamp_secs = now_secs + timeout_secs\n+\n+    # Wait for up to `timeout_secs` for the file to be available.\n+    while f is None and now_secs < timeout_timestamp_secs:\n+      now_secs = time.time()\n+      try:\n+        path = os.path.join(self._cache_dir, *self._labels)\n+        f = open(path, mode='r')\n+      except EnvironmentError as e:\n+        # For Python2 and Python3 compatibility, this checks the\n+        # EnvironmentError to see if the file exists.\n+        # TODO: Change this to a FileNotFoundError when Python3 migration is\n+        # complete.\n+        import errno\n+        if e.errno != errno.ENOENT:\n+          # Raise the exception if it is not a FileNotFoundError.\n+          raise\n+        time.sleep(1)\n+    if now_secs >= timeout_timestamp_secs:\n+      raise RuntimeError(\n+          \"Timed out waiting for file '{}' to be available\".format(path))\n+    return f\n+\n+  def _emit_from_file(self, fh, tail):\n+    \"\"\"Emits the TestStreamFile(Header|Record)s from file.\n+\n+    This returns a generator to be able to read all lines from the given file.\n+    If `tail` is True, then it will wait until the cache is complete to exit.\n+    Otherwise, it will read the file only once.\n+    \"\"\"\n+    # Always read at least once to read the whole file.\n+    while True:\n+      pos = fh.tell()\n+      line = fh.readline()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM3MTU2MQ=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTE1Njk2MA==", "bodyText": "We've discussed this. Sam will add the fix by re-reading from the initial position.", "url": "https://github.com/apache/beam/pull/11005#discussion_r391156960", "createdAt": "2020-03-11T17:52:35Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "diffHunk": "@@ -19,15 +19,298 @@\n \n from __future__ import absolute_import\n \n+import itertools\n+import os\n+import shutil\n+import tempfile\n+import time\n+\n+import apache_beam as beam\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileHeader\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileRecord\n from apache_beam.portability.api.beam_runner_api_pb2 import TestStreamPayload\n+from apache_beam.runners.interactive.cache_manager import CacheManager\n+from apache_beam.runners.interactive.cache_manager import SafeFastPrimitivesCoder\n+from apache_beam.testing.test_stream import ReverseTestStream\n from apache_beam.utils import timestamp\n \n \n-class StreamingCache(object):\n+class StreamingCacheSink(beam.PTransform):\n+  \"\"\"A PTransform that writes TestStreamFile(Header|Records)s to file.\n+\n+  This transform takes in an arbitrary element stream and writes the best-effort\n+  list of TestStream events (as TestStreamFileRecords) to file.\n+\n+  Note that this PTransform is assumed to be only run on a single machine where\n+  the following assumptions are correct: elements come in ordered, no two\n+  transforms are writing to the same file. This PTransform is assumed to only\n+  run correctly with the DirectRunner.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      filename,\n+      sample_resolution_sec,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._filename = filename\n+    self._sample_resolution_sec = sample_resolution_sec\n+    self._coder = coder\n+    self._path = os.path.join(self._cache_dir, self._filename)\n+\n+  @property\n+  def path(self):\n+    \"\"\"Returns the path the sink leads to.\"\"\"\n+    return self._path\n+\n+  def expand(self, pcoll):\n+    class StreamingWriteToText(beam.DoFn):\n+      \"\"\"DoFn that performs the writing.\n+\n+      Note that the other file writing methods cannot be used in streaming\n+      contexts.\n+      \"\"\"\n+      def __init__(self, full_path, coder=SafeFastPrimitivesCoder()):\n+        self._full_path = full_path\n+        self._coder = coder\n+\n+        # Try and make the given path.\n+        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n+\n+      def start_bundle(self):\n+        # Open the file for 'append-mode' and writing 'bytes'.\n+        self._fh = open(self._full_path, 'ab')\n+\n+      def finish_bundle(self):\n+        self._fh.close()\n+\n+      def process(self, e):\n+        \"\"\"Appends the given element to the file.\n+        \"\"\"\n+        self._fh.write(self._coder.encode(e))\n+        self._fh.write(b'\\n')\n+\n+    return (\n+        pcoll\n+        | ReverseTestStream(\n+            output_tag=self._filename,\n+            sample_resolution_sec=self._sample_resolution_sec,\n+            output_format=ReverseTestStream.Format.\n+            SERIALIZED_TEST_STREAM_FILE_RECORDS,\n+            coder=self._coder)\n+        | beam.ParDo(\n+            StreamingWriteToText(full_path=self._path, coder=self._coder)))\n+\n+\n+class StreamingCacheSource:\n+  \"\"\"A class that reads and parses TestStreamFile(Header|Reader)s.\n+\n+  This source operates in the following way:\n+\n+    1. Wait for up to `timeout_secs` for the file to be available.\n+    2. Read, parse, and emit the entire contents of the file\n+    3. Wait for more events to come or until `is_cache_complete` returns True\n+    4. If there are more events, then go to 2\n+    5. Otherwise, stop emitting.\n+\n+  This class is used to read from file and send its to the TestStream via the\n+  StreamingCacheManager.Reader.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      labels,\n+      is_cache_complete=None,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._coder = coder\n+    self._labels = labels\n+    self._is_cache_complete = (\n+        is_cache_complete if is_cache_complete else lambda: True)\n+\n+  def _wait_until_file_exists(self, timeout_secs=30):\n+    \"\"\"Blocks until the file exists for a maximum of timeout_secs.\n+    \"\"\"\n+    f = None\n+    now_secs = time.time()\n+    timeout_timestamp_secs = now_secs + timeout_secs\n+\n+    # Wait for up to `timeout_secs` for the file to be available.\n+    while f is None and now_secs < timeout_timestamp_secs:\n+      now_secs = time.time()\n+      try:\n+        path = os.path.join(self._cache_dir, *self._labels)\n+        f = open(path, mode='r')\n+      except EnvironmentError as e:\n+        # For Python2 and Python3 compatibility, this checks the\n+        # EnvironmentError to see if the file exists.\n+        # TODO: Change this to a FileNotFoundError when Python3 migration is\n+        # complete.\n+        import errno\n+        if e.errno != errno.ENOENT:\n+          # Raise the exception if it is not a FileNotFoundError.\n+          raise\n+        time.sleep(1)\n+    if now_secs >= timeout_timestamp_secs:\n+      raise RuntimeError(\n+          \"Timed out waiting for file '{}' to be available\".format(path))\n+    return f\n+\n+  def _emit_from_file(self, fh, tail):\n+    \"\"\"Emits the TestStreamFile(Header|Record)s from file.\n+\n+    This returns a generator to be able to read all lines from the given file.\n+    If `tail` is True, then it will wait until the cache is complete to exit.\n+    Otherwise, it will read the file only once.\n+    \"\"\"\n+    # Always read at least once to read the whole file.\n+    while True:\n+      pos = fh.tell()\n+      line = fh.readline()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM3MTU2MQ=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTE1NzA2Mg==", "bodyText": "Ah sorry, I misunderstood. I thought you were referring to multiple concurrent writers. I'll change the code to take this into account.", "url": "https://github.com/apache/beam/pull/11005#discussion_r391157062", "createdAt": "2020-03-11T17:52:45Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "diffHunk": "@@ -19,15 +19,298 @@\n \n from __future__ import absolute_import\n \n+import itertools\n+import os\n+import shutil\n+import tempfile\n+import time\n+\n+import apache_beam as beam\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileHeader\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileRecord\n from apache_beam.portability.api.beam_runner_api_pb2 import TestStreamPayload\n+from apache_beam.runners.interactive.cache_manager import CacheManager\n+from apache_beam.runners.interactive.cache_manager import SafeFastPrimitivesCoder\n+from apache_beam.testing.test_stream import ReverseTestStream\n from apache_beam.utils import timestamp\n \n \n-class StreamingCache(object):\n+class StreamingCacheSink(beam.PTransform):\n+  \"\"\"A PTransform that writes TestStreamFile(Header|Records)s to file.\n+\n+  This transform takes in an arbitrary element stream and writes the best-effort\n+  list of TestStream events (as TestStreamFileRecords) to file.\n+\n+  Note that this PTransform is assumed to be only run on a single machine where\n+  the following assumptions are correct: elements come in ordered, no two\n+  transforms are writing to the same file. This PTransform is assumed to only\n+  run correctly with the DirectRunner.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      filename,\n+      sample_resolution_sec,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._filename = filename\n+    self._sample_resolution_sec = sample_resolution_sec\n+    self._coder = coder\n+    self._path = os.path.join(self._cache_dir, self._filename)\n+\n+  @property\n+  def path(self):\n+    \"\"\"Returns the path the sink leads to.\"\"\"\n+    return self._path\n+\n+  def expand(self, pcoll):\n+    class StreamingWriteToText(beam.DoFn):\n+      \"\"\"DoFn that performs the writing.\n+\n+      Note that the other file writing methods cannot be used in streaming\n+      contexts.\n+      \"\"\"\n+      def __init__(self, full_path, coder=SafeFastPrimitivesCoder()):\n+        self._full_path = full_path\n+        self._coder = coder\n+\n+        # Try and make the given path.\n+        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n+\n+      def start_bundle(self):\n+        # Open the file for 'append-mode' and writing 'bytes'.\n+        self._fh = open(self._full_path, 'ab')\n+\n+      def finish_bundle(self):\n+        self._fh.close()\n+\n+      def process(self, e):\n+        \"\"\"Appends the given element to the file.\n+        \"\"\"\n+        self._fh.write(self._coder.encode(e))\n+        self._fh.write(b'\\n')\n+\n+    return (\n+        pcoll\n+        | ReverseTestStream(\n+            output_tag=self._filename,\n+            sample_resolution_sec=self._sample_resolution_sec,\n+            output_format=ReverseTestStream.Format.\n+            SERIALIZED_TEST_STREAM_FILE_RECORDS,\n+            coder=self._coder)\n+        | beam.ParDo(\n+            StreamingWriteToText(full_path=self._path, coder=self._coder)))\n+\n+\n+class StreamingCacheSource:\n+  \"\"\"A class that reads and parses TestStreamFile(Header|Reader)s.\n+\n+  This source operates in the following way:\n+\n+    1. Wait for up to `timeout_secs` for the file to be available.\n+    2. Read, parse, and emit the entire contents of the file\n+    3. Wait for more events to come or until `is_cache_complete` returns True\n+    4. If there are more events, then go to 2\n+    5. Otherwise, stop emitting.\n+\n+  This class is used to read from file and send its to the TestStream via the\n+  StreamingCacheManager.Reader.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      labels,\n+      is_cache_complete=None,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._coder = coder\n+    self._labels = labels\n+    self._is_cache_complete = (\n+        is_cache_complete if is_cache_complete else lambda: True)\n+\n+  def _wait_until_file_exists(self, timeout_secs=30):\n+    \"\"\"Blocks until the file exists for a maximum of timeout_secs.\n+    \"\"\"\n+    f = None\n+    now_secs = time.time()\n+    timeout_timestamp_secs = now_secs + timeout_secs\n+\n+    # Wait for up to `timeout_secs` for the file to be available.\n+    while f is None and now_secs < timeout_timestamp_secs:\n+      now_secs = time.time()\n+      try:\n+        path = os.path.join(self._cache_dir, *self._labels)\n+        f = open(path, mode='r')\n+      except EnvironmentError as e:\n+        # For Python2 and Python3 compatibility, this checks the\n+        # EnvironmentError to see if the file exists.\n+        # TODO: Change this to a FileNotFoundError when Python3 migration is\n+        # complete.\n+        import errno\n+        if e.errno != errno.ENOENT:\n+          # Raise the exception if it is not a FileNotFoundError.\n+          raise\n+        time.sleep(1)\n+    if now_secs >= timeout_timestamp_secs:\n+      raise RuntimeError(\n+          \"Timed out waiting for file '{}' to be available\".format(path))\n+    return f\n+\n+  def _emit_from_file(self, fh, tail):\n+    \"\"\"Emits the TestStreamFile(Header|Record)s from file.\n+\n+    This returns a generator to be able to read all lines from the given file.\n+    If `tail` is True, then it will wait until the cache is complete to exit.\n+    Otherwise, it will read the file only once.\n+    \"\"\"\n+    # Always read at least once to read the whole file.\n+    while True:\n+      pos = fh.tell()\n+      line = fh.readline()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM3MTU2MQ=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 152}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM5OTgxMjY3OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwMDoyMDo1M1rOFxbYIw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNVQwMDoxMzoyMFrOFyCQLA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM3MzA5MQ==", "bodyText": "Why is this not done in the loop below?", "url": "https://github.com/apache/beam/pull/11005#discussion_r387373091", "createdAt": "2020-03-04T00:20:53Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "diffHunk": "@@ -19,15 +19,298 @@\n \n from __future__ import absolute_import\n \n+import itertools\n+import os\n+import shutil\n+import tempfile\n+import time\n+\n+import apache_beam as beam\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileHeader\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileRecord\n from apache_beam.portability.api.beam_runner_api_pb2 import TestStreamPayload\n+from apache_beam.runners.interactive.cache_manager import CacheManager\n+from apache_beam.runners.interactive.cache_manager import SafeFastPrimitivesCoder\n+from apache_beam.testing.test_stream import ReverseTestStream\n from apache_beam.utils import timestamp\n \n \n-class StreamingCache(object):\n+class StreamingCacheSink(beam.PTransform):\n+  \"\"\"A PTransform that writes TestStreamFile(Header|Records)s to file.\n+\n+  This transform takes in an arbitrary element stream and writes the best-effort\n+  list of TestStream events (as TestStreamFileRecords) to file.\n+\n+  Note that this PTransform is assumed to be only run on a single machine where\n+  the following assumptions are correct: elements come in ordered, no two\n+  transforms are writing to the same file. This PTransform is assumed to only\n+  run correctly with the DirectRunner.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      filename,\n+      sample_resolution_sec,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._filename = filename\n+    self._sample_resolution_sec = sample_resolution_sec\n+    self._coder = coder\n+    self._path = os.path.join(self._cache_dir, self._filename)\n+\n+  @property\n+  def path(self):\n+    \"\"\"Returns the path the sink leads to.\"\"\"\n+    return self._path\n+\n+  def expand(self, pcoll):\n+    class StreamingWriteToText(beam.DoFn):\n+      \"\"\"DoFn that performs the writing.\n+\n+      Note that the other file writing methods cannot be used in streaming\n+      contexts.\n+      \"\"\"\n+      def __init__(self, full_path, coder=SafeFastPrimitivesCoder()):\n+        self._full_path = full_path\n+        self._coder = coder\n+\n+        # Try and make the given path.\n+        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n+\n+      def start_bundle(self):\n+        # Open the file for 'append-mode' and writing 'bytes'.\n+        self._fh = open(self._full_path, 'ab')\n+\n+      def finish_bundle(self):\n+        self._fh.close()\n+\n+      def process(self, e):\n+        \"\"\"Appends the given element to the file.\n+        \"\"\"\n+        self._fh.write(self._coder.encode(e))\n+        self._fh.write(b'\\n')\n+\n+    return (\n+        pcoll\n+        | ReverseTestStream(\n+            output_tag=self._filename,\n+            sample_resolution_sec=self._sample_resolution_sec,\n+            output_format=ReverseTestStream.Format.\n+            SERIALIZED_TEST_STREAM_FILE_RECORDS,\n+            coder=self._coder)\n+        | beam.ParDo(\n+            StreamingWriteToText(full_path=self._path, coder=self._coder)))\n+\n+\n+class StreamingCacheSource:\n+  \"\"\"A class that reads and parses TestStreamFile(Header|Reader)s.\n+\n+  This source operates in the following way:\n+\n+    1. Wait for up to `timeout_secs` for the file to be available.\n+    2. Read, parse, and emit the entire contents of the file\n+    3. Wait for more events to come or until `is_cache_complete` returns True\n+    4. If there are more events, then go to 2\n+    5. Otherwise, stop emitting.\n+\n+  This class is used to read from file and send its to the TestStream via the\n+  StreamingCacheManager.Reader.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      labels,\n+      is_cache_complete=None,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._coder = coder\n+    self._labels = labels\n+    self._is_cache_complete = (\n+        is_cache_complete if is_cache_complete else lambda: True)\n+\n+  def _wait_until_file_exists(self, timeout_secs=30):\n+    \"\"\"Blocks until the file exists for a maximum of timeout_secs.\n+    \"\"\"\n+    f = None\n+    now_secs = time.time()\n+    timeout_timestamp_secs = now_secs + timeout_secs\n+\n+    # Wait for up to `timeout_secs` for the file to be available.\n+    while f is None and now_secs < timeout_timestamp_secs:\n+      now_secs = time.time()\n+      try:\n+        path = os.path.join(self._cache_dir, *self._labels)\n+        f = open(path, mode='r')\n+      except EnvironmentError as e:\n+        # For Python2 and Python3 compatibility, this checks the\n+        # EnvironmentError to see if the file exists.\n+        # TODO: Change this to a FileNotFoundError when Python3 migration is\n+        # complete.\n+        import errno\n+        if e.errno != errno.ENOENT:\n+          # Raise the exception if it is not a FileNotFoundError.\n+          raise\n+        time.sleep(1)\n+    if now_secs >= timeout_timestamp_secs:\n+      raise RuntimeError(\n+          \"Timed out waiting for file '{}' to be available\".format(path))\n+    return f\n+\n+  def _emit_from_file(self, fh, tail):\n+    \"\"\"Emits the TestStreamFile(Header|Record)s from file.\n+\n+    This returns a generator to be able to read all lines from the given file.\n+    If `tail` is True, then it will wait until the cache is complete to exit.\n+    Otherwise, it will read the file only once.\n+    \"\"\"\n+    # Always read at least once to read the whole file.\n+    while True:\n+      pos = fh.tell()\n+      line = fh.readline()\n+\n+      # Check if we are at EOF.\n+      if not line:\n+        # Complete reading only when the cache is complete.\n+        if self._is_cache_complete():\n+          break\n+\n+        if not tail:\n+          break\n+\n+        # Otherwise wait for new data in the file to be written.\n+        time.sleep(0.5)\n+        fh.seek(pos)\n+      else:\n+        # The first line at pos = 0 is always the header. Read the line without\n+        # the new line.\n+        if pos == 0:\n+          header = TestStreamFileHeader()\n+          header.ParseFromString(self._coder.decode(line[:-1]))\n+          yield header\n+        else:\n+          record = TestStreamFileRecord()\n+          record.ParseFromString(self._coder.decode(line[:-1]))\n+          yield record\n+\n+  def read(self, tail):\n+    \"\"\"Reads all TestStreamFile(Header|TestStreamFileRecord)s from file.\n+\n+    This returns a generator to be able to read all lines from the given file.\n+    If `tail` is True, then it will wait until the cache is complete to exit.\n+    Otherwise, it will read the file only once.\n+    \"\"\"\n+    with self._wait_until_file_exists() as f:\n+      for e in self._emit_from_file(f, tail):\n+        yield e\n+\n+\n+class StreamingCache(CacheManager):\n   \"\"\"Abstraction that holds the logic for reading and writing to cache.\n   \"\"\"\n-  def __init__(self, readers):\n-    self._readers = readers\n+  def __init__(\n+      self, cache_dir, is_cache_complete=None, sample_resolution_sec=0.1):\n+    self._sample_resolution_sec = sample_resolution_sec\n+    self._is_cache_complete = is_cache_complete\n+\n+    if cache_dir:\n+      self._cache_dir = cache_dir\n+    else:\n+      self._cache_dir = tempfile.mkdtemp(\n+          prefix='interactive-temp-', dir=os.environ.get('TEST_TMPDIR', None))\n+\n+    # List of saved pcoders keyed by PCollection path. It is OK to keep this\n+    # list in memory because once FileBasedCacheManager object is\n+    # destroyed/re-created it loses the access to previously written cache\n+    # objects anyways even if cache_dir already exists. In other words,\n+    # it is not possible to resume execution of Beam pipeline from the\n+    # saved cache if FileBasedCacheManager has been reset.\n+    #\n+    # However, if we are to implement better cache persistence, one needs\n+    # to take care of keeping consistency between the cached PCollection\n+    # and its PCoder type.\n+    self._saved_pcoders = {}\n+    self._default_pcoder = SafeFastPrimitivesCoder()\n+\n+  def exists(self, *labels):\n+    path = os.path.join(self._cache_dir, *labels)\n+    return os.path.exists(path)\n+\n+  # TODO(srohde): Modify this to return the correct version.\n+  def read(self, *labels):\n+    \"\"\"Returns a generator to read all records from file.\n+\n+    Does not tail.\n+    \"\"\"\n+    if not self.exists(*labels):\n+      return [].__iter__(), -1\n+\n+    reader = StreamingCacheSource(\n+        self._cache_dir, labels,\n+        is_cache_complete=self._is_cache_complete).read(tail=False)\n+    header = next(reader)\n+    return StreamingCache.Reader([header], [reader]).read(), 1\n+\n+  def read_multiple(self, labels):\n+    \"\"\"Returns a generator to read all records from file.\n+\n+    Does tail until the cache is complete. This is because it is used in the\n+    TestStreamServiceController to read from file which is only used during\n+    pipeline runtime which needs to block.\n+    \"\"\"\n+    readers = [\n+        StreamingCacheSource(\n+            self._cache_dir, l,\n+            is_cache_complete=self._is_cache_complete).read(tail=True)\n+        for l in labels\n+    ]\n+    headers = [next(r) for r in readers]\n+    return StreamingCache.Reader(headers, readers).read()\n+\n+  def write(self, values, *labels):\n+    \"\"\"Writes the given values to cache.\n+    \"\"\"\n+    to_write = [v.SerializeToString() for v in values]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 257}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4ODAxMDAyOA==", "bodyText": "Moved to loop", "url": "https://github.com/apache/beam/pull/11005#discussion_r388010028", "createdAt": "2020-03-05T00:13:20Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "diffHunk": "@@ -19,15 +19,298 @@\n \n from __future__ import absolute_import\n \n+import itertools\n+import os\n+import shutil\n+import tempfile\n+import time\n+\n+import apache_beam as beam\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileHeader\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileRecord\n from apache_beam.portability.api.beam_runner_api_pb2 import TestStreamPayload\n+from apache_beam.runners.interactive.cache_manager import CacheManager\n+from apache_beam.runners.interactive.cache_manager import SafeFastPrimitivesCoder\n+from apache_beam.testing.test_stream import ReverseTestStream\n from apache_beam.utils import timestamp\n \n \n-class StreamingCache(object):\n+class StreamingCacheSink(beam.PTransform):\n+  \"\"\"A PTransform that writes TestStreamFile(Header|Records)s to file.\n+\n+  This transform takes in an arbitrary element stream and writes the best-effort\n+  list of TestStream events (as TestStreamFileRecords) to file.\n+\n+  Note that this PTransform is assumed to be only run on a single machine where\n+  the following assumptions are correct: elements come in ordered, no two\n+  transforms are writing to the same file. This PTransform is assumed to only\n+  run correctly with the DirectRunner.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      filename,\n+      sample_resolution_sec,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._filename = filename\n+    self._sample_resolution_sec = sample_resolution_sec\n+    self._coder = coder\n+    self._path = os.path.join(self._cache_dir, self._filename)\n+\n+  @property\n+  def path(self):\n+    \"\"\"Returns the path the sink leads to.\"\"\"\n+    return self._path\n+\n+  def expand(self, pcoll):\n+    class StreamingWriteToText(beam.DoFn):\n+      \"\"\"DoFn that performs the writing.\n+\n+      Note that the other file writing methods cannot be used in streaming\n+      contexts.\n+      \"\"\"\n+      def __init__(self, full_path, coder=SafeFastPrimitivesCoder()):\n+        self._full_path = full_path\n+        self._coder = coder\n+\n+        # Try and make the given path.\n+        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n+\n+      def start_bundle(self):\n+        # Open the file for 'append-mode' and writing 'bytes'.\n+        self._fh = open(self._full_path, 'ab')\n+\n+      def finish_bundle(self):\n+        self._fh.close()\n+\n+      def process(self, e):\n+        \"\"\"Appends the given element to the file.\n+        \"\"\"\n+        self._fh.write(self._coder.encode(e))\n+        self._fh.write(b'\\n')\n+\n+    return (\n+        pcoll\n+        | ReverseTestStream(\n+            output_tag=self._filename,\n+            sample_resolution_sec=self._sample_resolution_sec,\n+            output_format=ReverseTestStream.Format.\n+            SERIALIZED_TEST_STREAM_FILE_RECORDS,\n+            coder=self._coder)\n+        | beam.ParDo(\n+            StreamingWriteToText(full_path=self._path, coder=self._coder)))\n+\n+\n+class StreamingCacheSource:\n+  \"\"\"A class that reads and parses TestStreamFile(Header|Reader)s.\n+\n+  This source operates in the following way:\n+\n+    1. Wait for up to `timeout_secs` for the file to be available.\n+    2. Read, parse, and emit the entire contents of the file\n+    3. Wait for more events to come or until `is_cache_complete` returns True\n+    4. If there are more events, then go to 2\n+    5. Otherwise, stop emitting.\n+\n+  This class is used to read from file and send its to the TestStream via the\n+  StreamingCacheManager.Reader.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      labels,\n+      is_cache_complete=None,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._coder = coder\n+    self._labels = labels\n+    self._is_cache_complete = (\n+        is_cache_complete if is_cache_complete else lambda: True)\n+\n+  def _wait_until_file_exists(self, timeout_secs=30):\n+    \"\"\"Blocks until the file exists for a maximum of timeout_secs.\n+    \"\"\"\n+    f = None\n+    now_secs = time.time()\n+    timeout_timestamp_secs = now_secs + timeout_secs\n+\n+    # Wait for up to `timeout_secs` for the file to be available.\n+    while f is None and now_secs < timeout_timestamp_secs:\n+      now_secs = time.time()\n+      try:\n+        path = os.path.join(self._cache_dir, *self._labels)\n+        f = open(path, mode='r')\n+      except EnvironmentError as e:\n+        # For Python2 and Python3 compatibility, this checks the\n+        # EnvironmentError to see if the file exists.\n+        # TODO: Change this to a FileNotFoundError when Python3 migration is\n+        # complete.\n+        import errno\n+        if e.errno != errno.ENOENT:\n+          # Raise the exception if it is not a FileNotFoundError.\n+          raise\n+        time.sleep(1)\n+    if now_secs >= timeout_timestamp_secs:\n+      raise RuntimeError(\n+          \"Timed out waiting for file '{}' to be available\".format(path))\n+    return f\n+\n+  def _emit_from_file(self, fh, tail):\n+    \"\"\"Emits the TestStreamFile(Header|Record)s from file.\n+\n+    This returns a generator to be able to read all lines from the given file.\n+    If `tail` is True, then it will wait until the cache is complete to exit.\n+    Otherwise, it will read the file only once.\n+    \"\"\"\n+    # Always read at least once to read the whole file.\n+    while True:\n+      pos = fh.tell()\n+      line = fh.readline()\n+\n+      # Check if we are at EOF.\n+      if not line:\n+        # Complete reading only when the cache is complete.\n+        if self._is_cache_complete():\n+          break\n+\n+        if not tail:\n+          break\n+\n+        # Otherwise wait for new data in the file to be written.\n+        time.sleep(0.5)\n+        fh.seek(pos)\n+      else:\n+        # The first line at pos = 0 is always the header. Read the line without\n+        # the new line.\n+        if pos == 0:\n+          header = TestStreamFileHeader()\n+          header.ParseFromString(self._coder.decode(line[:-1]))\n+          yield header\n+        else:\n+          record = TestStreamFileRecord()\n+          record.ParseFromString(self._coder.decode(line[:-1]))\n+          yield record\n+\n+  def read(self, tail):\n+    \"\"\"Reads all TestStreamFile(Header|TestStreamFileRecord)s from file.\n+\n+    This returns a generator to be able to read all lines from the given file.\n+    If `tail` is True, then it will wait until the cache is complete to exit.\n+    Otherwise, it will read the file only once.\n+    \"\"\"\n+    with self._wait_until_file_exists() as f:\n+      for e in self._emit_from_file(f, tail):\n+        yield e\n+\n+\n+class StreamingCache(CacheManager):\n   \"\"\"Abstraction that holds the logic for reading and writing to cache.\n   \"\"\"\n-  def __init__(self, readers):\n-    self._readers = readers\n+  def __init__(\n+      self, cache_dir, is_cache_complete=None, sample_resolution_sec=0.1):\n+    self._sample_resolution_sec = sample_resolution_sec\n+    self._is_cache_complete = is_cache_complete\n+\n+    if cache_dir:\n+      self._cache_dir = cache_dir\n+    else:\n+      self._cache_dir = tempfile.mkdtemp(\n+          prefix='interactive-temp-', dir=os.environ.get('TEST_TMPDIR', None))\n+\n+    # List of saved pcoders keyed by PCollection path. It is OK to keep this\n+    # list in memory because once FileBasedCacheManager object is\n+    # destroyed/re-created it loses the access to previously written cache\n+    # objects anyways even if cache_dir already exists. In other words,\n+    # it is not possible to resume execution of Beam pipeline from the\n+    # saved cache if FileBasedCacheManager has been reset.\n+    #\n+    # However, if we are to implement better cache persistence, one needs\n+    # to take care of keeping consistency between the cached PCollection\n+    # and its PCoder type.\n+    self._saved_pcoders = {}\n+    self._default_pcoder = SafeFastPrimitivesCoder()\n+\n+  def exists(self, *labels):\n+    path = os.path.join(self._cache_dir, *labels)\n+    return os.path.exists(path)\n+\n+  # TODO(srohde): Modify this to return the correct version.\n+  def read(self, *labels):\n+    \"\"\"Returns a generator to read all records from file.\n+\n+    Does not tail.\n+    \"\"\"\n+    if not self.exists(*labels):\n+      return [].__iter__(), -1\n+\n+    reader = StreamingCacheSource(\n+        self._cache_dir, labels,\n+        is_cache_complete=self._is_cache_complete).read(tail=False)\n+    header = next(reader)\n+    return StreamingCache.Reader([header], [reader]).read(), 1\n+\n+  def read_multiple(self, labels):\n+    \"\"\"Returns a generator to read all records from file.\n+\n+    Does tail until the cache is complete. This is because it is used in the\n+    TestStreamServiceController to read from file which is only used during\n+    pipeline runtime which needs to block.\n+    \"\"\"\n+    readers = [\n+        StreamingCacheSource(\n+            self._cache_dir, l,\n+            is_cache_complete=self._is_cache_complete).read(tail=True)\n+        for l in labels\n+    ]\n+    headers = [next(r) for r in readers]\n+    return StreamingCache.Reader(headers, readers).read()\n+\n+  def write(self, values, *labels):\n+    \"\"\"Writes the given values to cache.\n+    \"\"\"\n+    to_write = [v.SerializeToString() for v in values]", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM3MzA5MQ=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 257}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMDk5MTk0OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/direct/test_stream_impl.py", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDoxODoyMVrOFzGN9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxODoyMjo1OVrOFz0PBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyMzU3Mw==", "bodyText": "Why can't we just pass the [None]? Is it because None is not a string?", "url": "https://github.com/apache/beam/pull/11005#discussion_r389123573", "createdAt": "2020-03-06T20:18:21Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/test_stream_impl.py", "diffHunk": "@@ -226,17 +237,53 @@ def expand(self, pcoll):\n   def _infer_output_coder(self, input_type=None, input_coder=None):\n     return self.coder\n \n-  def _events_from_script(self, index):\n-    yield self._events[index]\n-\n-  def events(self, index):\n-    return self._events_from_script(index)\n-\n-  def begin(self):\n-    return 0\n-\n-  def end(self, index):\n-    return index >= len(self._events)\n+  @staticmethod\n+  def events_from_script(events):\n+    \"\"\"Yields the in-memory events.\n+    \"\"\"\n+    return itertools.chain(events)\n \n-  def next(self, index):\n-    return index + 1\n+  @staticmethod\n+  def events_from_rpc(endpoint, output_tags, coder):\n+    \"\"\"Yields the events received from the given endpoint.\n+    \"\"\"\n+    stub_channel = grpc.insecure_channel(endpoint)\n+    stub = beam_runner_api_pb2_grpc.TestStreamServiceStub(stub_channel)\n+\n+    # Request the PCollections that we are looking for from the service.\n+    if list(output_tags) == [None]:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTE2ODU3MQ==", "bodyText": "Yes, the proto expects a string", "url": "https://github.com/apache/beam/pull/11005#discussion_r389168571", "createdAt": "2020-03-06T22:10:19Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/direct/test_stream_impl.py", "diffHunk": "@@ -226,17 +237,53 @@ def expand(self, pcoll):\n   def _infer_output_coder(self, input_type=None, input_coder=None):\n     return self.coder\n \n-  def _events_from_script(self, index):\n-    yield self._events[index]\n-\n-  def events(self, index):\n-    return self._events_from_script(index)\n-\n-  def begin(self):\n-    return 0\n-\n-  def end(self, index):\n-    return index >= len(self._events)\n+  @staticmethod\n+  def events_from_script(events):\n+    \"\"\"Yields the in-memory events.\n+    \"\"\"\n+    return itertools.chain(events)\n \n-  def next(self, index):\n-    return index + 1\n+  @staticmethod\n+  def events_from_rpc(endpoint, output_tags, coder):\n+    \"\"\"Yields the events received from the given endpoint.\n+    \"\"\"\n+    stub_channel = grpc.insecure_channel(endpoint)\n+    stub = beam_runner_api_pb2_grpc.TestStreamServiceStub(stub_channel)\n+\n+    # Request the PCollections that we are looking for from the service.\n+    if list(output_tags) == [None]:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyMzU3Mw=="}, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTIwODYyMQ==", "bodyText": "What's the convention for this list being empty then? (And if it can be empty, in what cases do we have to provide it? Or should we explicitly be turning None to 'None'?", "url": "https://github.com/apache/beam/pull/11005#discussion_r389208621", "createdAt": "2020-03-07T00:53:52Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/test_stream_impl.py", "diffHunk": "@@ -226,17 +237,53 @@ def expand(self, pcoll):\n   def _infer_output_coder(self, input_type=None, input_coder=None):\n     return self.coder\n \n-  def _events_from_script(self, index):\n-    yield self._events[index]\n-\n-  def events(self, index):\n-    return self._events_from_script(index)\n-\n-  def begin(self):\n-    return 0\n-\n-  def end(self, index):\n-    return index >= len(self._events)\n+  @staticmethod\n+  def events_from_script(events):\n+    \"\"\"Yields the in-memory events.\n+    \"\"\"\n+    return itertools.chain(events)\n \n-  def next(self, index):\n-    return index + 1\n+  @staticmethod\n+  def events_from_rpc(endpoint, output_tags, coder):\n+    \"\"\"Yields the events received from the given endpoint.\n+    \"\"\"\n+    stub_channel = grpc.insecure_channel(endpoint)\n+    stub = beam_runner_api_pb2_grpc.TestStreamServiceStub(stub_channel)\n+\n+    # Request the PCollections that we are looking for from the service.\n+    if list(output_tags) == [None]:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyMzU3Mw=="}, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg3NzUwOA==", "bodyText": "Good point, I changed it to stringify the list then convert 'None' to None on the service side.", "url": "https://github.com/apache/beam/pull/11005#discussion_r389877508", "createdAt": "2020-03-09T18:22:59Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/direct/test_stream_impl.py", "diffHunk": "@@ -226,17 +237,53 @@ def expand(self, pcoll):\n   def _infer_output_coder(self, input_type=None, input_coder=None):\n     return self.coder\n \n-  def _events_from_script(self, index):\n-    yield self._events[index]\n-\n-  def events(self, index):\n-    return self._events_from_script(index)\n-\n-  def begin(self):\n-    return 0\n-\n-  def end(self, index):\n-    return index >= len(self._events)\n+  @staticmethod\n+  def events_from_script(events):\n+    \"\"\"Yields the in-memory events.\n+    \"\"\"\n+    return itertools.chain(events)\n \n-  def next(self, index):\n-    return index + 1\n+  @staticmethod\n+  def events_from_rpc(endpoint, output_tags, coder):\n+    \"\"\"Yields the events received from the given endpoint.\n+    \"\"\"\n+    stub_channel = grpc.insecure_channel(endpoint)\n+    stub = beam_runner_api_pb2_grpc.TestStreamServiceStub(stub_channel)\n+\n+    # Request the PCollections that we are looking for from the service.\n+    if list(output_tags) == [None]:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyMzU3Mw=="}, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 57}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTAyNzY4OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDozMjowNVrOFzGkqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQyMzoxNTowM1rOFz8b9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyOTM4Nw==", "bodyText": "No, there should only be one TestStream per pipeline. That's the reason we introduced test streams with multiple outputs. (I also fail to see how multiple TestStreams would work, as they would all seem to share the same global context and so it seems all test stream evaluators would emit all events).", "url": "https://github.com/apache/beam/pull/11005#discussion_r389129387", "createdAt": "2020-03-06T20:32:05Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -214,11 +214,31 @@ class _TestStreamRootBundleProvider(RootBundleProvider):\n   \"\"\"\n   def get_root_bundles(self):\n     test_stream = self._applied_ptransform.transform\n+\n+    # The TestStream specification allows for multiple TestStreams in the same", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTIwNTY5MA==", "bodyText": "Ack I updated the TestStreamTests to use the multi-output teststream instead of two teststreams. And changed this back to a singleton.", "url": "https://github.com/apache/beam/pull/11005#discussion_r389205690", "createdAt": "2020-03-07T00:35:12Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -214,11 +214,31 @@ class _TestStreamRootBundleProvider(RootBundleProvider):\n   \"\"\"\n   def get_root_bundles(self):\n     test_stream = self._applied_ptransform.transform\n+\n+    # The TestStream specification allows for multiple TestStreams in the same", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyOTM4Nw=="}, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MDAxMTg5NQ==", "bodyText": "I'm happy to see this.", "url": "https://github.com/apache/beam/pull/11005#discussion_r390011895", "createdAt": "2020-03-09T23:15:03Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -214,11 +214,31 @@ class _TestStreamRootBundleProvider(RootBundleProvider):\n   \"\"\"\n   def get_root_bundles(self):\n     test_stream = self._applied_ptransform.transform\n+\n+    # The TestStream specification allows for multiple TestStreams in the same", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyOTM4Nw=="}, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTA0MDYxOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/direct/direct_runner.py", "isResolved": true, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDozNzowN1rOFzGsxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxODoyMjoxOVrOFz0NtQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMTQ2Mw==", "bodyText": "Why is this passed via the context, instead of accessed directly on the test stream object when it is being processed?", "url": "https://github.com/apache/beam/pull/11005#discussion_r389131463", "createdAt": "2020-03-06T20:37:07Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/direct_runner.py", "diffHunk": "@@ -408,7 +410,8 @@ def visit_transform(self, applied_ptransform):\n         self.consumer_tracking_visitor.value_to_consumers,\n         self.consumer_tracking_visitor.step_names,\n         self.consumer_tracking_visitor.views,\n-        clock)\n+        clock,\n+        test_stream_visitor.endpoint)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTIwNTcwNA==", "bodyText": "Ack, changed to get endpoint from TestStream object.", "url": "https://github.com/apache/beam/pull/11005#discussion_r389205704", "createdAt": "2020-03-07T00:35:18Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/direct/direct_runner.py", "diffHunk": "@@ -408,7 +410,8 @@ def visit_transform(self, applied_ptransform):\n         self.consumer_tracking_visitor.value_to_consumers,\n         self.consumer_tracking_visitor.step_names,\n         self.consumer_tracking_visitor.views,\n-        clock)\n+        clock,\n+        test_stream_visitor.endpoint)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMTQ2Mw=="}, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTIwOTk2Nw==", "bodyText": "Not changed?", "url": "https://github.com/apache/beam/pull/11005#discussion_r389209967", "createdAt": "2020-03-07T01:02:25Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/direct_runner.py", "diffHunk": "@@ -408,7 +410,8 @@ def visit_transform(self, applied_ptransform):\n         self.consumer_tracking_visitor.value_to_consumers,\n         self.consumer_tracking_visitor.step_names,\n         self.consumer_tracking_visitor.views,\n-        clock)\n+        clock,\n+        test_stream_visitor.endpoint)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMTQ2Mw=="}, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg3NzE3Mw==", "bodyText": "I pushed this particular change to #10994", "url": "https://github.com/apache/beam/pull/11005#discussion_r389877173", "createdAt": "2020-03-09T18:22:19Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/direct/direct_runner.py", "diffHunk": "@@ -408,7 +410,8 @@ def visit_transform(self, applied_ptransform):\n         self.consumer_tracking_visitor.value_to_consumers,\n         self.consumer_tracking_visitor.step_names,\n         self.consumer_tracking_visitor.views,\n-        clock)\n+        clock,\n+        test_stream_visitor.endpoint)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMTQ2Mw=="}, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTA0Mjk3OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDozNzo1N1rOFzGuQg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQyMDo0MzoxNFrOFz4qWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMTg0Mg==", "bodyText": "(Regarding the comment above) we should get this from test_stream, rather than plumb it through the context.", "url": "https://github.com/apache/beam/pull/11005#discussion_r389131842", "createdAt": "2020-03-06T20:37:57Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -214,11 +214,31 @@ class _TestStreamRootBundleProvider(RootBundleProvider):\n   \"\"\"\n   def get_root_bundles(self):\n     test_stream = self._applied_ptransform.transform\n+\n+    # The TestStream specification allows for multiple TestStreams in the same\n+    # pipeline (with only one controlling the clock). Here, we use an array in\n+    # the global EvaluationContext state to keep track of the iterator for each\n+    # event stream.\n+    idx = len(self._evaluation_context._test_stream_events)\n+\n+    # If there was an endpoint defined then get the events from the\n+    # TestStreamService.\n+    if self._evaluation_context._test_stream_endpoint:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzNDMyOQ==", "bodyText": "Likewise, why are we plumbing  _test_stream_events through the context instead of creating it in _TestStreamEvaluator?", "url": "https://github.com/apache/beam/pull/11005#discussion_r389134329", "createdAt": "2020-03-06T20:43:44Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -214,11 +214,31 @@ class _TestStreamRootBundleProvider(RootBundleProvider):\n   \"\"\"\n   def get_root_bundles(self):\n     test_stream = self._applied_ptransform.transform\n+\n+    # The TestStream specification allows for multiple TestStreams in the same\n+    # pipeline (with only one controlling the clock). Here, we use an array in\n+    # the global EvaluationContext state to keep track of the iterator for each\n+    # event stream.\n+    idx = len(self._evaluation_context._test_stream_events)\n+\n+    # If there was an endpoint defined then get the events from the\n+    # TestStreamService.\n+    if self._evaluation_context._test_stream_endpoint:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMTg0Mg=="}, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTIwNTcyMg==", "bodyText": "This is a way to keep track of where in the event stream the TestStream is. Because the _TestStreamEvaluator can be created multiple times, the iterator will become invalid once we try to make a new one. This is actually a cleanup of the old implementation where the state was kept in an unprocessed bundle that was passed to itself.", "url": "https://github.com/apache/beam/pull/11005#discussion_r389205722", "createdAt": "2020-03-07T00:35:24Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -214,11 +214,31 @@ class _TestStreamRootBundleProvider(RootBundleProvider):\n   \"\"\"\n   def get_root_bundles(self):\n     test_stream = self._applied_ptransform.transform\n+\n+    # The TestStream specification allows for multiple TestStreams in the same\n+    # pipeline (with only one controlling the clock). Here, we use an array in\n+    # the global EvaluationContext state to keep track of the iterator for each\n+    # event stream.\n+    idx = len(self._evaluation_context._test_stream_events)\n+\n+    # If there was an endpoint defined then get the events from the\n+    # TestStreamService.\n+    if self._evaluation_context._test_stream_endpoint:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMTg0Mg=="}, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTIwOTAwNQ==", "bodyText": "I'm not convinced that storing it on the global context is an improvement on keeping it on the bundle that gets passed back to the (new) _TestStreamEvaluator instance. It seems harder to track who owns/accesses it (and also that it only gets written to once). I agree the original way of doing things may not have been ideal either.", "url": "https://github.com/apache/beam/pull/11005#discussion_r389209005", "createdAt": "2020-03-07T00:56:17Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -214,11 +214,31 @@ class _TestStreamRootBundleProvider(RootBundleProvider):\n   \"\"\"\n   def get_root_bundles(self):\n     test_stream = self._applied_ptransform.transform\n+\n+    # The TestStream specification allows for multiple TestStreams in the same\n+    # pipeline (with only one controlling the clock). Here, we use an array in\n+    # the global EvaluationContext state to keep track of the iterator for each\n+    # event stream.\n+    idx = len(self._evaluation_context._test_stream_events)\n+\n+    # If there was an endpoint defined then get the events from the\n+    # TestStreamService.\n+    if self._evaluation_context._test_stream_endpoint:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMTg0Mg=="}, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTk1MDA0Mg==", "bodyText": "Gotcha, I'll set the global context once and move the state to a global on the _TestStreamEvaluator. That way the owner is strictly the _TestStreamEvaluator.", "url": "https://github.com/apache/beam/pull/11005#discussion_r389950042", "createdAt": "2020-03-09T20:43:14Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -214,11 +214,31 @@ class _TestStreamRootBundleProvider(RootBundleProvider):\n   \"\"\"\n   def get_root_bundles(self):\n     test_stream = self._applied_ptransform.transform\n+\n+    # The TestStream specification allows for multiple TestStreams in the same\n+    # pipeline (with only one controlling the clock). Here, we use an array in\n+    # the global EvaluationContext state to keep track of the iterator for each\n+    # event stream.\n+    idx = len(self._evaluation_context._test_stream_events)\n+\n+    # If there was an endpoint defined then get the events from the\n+    # TestStreamService.\n+    if self._evaluation_context._test_stream_endpoint:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMTg0Mg=="}, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTA3NTE3OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDo0OTo0NlrOFzHCEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wN1QwMDozNTozMFrOFzLO_g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzNjkxNA==", "bodyText": "Why did this have to move out of start_bundle? (Similarly, why do we need is_done now and didn't before?)", "url": "https://github.com/apache/beam/pull/11005#discussion_r389136914", "createdAt": "2020-03-06T20:49:46Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -500,23 +523,21 @@ def __init__(\n         input_committed_bundle,\n         side_inputs)\n     self.test_stream = applied_ptransform.transform\n+    self.event_index = 0", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 53}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTIwNTc1OA==", "bodyText": "Removed this. Also, the is_done is needed because of the small cleanup. There used to be a weird C++ iterator style on the TestStream object to get the event. So there needs to be a small state variable from the process_elements call to the finish_bundle call informing itself if there are any more elements to process.", "url": "https://github.com/apache/beam/pull/11005#discussion_r389205758", "createdAt": "2020-03-07T00:35:30Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -500,23 +523,21 @@ def __init__(\n         input_committed_bundle,\n         side_inputs)\n     self.test_stream = applied_ptransform.transform\n+    self.event_index = 0", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzNjkxNA=="}, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 53}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTU0MTgxOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/interactive/cache_manager.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wN1QwMTowMToyNFrOFzLe7A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxODoyMToyNFrOFz0LvA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTIwOTgzNg==", "bodyText": "Please leave the types (it makes it easier to follow).", "url": "https://github.com/apache/beam/pull/11005#discussion_r389209836", "createdAt": "2020-03-07T01:01:24Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/interactive/cache_manager.py", "diffHunk": "@@ -69,8 +69,8 @@ def read(self, *labels):\n       *labels: List of labels for PCollection instance.\n \n     Returns:\n-      Tuple[List[Any], int]: A tuple containing a list of items in the\n-        PCollection and the version number.\n+      A tuple containing an iterator for the items in the PCollection and the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg3NjY2OA==", "bodyText": "I was getting some pydoc errors and removed it. I put them back in as type hints.", "url": "https://github.com/apache/beam/pull/11005#discussion_r389876668", "createdAt": "2020-03-09T18:21:24Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/interactive/cache_manager.py", "diffHunk": "@@ -69,8 +69,8 @@ def read(self, *labels):\n       *labels: List of labels for PCollection instance.\n \n     Returns:\n-      Tuple[List[Any], int]: A tuple containing a list of items in the\n-        PCollection and the version number.\n+      A tuple containing an iterator for the items in the PCollection and the", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTIwOTgzNg=="}, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQxMTU0NDIwOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wN1QwMTowNDowOVrOFzLgWQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wOVQxODozMDowNlrOFz0d3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTIxMDIwMQ==", "bodyText": "Is this ever non-empty? It seems repeated calls would append everything. (Or is it populated elsewhere?)", "url": "https://github.com/apache/beam/pull/11005#discussion_r389210201", "createdAt": "2020-03-07T01:04:09Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -214,11 +214,31 @@ class _TestStreamRootBundleProvider(RootBundleProvider):\n   \"\"\"\n   def get_root_bundles(self):\n     test_stream = self._applied_ptransform.transform\n+\n+    # The TestStream specification allows for multiple TestStreams in the same\n+    # pipeline (with only one controlling the clock). Here, we use an array in\n+    # the global EvaluationContext state to keep track of the iterator for each\n+    # event stream.\n+    idx = len(self._evaluation_context._test_stream_events)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTg4MTMxMQ==", "bodyText": "In #10994 I changed this to a singleton", "url": "https://github.com/apache/beam/pull/11005#discussion_r389881311", "createdAt": "2020-03-09T18:30:06Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -214,11 +214,31 @@ class _TestStreamRootBundleProvider(RootBundleProvider):\n   \"\"\"\n   def get_root_bundles(self):\n     test_stream = self._applied_ptransform.transform\n+\n+    # The TestStream specification allows for multiple TestStreams in the same\n+    # pipeline (with only one controlling the clock). Here, we use an array in\n+    # the global EvaluationContext state to keep track of the iterator for each\n+    # event stream.\n+    idx = len(self._evaluation_context._test_stream_events)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTIxMDIwMQ=="}, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 9}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyNDAzOTE3OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/interactive/cache_manager.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxNzoyNjozMFrOF1BPxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxODo0ODoyMlrOF1EUBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEzOTI3MQ==", "bodyText": "it seems like you can just return the result of self._reader_class(...) all the way. This is a PTransform, so it is consistent with the itnerface (instead of re-wrapping the source / sink)", "url": "https://github.com/apache/beam/pull/11005#discussion_r391139271", "createdAt": "2020-03-11T17:26:30Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/runners/interactive/cache_manager.py", "diffHunk": "@@ -167,20 +196,38 @@ def load_pcoder(self, *labels):\n         self._saved_pcoders[self._path(*labels)])\n \n   def read(self, *labels):\n+    # Return an iterator to an empty list if it doesn't exist.\n     if not self.exists(*labels):\n-      return [], -1\n+      return iter([]), -1\n \n-    source = self.source(*labels)\n+    # Otherwise, return a generator to the cached PCollection.\n+    source = self._source(*labels)\n     range_tracker = source.get_range_tracker(None, None)\n-    result = list(source.read(range_tracker))\n+    reader = source.read(range_tracker)\n     version = self._latest_version(*labels)\n-    return result, version\n+    return reader, version\n+\n+  def write(self, values, *labels):\n+    sink = self._sink(*labels)\n+    path = self._path(*labels)\n+\n+    init_result = sink.initialize_write()\n+    writer = sink.open_writer(init_result, path)\n+    for v in values:\n+      writer.write(v)\n+    writer.close()\n \n   def source(self, *labels):\n+    return beam.io.Read(self._source(*labels))\n+\n+  def sink(self, labels):\n+    return beam.io.Write(self._sink(*labels))\n+\n+  def _source(self, *labels):\n     return self._reader_class(\n         self._glob_path(*labels), coder=self.load_pcoder(*labels))._source", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a6a4c6e9b6cb19bd5e21d2a5156a77e3b3c9b1c"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTE4OTUxMA==", "bodyText": "Done", "url": "https://github.com/apache/beam/pull/11005#discussion_r391189510", "createdAt": "2020-03-11T18:48:22Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/interactive/cache_manager.py", "diffHunk": "@@ -167,20 +196,38 @@ def load_pcoder(self, *labels):\n         self._saved_pcoders[self._path(*labels)])\n \n   def read(self, *labels):\n+    # Return an iterator to an empty list if it doesn't exist.\n     if not self.exists(*labels):\n-      return [], -1\n+      return iter([]), -1\n \n-    source = self.source(*labels)\n+    # Otherwise, return a generator to the cached PCollection.\n+    source = self._source(*labels)\n     range_tracker = source.get_range_tracker(None, None)\n-    result = list(source.read(range_tracker))\n+    reader = source.read(range_tracker)\n     version = self._latest_version(*labels)\n-    return result, version\n+    return reader, version\n+\n+  def write(self, values, *labels):\n+    sink = self._sink(*labels)\n+    path = self._path(*labels)\n+\n+    init_result = sink.initialize_write()\n+    writer = sink.open_writer(init_result, path)\n+    for v in values:\n+      writer.write(v)\n+    writer.close()\n \n   def source(self, *labels):\n+    return beam.io.Read(self._source(*labels))\n+\n+  def sink(self, labels):\n+    return beam.io.Write(self._sink(*labels))\n+\n+  def _source(self, *labels):\n     return self._reader_class(\n         self._glob_path(*labels), coder=self.load_pcoder(*labels))._source", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEzOTI3MQ=="}, "originalCommit": {"oid": "7a6a4c6e9b6cb19bd5e21d2a5156a77e3b3c9b1c"}, "originalPosition": 125}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQyNDA0MDIxOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/interactive/cache_manager.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxNzoyNjo0MlrOF1BQYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxODo0ODoyOVrOF1EUTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEzOTQyNQ==", "bodyText": "it seems like you can just return the result of self._writer_class(...) all the way. This is a PTransform, so it is consistent with the itnerface (instead of re-wrapping the source / sink)", "url": "https://github.com/apache/beam/pull/11005#discussion_r391139425", "createdAt": "2020-03-11T17:26:42Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/runners/interactive/cache_manager.py", "diffHunk": "@@ -167,20 +196,38 @@ def load_pcoder(self, *labels):\n         self._saved_pcoders[self._path(*labels)])\n \n   def read(self, *labels):\n+    # Return an iterator to an empty list if it doesn't exist.\n     if not self.exists(*labels):\n-      return [], -1\n+      return iter([]), -1\n \n-    source = self.source(*labels)\n+    # Otherwise, return a generator to the cached PCollection.\n+    source = self._source(*labels)\n     range_tracker = source.get_range_tracker(None, None)\n-    result = list(source.read(range_tracker))\n+    reader = source.read(range_tracker)\n     version = self._latest_version(*labels)\n-    return result, version\n+    return reader, version\n+\n+  def write(self, values, *labels):\n+    sink = self._sink(*labels)\n+    path = self._path(*labels)\n+\n+    init_result = sink.initialize_write()\n+    writer = sink.open_writer(init_result, path)\n+    for v in values:\n+      writer.write(v)\n+    writer.close()\n \n   def source(self, *labels):\n+    return beam.io.Read(self._source(*labels))\n+\n+  def sink(self, labels):\n+    return beam.io.Write(self._sink(*labels))\n+\n+  def _source(self, *labels):\n     return self._reader_class(\n         self._glob_path(*labels), coder=self.load_pcoder(*labels))._source\n \n-  def sink(self, *labels):\n+  def _sink(self, *labels):\n     return self._writer_class(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a6a4c6e9b6cb19bd5e21d2a5156a77e3b3c9b1c"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTE4OTU4MQ==", "bodyText": "Done", "url": "https://github.com/apache/beam/pull/11005#discussion_r391189581", "createdAt": "2020-03-11T18:48:29Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/interactive/cache_manager.py", "diffHunk": "@@ -167,20 +196,38 @@ def load_pcoder(self, *labels):\n         self._saved_pcoders[self._path(*labels)])\n \n   def read(self, *labels):\n+    # Return an iterator to an empty list if it doesn't exist.\n     if not self.exists(*labels):\n-      return [], -1\n+      return iter([]), -1\n \n-    source = self.source(*labels)\n+    # Otherwise, return a generator to the cached PCollection.\n+    source = self._source(*labels)\n     range_tracker = source.get_range_tracker(None, None)\n-    result = list(source.read(range_tracker))\n+    reader = source.read(range_tracker)\n     version = self._latest_version(*labels)\n-    return result, version\n+    return reader, version\n+\n+  def write(self, values, *labels):\n+    sink = self._sink(*labels)\n+    path = self._path(*labels)\n+\n+    init_result = sink.initialize_write()\n+    writer = sink.open_writer(init_result, path)\n+    for v in values:\n+      writer.write(v)\n+    writer.close()\n \n   def source(self, *labels):\n+    return beam.io.Read(self._source(*labels))\n+\n+  def sink(self, labels):\n+    return beam.io.Write(self._sink(*labels))\n+\n+  def _source(self, *labels):\n     return self._reader_class(\n         self._glob_path(*labels), coder=self.load_pcoder(*labels))._source\n \n-  def sink(self, *labels):\n+  def _sink(self, *labels):\n     return self._writer_class(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEzOTQyNQ=="}, "originalCommit": {"oid": "7a6a4c6e9b6cb19bd5e21d2a5156a77e3b3c9b1c"}, "originalPosition": 129}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1960, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}