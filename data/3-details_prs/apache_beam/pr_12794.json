{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDgyNzI1OTg5", "number": 12794, "title": "[BEAM-10865] Support for Kafka deserialization API with headers (since Kafka API 2.1.0)", "bodyText": "TLDR  Let KafkaIO support the deserializer API with headers\nReferences mailing list posts:\n\nOriginal https://lists.apache.org/thread.html/rdac09a286cab86c237cf17ed35cdc592b4079d116fc682a6f797d68b%40%3Cdev.beam.apache.org%3E\nReply from Luke https://lists.apache.org/thread.html/rfde58381e9c34da7894b2dd5325c02944411539235f2668adea5bf24%40%3Cdev.beam.apache.org%3E\n\nDesign decisions\nThe reason for SpEL is because with kafka-clients API < 2.1.0 as dependency, compilation fails with:\n  required: String,byte[]\n  found: String,Headers,byte[]\n  reason: actual and formal argument lists differ in length\n  where T is a type-variable:\n\nBecause the headers default API only landed in 2.1.0 via apache/kafka@f1f7192#diff-a4f4aee88ce5091db576139f6c610ced\nI opted for ConsumerSpEL#deserializeKey and ConsumerSpEL#deserializeValue as API to ensure forward looking consistency for both KafkaUnboundedReader and ReadFromKafkaDoFn as both already depended on an instance thereof.\nNot so great things\nUsing the SpEL for kafka-client API 2.1.0 onwards effectively turns the deserialization path into a more expensive indirection by calling the deserializer methods using reflection (2x per record, 1 x key, 1 x value):\n \tat org.apache.kafka.common.serialization.Deserializer.deserialize(Deserializer.java:58) <<<<<<<<<<<\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method) <<<<<<<<<<<\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62) <<<<<<<<<<<\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) <<<<<<<<<<<\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566) <<<<<<<<<<<\n\tat org.springframework.expression.spel.support.ReflectiveMethodExecutor.execute(ReflectiveMethodExecutor.java:117)\n\tat org.springframework.expression.spel.ast.MethodReference.getValueInternal(MethodReference.java:134)\n\tat org.springframework.expression.spel.ast.MethodReference.access$000(MethodReference.java:52)\n\tat org.springframework.expression.spel.ast.MethodReference$MethodValueRef.getValue(MethodReference.java:377)\n\tat org.springframework.expression.spel.ast.CompoundExpression.getValueInternal(CompoundExpression.java:88)\n\tat org.springframework.expression.spel.ast.SpelNodeImpl.getValue(SpelNodeImpl.java:121)\n\tat org.springframework.expression.spel.standard.SpelExpression.getValue(SpelExpression.java:262)\n\tat org.apache.beam.sdk.io.kafka.ConsumerSpEL.evaluateDeserializeWithHeaders(ConsumerSpEL.java:134)\n\tat org.apache.beam.sdk.io.kafka.ConsumerSpEL.deserializeValue(ConsumerSpEL.java:174)\n\tat org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.advance(KafkaUnboundedReader.java:195)\n\tat org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.start(KafkaUnboundedReader.java:137)\n\nAnd effectively this penalizes the more recent Kafka API versions in favor of the older ones. I have not measured the overhead thereof, yet.\nOther avenues explored\nFor runtime deserialization:\n\nNaively tried conditional compile options but the compiler cannot know which kafka-clients version could be used at runtime\n\nFor regression tests (that we don't stop passing headers in the future):\n\n\nI tried Mockito and Powermock implementations on both LocalDeserializerProvider and the Integer and Long serializers in tests, but found the stack to be too deep and backed out of that.\n\n\nDitto for attempting to spy on ConsumerRecord#headers() (expect it to be called twice as much for the newer API), but again deep stack and hard to assert. Just the call is interesting because the constructor used for ConsumerRecord in the tests does not use the one that sets headers, presumably for client API compatibility too.\n\n\nEvaluated ExtendedSerializer\u00b4s wrapper, but ExtendedSerializer is deprecated API and no point in bringing that in as a dependency\n\n\nHow the current regression test works\nI figured it makes sense given this feature tests deserialization and the whole test suite depends on the Integer (for keys) and Long (for values) ones to implement a key and value deserializer that can assert the behaviour. And herein also lies somewhat of a problem because the test case is a bit weak as I relied on stack frames (wide array of suppored client versions makes anything else super complex) to infer the caller of the deserialize method, but unfortunately only class and method name context is provide and no arguments size of 3 or even types on those to assert on.\nKafka client API 1.0.0 :\nFrame 0: java.lang.Thread.getStackTrace\nFrame 1: org.apache.beam.sdk.io.kafka.KafkaIOTest$IntegerDeserializerWithHeadersAssertor#deserialize\nFrame 2: org.apache.beam.sdk.io.kafka.KafkaIOTest$IntegerDeserializerWithHeadersAssertor#deserialize\nFrame 3: org.apache.beam.sdk.io.kafka.ConsumerSpEL#deserializeKey\n\nFor clients before 2.1.0, frame 3 is ConsumerSpEL#deserializeKey, meaning it was called directly and not via a default or actual implementation on Deserializer. Frames 1 and 2 being equal is because of the super.deserialize call.\nKafka client API 2.1.0+ :\nFrame 0: java.lang.Thread.getStackTrace\nFrame 1: org.apache.beam.sdk.io.kafka.KafkaIOTest$IntegerDeserializerWithHeadersAssertor#deserialize\nFrame 2: org.apache.beam.sdk.io.kafka.KafkaIOTest$IntegerDeserializerWithHeadersAssertor#deserialize\nFrame 3: org.apache.kafka.common.serialization.Deserializer#deserialize\n\nFor clients 2.1.0 and beyond, frame 3 is org.apache.kafka.common.serialization.Deserializer#deserialize. This is true for the bundled deserializers used in the tests because they delegate the call to the implementation on Deserializer. In practice this may refer to an actual override implementation.\nFeedback items for me\n\nAny alternatives for the SpEL evaluation for this hot path API? consumer.seekToEnd and consumer.assign are once off / periodic APIs and not called as often as twice per record.\nIdeas for a better way to test for regressions?\n\nQuestions\n\nWould it make sense to consider raising the minimum supported client API in order to\nIf this implementation (and very likely iterations thereof :-)), would support for the same API on serialization be appreciated as well?\n\n\nThank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:\n\n Choose reviewer(s) and mention them in a comment (R: @username).\n Format the pull request title like [BEAM-XXX] Fixes bug in ApproximateQuantiles, where you replace BEAM-XXX with the appropriate JIRA issue, if applicable. This will automatically link the pull request to the issue.\n Update CHANGES.md with noteworthy changes.\n If this contribution is large, please file an Apache Individual Contributor License Agreement.\n\nSee the Contributor Guide for more tips on how to make review process smoother.\nPost-Commit Tests Status (on master branch)\n\n\n\nLang\nSDK\nDataflow\nFlink\nSamza\nSpark\nTwister2\n\n\n\n\nGo\n\n---\n\n---\n\n---\n\n\nJava\n\n\n\n\n\n\n\n\nPython\n\n\n\n---\n\n---\n\n\nXLang\n\n---\n\n---\n\n---\n\n\n\nPre-Commit Tests Status (on master branch)\n\n\n\n---\nJava\nPython\nGo\nWebsite\nWhitespace\nTypescript\n\n\n\n\nNon-portable\n\n \n\n\n\n\n\n\nPortable\n---\n\n---\n---\n---\n---\n\n\n\nSee .test-infra/jenkins/README for trigger phrase, status and link of all Jenkins jobs.\nGitHub Actions Tests Status (on master branch)\n\n\n\nSee CI.md for more information about GitHub Actions CI.", "createdAt": "2020-09-09T10:01:22Z", "url": "https://github.com/apache/beam/pull/12794", "merged": true, "mergeCommit": {"oid": "7f474b544f868d2addbc2463cb26e0fed31061b1"}, "closed": true, "closedAt": "2020-09-21T21:13:02Z", "author": {"login": "methodmissing"}, "timelineItems": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdG4jeogH2gAyNDgyNzI1OTg5OmE2YmI0MWRkNDZkNjA1ZTdiMjNlMjYwYzRmMGU2NmI5YTE4ZjE4YmY=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdLKDJ_AFqTQ5Mjg1NjkzOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "a6bb41dd46d605e7b23e260c4f0e66b9a18f18bf", "author": {"user": {"login": "methodmissing", "name": "Lourens Naud\u00e9"}}, "url": "https://github.com/apache/beam/commit/a6bb41dd46d605e7b23e260c4f0e66b9a18f18bf", "committedDate": "2020-09-08T14:34:13Z", "message": "Support for Kafka deserialization API with headers (since Kafka API 2.1.0)"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDg2MTExMDEx", "url": "https://github.com/apache/beam/pull/12794#pullrequestreview-486111011", "createdAt": "2020-09-10T16:28:27Z", "commit": {"oid": "a6bb41dd46d605e7b23e260c4f0e66b9a18f18bf"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNjoyODoyN1rOHP8NZw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xMFQxNjozMjo0MVrOHP8X3w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ3NzE1OQ==", "bodyText": "Why does this matter that it is a default method?\nShouldn't we be setting this to true as long as the method exists?", "url": "https://github.com/apache/beam/pull/12794#discussion_r486477159", "createdAt": "2020-09-10T16:28:27Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ConsumerSpEL.java", "diffHunk": "@@ -90,6 +98,16 @@ public ConsumerSpEL() {\n     } catch (NoSuchMethodException | SecurityException e) {\n       LOG.debug(\"OffsetsForTimes is not available.\");\n     }\n+\n+    try {\n+      // It is supported by Kafka Client 2.1.0 onwards.\n+      Method method =\n+          Deserializer.class.getDeclaredMethod(\n+              \"deserialize\", String.class, Headers.class, byte[].class);\n+      deserializerSupportsHeaders = method.isDefault();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a6bb41dd46d605e7b23e260c4f0e66b9a18f18bf"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NjQ3OTgzOQ==", "bodyText": "In the build.gradle you'll want to setup a junit test run that uses Kafka 2.1.0 and executes these tests.\nThis is a pretty good example of how to get this going:\nhttps://docs.gradle.org/current/userguide/java_testing.html#sec:configuring_java_integration_tests\nYou shouldn't need any additional source sets but you'll want to add a configuration for kafka210 that adds the dependency and also the test task that uses it.", "url": "https://github.com/apache/beam/pull/12794#discussion_r486479839", "createdAt": "2020-09-10T16:32:41Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/test/java/org/apache/beam/sdk/io/kafka/KafkaIOTest.java", "diffHunk": "@@ -501,6 +503,83 @@ public void testReadAvroSpecificRecordsWithConfluentSchemaRegistry() {\n     p.run();\n   }\n \n+  public static class IntegerDeserializerWithHeadersAssertor extends IntegerDeserializer\n+      implements Deserializer<Integer> {\n+    ConsumerSpEL consumerSpEL = null;\n+\n+    @Override\n+    public Integer deserialize(String topic, byte[] data) {\n+      StackTraceElement[] stackTraceElements = Thread.currentThread().getStackTrace();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a6bb41dd46d605e7b23e260c4f0e66b9a18f18bf"}, "originalPosition": 26}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f79db7c188113dc2fb12d6e8324e9fe2bdc3bfd8", "author": {"user": {"login": "methodmissing", "name": "Lourens Naud\u00e9"}}, "url": "https://github.com/apache/beam/commit/f79db7c188113dc2fb12d6e8324e9fe2bdc3bfd8", "committedDate": "2020-09-10T20:09:28Z", "message": "Assert the deserializer method with a Headers argument exists and disregard it being a default or not"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2bc407dc5491e967065adbb36b1aaf7725e72634", "author": {"user": {"login": "methodmissing", "name": "Lourens Naud\u00e9"}}, "url": "https://github.com/apache/beam/commit/2bc407dc5491e967065adbb36b1aaf7725e72634", "committedDate": "2020-09-15T13:10:06Z", "message": "Introduce a kafkaVersion210Test for testing KafkaIOTest against kafka-client 2.1.0"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b3bddff2f6b326d376914e0d315e53b97e12f729", "author": {"user": {"login": "methodmissing", "name": "Lourens Naud\u00e9"}}, "url": "https://github.com/apache/beam/commit/b3bddff2f6b326d376914e0d315e53b97e12f729", "committedDate": "2020-09-20T01:47:55Z", "message": "Let the kafkaVersion210 configuration use a resolution strategy to force Kafka clients API 2.1.0"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDkyODU2OTM4", "url": "https://github.com/apache/beam/pull/12794#pullrequestreview-492856938", "createdAt": "2020-09-21T18:02:18Z", "commit": {"oid": "b3bddff2f6b326d376914e0d315e53b97e12f729"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMVQxODowMjozNlrOHVceAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMVQyMToxMDowM1rOHVih6g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjI0ODU3Nw==", "bodyText": "Can we run all the unit tests?\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              include '**/KafkaIOTest.class'\n          \n          \n            \n              include '**/KafkaIOTest.class'", "url": "https://github.com/apache/beam/pull/12794#discussion_r492248577", "createdAt": "2020-09-21T18:02:36Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/build.gradle", "diffHunk": "@@ -64,4 +69,24 @@ dependencies {\n   testCompile library.java.powermock_mockito\n   testRuntimeOnly library.java.slf4j_jdk14\n   testRuntimeOnly project(path: \":runners:direct-java\", configuration: \"shadow\")\n+  kafkaVersion210 \"org.apache.kafka:kafka-clients:2.1.0\"\n+}\n+\n+configurations.kafkaVersion210 {\n+  resolutionStrategy {\n+    force \"org.apache.kafka:kafka-clients:2.1.0\"\n+  }\n+}\n+\n+task kafkaVersion210Test(type: Test) {\n+  group = \"Verification\"\n+  description = 'Runs KafkaIO tests with Kafka clients API 2.1.0'\n+  outputs.upToDateWhen { false }\n+  testClassesDirs = sourceSets.test.output.classesDirs\n+  classpath =  configurations.kafkaVersion210 + sourceSets.test.runtimeClasspath\n+  include '**/KafkaIOTest.class'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3bddff2f6b326d376914e0d315e53b97e12f729"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjM0Nzg4Mg==", "bodyText": "Instead of doing outputs.upToDateWhen { false } to have this run every time, please include the set of inputs/outputs that makes sense in a follow-up PR so that an incremental build can be supported.", "url": "https://github.com/apache/beam/pull/12794#discussion_r492347882", "createdAt": "2020-09-21T21:10:03Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/build.gradle", "diffHunk": "@@ -64,4 +69,24 @@ dependencies {\n   testCompile library.java.powermock_mockito\n   testRuntimeOnly library.java.slf4j_jdk14\n   testRuntimeOnly project(path: \":runners:direct-java\", configuration: \"shadow\")\n+  kafkaVersion210 \"org.apache.kafka:kafka-clients:2.1.0\"\n+}\n+\n+configurations.kafkaVersion210 {\n+  resolutionStrategy {\n+    force \"org.apache.kafka:kafka-clients:2.1.0\"\n+  }\n+}\n+\n+task kafkaVersion210Test(type: Test) {\n+  group = \"Verification\"\n+  description = 'Runs KafkaIO tests with Kafka clients API 2.1.0'\n+  outputs.upToDateWhen { false }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3bddff2f6b326d376914e0d315e53b97e12f729"}, "originalPosition": 28}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4699, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}