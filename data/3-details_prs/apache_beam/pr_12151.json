{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQyNzc3NTEw", "number": 12151, "title": "[BEAM-9896] Add streaming for SnowflakeIO.Write to Java SDK", "bodyText": "SnowflakeIO streaming write support.\n\nThank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:\n\n Choose reviewer(s) and mention them in a comment (R: @username).\n Format the pull request title like [BEAM-XXX] Fixes bug in ApproximateQuantiles, where you replace BEAM-XXX with the appropriate JIRA issue, if applicable. This will automatically link the pull request to the issue.\n Update CHANGES.md with noteworthy changes.\n If this contribution is large, please file an Apache Individual Contributor License Agreement.\n\nSee the Contributor Guide for more tips on how to make review process smoother.\nPost-Commit Tests Status (on master branch)\n\n\n\nLang\nSDK\nDataflow\nFlink\nSamza\nSpark\nTwister2\n\n\n\n\nGo\n\n---\n\n---\n\n---\n\n\nJava\n\n\n\n\n\n\n\n\nPython\n\n\n\n---\n\n---\n\n\nXLang\n---\n---\n\n---\n---\n\n\n\n\nPre-Commit Tests Status (on master branch)\n\n\n\n---\nJava\nPython\nGo\nWebsite\n\n\n\n\nNon-portable\n\n\n\n\n\n\nPortable\n---\n\n---\n---\n\n\n\nSee .test-infra/jenkins/README for trigger phrase, status and link of all Jenkins jobs.", "createdAt": "2020-07-01T15:31:38Z", "url": "https://github.com/apache/beam/pull/12151", "merged": true, "mergeCommit": {"oid": "86b8326b4ebc4e217585847102743cc1d1af367a"}, "closed": true, "closedAt": "2020-08-05T13:53:15Z", "author": {"login": "kkucharc"}, "timelineItems": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcynJ0VgBqjM1MjA4ODQ3NTU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABc76voYABqjM2MjQ1MzE2MTE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "65aef453e7354506c808d593ae60c662a70ffe4a", "author": {"user": {"login": "kkucharc", "name": "Kasia Kucharczyk"}}, "url": "https://github.com/apache/beam/commit/65aef453e7354506c808d593ae60c662a70ffe4a", "committedDate": "2020-07-01T15:25:11Z", "message": "[BEAM-9896] Spotless Apply"}, "afterCommit": {"oid": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd", "author": {"user": {"login": "kkucharc", "name": "Kasia Kucharczyk"}}, "url": "https://github.com/apache/beam/commit/8a16ec3f427cb88bf59781000ed23a84b6cc4bbd", "committedDate": "2020-07-06T14:38:02Z", "message": "[BEAM-9896] Spotless Apply"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQ0NDE3MTQz", "url": "https://github.com/apache/beam/pull/12151#pullrequestreview-444417143", "createdAt": "2020-07-08T05:10:57Z", "commit": {"oid": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd"}, "state": "COMMENTED", "comments": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOFQwNToxMDo1N1rOGuYV0g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wOFQwNjowNjowNVrOGuZXwQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI4NjQ4Mg==", "bodyText": "Please add detailed Javadoc for these functions, as they will be the point of entry for users - they also will help me understand what's a snow pipe : )", "url": "https://github.com/apache/beam/pull/12151#discussion_r451286482", "createdAt": "2020-07-08T05:10:57Z", "author": {"login": "pabloem"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -644,6 +747,26 @@ public void populateDisplayData(DisplayData.Builder builder) {\n       return toBuilder().setUserDataMapper(userDataMapper).build();\n     }\n \n+    public Write<T> withFlushTimeLimit(Duration triggeringFrequency) {\n+      return toBuilder().setFlushTimeLimit(triggeringFrequency).build();\n+    }\n+\n+    public Write<T> withSnowPipe(String snowPipe) {\n+      return toBuilder().setSnowPipe(ValueProvider.StaticValueProvider.of(snowPipe)).build();\n+    }\n+\n+    public Write<T> withSnowPipe(ValueProvider<String> snowPipe) {\n+      return toBuilder().setSnowPipe(snowPipe).build();\n+    }\n+\n+    public Write<T> withShardsNumber(Integer shardsNumber) {\n+      return toBuilder().setShardsNumber(shardsNumber).build();\n+    }\n+\n+    public Write<T> withFlushRowLimit(Integer rowsCount) {\n+      return toBuilder().setFlushRowLimit(rowsCount).build();\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd"}, "originalPosition": 371}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI4NzExMw==", "bodyText": "Please add javadoc", "url": "https://github.com/apache/beam/pull/12151#discussion_r451287113", "createdAt": "2020-07-08T05:13:13Z", "author": {"login": "pabloem"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -654,28 +777,40 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     }\n \n     /**\n-     * A snowflake service which is supposed to be used. Note: Currently we have {@link\n-     * SnowflakeServiceImpl} with corresponding {@link FakeSnowflakeServiceImpl} used for testing.\n+     * A snowflake service {@link SnowflakeService} implementation which is supposed to be used.\n      *\n      * @param snowflakeService - an instance of {@link SnowflakeService}.\n      */\n     public Write<T> withSnowflakeService(SnowflakeService snowflakeService) {\n       return toBuilder().setSnowflakeService(snowflakeService).build();\n     }\n \n+    public Write<T> withQuotationMark(String quotationMark) {\n+      return toBuilder().setQuotationMark(quotationMark).build();\n+    }\n+\n+    public Write<T> withDebugMode(StreamingLogLevel debugLevel) {\n+      return toBuilder().setDebugMode(debugLevel).build();\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd"}, "originalPosition": 396}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI4NzIwOQ==", "bodyText": "Javadoc please", "url": "https://github.com/apache/beam/pull/12151#discussion_r451287209", "createdAt": "2020-07-08T05:13:36Z", "author": {"login": "pabloem"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -352,15 +385,16 @@\n       return toBuilder().setCoder(coder).build();\n     }\n \n+    public Read<T> withQuotationMark(String quotationMark) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd"}, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI4Nzk2NA==", "bodyText": "Please improve the error message specifying that withSnowPipe is required for streaming / unbounded PCollections", "url": "https://github.com/apache/beam/pull/12151#discussion_r451287964", "createdAt": "2020-07-08T05:15:57Z", "author": {"login": "pabloem"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -684,14 +819,61 @@ private void checkArguments() {\n           (getDataSourceProviderFn() != null),\n           \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n \n-      checkArgument(getTable() != null, \"withTable() is required\");\n+      if (input.isBounded() == PCollection.IsBounded.UNBOUNDED) {\n+        checkArgument(getSnowPipe() != null, \"withSnowPipe() is required\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd"}, "originalPosition": 429}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI4ODAyOQ==", "bodyText": "Please make the error message more informative", "url": "https://github.com/apache/beam/pull/12151#discussion_r451288029", "createdAt": "2020-07-08T05:16:14Z", "author": {"login": "pabloem"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -684,14 +819,61 @@ private void checkArguments() {\n           (getDataSourceProviderFn() != null),\n           \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n \n-      checkArgument(getTable() != null, \"withTable() is required\");\n+      if (input.isBounded() == PCollection.IsBounded.UNBOUNDED) {\n+        checkArgument(getSnowPipe() != null, \"withSnowPipe() is required\");\n+      } else {\n+        checkArgument(getTable() != null, \"to() is required\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd"}, "originalPosition": 431}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5NDQ2NA==", "bodyText": "A PCollection after a combine / GBK / etc - keeps the trigger configuration from upstream. Did you find that you needed to restate the trigger like this?", "url": "https://github.com/apache/beam/pull/12151#discussion_r451294464", "createdAt": "2020-07-08T05:38:41Z", "author": {"login": "pabloem"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -684,14 +819,61 @@ private void checkArguments() {\n           (getDataSourceProviderFn() != null),\n           \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n \n-      checkArgument(getTable() != null, \"withTable() is required\");\n+      if (input.isBounded() == PCollection.IsBounded.UNBOUNDED) {\n+        checkArgument(getSnowPipe() != null, \"withSnowPipe() is required\");\n+      } else {\n+        checkArgument(getTable() != null, \"to() is required\");\n+      }\n     }\n \n-    private PCollection<String> write(PCollection<T> input, String stagingBucketDir) {\n+    private PCollection<T> writeStream(PCollection<T> input, String stagingBucketDir) {\n       SnowflakeService snowflakeService =\n-          getSnowflakeService() != null ? getSnowflakeService() : new SnowflakeServiceImpl();\n+          getSnowflakeService() != null\n+              ? getSnowflakeService()\n+              : new SnowflakeStreamingServiceImpl();\n+\n+      /* Ensure that files will be created after specific record count or duration specified */\n+      PCollection<T> inputInGlobalWindow =\n+          input.apply(\n+              \"rewindowIntoGlobal\",\n+              Window.<T>into(new GlobalWindows())\n+                  .triggering(\n+                      Repeatedly.forever(\n+                          AfterFirst.of(\n+                              AfterProcessingTime.pastFirstElementInPane()\n+                                  .plusDelayOf(getFlushTimeLimit()),\n+                              AfterPane.elementCountAtLeast(getFlushRowLimit()))))\n+                  .discardingFiredPanes());\n+\n+      int shards = (getShardsNumber() > 0) ? getShardsNumber() : DEFAULT_STREAMING_SHARDS_NUMBER;\n+      PCollection files = writeFiles(inputInGlobalWindow, stagingBucketDir, shards);\n+\n+      /* Ensuring that files will be ingested after flush time */\n+      files =\n+          (PCollection)\n+              files.apply(\n+                  \"applyUserTrigger\",\n+                  Window.<T>into(new GlobalWindows())\n+                      .triggering(\n+                          Repeatedly.forever(\n+                              AfterProcessingTime.pastFirstElementInPane()\n+                                  .plusDelayOf(getFlushTimeLimit())))\n+                      .discardingFiredPanes());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd"}, "originalPosition": 469}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5NDgwMg==", "bodyText": "We are concatenating this in memory. Is this intended? I guess you would end up with a number of files equal to the number shards, which shouldn't be too large?", "url": "https://github.com/apache/beam/pull/12151#discussion_r451294802", "createdAt": "2020-07-08T05:39:52Z", "author": {"login": "pabloem"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -684,14 +819,61 @@ private void checkArguments() {\n           (getDataSourceProviderFn() != null),\n           \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n \n-      checkArgument(getTable() != null, \"withTable() is required\");\n+      if (input.isBounded() == PCollection.IsBounded.UNBOUNDED) {\n+        checkArgument(getSnowPipe() != null, \"withSnowPipe() is required\");\n+      } else {\n+        checkArgument(getTable() != null, \"to() is required\");\n+      }\n     }\n \n-    private PCollection<String> write(PCollection<T> input, String stagingBucketDir) {\n+    private PCollection<T> writeStream(PCollection<T> input, String stagingBucketDir) {\n       SnowflakeService snowflakeService =\n-          getSnowflakeService() != null ? getSnowflakeService() : new SnowflakeServiceImpl();\n+          getSnowflakeService() != null\n+              ? getSnowflakeService()\n+              : new SnowflakeStreamingServiceImpl();\n+\n+      /* Ensure that files will be created after specific record count or duration specified */\n+      PCollection<T> inputInGlobalWindow =\n+          input.apply(\n+              \"rewindowIntoGlobal\",\n+              Window.<T>into(new GlobalWindows())\n+                  .triggering(\n+                      Repeatedly.forever(\n+                          AfterFirst.of(\n+                              AfterProcessingTime.pastFirstElementInPane()\n+                                  .plusDelayOf(getFlushTimeLimit()),\n+                              AfterPane.elementCountAtLeast(getFlushRowLimit()))))\n+                  .discardingFiredPanes());\n+\n+      int shards = (getShardsNumber() > 0) ? getShardsNumber() : DEFAULT_STREAMING_SHARDS_NUMBER;\n+      PCollection files = writeFiles(inputInGlobalWindow, stagingBucketDir, shards);\n+\n+      /* Ensuring that files will be ingested after flush time */\n+      files =\n+          (PCollection)\n+              files.apply(\n+                  \"applyUserTrigger\",\n+                  Window.<T>into(new GlobalWindows())\n+                      .triggering(\n+                          Repeatedly.forever(\n+                              AfterProcessingTime.pastFirstElementInPane()\n+                                  .plusDelayOf(getFlushTimeLimit())))\n+                      .discardingFiredPanes());\n+      files =\n+          (PCollection)\n+              files.apply(\n+                  \"Create list of logs to copy\",\n+                  Combine.globally(new Concatenate()).withoutDefaults());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd"}, "originalPosition": 474}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5NjE0MQ==", "bodyText": "TODO(pabloem) Review this section", "url": "https://github.com/apache/beam/pull/12151#discussion_r451296141", "createdAt": "2020-07-08T05:44:19Z", "author": {"login": "pabloem"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -828,31 +1037,152 @@ private String quoteField(String field, String quotation) {\n         String stagingBucketDir,\n         String storageIntegrationName,\n         WriteDisposition writeDisposition,\n-        SnowflakeService snowflakeService) {\n+        SnowflakeService snowflakeService,\n+        String quotationMark) {\n       this.dataSourceProviderFn = dataSourceProviderFn;\n-      this.table = table;\n       this.query = query;\n+      this.table = table;\n       this.stagingBucketDir = stagingBucketDir;\n       this.storageIntegrationName = storageIntegrationName;\n       this.writeDisposition = writeDisposition;\n       this.snowflakeService = snowflakeService;\n+      this.quotationMark = quotationMark;\n+\n+      DataSourceProviderFromDataSourceConfiguration dataSourceProviderFromDataSourceConfiguration =\n+          (DataSourceProviderFromDataSourceConfiguration) this.dataSourceProviderFn;\n+      DataSourceConfiguration config = dataSourceProviderFromDataSourceConfiguration.getConfig();\n+\n+      this.database = config.getDatabase();\n+      this.schema = config.getSchema();\n     }\n \n     @ProcessElement\n     public void processElement(ProcessContext context) throws Exception {\n-      SnowflakeServiceConfig config =\n-          new SnowflakeServiceConfig(\n+      SnowflakeBatchServiceConfig config =\n+          new SnowflakeBatchServiceConfig(\n               dataSourceProviderFn,\n               (List<String>) context.element(),\n+              database,\n+              schema,\n               table,\n               query,\n               writeDisposition,\n               storageIntegrationName,\n-              stagingBucketDir);\n+              stagingBucketDir,\n+              quotationMark);\n       snowflakeService.write(config);\n     }\n   }\n \n+  /** Custom DoFn that streams data to Snowflake table. */\n+  private static class StreamToTableFn<ParameterT, OutputT> extends DoFn<ParameterT, OutputT> {\n+    private final SerializableFunction<Void, DataSource> dataSourceProviderFn;\n+    private final String stagingBucketDir;\n+    private final ValueProvider<String> snowPipe;\n+    private final StreamingLogLevel debugMode;\n+    private final SnowflakeService snowflakeService;\n+    private transient SimpleIngestManager ingestManager;\n+\n+    private transient DataSource dataSource;\n+    ArrayList<String> trackedFilesNames;\n+\n+    StreamToTableFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn,\n+        ValueProvider<String> snowPipe,\n+        String stagingBucketDir,\n+        StreamingLogLevel debugMode,\n+        SnowflakeService snowflakeService) {\n+      this.dataSourceProviderFn = dataSourceProviderFn;\n+      this.stagingBucketDir = stagingBucketDir;\n+      this.snowPipe = snowPipe;\n+      this.debugMode = debugMode;\n+      this.snowflakeService = snowflakeService;\n+      trackedFilesNames = new ArrayList<>();\n+    }\n+\n+    @Setup\n+    public void setup() throws Exception {\n+      dataSource = dataSourceProviderFn.apply(null);\n+\n+      DataSourceProviderFromDataSourceConfiguration dataSourceProviderFromDataSourceConfiguration =\n+          (DataSourceProviderFromDataSourceConfiguration) this.dataSourceProviderFn;\n+      DataSourceConfiguration config = dataSourceProviderFromDataSourceConfiguration.getConfig();\n+\n+      checkArgument(config.getPrivateKey() != null, \"KeyPair is required for authentication\");\n+\n+      String hostName = config.getServerName();\n+      List<String> path = Splitter.on('.').splitToList(hostName);\n+      String account = path.get(0);\n+      String username = config.getUsername();\n+      PrivateKey privateKey = config.getPrivateKey();\n+      String schema = config.getSchema();\n+      String database = config.getDatabase();\n+      String snowPipeName = String.format(\"%s.%s.%s\", database, schema, snowPipe.get());\n+\n+      this.ingestManager =\n+          new SimpleIngestManager(\n+              account, username, snowPipeName, privateKey, \"https\", hostName, 443);\n+    }\n+\n+    @ProcessElement\n+    public void processElement(ProcessContext context) throws Exception {\n+      List<String> filesList = (List<String>) context.element();\n+\n+      if (debugMode != null) {\n+        trackedFilesNames.addAll(filesList);\n+      }\n+      SnowflakeStreamingServiceConfig config =\n+          new SnowflakeStreamingServiceConfig(filesList, this.stagingBucketDir, this.ingestManager);\n+      snowflakeService.write(config);\n+    }\n+\n+    @FinishBundle\n+    public void finishBundle() throws Exception {\n+      if (debugMode != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd"}, "originalPosition": 688}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5NjY2MA==", "bodyText": "Add Javadoc to this class? (including whether it's considered part of the public API)", "url": "https://github.com/apache/beam/pull/12151#discussion_r451296660", "createdAt": "2020-07-08T05:45:51Z", "author": {"login": "pabloem"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/services/SnowflakeStreamingServiceConfig.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake.services;\n+\n+import java.util.List;\n+import net.snowflake.ingest.SimpleIngestManager;\n+\n+public class SnowflakeStreamingServiceConfig extends ServiceConfig {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5NzgwNQ==", "bodyText": "Maybe these fields should be private and final since we have getters?", "url": "https://github.com/apache/beam/pull/12151#discussion_r451297805", "createdAt": "2020-07-08T05:49:18Z", "author": {"login": "pabloem"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/services/SnowflakeStreamingServiceConfig.java", "diffHunk": "@@ -0,0 +1,46 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake.services;\n+\n+import java.util.List;\n+import net.snowflake.ingest.SimpleIngestManager;\n+\n+public class SnowflakeStreamingServiceConfig extends ServiceConfig {\n+  SimpleIngestManager ingestManager;\n+  List<String> filesList;\n+  String stagingBucketDir;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5ODIwMg==", "bodyText": "I am a little confused about these lines (53-62). What does this do?", "url": "https://github.com/apache/beam/pull/12151#discussion_r451298202", "createdAt": "2020-07-08T05:50:26Z", "author": {"login": "pabloem"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/services/SnowflakeStreamingServiceImpl.java", "diffHunk": "@@ -0,0 +1,68 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake.services;\n+\n+import java.io.IOException;\n+import java.net.URISyntaxException;\n+import java.util.List;\n+import java.util.Set;\n+import java.util.TreeSet;\n+import java.util.stream.Collectors;\n+import net.snowflake.ingest.SimpleIngestManager;\n+import net.snowflake.ingest.connection.IngestResponseException;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** Implemenation of {@link SnowflakeService} used in production. */\n+public class SnowflakeStreamingServiceImpl\n+    implements SnowflakeService<SnowflakeStreamingServiceConfig> {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeStreamingServiceImpl.class);\n+  private transient SimpleIngestManager ingestManager;\n+\n+  @Override\n+  public void write(SnowflakeStreamingServiceConfig config) throws Exception {\n+    ingest(config);\n+  }\n+\n+  @Override\n+  public String read(SnowflakeStreamingServiceConfig config) throws Exception {\n+    throw new UnsupportedOperationException(\"Not supported by SnowflakeIO.\");\n+  }\n+\n+  public void ingest(SnowflakeStreamingServiceConfig config)\n+      throws IngestResponseException, IOException, URISyntaxException {\n+    List<String> filesList = config.filesList;\n+    String stagingBucketDir = config.stagingBucketDir;\n+    ingestManager = config.ingestManager;\n+\n+    List<String> newList =\n+        filesList.stream()\n+            .map(e -> e.replaceAll(String.valueOf(stagingBucketDir), \"\"))\n+            .map(e -> e.replaceAll(\"'\", \"\"))\n+            .collect(Collectors.toList());\n+\n+    Set<String> files = new TreeSet<>();\n+    for (String file : newList) {\n+      files.add(file);\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTMwMTg5Mw==", "bodyText": "TODO(pabloem) Review these tests", "url": "https://github.com/apache/beam/pull/12151#discussion_r451301893", "createdAt": "2020-07-08T06:01:44Z", "author": {"login": "pabloem"}, "path": "sdks/java/io/snowflake/src/test/java/org/apache/beam/sdk/io/snowflake/test/unit/write/StreamingWriteTest.java", "diffHunk": "@@ -0,0 +1,321 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake.test.unit.write;\n+\n+import static org.hamcrest.CoreMatchers.equalTo;\n+\n+import java.io.IOException;\n+import java.nio.file.DirectoryStream;\n+import java.nio.file.Files;\n+import java.nio.file.Path;\n+import java.nio.file.Paths;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import java.util.stream.LongStream;\n+import net.snowflake.client.jdbc.SnowflakeSQLException;\n+import org.apache.beam.sdk.coders.StringUtf8Coder;\n+import org.apache.beam.sdk.io.snowflake.SnowflakeIO;\n+import org.apache.beam.sdk.io.snowflake.SnowflakePipelineOptions;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory;\n+import org.apache.beam.sdk.io.snowflake.services.SnowflakeService;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeBasicDataSource;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeDatabase;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeStreamingServiceImpl;\n+import org.apache.beam.sdk.io.snowflake.test.TestUtils;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.testing.TestStream;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n+import org.apache.beam.sdk.values.TimestampedValue;\n+import org.hamcrest.MatcherAssert;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.junit.After;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+@RunWith(JUnit4.class)\n+public class StreamingWriteTest {\n+  private static final Logger LOG = LoggerFactory.getLogger(StreamingWriteTest.class);\n+  private static final String FAKE_TABLE = \"TEST_TABLE\";\n+  private static final String STAGING_BUCKET_NAME = \"BUCKET/\";\n+  private static final String STORAGE_INTEGRATION_NAME = \"STORAGE_INTEGRATION\";\n+  private static final String SNOW_PIPE = \"Snowpipe\";\n+  private static final Instant START_TIME = new Instant(0);\n+\n+  @Rule public final transient TestPipeline pipeline = TestPipeline.create();\n+\n+  @Rule public ExpectedException exceptionRule = ExpectedException.none();\n+  private static SnowflakeIO.DataSourceConfiguration dataSourceConfiguration;\n+  private static SnowflakeService snowflakeService;\n+  private static SnowflakePipelineOptions options;\n+  private static List<Long> testData;\n+\n+  private static final List<String> SENTENCES =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTMwMjQ2MA==", "bodyText": "Can you add javadoc for this class? It'll help understand its role, and it'll help users figure out if they're meant to use this class or not.", "url": "https://github.com/apache/beam/pull/12151#discussion_r451302460", "createdAt": "2020-07-08T06:03:34Z", "author": {"login": "pabloem"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/services/SnowflakeBatchServiceConfig.java", "diffHunk": "@@ -0,0 +1,120 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake.services;\n+\n+import java.util.List;\n+import javax.sql.DataSource;\n+import org.apache.beam.sdk.io.snowflake.enums.WriteDisposition;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+\n+public class SnowflakeBatchServiceConfig extends ServiceConfig {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTMwMzM2MQ==", "bodyText": "I know this was discussed earlier, but I am somewhat concerned about using CSV as the method to write. Is this not risky? Have you verified that it works well for all types (Numeric, Date/Time, Decimal, Strings with newlines, etc.)?\nI recommend you add an integration test that tests all - or most - data types once we have the Snowflake instance to test.", "url": "https://github.com/apache/beam/pull/12151#discussion_r451303361", "createdAt": "2020-07-08T06:06:05Z", "author": {"login": "pabloem"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -719,7 +906,9 @@ private void checkArguments() {\n                           return getUserDataMapper().mapRow(element);\n                         }\n                       }))\n-              .apply(\"Map Objects array to CSV lines\", ParDo.of(new MapObjectsArrayToCsvFn()))\n+              .apply(\n+                  \"Map Objects array to CSV lines\",\n+                  ParDo.of(new MapObjectsArrayToCsvFn(getQuotationMark())))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd"}, "originalPosition": 510}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d1d838f737137ff310b117f0519ff83e6455a523", "author": {"user": {"login": "kkucharc", "name": "Kasia Kucharczyk"}}, "url": "https://github.com/apache/beam/commit/d1d838f737137ff310b117f0519ff83e6455a523", "committedDate": "2020-07-15T15:55:33Z", "message": "[BEAM-9896] Added Snowflake streaming write with debug mode and unit tests."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "076be87a131b0a619449e9d6daf54fe400ea6db2", "author": {"user": {"login": "kkucharc", "name": "Kasia Kucharczyk"}}, "url": "https://github.com/apache/beam/commit/076be87a131b0a619449e9d6daf54fe400ea6db2", "committedDate": "2020-07-15T15:55:33Z", "message": "[BEAM-9896] Removed default /data directory in Snowflake write and added parametrized quotation mark."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5b466ad69248e53976ae57cd94a73f320f3c700c", "author": {"user": {"login": "kkucharc", "name": "Kasia Kucharczyk"}}, "url": "https://github.com/apache/beam/commit/5b466ad69248e53976ae57cd94a73f320f3c700c", "committedDate": "2020-07-15T15:55:33Z", "message": "[BEAM-9896] Changed default name."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dc7db2882c7b1a6cde15915231956586e9269e44", "author": {"user": {"login": "kkucharc", "name": "Kasia Kucharczyk"}}, "url": "https://github.com/apache/beam/commit/dc7db2882c7b1a6cde15915231956586e9269e44", "committedDate": "2020-07-15T15:55:33Z", "message": "[BEAM-9896] Added enum for streaming log level."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c5e872d63a82f9e1fb171b22c9ccee4f557ff672", "author": {"user": {"login": "kkucharc", "name": "Kasia Kucharczyk"}}, "url": "https://github.com/apache/beam/commit/c5e872d63a82f9e1fb171b22c9ccee4f557ff672", "committedDate": "2020-07-15T15:55:33Z", "message": "[BEAM-9896] Spotless Apply"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dd38f056d66744e5b6c6d04ede385840c0e55582", "author": {"user": {"login": "kkucharc", "name": "Kasia Kucharczyk"}}, "url": "https://github.com/apache/beam/commit/dd38f056d66744e5b6c6d04ede385840c0e55582", "committedDate": "2020-07-15T16:09:57Z", "message": "[BEAM-9896] Updated javadocs, error messages and parsing filenames in streaming"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4a6c7b5b100687137a619742ad8d357de5869dc2", "author": {"user": {"login": "kkucharc", "name": "Kasia Kucharczyk"}}, "url": "https://github.com/apache/beam/commit/4a6c7b5b100687137a619742ad8d357de5869dc2", "committedDate": "2020-07-15T14:13:51Z", "message": "[BEAM-9896] Updated javadocs, error messages and parsing filenames in streaming"}, "afterCommit": {"oid": "1f8230c476344939fdf0be3a4bd747471735ee1a", "author": {"user": {"login": "kkucharc", "name": "Kasia Kucharczyk"}}, "url": "https://github.com/apache/beam/commit/1f8230c476344939fdf0be3a4bd747471735ee1a", "committedDate": "2020-07-15T15:18:18Z", "message": "[BEAM-9896] Updated javadocs, error messages and parsing filenames in streaming"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1f8230c476344939fdf0be3a4bd747471735ee1a", "author": {"user": {"login": "kkucharc", "name": "Kasia Kucharczyk"}}, "url": "https://github.com/apache/beam/commit/1f8230c476344939fdf0be3a4bd747471735ee1a", "committedDate": "2020-07-15T15:18:18Z", "message": "[BEAM-9896] Updated javadocs, error messages and parsing filenames in streaming"}, "afterCommit": {"oid": "daa675e56a556a20703afd102cdce684385d3e41", "author": {"user": {"login": "kkucharc", "name": "Kasia Kucharczyk"}}, "url": "https://github.com/apache/beam/commit/daa675e56a556a20703afd102cdce684385d3e41", "committedDate": "2020-07-15T15:55:33Z", "message": "[BEAM-9896] Updated javadocs, error messages and parsing filenames in streaming"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "daa675e56a556a20703afd102cdce684385d3e41", "author": {"user": {"login": "kkucharc", "name": "Kasia Kucharczyk"}}, "url": "https://github.com/apache/beam/commit/daa675e56a556a20703afd102cdce684385d3e41", "committedDate": "2020-07-15T15:55:33Z", "message": "[BEAM-9896] Updated javadocs, error messages and parsing filenames in streaming"}, "afterCommit": {"oid": "dd38f056d66744e5b6c6d04ede385840c0e55582", "author": {"user": {"login": "kkucharc", "name": "Kasia Kucharczyk"}}, "url": "https://github.com/apache/beam/commit/dd38f056d66744e5b6c6d04ede385840c0e55582", "committedDate": "2020-07-15T16:09:57Z", "message": "[BEAM-9896] Updated javadocs, error messages and parsing filenames in streaming"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "24fc6e3ba74dd5ba3edef29df11631426b4f24dc", "author": {"user": {"login": "kkucharc", "name": "Kasia Kucharczyk"}}, "url": "https://github.com/apache/beam/commit/24fc6e3ba74dd5ba3edef29df11631426b4f24dc", "committedDate": "2020-07-27T09:19:11Z", "message": "Merge remote-tracking branch 'origin/master' into snowflake-streaming-write"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4NjkwNjAw", "url": "https://github.com/apache/beam/pull/12151#pullrequestreview-458690600", "createdAt": "2020-07-30T19:24:26Z", "commit": {"oid": "24fc6e3ba74dd5ba3edef29df11631426b4f24dc"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxOToyNDoyN1rOG5wsCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQxOToyNDo1MFrOG5ws3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzIxOTcyMw==", "bodyText": "I think we can leave this as it is. I see that the trigger is not the exact same.", "url": "https://github.com/apache/beam/pull/12151#discussion_r463219723", "createdAt": "2020-07-30T19:24:27Z", "author": {"login": "pabloem"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -684,14 +819,61 @@ private void checkArguments() {\n           (getDataSourceProviderFn() != null),\n           \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n \n-      checkArgument(getTable() != null, \"withTable() is required\");\n+      if (input.isBounded() == PCollection.IsBounded.UNBOUNDED) {\n+        checkArgument(getSnowPipe() != null, \"withSnowPipe() is required\");\n+      } else {\n+        checkArgument(getTable() != null, \"to() is required\");\n+      }\n     }\n \n-    private PCollection<String> write(PCollection<T> input, String stagingBucketDir) {\n+    private PCollection<T> writeStream(PCollection<T> input, String stagingBucketDir) {\n       SnowflakeService snowflakeService =\n-          getSnowflakeService() != null ? getSnowflakeService() : new SnowflakeServiceImpl();\n+          getSnowflakeService() != null\n+              ? getSnowflakeService()\n+              : new SnowflakeStreamingServiceImpl();\n+\n+      /* Ensure that files will be created after specific record count or duration specified */\n+      PCollection<T> inputInGlobalWindow =\n+          input.apply(\n+              \"rewindowIntoGlobal\",\n+              Window.<T>into(new GlobalWindows())\n+                  .triggering(\n+                      Repeatedly.forever(\n+                          AfterFirst.of(\n+                              AfterProcessingTime.pastFirstElementInPane()\n+                                  .plusDelayOf(getFlushTimeLimit()),\n+                              AfterPane.elementCountAtLeast(getFlushRowLimit()))))\n+                  .discardingFiredPanes());\n+\n+      int shards = (getShardsNumber() > 0) ? getShardsNumber() : DEFAULT_STREAMING_SHARDS_NUMBER;\n+      PCollection files = writeFiles(inputInGlobalWindow, stagingBucketDir, shards);\n+\n+      /* Ensuring that files will be ingested after flush time */\n+      files =\n+          (PCollection)\n+              files.apply(\n+                  \"applyUserTrigger\",\n+                  Window.<T>into(new GlobalWindows())\n+                      .triggering(\n+                          Repeatedly.forever(\n+                              AfterProcessingTime.pastFirstElementInPane()\n+                                  .plusDelayOf(getFlushTimeLimit())))\n+                      .discardingFiredPanes());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTI5NDQ2NA=="}, "originalCommit": {"oid": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd"}, "originalPosition": 469}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzIxOTkzMg==", "bodyText": "that makes sense to me. Thanks Kasia!", "url": "https://github.com/apache/beam/pull/12151#discussion_r463219932", "createdAt": "2020-07-30T19:24:50Z", "author": {"login": "pabloem"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -719,7 +906,9 @@ private void checkArguments() {\n                           return getUserDataMapper().mapRow(element);\n                         }\n                       }))\n-              .apply(\"Map Objects array to CSV lines\", ParDo.of(new MapObjectsArrayToCsvFn()))\n+              .apply(\n+                  \"Map Objects array to CSV lines\",\n+                  ParDo.of(new MapObjectsArrayToCsvFn(getQuotationMark())))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MTMwMzM2MQ=="}, "originalCommit": {"oid": "8a16ec3f427cb88bf59781000ed23a84b6cc4bbd"}, "originalPosition": 510}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU4NzU5NDE5", "url": "https://github.com/apache/beam/pull/12151#pullrequestreview-458759419", "createdAt": "2020-07-30T21:10:52Z", "commit": {"oid": "dd38f056d66744e5b6c6d04ede385840c0e55582"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQyMToxMDo1M1rOG5z9kw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMFQyMToxMDo1M1rOG5z9kw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzI3MzM2Mw==", "bodyText": "should these attributes also be final?", "url": "https://github.com/apache/beam/pull/12151#discussion_r463273363", "createdAt": "2020-07-30T21:10:53Z", "author": {"login": "pabloem"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/services/SnowflakeStreamingServiceConfig.java", "diffHunk": "@@ -20,26 +20,51 @@\n import java.util.List;\n import net.snowflake.ingest.SimpleIngestManager;\n \n+/** Class for preparing configuration for streaming write. */\n public class SnowflakeStreamingServiceConfig extends ServiceConfig {\n-  SimpleIngestManager ingestManager;\n-  List<String> filesList;\n-  String stagingBucketDir;\n+  private SimpleIngestManager ingestManager;\n+  private List<String> filesList;\n+  private String stagingBucketDir;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "dd38f056d66744e5b6c6d04ede385840c0e55582"}, "originalPosition": 11}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "babebae8fd6822dbf4de8f657e5761950e54e1ca", "author": {"user": {"login": "kkucharc", "name": "Kasia Kucharczyk"}}, "url": "https://github.com/apache/beam/commit/babebae8fd6822dbf4de8f657e5761950e54e1ca", "committedDate": "2020-08-05T11:23:01Z", "message": "[BEAM-9896] Added final keyword to parameters in Snowflake Batch and Streaming Configs"}, "afterCommit": {"oid": "d92ddb97147e32b0864881165c042ff033f25e5e", "author": {"user": {"login": "kkucharc", "name": "Kasia Kucharczyk"}}, "url": "https://github.com/apache/beam/commit/d92ddb97147e32b0864881165c042ff033f25e5e", "committedDate": "2020-08-05T11:26:57Z", "message": "[BEAM-9896] Added final keyword to parameters in Snowflake Batch and Streaming configs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bc980d7cfbb8421d729888b78f956ce1305a32d8", "author": {"user": {"login": "kkucharc", "name": "Kasia Kucharczyk"}}, "url": "https://github.com/apache/beam/commit/bc980d7cfbb8421d729888b78f956ce1305a32d8", "committedDate": "2020-08-05T12:53:32Z", "message": "[BEAM-9896] Updated CHANGES.md"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eadd31bba3994f4d488dec5f36a13700cd03dfcf", "author": {"user": {"login": "kkucharc", "name": "Kasia Kucharczyk"}}, "url": "https://github.com/apache/beam/commit/eadd31bba3994f4d488dec5f36a13700cd03dfcf", "committedDate": "2020-08-05T12:53:56Z", "message": "[BEAM-9896] Added final keyword to parameters in Snowflake Batch and Streaming configs"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d92ddb97147e32b0864881165c042ff033f25e5e", "author": {"user": {"login": "kkucharc", "name": "Kasia Kucharczyk"}}, "url": "https://github.com/apache/beam/commit/d92ddb97147e32b0864881165c042ff033f25e5e", "committedDate": "2020-08-05T11:26:57Z", "message": "[BEAM-9896] Added final keyword to parameters in Snowflake Batch and Streaming configs"}, "afterCommit": {"oid": "eadd31bba3994f4d488dec5f36a13700cd03dfcf", "author": {"user": {"login": "kkucharc", "name": "Kasia Kucharczyk"}}, "url": "https://github.com/apache/beam/commit/eadd31bba3994f4d488dec5f36a13700cd03dfcf", "committedDate": "2020-08-05T12:53:56Z", "message": "[BEAM-9896] Added final keyword to parameters in Snowflake Batch and Streaming configs"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3304, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}