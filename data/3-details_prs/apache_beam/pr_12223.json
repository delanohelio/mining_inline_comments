{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ3NjE0NjIz", "number": 12223, "title": "[Beam-4379] Make ParquetIO read splittable", "bodyText": "Please add a meaningful description for your change here\n\nThank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:\n\n Choose reviewer(s) and mention them in a comment (R: @username).\n Format the pull request title like [BEAM-XXX] Fixes bug in ApproximateQuantiles, where you replace BEAM-XXX with the appropriate JIRA issue, if applicable. This will automatically link the pull request to the issue.\n Update CHANGES.md with noteworthy changes.\n If this contribution is large, please file an Apache Individual Contributor License Agreement.\n\nSee the Contributor Guide for more tips on how to make review process smoother.\nPost-Commit Tests Status (on master branch)\n\n\n\nLang\nSDK\nDataflow\nFlink\nSamza\nSpark\nTwister2\n\n\n\n\nGo\n\n---\n\n---\n\n---\n\n\nJava\n\n\n\n\n\n\n\n\nPython\n\n\n\n---\n\n---\n\n\nXLang\n---\n---\n\n---\n\n---\n\n\n\nPre-Commit Tests Status (on master branch)\n\n\n\n---\nJava\nPython\nGo\nWebsite\n\n\n\n\nNon-portable\n\n\n\n\n\n\nPortable\n---\n\n---\n---\n\n\n\nSee .test-infra/jenkins/README for trigger phrase, status and link of all Jenkins jobs.", "createdAt": "2020-07-10T19:45:55Z", "url": "https://github.com/apache/beam/pull/12223", "merged": true, "mergeCommit": {"oid": "d5944974fbe128c289000acb938f0702376eda34"}, "closed": true, "closedAt": "2020-09-03T19:25:03Z", "author": {"login": "danielxjd"}, "timelineItems": {"totalCount": 54, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABczYBq9AH2gAyNDQ3NjE0NjIzOjhhYmQwM2ZkMjlkMTJiNGFlODBiMzkxODZiZTk5MzE1ZDMxY2RiYTA=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdFDsWkgFqTQ4MTM1NjEzOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "8abd03fd29d12b4ae80b39186be99315d31cdba0", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/8abd03fd29d12b4ae80b39186be99315d31cdba0", "committedDate": "2020-07-09T23:55:46Z", "message": "first approach"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f08f5092d2a417ef479a619409f7ee739af1f164", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/f08f5092d2a417ef479a619409f7ee739af1f164", "committedDate": "2020-07-10T19:42:58Z", "message": "splittable dofn"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3a72ed67f45b15d78cc35a2cfe90b730eff2c662", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/3a72ed67f45b15d78cc35a2cfe90b730eff2c662", "committedDate": "2020-07-10T19:48:12Z", "message": "Merge branch 'master' into BEAM-4379"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "528c43d4267d7a14ec32bb297914895b35f1ae6b", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/528c43d4267d7a14ec32bb297914895b35f1ae6b", "committedDate": "2020-07-10T20:08:39Z", "message": "spotless"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "589c1b5ace602ad02811ef6a2973671627cccc9f", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/589c1b5ace602ad02811ef6a2973671627cccc9f", "committedDate": "2020-07-10T20:21:59Z", "message": "git change"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a627b54bd6c3c0e9d4d1d9641ae55d0f3250c367", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/a627b54bd6c3c0e9d4d1d9641ae55d0f3250c367", "committedDate": "2020-07-10T21:33:15Z", "message": "change function name"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "01f5b8b81a72218da0a389a7174fd91fdc5a4342", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/01f5b8b81a72218da0a389a7174fd91fdc5a4342", "committedDate": "2020-07-13T21:19:35Z", "message": "fixing spittable dofn part,stoll has problem in function"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "515dbb3338559e15b937a87c32fcfccbf7e87e69", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/515dbb3338559e15b937a87c32fcfccbf7e87e69", "committedDate": "2020-07-16T19:20:52Z", "message": "bug fixed"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8d3a0f3efa853d10c6918d2eca40ad4c426ee72a", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/8d3a0f3efa853d10c6918d2eca40ad4c426ee72a", "committedDate": "2020-07-16T20:05:38Z", "message": "add build option and unit test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ce22bf692c0939f6d4c9f7ff616cfe044dd0bbc4", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/ce22bf692c0939f6d4c9f7ff616cfe044dd0bbc4", "committedDate": "2020-07-16T21:47:05Z", "message": "avro change"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUwOTA0ODI1", "url": "https://github.com/apache/beam/pull/12223#pullrequestreview-450904825", "createdAt": "2020-07-17T19:24:55Z", "commit": {"oid": "ce22bf692c0939f6d4c9f7ff616cfe044dd0bbc4"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QxOToyNDo1NlrOGzeovw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QyMDoxNTozOVrOGzf91Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYzMjUxMQ==", "bodyText": "getSplit() can be False. You can mark getSplit() as not null with False as default value.", "url": "https://github.com/apache/beam/pull/12223#discussion_r456632511", "createdAt": "2020-07-17T19:24:56Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -190,12 +227,19 @@ public Read withAvroDataModel(GenericData model) {\n     @Override\n     public PCollection<GenericRecord> expand(PBegin input) {\n       checkNotNull(getFilepattern(), \"Filepattern cannot be null.\");\n-\n-      return input\n-          .apply(\"Create filepattern\", Create.ofProvider(getFilepattern(), StringUtf8Coder.of()))\n-          .apply(FileIO.matchAll())\n-          .apply(FileIO.readMatches())\n-          .apply(readFiles(getSchema()).withAvroDataModel(getAvroDataModel()));\n+      if (getSplit() != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ce22bf692c0939f6d4c9f7ff616cfe044dd0bbc4"}, "originalPosition": 111}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjYzMjc5Mw==", "bodyText": "Same above.", "url": "https://github.com/apache/beam/pull/12223#discussion_r456632793", "createdAt": "2020-07-17T19:25:36Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -235,12 +284,151 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (getSplit() != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ce22bf692c0939f6d4c9f7ff616cfe044dd0bbc4"}, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY0MTIwNw==", "bodyText": "L231-L234 and L237-L240 are the same. We can make them as common part and set readFiles differently.", "url": "https://github.com/apache/beam/pull/12223#discussion_r456641207", "createdAt": "2020-07-17T19:45:22Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -190,12 +227,19 @@ public Read withAvroDataModel(GenericData model) {\n     @Override\n     public PCollection<GenericRecord> expand(PBegin input) {\n       checkNotNull(getFilepattern(), \"Filepattern cannot be null.\");\n-\n-      return input\n-          .apply(\"Create filepattern\", Create.ofProvider(getFilepattern(), StringUtf8Coder.of()))\n-          .apply(FileIO.matchAll())\n-          .apply(FileIO.readMatches())\n-          .apply(readFiles(getSchema()).withAvroDataModel(getAvroDataModel()));\n+      if (getSplit() != null) {\n+        return input", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ce22bf692c0939f6d4c9f7ff616cfe044dd0bbc4"}, "originalPosition": 112}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY0MjcxMQ==", "bodyText": "you can replace filename  with file.getMetadata().resourceId() directly instead of creating a local var.", "url": "https://github.com/apache/beam/pull/12223#discussion_r456642711", "createdAt": "2020-07-17T19:48:40Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -235,12 +284,151 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (getSplit() != null) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          ResourceId filename = file.getMetadata().resourceId();\n+          throw new RuntimeException(String.format(\"File has to be seekable: %s\", filename));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ce22bf692c0939f6d4c9f7ff616cfe044dd0bbc4"}, "originalPosition": 199}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY0NTEyNw==", "bodyText": "Are you sure you want to split the OffsetRange initially into one offset per range? It seems too much to me. Also one offset per range means no more further split could happen.", "url": "https://github.com/apache/beam/pull/12223#discussion_r456645127", "createdAt": "2020-07-17T19:54:00Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -235,12 +284,151 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (getSplit() != null) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          ResourceId filename = file.getMetadata().resourceId();\n+          throw new RuntimeException(String.format(\"File has to be seekable: %s\", filename));\n+        }\n+\n+        SeekableByteChannel seekableByteChannel = file.openSeekable();\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = new BeamParquetInputFile(seekableByteChannel);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        for (int i = 0; i < reader.getRowGroups().size(); i++) {\n+          if (i < tracker.currentRestriction().getFrom()) {\n+            reader.skipNextRowGroup();\n+            continue;\n+          }\n+          if (tracker.tryClaim((long) i)) {\n+            PageReadStore pages = reader.readNextRowGroup();\n+            i += 1;\n+            RecordReader<GenericRecord> recordReader =\n+                columnIO.getRecordReader(\n+                    pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+            GenericRecord read;\n+            long current = 0;\n+            long totalRows = pages.getRowCount();\n+            while (current < totalRows) {\n+              read = recordReader.read();\n+              outputReceiver.output(read);\n+              current += 1;\n+            }\n+          } else {\n+            break;\n+          }\n+        }\n+      }\n+\n+      private Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          ResourceId filename = file.getMetadata().resourceId();\n+          throw new RuntimeException(String.format(\"File has to be seekable: %s\", filename));\n+        }\n+        SeekableByteChannel seekableByteChannel = file.openSeekable();\n+        InputFile inputFile = new BeamParquetInputFile(seekableByteChannel);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(@Restriction OffsetRange restriction, OutputReceiver<OffsetRange> out) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ce22bf692c0939f6d4c9f7ff616cfe044dd0bbc4"}, "originalPosition": 288}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY1MzkzMA==", "bodyText": "I would get the start position by tracker.currentRestriction().getFrom() and have a util function to move the cursor of reader to such index. Then you can do a while loop like while(tracker.tryClaim(i)).", "url": "https://github.com/apache/beam/pull/12223#discussion_r456653930", "createdAt": "2020-07-17T20:14:47Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -235,12 +284,151 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (getSplit() != null) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          ResourceId filename = file.getMetadata().resourceId();\n+          throw new RuntimeException(String.format(\"File has to be seekable: %s\", filename));\n+        }\n+\n+        SeekableByteChannel seekableByteChannel = file.openSeekable();\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = new BeamParquetInputFile(seekableByteChannel);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        for (int i = 0; i < reader.getRowGroups().size(); i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ce22bf692c0939f6d4c9f7ff616cfe044dd0bbc4"}, "originalPosition": 233}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY1NDI5Mw==", "bodyText": "You could do  outputReceiver.output(recordReader.read()) to get rid of local var.", "url": "https://github.com/apache/beam/pull/12223#discussion_r456654293", "createdAt": "2020-07-17T20:15:39Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -235,12 +284,151 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (getSplit() != null) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          ResourceId filename = file.getMetadata().resourceId();\n+          throw new RuntimeException(String.format(\"File has to be seekable: %s\", filename));\n+        }\n+\n+        SeekableByteChannel seekableByteChannel = file.openSeekable();\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = new BeamParquetInputFile(seekableByteChannel);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        for (int i = 0; i < reader.getRowGroups().size(); i++) {\n+          if (i < tracker.currentRestriction().getFrom()) {\n+            reader.skipNextRowGroup();\n+            continue;\n+          }\n+          if (tracker.tryClaim((long) i)) {\n+            PageReadStore pages = reader.readNextRowGroup();\n+            i += 1;\n+            RecordReader<GenericRecord> recordReader =\n+                columnIO.getRecordReader(\n+                    pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+            GenericRecord read;\n+            long current = 0;\n+            long totalRows = pages.getRowCount();\n+            while (current < totalRows) {\n+              read = recordReader.read();\n+              outputReceiver.output(read);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ce22bf692c0939f6d4c9f7ff616cfe044dd0bbc4"}, "originalPosition": 249}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0c164c34ce06e6b8450357d3c6f7c16553c0cd11", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/0c164c34ce06e6b8450357d3c6f7c16553c0cd11", "committedDate": "2020-07-17T21:13:05Z", "message": "remove replicate codes"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUwOTk1OTgx", "url": "https://github.com/apache/beam/pull/12223#pullrequestreview-450995981", "createdAt": "2020-07-17T23:00:46Z", "commit": {"oid": "0c164c34ce06e6b8450357d3c6f7c16553c0cd11"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xN1QyMzowMDo0NlrOGzjOcQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xOFQwMzozMzoyNFrOGzlY_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcwNzY5Nw==", "bodyText": "You will also need to implement @ GetRestrictionCoder which returns a OffsetRange.Coder. Also you may want to consider implementing @GetSize to give a better sizing information.\nFor more references:\nhttps://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/DoFn.java#L661-L698\nhttps://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/DoFn.java#L1006\nhttps://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/DoFn.java#L995", "url": "https://github.com/apache/beam/pull/12223#discussion_r456707697", "createdAt": "2020-07-17T23:00:46Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -235,12 +281,147 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c164c34ce06e6b8450357d3c6f7c16553c0cd11"}, "originalPosition": 187}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjcxMzM5MA==", "bodyText": "It's possible that model is null. Is it expected?", "url": "https://github.com/apache/beam/pull/12223#discussion_r456713390", "createdAt": "2020-07-17T23:26:53Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -235,12 +281,147 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      private InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0c164c34ce06e6b8450357d3c6f7c16553c0cd11"}, "originalPosition": 227}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Njc0MzE2NQ==", "bodyText": "Yeah I understand. You could do something like:\ncurrent = tracker.currentRestriction().getFrom();\nmoveReadToCurrent(reader, current);\nwhile(tracker.tryClaim(current)) {\n  doSomething;\n  current += 1;\n}", "url": "https://github.com/apache/beam/pull/12223#discussion_r456743165", "createdAt": "2020-07-18T03:33:24Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -235,12 +284,151 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (getSplit() != null) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          ResourceId filename = file.getMetadata().resourceId();\n+          throw new RuntimeException(String.format(\"File has to be seekable: %s\", filename));\n+        }\n+\n+        SeekableByteChannel seekableByteChannel = file.openSeekable();\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = new BeamParquetInputFile(seekableByteChannel);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        for (int i = 0; i < reader.getRowGroups().size(); i++) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY1MzkzMA=="}, "originalCommit": {"oid": "ce22bf692c0939f6d4c9f7ff616cfe044dd0bbc4"}, "originalPosition": 233}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2d529b85b19ffc263c2285fcfacc7dfb4446497e", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/2d529b85b19ffc263c2285fcfacc7dfb4446497e", "committedDate": "2020-07-18T19:45:34Z", "message": "add getSize"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUxOTgyMjE5", "url": "https://github.com/apache/beam/pull/12223#pullrequestreview-451982219", "createdAt": "2020-07-20T21:46:47Z", "commit": {"oid": "2d529b85b19ffc263c2285fcfacc7dfb4446497e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQyMTo0Njo0N1rOG0gaMg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQyMTo0Njo0N1rOG0gaMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcxMDEzMA==", "bodyText": "If the restriction is null, it should means there is no more work.", "url": "https://github.com/apache/beam/pull/12223#discussion_r457710130", "createdAt": "2020-07-20T21:46:47Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -235,12 +283,164 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      private InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+        while (tracker.tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          GenericRecord read;\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            outputReceiver.output(recordReader.read());\n+            currentRow += 1;\n+          }\n+        }\n+      }\n+\n+      private Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(@Restriction OffsetRange restriction, OutputReceiver<OffsetRange> out) {\n+        for (OffsetRange range : restriction.split(1, 0)) {\n+          out.output(range);\n+        }\n+      }\n+\n+      @NewTracker\n+      public OffsetRangeTracker newTracker(@Restriction OffsetRange restriction) {\n+        return new OffsetRangeTracker(restriction);\n+      }\n+\n+      @GetSize\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        if (restriction == null) {\n+          return reader.getRecordCount();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d529b85b19ffc263c2285fcfacc7dfb4446497e"}, "originalPosition": 316}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDUxOTg1MjM4", "url": "https://github.com/apache/beam/pull/12223#pullrequestreview-451985238", "createdAt": "2020-07-20T21:52:28Z", "commit": {"oid": "2d529b85b19ffc263c2285fcfacc7dfb4446497e"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQyMTo1MjoyOFrOG0gj3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMFQyMTo1MjoyOFrOG0gj3g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcxMjYwNg==", "bodyText": "If it's feasible, it would be nice to track an estimated avg row size like KafkaIO. Then the size will be avgSize * rowCount. @chamikaramj Do you think it's necessary to do so?", "url": "https://github.com/apache/beam/pull/12223#discussion_r457712606", "createdAt": "2020-07-20T21:52:28Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -235,12 +283,164 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      private InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+        while (tracker.tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          GenericRecord read;\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            outputReceiver.output(recordReader.read());\n+            currentRow += 1;\n+          }\n+        }\n+      }\n+\n+      private Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(@Restriction OffsetRange restriction, OutputReceiver<OffsetRange> out) {\n+        for (OffsetRange range : restriction.split(1, 0)) {\n+          out.output(range);\n+        }\n+      }\n+\n+      @NewTracker\n+      public OffsetRangeTracker newTracker(@Restriction OffsetRange restriction) {\n+        return new OffsetRangeTracker(restriction);\n+      }\n+\n+      @GetSize\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        if (restriction == null) {\n+          return reader.getRecordCount();\n+        } else {\n+          long start = restriction.getFrom();\n+          long end = restriction.getTo();\n+          List<BlockMetaData> blocks = reader.getRowGroups();\n+          double size = 0;\n+          for (long i = start; i < end; i++) {\n+            size += blocks.get((int) i).getRowCount();\n+          }\n+          return size;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2d529b85b19ffc263c2285fcfacc7dfb4446497e"}, "originalPosition": 325}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d9f7709307f5456fd286c524ace4688255d19fbe", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/d9f7709307f5456fd286c524ace4688255d19fbe", "committedDate": "2020-07-21T18:10:40Z", "message": "set coder"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f507a33b506eb7c0aac011b4b021eb22dfffb277", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/f507a33b506eb7c0aac011b4b021eb22dfffb277", "committedDate": "2020-07-27T21:06:46Z", "message": "change"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDU2MTYwNDk1", "url": "https://github.com/apache/beam/pull/12223#pullrequestreview-456160495", "createdAt": "2020-07-27T21:49:58Z", "commit": {"oid": "f507a33b506eb7c0aac011b4b021eb22dfffb277"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QyMTo0OTo1OFrOG3024Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QyMTo1Mjo0N1rOG3073A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTE5MDg4MQ==", "bodyText": "Usual pattern for files is to initial split into 64MB blocks and dynamic split further as needed. I believe this is the tried and tested method for existing file-based sources. This might require changing to a tracker that use byte positions (0 to size of file) instead of a tracker that goes from 0 to number of blocks.", "url": "https://github.com/apache/beam/pull/12223#discussion_r461190881", "createdAt": "2020-07-27T21:49:58Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -235,12 +284,151 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (getSplit() != null) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          ResourceId filename = file.getMetadata().resourceId();\n+          throw new RuntimeException(String.format(\"File has to be seekable: %s\", filename));\n+        }\n+\n+        SeekableByteChannel seekableByteChannel = file.openSeekable();\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = new BeamParquetInputFile(seekableByteChannel);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        for (int i = 0; i < reader.getRowGroups().size(); i++) {\n+          if (i < tracker.currentRestriction().getFrom()) {\n+            reader.skipNextRowGroup();\n+            continue;\n+          }\n+          if (tracker.tryClaim((long) i)) {\n+            PageReadStore pages = reader.readNextRowGroup();\n+            i += 1;\n+            RecordReader<GenericRecord> recordReader =\n+                columnIO.getRecordReader(\n+                    pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+            GenericRecord read;\n+            long current = 0;\n+            long totalRows = pages.getRowCount();\n+            while (current < totalRows) {\n+              read = recordReader.read();\n+              outputReceiver.output(read);\n+              current += 1;\n+            }\n+          } else {\n+            break;\n+          }\n+        }\n+      }\n+\n+      private Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          ResourceId filename = file.getMetadata().resourceId();\n+          throw new RuntimeException(String.format(\"File has to be seekable: %s\", filename));\n+        }\n+        SeekableByteChannel seekableByteChannel = file.openSeekable();\n+        InputFile inputFile = new BeamParquetInputFile(seekableByteChannel);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(@Restriction OffsetRange restriction, OutputReceiver<OffsetRange> out) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NjY0NTEyNw=="}, "originalCommit": {"oid": "ce22bf692c0939f6d4c9f7ff616cfe044dd0bbc4"}, "originalPosition": 288}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTE5MTcxNw==", "bodyText": "If you use byte sizes for tracking implementation here will end up being trivial.", "url": "https://github.com/apache/beam/pull/12223#discussion_r461191717", "createdAt": "2020-07-27T21:51:46Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -235,12 +283,164 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      private InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+        while (tracker.tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          GenericRecord read;\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            outputReceiver.output(recordReader.read());\n+            currentRow += 1;\n+          }\n+        }\n+      }\n+\n+      private Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(@Restriction OffsetRange restriction, OutputReceiver<OffsetRange> out) {\n+        for (OffsetRange range : restriction.split(1, 0)) {\n+          out.output(range);\n+        }\n+      }\n+\n+      @NewTracker\n+      public OffsetRangeTracker newTracker(@Restriction OffsetRange restriction) {\n+        return new OffsetRangeTracker(restriction);\n+      }\n+\n+      @GetSize\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        if (restriction == null) {\n+          return reader.getRecordCount();\n+        } else {\n+          long start = restriction.getFrom();\n+          long end = restriction.getTo();\n+          List<BlockMetaData> blocks = reader.getRowGroups();\n+          double size = 0;\n+          for (long i = start; i < end; i++) {\n+            size += blocks.get((int) i).getRowCount();\n+          }\n+          return size;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzcxMjYwNg=="}, "originalCommit": {"oid": "2d529b85b19ffc263c2285fcfacc7dfb4446497e"}, "originalPosition": 325}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTE5MjE1Ng==", "bodyText": "Does this mean that we will be incrementing progress (obtained by the runner through restriction tracker) only when a new block  is read ? This might be too coarse grained.", "url": "https://github.com/apache/beam/pull/12223#discussion_r461192156", "createdAt": "2020-07-27T21:52:47Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -235,12 +277,195 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      private InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+        while (tracker.tryClaim(currentBlock)) {\n+\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+                  columnIO.getRecordReader(\n+                          pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+              throw new ParquetDecodingException(format(\"Can not read value at %d in block %d in file %s\", currentRow, currentBlock, file.toString()), e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+\n+\n+\n+      }\n+\n+      private Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f507a33b506eb7c0aac011b4b021eb22dfffb277"}, "originalPosition": 319}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7b93829dab22a33226dc313d1231001c9c334416", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/7b93829dab22a33226dc313d1231001c9c334416", "committedDate": "2020-07-27T22:51:55Z", "message": "Merge branch 'master' into BEAM-4379"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a44061cf10d23309e8bc418147c04e54993ac6e3", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/a44061cf10d23309e8bc418147c04e54993ac6e3", "committedDate": "2020-07-28T16:42:28Z", "message": "spotless"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "03ba0c3e9056eb25d66f61554bc13c7605593737", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/03ba0c3e9056eb25d66f61554bc13c7605593737", "committedDate": "2020-07-28T16:43:09Z", "message": "Merge branch 'BEAM-4379' of https://github.com/danielxjd/beam into BEAM-4379"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "55c17ead8439b058b5418c3332abdc88572db4ab", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/55c17ead8439b058b5418c3332abdc88572db4ab", "committedDate": "2020-07-28T16:43:39Z", "message": "spotless"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "62b6c6362a62ae1d5f54899111761db2b7484268", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/62b6c6362a62ae1d5f54899111761db2b7484268", "committedDate": "2020-07-28T16:44:24Z", "message": "spotless"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5641c96a41a7de8f6088a9de3b385b51adee66b1", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/5641c96a41a7de8f6088a9de3b385b51adee66b1", "committedDate": "2020-07-28T16:49:04Z", "message": "spotless"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "032715d1f30f2038bcada16c1402f0385d31b6ed", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/032715d1f30f2038bcada16c1402f0385d31b6ed", "committedDate": "2020-07-28T18:08:52Z", "message": "add block tracker"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "99cd39ccaf60b0f0ace7d3c1e4be3c04e6e52d59", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/99cd39ccaf60b0f0ace7d3c1e4be3c04e6e52d59", "committedDate": "2020-07-30T20:41:32Z", "message": "change split function"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYwMzM3NDQy", "url": "https://github.com/apache/beam/pull/12223#pullrequestreview-460337442", "createdAt": "2020-08-03T21:00:06Z", "commit": {"oid": "99cd39ccaf60b0f0ace7d3c1e4be3c04e6e52d59"}, "state": "COMMENTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMTowMDowN1rOG7Ij1w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMToyMzozOFrOG7JL1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY1OTQxNQ==", "bodyText": "Why do we need this option ? We should be able to safely always enable splitting for runners that support SDF without an explicit flag from the user.", "url": "https://github.com/apache/beam/pull/12223#discussion_r464659415", "createdAt": "2020-08-03T21:00:07Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -177,6 +214,10 @@ public Read from(String filepattern) {\n       return from(ValueProvider.StaticValueProvider.of(filepattern));\n     }\n \n+    public Read withSplit() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "99cd39ccaf60b0f0ace7d3c1e4be3c04e6e52d59"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY1OTYzNw==", "bodyText": "We should be able to get rid of this fork by using a single DoFn.", "url": "https://github.com/apache/beam/pull/12223#discussion_r464659637", "createdAt": "2020-08-03T21:00:32Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,12 +281,271 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "99cd39ccaf60b0f0ace7d3c1e4be3c04e6e52d59"}, "originalPosition": 183}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY2MDcwOQ==", "bodyText": "Why do we need a separate DoFn ? We should be able to use the same DoFn but implement SDF specific methods as well (which will also allow us to better reuse code).", "url": "https://github.com/apache/beam/pull/12223#discussion_r464660709", "createdAt": "2020-08-03T21:02:55Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,12 +281,271 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "99cd39ccaf60b0f0ace7d3c1e4be3c04e6e52d59"}, "originalPosition": 195}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY2NDQ1MA==", "bodyText": "How expensive will this be ? Note that input file can be arbitrary long (for example, petabytes) and we don't want the newTracker() call to hang forever.  BTW I think we can safely assume that records are of equal size. So if we can more efficiently determine the number of records per block/row group that should be adequate (for example, we do this for Avro).", "url": "https://github.com/apache/beam/pull/12223#discussion_r464664450", "createdAt": "2020-08-03T21:11:34Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,12 +281,271 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      private InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+        while (tracker.tryClaim(currentBlock)) {\n+\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      private Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        long start = restriction.getFrom();\n+        long end = restriction.getFrom();\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        long totalSize = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          totalSize += rowGroups.get((int) i).getTotalByteSize();\n+          end += 1;\n+          if (totalSize > SPLIT_LIMIT) {\n+            start = end;\n+            totalSize = 0;\n+            out.output(new OffsetRange(start, end));\n+          }\n+        }\n+        if (totalSize != 0) {\n+          out.output(new OffsetRange(start, end));\n+        }\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        if (restriction == null) {\n+          return 0;\n+        } else {\n+          long recordCount = 0;\n+          for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "99cd39ccaf60b0f0ace7d3c1e4be3c04e6e52d59"}, "originalPosition": 385}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY2NTY0Ng==", "bodyText": "Nice!", "url": "https://github.com/apache/beam/pull/12223#discussion_r464665646", "createdAt": "2020-08-03T21:14:19Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,12 +281,271 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      private InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+        while (tracker.tryClaim(currentBlock)) {\n+\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "99cd39ccaf60b0f0ace7d3c1e4be3c04e6e52d59"}, "originalPosition": 294}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY2NzMyOQ==", "bodyText": "Let's call this approximateRecordSize", "url": "https://github.com/apache/beam/pull/12223#discussion_r464667329", "createdAt": "2020-08-03T21:17:57Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,12 +281,271 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      private InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+        while (tracker.tryClaim(currentBlock)) {\n+\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      private Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        long start = restriction.getFrom();\n+        long end = restriction.getFrom();\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        long totalSize = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          totalSize += rowGroups.get((int) i).getTotalByteSize();\n+          end += 1;\n+          if (totalSize > SPLIT_LIMIT) {\n+            start = end;\n+            totalSize = 0;\n+            out.output(new OffsetRange(start, end));\n+          }\n+        }\n+        if (totalSize != 0) {\n+          out.output(new OffsetRange(start, end));\n+        }\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        if (restriction == null) {\n+          return 0;\n+        } else {\n+          long recordCount = 0;\n+          for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+            recordCount += reader.getRowGroups().get((int) i).getRowCount();\n+          }\n+          return recordCount;\n+        }\n+      }\n+\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        if (restriction == null) {\n+          return 0;\n+        } else {\n+          double size = 0;\n+          for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+            size += reader.getRowGroups().get((int) i).getTotalByteSize();\n+          }\n+          return size;\n+        }\n+      }\n+    }\n+\n+    private static class BlockTracker extends OffsetRangeTracker {\n+      private long totalWork;\n+      private long progress;\n+      private long recordSize;\n+\n+      public BlockTracker(OffsetRange range, long totalWork, long recordCount) {\n+        super(range);\n+        this.recordSize = totalWork / recordCount;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "99cd39ccaf60b0f0ace7d3c1e4be3c04e6e52d59"}, "originalPosition": 417}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY2NzcyMg==", "bodyText": "this.totalWork = totalWork", "url": "https://github.com/apache/beam/pull/12223#discussion_r464667722", "createdAt": "2020-08-03T21:18:56Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,12 +281,271 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      private InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+        while (tracker.tryClaim(currentBlock)) {\n+\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      private Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        long start = restriction.getFrom();\n+        long end = restriction.getFrom();\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        long totalSize = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          totalSize += rowGroups.get((int) i).getTotalByteSize();\n+          end += 1;\n+          if (totalSize > SPLIT_LIMIT) {\n+            start = end;\n+            totalSize = 0;\n+            out.output(new OffsetRange(start, end));\n+          }\n+        }\n+        if (totalSize != 0) {\n+          out.output(new OffsetRange(start, end));\n+        }\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        if (restriction == null) {\n+          return 0;\n+        } else {\n+          long recordCount = 0;\n+          for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+            recordCount += reader.getRowGroups().get((int) i).getRowCount();\n+          }\n+          return recordCount;\n+        }\n+      }\n+\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        if (restriction == null) {\n+          return 0;\n+        } else {\n+          double size = 0;\n+          for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+            size += reader.getRowGroups().get((int) i).getTotalByteSize();\n+          }\n+          return size;\n+        }\n+      }\n+    }\n+\n+    private static class BlockTracker extends OffsetRangeTracker {\n+      private long totalWork;\n+      private long progress;\n+      private long recordSize;\n+\n+      public BlockTracker(OffsetRange range, long totalWork, long recordCount) {\n+        super(range);\n+        this.recordSize = totalWork / recordCount;\n+        this.totalWork = recordSize * recordCount;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "99cd39ccaf60b0f0ace7d3c1e4be3c04e6e52d59"}, "originalPosition": 418}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY2ODQ5NA==", "bodyText": "If restriction is null that means we'll be reading the whole file. So what we return here should be the size of the file I believe.", "url": "https://github.com/apache/beam/pull/12223#discussion_r464668494", "createdAt": "2020-08-03T21:20:44Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,12 +281,271 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      private InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+        while (tracker.tryClaim(currentBlock)) {\n+\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      private Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        long start = restriction.getFrom();\n+        long end = restriction.getFrom();\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        long totalSize = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          totalSize += rowGroups.get((int) i).getTotalByteSize();\n+          end += 1;\n+          if (totalSize > SPLIT_LIMIT) {\n+            start = end;\n+            totalSize = 0;\n+            out.output(new OffsetRange(start, end));\n+          }\n+        }\n+        if (totalSize != 0) {\n+          out.output(new OffsetRange(start, end));\n+        }\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        if (restriction == null) {\n+          return 0;\n+        } else {\n+          long recordCount = 0;\n+          for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+            recordCount += reader.getRowGroups().get((int) i).getRowCount();\n+          }\n+          return recordCount;\n+        }\n+      }\n+\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        if (restriction == null) {\n+          return 0;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "99cd39ccaf60b0f0ace7d3c1e4be3c04e6e52d59"}, "originalPosition": 399}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY2ODg2MQ==", "bodyText": "How expensive is this operation ?", "url": "https://github.com/apache/beam/pull/12223#discussion_r464668861", "createdAt": "2020-08-03T21:21:39Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,12 +281,271 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n-      return input\n-          .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n-          .setCoder(AvroCoder.of(getSchema()));\n+      if (!getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      } else {\n+        return input\n+            .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n+    }\n+\n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      private InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          conf.set(property, options.getProperty(property));\n+        }\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+        while (tracker.tryClaim(currentBlock)) {\n+\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      private Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        long start = restriction.getFrom();\n+        long end = restriction.getFrom();\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        long totalSize = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          totalSize += rowGroups.get((int) i).getTotalByteSize();\n+          end += 1;\n+          if (totalSize > SPLIT_LIMIT) {\n+            start = end;\n+            totalSize = 0;\n+            out.output(new OffsetRange(start, end));\n+          }\n+        }\n+        if (totalSize != 0) {\n+          out.output(new OffsetRange(start, end));\n+        }\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        if (restriction == null) {\n+          return 0;\n+        } else {\n+          long recordCount = 0;\n+          for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+            recordCount += reader.getRowGroups().get((int) i).getRowCount();\n+          }\n+          return recordCount;\n+        }\n+      }\n+\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        if (restriction == null) {\n+          return 0;\n+        } else {\n+          double size = 0;\n+          for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+            size += reader.getRowGroups().get((int) i).getTotalByteSize();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "99cd39ccaf60b0f0ace7d3c1e4be3c04e6e52d59"}, "originalPosition": 403}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY2OTY1Mw==", "bodyText": "Please add a comprehensive set of unit tests for ParquetIO and BlockTracker splitting and progress tracking. You can use following for inspiration.\nhttps://github.com/apache/beam/blob/master/sdks/java/core/src/test/java/org/apache/beam/sdk/io/AvroSourceTest.java", "url": "https://github.com/apache/beam/pull/12223#discussion_r464669653", "createdAt": "2020-08-03T21:23:38Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java", "diffHunk": "@@ -84,11 +85,31 @@ public void testWriteAndRead() {\n     PCollection<GenericRecord> readBack =\n         readPipeline.apply(\n             ParquetIO.read(SCHEMA).from(temporaryFolder.getRoot().getAbsolutePath() + \"/*\"));\n-\n     PAssert.that(readBack).containsInAnyOrder(records);\n     readPipeline.run().waitUntilFinish();\n   }\n \n+  @Test\n+  public void testWriteAndReadWithSplit() {\n+    List<GenericRecord> records = generateGenericRecords(1000);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "99cd39ccaf60b0f0ace7d3c1e4be3c04e6e52d59"}, "originalPosition": 21}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b3ec41b3bac0a1b37a537880aabc29faba8d7ccb", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/b3ec41b3bac0a1b37a537880aabc29faba8d7ccb", "committedDate": "2020-08-04T19:38:08Z", "message": "unit test added"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b48b28cade88ab3e40ff355edb07d85051678dc0", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/b48b28cade88ab3e40ff355edb07d85051678dc0", "committedDate": "2020-08-04T22:49:54Z", "message": "[Beam-4379] add exception handling"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a42d3ee599da1c0b8ba9e2703a3dc83e4feaedfd", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/a42d3ee599da1c0b8ba9e2703a3dc83e4feaedfd", "committedDate": "2020-08-05T21:28:12Z", "message": "[Beam-4379]Add unit test"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYyMDQzNjE0", "url": "https://github.com/apache/beam/pull/12223#pullrequestreview-462043614", "createdAt": "2020-08-05T21:40:18Z", "commit": {"oid": "b48b28cade88ab3e40ff355edb07d85051678dc0"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMTo0MDoxOFrOG8bm0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMTo0Mjo1NVrOG8brKQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAyMDA0OA==", "bodyText": "Should it be if (getSplit()) ?", "url": "https://github.com/apache/beam/pull/12223#discussion_r466020048", "createdAt": "2020-08-05T21:40:18Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -187,12 +229,18 @@ public Read withAvroDataModel(GenericData model) {\n     @Override\n     public PCollection<GenericRecord> expand(PBegin input) {\n       checkNotNull(getFilepattern(), \"Filepattern cannot be null.\");\n-\n-      return input\n-          .apply(\"Create filepattern\", Create.ofProvider(getFilepattern(), StringUtf8Coder.of()))\n-          .apply(FileIO.matchAll())\n-          .apply(FileIO.readMatches())\n-          .apply(readFiles(getSchema()).withAvroDataModel(getAvroDataModel()));\n+      PCollection<FileIO.ReadableFile> inputFiles =\n+          input\n+              .apply(\n+                  \"Create filepattern\", Create.ofProvider(getFilepattern(), StringUtf8Coder.of()))\n+              .apply(FileIO.matchAll())\n+              .apply(FileIO.readMatches());\n+      if (!getSplit()) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b48b28cade88ab3e40ff355edb07d85051678dc0"}, "originalPosition": 143}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAyMDQ0Ng==", "bodyText": "isSplittable?", "url": "https://github.com/apache/beam/pull/12223#discussion_r466020446", "createdAt": "2020-08-05T21:41:19Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -154,10 +187,15 @@ public static ReadFiles readFiles(Schema schema) {\n \n     abstract @Nullable GenericData getAvroDataModel();\n \n+    abstract boolean getSplit();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b48b28cade88ab3e40ff355edb07d85051678dc0"}, "originalPosition": 104}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAyMTE2MQ==", "bodyText": "We can drop this redundant else.", "url": "https://github.com/apache/beam/pull/12223#discussion_r466021161", "createdAt": "2020-08-05T21:42:55Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -187,12 +229,18 @@ public Read withAvroDataModel(GenericData model) {\n     @Override\n     public PCollection<GenericRecord> expand(PBegin input) {\n       checkNotNull(getFilepattern(), \"Filepattern cannot be null.\");\n-\n-      return input\n-          .apply(\"Create filepattern\", Create.ofProvider(getFilepattern(), StringUtf8Coder.of()))\n-          .apply(FileIO.matchAll())\n-          .apply(FileIO.readMatches())\n-          .apply(readFiles(getSchema()).withAvroDataModel(getAvroDataModel()));\n+      PCollection<FileIO.ReadableFile> inputFiles =\n+          input\n+              .apply(\n+                  \"Create filepattern\", Create.ofProvider(getFilepattern(), StringUtf8Coder.of()))\n+              .apply(FileIO.matchAll())\n+              .apply(FileIO.readMatches());\n+      if (!getSplit()) {\n+        return inputFiles.apply(\n+            readFiles(getSchema()).withSplit().withAvroDataModel(getAvroDataModel()));\n+      } else {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b48b28cade88ab3e40ff355edb07d85051678dc0"}, "originalPosition": 146}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/8b8194f52cc0971ba5f2befcf91aba01a5556485", "committedDate": "2020-08-06T22:23:51Z", "message": "delete else"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDYyOTQ2MjI0", "url": "https://github.com/apache/beam/pull/12223#pullrequestreview-462946224", "createdAt": "2020-08-06T23:38:25Z", "commit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 23, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMzozODoyNVrOG9HppA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QwMzoxMzowNVrOG9LBqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc0MTY2OA==", "bodyText": "What's the unit of SPLIT_LIMIT? Please put a comment that how to interpret the number.", "url": "https://github.com/apache/beam/pull/12223#discussion_r466741668", "createdAt": "2020-08-06T23:38:25Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 194}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc0NTQyMA==", "bodyText": "one liner: Maps.transformValues(map, ImmutableSet::of)", "url": "https://github.com/apache/beam/pull/12223#discussion_r466745420", "createdAt": "2020-08-06T23:51:47Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 201}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc0NjEwMQ==", "bodyText": "one liner: new BeamParquetInputFile(file.openSeekable())\nisReadSeekEfficient check is not necessary.", "url": "https://github.com/apache/beam/pull/12223#discussion_r466746101", "createdAt": "2020-08-06T23:54:23Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 211}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc1NDAwMA==", "bodyText": "I think this method needs better name that explains what it exactly does e.g. getConfWithDefaultAvroCompatibility", "url": "https://github.com/apache/beam/pull/12223#discussion_r466754000", "createdAt": "2020-08-07T00:23:16Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 303}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc1NTM2MQ==", "bodyText": "Should we create a new instance of Configuration  and ParquetReadOptions multiple times in each method? Is it possible to make options an instance variable?", "url": "https://github.com/apache/beam/pull/12223#discussion_r466755361", "createdAt": "2020-08-07T00:28:39Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 321}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc1NTg2Mw==", "bodyText": "please avoid reassignment. use conf from above or create a new variable with a meaningful name.", "url": "https://github.com/apache/beam/pull/12223#discussion_r466755863", "createdAt": "2020-08-07T00:30:30Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 236}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc1OTM1OA==", "bodyText": "The rhs expression could be embedded since it's only used onetime.", "url": "https://github.com/apache/beam/pull/12223#discussion_r466759358", "createdAt": "2020-08-07T00:43:58Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 247}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc1OTQzMA==", "bodyText": "ditto.", "url": "https://github.com/apache/beam/pull/12223#discussion_r466759430", "createdAt": "2020-08-07T00:44:16Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 248}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc3ODA1Mg==", "bodyText": "How many logs will this print? How important is this log? Why not debug?", "url": "https://github.com/apache/beam/pull/12223#discussion_r466778052", "createdAt": "2020-08-07T01:56:37Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 258}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc3ODg5NA==", "bodyText": "Why debug? It seems more important than debug since it's a failure.", "url": "https://github.com/apache/beam/pull/12223#discussion_r466778894", "createdAt": "2020-08-07T01:59:48Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 273}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4MDQ5OA==", "bodyText": "This log doesn't deliver enough information for debugging. Probably there will be too many of them also.", "url": "https://github.com/apache/beam/pull/12223#discussion_r466780498", "createdAt": "2020-08-07T02:06:24Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 286}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4MTA0MQ==", "bodyText": "not so much helpful. better to be something like \"finished reading # rows in row group #\"", "url": "https://github.com/apache/beam/pull/12223#discussion_r466781041", "createdAt": "2020-08-07T02:08:37Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 278}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4MTkxNw==", "bodyText": "why 3?", "url": "https://github.com/apache/beam/pull/12223#discussion_r466781917", "createdAt": "2020-08-07T02:11:59Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 3)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 340}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4MjU1Mg==", "bodyText": "Isn't rangeEnd == i+1?", "url": "https://github.com/apache/beam/pull/12223#discussion_r466782552", "createdAt": "2020-08-07T02:14:11Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 3)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 353}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4NDkyOA==", "bodyText": "These four lines\nInputFile inputFile = getInputFile(file);\nConfiguration conf = setConf();\nParquetReadOptions options = HadoopReadOptions.builder(conf).build();\nParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n\nare repeated in many methods. Could we extract them into a new method?", "url": "https://github.com/apache/beam/pull/12223#discussion_r466784928", "createdAt": "2020-08-07T02:23:48Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 3)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 385}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4NTczMg==", "bodyText": "for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {", "url": "https://github.com/apache/beam/pull/12223#discussion_r466785732", "createdAt": "2020-08-07T02:27:03Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 3)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        long start = 0;\n+        long end = 0;\n+        start = restriction.getFrom();\n+        end = restriction.getTo();\n+        long recordCount = 0;\n+        for (long i = start; i < end; i++) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 391}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4NjA3Nw==", "bodyText": "Is this downcasting safe?", "url": "https://github.com/apache/beam/pull/12223#discussion_r466786077", "createdAt": "2020-08-07T02:28:19Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 3)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        long start = 0;\n+        long end = 0;\n+        start = restriction.getFrom();\n+        end = restriction.getTo();\n+        long recordCount = 0;\n+        for (long i = start; i < end; i++) {\n+          recordCount += reader.getRowGroups().get((int) i).getRowCount();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 392}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4NjY2MA==", "bodyText": "We should merge this method with getRecordCount. Don't need to iterate twice.", "url": "https://github.com/apache/beam/pull/12223#discussion_r466786660", "createdAt": "2020-08-07T02:30:36Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 3)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        long start = 0;\n+        long end = 0;\n+        start = restriction.getFrom();\n+        end = restriction.getTo();\n+        long recordCount = 0;\n+        for (long i = start; i < end; i++) {\n+          recordCount += reader.getRowGroups().get((int) i).getRowCount();\n+        }\n+        return recordCount;\n+      }\n+\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 397}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4ODAwMQ==", "bodyText": "work is too ambiguous. Please rename it to totalBytesSize", "url": "https://github.com/apache/beam/pull/12223#discussion_r466788001", "createdAt": "2020-08-07T02:36:00Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 3)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        long start = 0;\n+        long end = 0;\n+        start = restriction.getFrom();\n+        end = restriction.getTo();\n+        long recordCount = 0;\n+        for (long i = start; i < end; i++) {\n+          recordCount += reader.getRowGroups().get((int) i).getRowCount();\n+        }\n+        return recordCount;\n+      }\n+\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        double size = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          size += reader.getRowGroups().get((int) i).getTotalByteSize();\n+        }\n+        return size;\n+      }\n+    }\n+\n+    public static class BlockTracker extends OffsetRangeTracker {\n+      private static final Logger LOG = LoggerFactory.getLogger(BlockTracker.class);\n+      private long totalWork;\n+      private long progress;\n+      private long approximateRecordSize;\n+\n+      public BlockTracker(OffsetRange range, long work, long recordCount) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 417}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4ODY4OQ==", "bodyText": "Why not just use work? What's difference between work and totalWork?", "url": "https://github.com/apache/beam/pull/12223#discussion_r466788689", "createdAt": "2020-08-07T02:38:41Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 3)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        long start = 0;\n+        long end = 0;\n+        start = restriction.getFrom();\n+        end = restriction.getTo();\n+        long recordCount = 0;\n+        for (long i = start; i < end; i++) {\n+          recordCount += reader.getRowGroups().get((int) i).getRowCount();\n+        }\n+        return recordCount;\n+      }\n+\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        double size = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          size += reader.getRowGroups().get((int) i).getTotalByteSize();\n+        }\n+        return size;\n+      }\n+    }\n+\n+    public static class BlockTracker extends OffsetRangeTracker {\n+      private static final Logger LOG = LoggerFactory.getLogger(BlockTracker.class);\n+      private long totalWork;\n+      private long progress;\n+      private long approximateRecordSize;\n+\n+      public BlockTracker(OffsetRange range, long work, long recordCount) {\n+        super(range);\n+        if (recordCount != 0) {\n+          this.approximateRecordSize = work / recordCount;\n+          this.totalWork = approximateRecordSize * recordCount;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 421}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5NTM4OQ==", "bodyText": "Do we need BigDecimal conversion? I think Progress.from(0, this.totalWork) would just work.", "url": "https://github.com/apache/beam/pull/12223#discussion_r466795389", "createdAt": "2020-08-07T03:06:32Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 3)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        long start = 0;\n+        long end = 0;\n+        start = restriction.getFrom();\n+        end = restriction.getTo();\n+        long recordCount = 0;\n+        for (long i = start; i < end; i++) {\n+          recordCount += reader.getRowGroups().get((int) i).getRowCount();\n+        }\n+        return recordCount;\n+      }\n+\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        double size = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          size += reader.getRowGroups().get((int) i).getTotalByteSize();\n+        }\n+        return size;\n+      }\n+    }\n+\n+    public static class BlockTracker extends OffsetRangeTracker {\n+      private static final Logger LOG = LoggerFactory.getLogger(BlockTracker.class);\n+      private long totalWork;\n+      private long progress;\n+      private long approximateRecordSize;\n+\n+      public BlockTracker(OffsetRange range, long work, long recordCount) {\n+        super(range);\n+        if (recordCount != 0) {\n+          this.approximateRecordSize = work / recordCount;\n+          this.totalWork = approximateRecordSize * recordCount;\n+          this.progress = 0;\n+        }\n+      }\n+\n+      public void makeProgress() throws Exception {\n+        progress += approximateRecordSize;\n+        if (progress > totalWork) {\n+          throw new IOException(\"Making progress out of range\");\n+        }\n+      }\n+\n+      @Override\n+      public Progress getProgress() {\n+        if (this.lastAttemptedOffset == null) {\n+          return Progress.from(0.0D, BigDecimal.valueOf(this.totalWork).doubleValue());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 436}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5NTk3NQ==", "bodyText": "How many logs will this print? Why not debug?", "url": "https://github.com/apache/beam/pull/12223#discussion_r466795975", "createdAt": "2020-08-07T03:08:49Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 3)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        long start = 0;\n+        long end = 0;\n+        start = restriction.getFrom();\n+        end = restriction.getTo();\n+        long recordCount = 0;\n+        for (long i = start; i < end; i++) {\n+          recordCount += reader.getRowGroups().get((int) i).getRowCount();\n+        }\n+        return recordCount;\n+      }\n+\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        double size = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          size += reader.getRowGroups().get((int) i).getTotalByteSize();\n+        }\n+        return size;\n+      }\n+    }\n+\n+    public static class BlockTracker extends OffsetRangeTracker {\n+      private static final Logger LOG = LoggerFactory.getLogger(BlockTracker.class);\n+      private long totalWork;\n+      private long progress;\n+      private long approximateRecordSize;\n+\n+      public BlockTracker(OffsetRange range, long work, long recordCount) {\n+        super(range);\n+        if (recordCount != 0) {\n+          this.approximateRecordSize = work / recordCount;\n+          this.totalWork = approximateRecordSize * recordCount;\n+          this.progress = 0;\n+        }\n+      }\n+\n+      public void makeProgress() throws Exception {\n+        progress += approximateRecordSize;\n+        if (progress > totalWork) {\n+          throw new IOException(\"Making progress out of range\");\n+        }\n+      }\n+\n+      @Override\n+      public Progress getProgress() {\n+        if (this.lastAttemptedOffset == null) {\n+          return Progress.from(0.0D, BigDecimal.valueOf(this.totalWork).doubleValue());\n+        } else {\n+          BigDecimal workRemaining =\n+              BigDecimal.valueOf(this.totalWork)\n+                  .subtract(BigDecimal.valueOf(this.progress), MathContext.DECIMAL128)\n+                  .max(BigDecimal.ZERO);\n+          BigDecimal work = BigDecimal.valueOf(this.totalWork);\n+          LOG.info(\"total work: \" + work + \" work remaining: \" + workRemaining);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 443}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc5Njk2OQ==", "bodyText": "I think we could just write Progress.from(this.progress, this.totalWork - this.progress).", "url": "https://github.com/apache/beam/pull/12223#discussion_r466796969", "createdAt": "2020-08-07T03:13:05Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 3)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        long start = 0;\n+        long end = 0;\n+        start = restriction.getFrom();\n+        end = restriction.getTo();\n+        long recordCount = 0;\n+        for (long i = start; i < end; i++) {\n+          recordCount += reader.getRowGroups().get((int) i).getRowCount();\n+        }\n+        return recordCount;\n+      }\n+\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        double size = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          size += reader.getRowGroups().get((int) i).getTotalByteSize();\n+        }\n+        return size;\n+      }\n+    }\n+\n+    public static class BlockTracker extends OffsetRangeTracker {\n+      private static final Logger LOG = LoggerFactory.getLogger(BlockTracker.class);\n+      private long totalWork;\n+      private long progress;\n+      private long approximateRecordSize;\n+\n+      public BlockTracker(OffsetRange range, long work, long recordCount) {\n+        super(range);\n+        if (recordCount != 0) {\n+          this.approximateRecordSize = work / recordCount;\n+          this.totalWork = approximateRecordSize * recordCount;\n+          this.progress = 0;\n+        }\n+      }\n+\n+      public void makeProgress() throws Exception {\n+        progress += approximateRecordSize;\n+        if (progress > totalWork) {\n+          throw new IOException(\"Making progress out of range\");\n+        }\n+      }\n+\n+      @Override\n+      public Progress getProgress() {\n+        if (this.lastAttemptedOffset == null) {\n+          return Progress.from(0.0D, BigDecimal.valueOf(this.totalWork).doubleValue());\n+        } else {\n+          BigDecimal workRemaining =\n+              BigDecimal.valueOf(this.totalWork)\n+                  .subtract(BigDecimal.valueOf(this.progress), MathContext.DECIMAL128)\n+                  .max(BigDecimal.ZERO);\n+          BigDecimal work = BigDecimal.valueOf(this.totalWork);\n+          LOG.info(\"total work: \" + work + \" work remaining: \" + workRemaining);\n+          return Progress.from(\n+              work.subtract(workRemaining, MathContext.DECIMAL128).doubleValue(),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 445}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6c0c72a299265070079e1417d63d51946c13197c", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/6c0c72a299265070079e1417d63d51946c13197c", "committedDate": "2020-08-07T18:07:21Z", "message": "extracting methods"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "63f8efa249828964ff68009b406b8bb89e8e4fa3", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/63f8efa249828964ff68009b406b8bb89e8e4fa3", "committedDate": "2020-08-07T18:27:20Z", "message": "change method names"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b554bd1079f7fada6db96b073736438691f183d3", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/b554bd1079f7fada6db96b073736438691f183d3", "committedDate": "2020-08-07T21:54:08Z", "message": "[BEAM-4379]additional unit test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a5fcb46d2c14dfd2aea8bfa48b172902916093c8", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/a5fcb46d2c14dfd2aea8bfa48b172902916093c8", "committedDate": "2020-08-12T22:59:56Z", "message": "[BEAM-4379] update logger"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b16196328f72d3812f0da9cfd6eb3d8711e7fb17", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/b16196328f72d3812f0da9cfd6eb3d8711e7fb17", "committedDate": "2020-08-26T18:16:21Z", "message": "[BEAM-4379] change info"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "efda4310ad3006b37997de227de3d2aaa0ea055a", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/efda4310ad3006b37997de227de3d2aaa0ea055a", "committedDate": "2020-08-26T18:16:59Z", "message": "[BEAM-4379] change info"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1149881eee8d9508f423a8a45306fd826ca8bbbd", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/1149881eee8d9508f423a8a45306fd826ca8bbbd", "committedDate": "2020-08-28T18:45:05Z", "message": "[BEAM-4379] change import"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc4MDA0Nzcy", "url": "https://github.com/apache/beam/pull/12223#pullrequestreview-478004772", "createdAt": "2020-08-28T20:23:12Z", "commit": {"oid": "1149881eee8d9508f423a8a45306fd826ca8bbbd"}, "state": "COMMENTED", "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMDoyMzoxMlrOHJTdDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQyMTozNDo0OVrOHJVE4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUxNzk2NA==", "bodyText": "Please add javadoc above to explain what withSplit() will do.", "url": "https://github.com/apache/beam/pull/12223#discussion_r479517964", "createdAt": "2020-08-28T20:23:12Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -133,15 +161,18 @@\n    * pattern).\n    */\n   public static Read read(Schema schema) {\n-    return new AutoValue_ParquetIO_Read.Builder().setSchema(schema).build();\n+    return new AutoValue_ParquetIO_Read.Builder().setSchema(schema).setSplittable(false).build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1149881eee8d9508f423a8a45306fd826ca8bbbd"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUxODM3Nw==", "bodyText": "Do we really need this log?", "url": "https://github.com/apache/beam/pull/12223#discussion_r479518377", "createdAt": "2020-08-28T20:24:18Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +279,255 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.info(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1149881eee8d9508f423a8a45306fd826ca8bbbd"}, "originalPosition": 210}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUyMzc2NA==", "bodyText": "Same above. I'm thinking about whether we really these INFO log since it looks too much if per element per offset.", "url": "https://github.com/apache/beam/pull/12223#discussion_r479523764", "createdAt": "2020-08-28T20:38:00Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +279,255 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.info(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          LOG.info(\"block {} read in memory. row count = {}\", currentBlock, pages.getRowCount());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1149881eee8d9508f423a8a45306fd826ca8bbbd"}, "originalPosition": 247}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUyNDY2Mw==", "bodyText": "Same above.", "url": "https://github.com/apache/beam/pull/12223#discussion_r479524663", "createdAt": "2020-08-28T20:40:32Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +279,255 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.info(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          LOG.info(\"block {} read in memory. row count = {}\", currentBlock, pages.getRowCount());\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, options.useRecordFilter() ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"Finish processing \" + currentRow + \" rows from block \" + (currentBlock - 1));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1149881eee8d9508f423a8a45306fd826ca8bbbd"}, "originalPosition": 287}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUyNTE5NA==", "bodyText": "Any reason to use SPLIT_LIMIT / 1000?", "url": "https://github.com/apache/beam/pull/12223#discussion_r479525194", "createdAt": "2020-08-28T20:41:57Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +279,255 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.info(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          LOG.info(\"block {} read in memory. row count = {}\", currentBlock, pages.getRowCount());\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, options.useRecordFilter() ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"Finish processing \" + currentRow + \" rows from block \" + (currentBlock - 1));\n+        }\n+      }\n+\n+      public Configuration getConfWithModelClass() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 1000)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1149881eee8d9508f423a8a45306fd826ca8bbbd"}, "originalPosition": 322}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUzMDcxNg==", "bodyText": "Can we have a simple AutoValue class for countAndSize?", "url": "https://github.com/apache/beam/pull/12223#discussion_r479530716", "createdAt": "2020-08-28T20:56:24Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +279,255 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.info(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          LOG.info(\"block {} read in memory. row count = {}\", currentBlock, pages.getRowCount());\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, options.useRecordFilter() ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"Finish processing \" + currentRow + \" rows from block \" + (currentBlock - 1));\n+        }\n+      }\n+\n+      public Configuration getConfWithModelClass() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 1000)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        List<Double> recordCountAndSize = getRecordCountAndSize(file, restriction);\n+        return new BlockTracker(\n+            restriction,\n+            Math.round(recordCountAndSize.get(1)),\n+            Math.round(recordCountAndSize.get(0)));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      @GetSize\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        return getRecordCountAndSize(file, restriction).get(1);\n+      }\n+\n+      public List<Double> getRecordCountAndSize(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        double size = 0;\n+        double recordCount = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          BlockMetaData block = reader.getRowGroups().get((int) i);\n+          recordCount += block.getRowCount();\n+          size += block.getTotalByteSize();\n+        }\n+        List<Double> countAndSize = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1149881eee8d9508f423a8a45306fd826ca8bbbd"}, "originalPosition": 381}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUzMTU1NQ==", "bodyText": "To compare 2 double, you can use assertEquals(double expected, double actual, double epsilon)", "url": "https://github.com/apache/beam/pull/12223#discussion_r479531555", "createdAt": "2020-08-28T20:58:41Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java", "diffHunk": "@@ -69,6 +76,44 @@\n         \"Faraday\", \"Newton\", \"Bohr\", \"Galilei\", \"Maxwell\"\n       };\n \n+  @Test\n+  public void testBlockTracker() throws Exception {\n+    OffsetRange range = new OffsetRange(0, 1);\n+    ParquetIO.ReadFiles.BlockTracker tracker = new ParquetIO.ReadFiles.BlockTracker(range, 7, 3);\n+    assertTrue(Math.abs(tracker.getProgress().getWorkRemaining() - 6) < 0.01);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1149881eee8d9508f423a8a45306fd826ca8bbbd"}, "originalPosition": 54}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUzMjEwOQ==", "bodyText": "assertEquals?", "url": "https://github.com/apache/beam/pull/12223#discussion_r479532109", "createdAt": "2020-08-28T21:00:04Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java", "diffHunk": "@@ -69,6 +76,44 @@\n         \"Faraday\", \"Newton\", \"Bohr\", \"Galilei\", \"Maxwell\"\n       };\n \n+  @Test\n+  public void testBlockTracker() throws Exception {\n+    OffsetRange range = new OffsetRange(0, 1);\n+    ParquetIO.ReadFiles.BlockTracker tracker = new ParquetIO.ReadFiles.BlockTracker(range, 7, 3);\n+    assertTrue(Math.abs(tracker.getProgress().getWorkRemaining() - 6) < 0.01);\n+    assertTrue(Math.abs(tracker.getProgress().getWorkCompleted()) < 0.01);\n+    tracker.tryClaim((long) 0);\n+    tracker.makeProgress();\n+    assertTrue(Math.abs(tracker.getProgress().getWorkRemaining() - 4) < 0.01);\n+    assertTrue(Math.abs(tracker.getProgress().getWorkCompleted() - 2) < 0.01);\n+    assertThrows(RuntimeException.class, () -> tracker.tryClaim((long) 0));\n+    tracker.makeProgress();\n+    tracker.makeProgress();\n+    assertTrue(Math.abs(tracker.getProgress().getWorkRemaining() - 0) < 0.01);\n+    assertTrue(Math.abs(tracker.getProgress().getWorkCompleted() - 6) < 0.01);\n+    assertThrows(\"Making progress out of range\", IOException.class, () -> tracker.makeProgress());\n+  }\n+\n+  @Test\n+  public void testSplitBlockWithLimit() {\n+    ParquetIO.ReadFiles.SplitReadFn testFn = new ParquetIO.ReadFiles.SplitReadFn(null);\n+    ArrayList<BlockMetaData> blockList = new ArrayList<BlockMetaData>();\n+    ArrayList<OffsetRange> rangeList;\n+    BlockMetaData testBlock = mock(BlockMetaData.class);\n+    when(testBlock.getTotalByteSize()).thenReturn((long) 60);\n+    rangeList = testFn.splitBlockWithLimit(0, blockList.size(), blockList, 200);\n+    assertTrue(rangeList.isEmpty());\n+    for (int i = 0; i < 6; i++) {\n+      blockList.add(testBlock);\n+    }\n+    rangeList = testFn.splitBlockWithLimit(1, blockList.size(), blockList, 200);\n+    assertTrue(rangeList.get(0).getFrom() == (long) 1);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1149881eee8d9508f423a8a45306fd826ca8bbbd"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUzMjY0OQ==", "bodyText": "you can use 0L to represent that 0 is a long.", "url": "https://github.com/apache/beam/pull/12223#discussion_r479532649", "createdAt": "2020-08-28T21:01:29Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java", "diffHunk": "@@ -69,6 +76,44 @@\n         \"Faraday\", \"Newton\", \"Bohr\", \"Galilei\", \"Maxwell\"\n       };\n \n+  @Test\n+  public void testBlockTracker() throws Exception {\n+    OffsetRange range = new OffsetRange(0, 1);\n+    ParquetIO.ReadFiles.BlockTracker tracker = new ParquetIO.ReadFiles.BlockTracker(range, 7, 3);\n+    assertTrue(Math.abs(tracker.getProgress().getWorkRemaining() - 6) < 0.01);\n+    assertTrue(Math.abs(tracker.getProgress().getWorkCompleted()) < 0.01);\n+    tracker.tryClaim((long) 0);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1149881eee8d9508f423a8a45306fd826ca8bbbd"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0MTU0Mg==", "bodyText": "It seems like in the DoFn body, you will call tracker.makeProgress() for each record. It doesn't work under fnapi context since we only expose limit APIs: \n  \n    \n      beam/sdks/java/fn-execution/src/main/java/org/apache/beam/sdk/fn/splittabledofn/RestrictionTrackers.java\n    \n    \n        Lines 91 to 104\n      in\n      7fb07ff\n    \n    \n    \n    \n\n        \n          \n           private static class RestrictionTrackerObserverWithProgress<RestrictionT, PositionT> \n        \n\n        \n          \n               extends RestrictionTrackerObserver<RestrictionT, PositionT> implements HasProgress { \n        \n\n        \n          \n            \n        \n\n        \n          \n             protected RestrictionTrackerObserverWithProgress( \n        \n\n        \n          \n                 RestrictionTracker<RestrictionT, PositionT> delegate, \n        \n\n        \n          \n                 ClaimObserver<PositionT> claimObserver) { \n        \n\n        \n          \n               super(delegate, claimObserver); \n        \n\n        \n          \n             } \n        \n\n        \n          \n            \n        \n\n        \n          \n             @Override \n        \n\n        \n          \n             public synchronized Progress getProgress() { \n        \n\n        \n          \n               return ((HasProgress) delegate).getProgress(); \n        \n\n        \n          \n             } \n        \n\n        \n          \n           }", "url": "https://github.com/apache/beam/pull/12223#discussion_r479541542", "createdAt": "2020-08-28T21:25:50Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +279,255 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.info(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          LOG.info(\"block {} read in memory. row count = {}\", currentBlock, pages.getRowCount());\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, options.useRecordFilter() ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"Finish processing \" + currentRow + \" rows from block \" + (currentBlock - 1));\n+        }\n+      }\n+\n+      public Configuration getConfWithModelClass() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 1000)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        List<Double> recordCountAndSize = getRecordCountAndSize(file, restriction);\n+        return new BlockTracker(\n+            restriction,\n+            Math.round(recordCountAndSize.get(1)),\n+            Math.round(recordCountAndSize.get(0)));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      @GetSize\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        return getRecordCountAndSize(file, restriction).get(1);\n+      }\n+\n+      public List<Double> getRecordCountAndSize(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        double size = 0;\n+        double recordCount = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          BlockMetaData block = reader.getRowGroups().get((int) i);\n+          recordCount += block.getRowCount();\n+          size += block.getTotalByteSize();\n+        }\n+        List<Double> countAndSize = new ArrayList<>();\n+        countAndSize.add(recordCount);\n+        countAndSize.add(size);\n+        return countAndSize;\n+      }\n+    }\n+\n+    public static class BlockTracker extends OffsetRangeTracker {\n+      private long totalWork;\n+      private long progress;\n+      private long approximateRecordSize;\n+\n+      public BlockTracker(OffsetRange range, long totalByteSize, long recordCount) {\n+        super(range);\n+        if (recordCount != 0) {\n+          this.approximateRecordSize = totalByteSize / recordCount;\n+          this.totalWork = approximateRecordSize * recordCount;\n+          this.progress = 0;\n+        }\n+      }\n+\n+      public void makeProgress() throws Exception {\n+        progress += approximateRecordSize;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1149881eee8d9508f423a8a45306fd826ca8bbbd"}, "originalPosition": 403}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTU0NDU0Ng==", "bodyText": "Also if makeProgress() is never called because of the guard you have, the progress will always be (0, totalWork), which is not correct to some extent.", "url": "https://github.com/apache/beam/pull/12223#discussion_r479544546", "createdAt": "2020-08-28T21:34:49Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +279,255 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.info(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          LOG.info(\"block {} read in memory. row count = {}\", currentBlock, pages.getRowCount());\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, options.useRecordFilter() ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"Finish processing \" + currentRow + \" rows from block \" + (currentBlock - 1));\n+        }\n+      }\n+\n+      public Configuration getConfWithModelClass() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 1000)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        List<Double> recordCountAndSize = getRecordCountAndSize(file, restriction);\n+        return new BlockTracker(\n+            restriction,\n+            Math.round(recordCountAndSize.get(1)),\n+            Math.round(recordCountAndSize.get(0)));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      @GetSize\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        return getRecordCountAndSize(file, restriction).get(1);\n+      }\n+\n+      public List<Double> getRecordCountAndSize(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        double size = 0;\n+        double recordCount = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          BlockMetaData block = reader.getRowGroups().get((int) i);\n+          recordCount += block.getRowCount();\n+          size += block.getTotalByteSize();\n+        }\n+        List<Double> countAndSize = new ArrayList<>();\n+        countAndSize.add(recordCount);\n+        countAndSize.add(size);\n+        return countAndSize;\n+      }\n+    }\n+\n+    public static class BlockTracker extends OffsetRangeTracker {\n+      private long totalWork;\n+      private long progress;\n+      private long approximateRecordSize;\n+\n+      public BlockTracker(OffsetRange range, long totalByteSize, long recordCount) {\n+        super(range);\n+        if (recordCount != 0) {\n+          this.approximateRecordSize = totalByteSize / recordCount;\n+          this.totalWork = approximateRecordSize * recordCount;\n+          this.progress = 0;\n+        }\n+      }\n+\n+      public void makeProgress() throws Exception {\n+        progress += approximateRecordSize;\n+        if (progress > totalWork) {\n+          throw new IOException(\"Making progress out of range\");\n+        }\n+      }\n+\n+      @Override\n+      public Progress getProgress() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1149881eee8d9508f423a8a45306fd826ca8bbbd"}, "originalPosition": 410}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8c80d30e298f8e1bd2495fa6a9998d36a8ddcd4b", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/8c80d30e298f8e1bd2495fa6a9998d36a8ddcd4b", "committedDate": "2020-08-31T21:28:34Z", "message": "[BEAM-4379] Adding TODO"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDc5MzYzMjg5", "url": "https://github.com/apache/beam/pull/12223#pullrequestreview-479363289", "createdAt": "2020-09-01T02:44:45Z", "commit": {"oid": "8c80d30e298f8e1bd2495fa6a9998d36a8ddcd4b"}, "state": "APPROVED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwMjo0NDo0NVrOHKX_6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQwMzowODozN1rOHKZ-tg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY0MTAwMA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  // TODO:[BEAM-10842] A more precise progress update\n          \n          \n            \n                  // TODO(BEAM-10842): Refine the BlockTracker to provide better progress.", "url": "https://github.com/apache/beam/pull/12223#discussion_r480641000", "createdAt": "2020-09-01T02:44:45Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -229,15 +279,259 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n+    /** Enable the Splittable reading. */\n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n \n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.debug(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          LOG.debug(\"block {} read in memory. row count = {}\", currentBlock, pages.getRowCount());\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, options.useRecordFilter() ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.debug(\"Finish processing \" + currentRow + \" rows from block \" + (currentBlock - 1));\n+        }\n+      }\n+\n+      public Configuration getConfWithModelClass() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        CountAndSize recordCountAndSize = getRecordCountAndSize(file, restriction);\n+        return new BlockTracker(\n+            restriction,\n+            Math.round(recordCountAndSize.getSize()),\n+            Math.round(recordCountAndSize.getCount()));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      @GetSize\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        return getRecordCountAndSize(file, restriction).getSize();\n+      }\n+\n+      public CountAndSize getRecordCountAndSize(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        double size = 0;\n+        double recordCount = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          BlockMetaData block = reader.getRowGroups().get((int) i);\n+          recordCount += block.getRowCount();\n+          size += block.getTotalByteSize();\n+        }\n+        CountAndSize countAndSize = CountAndSize.create(recordCount, size);\n+        return countAndSize;\n+      }\n+\n+      @AutoValue\n+      abstract static class CountAndSize {\n+        static CountAndSize create(double count, double size) {\n+          return new AutoValue_ParquetIO_ReadFiles_SplitReadFn_CountAndSize(count, size);\n+        }\n+\n+        abstract double getCount();\n+\n+        abstract double getSize();\n+      }\n+    }\n+\n+    public static class BlockTracker extends OffsetRangeTracker {\n+      private long totalWork;\n+      private long progress;\n+      private long approximateRecordSize;\n+\n+      public BlockTracker(OffsetRange range, long totalByteSize, long recordCount) {\n+        super(range);\n+        if (recordCount != 0) {\n+          this.approximateRecordSize = totalByteSize / recordCount;\n+          this.totalWork = approximateRecordSize * recordCount;\n+          this.progress = 0;\n+        }\n+      }\n+\n+      public void makeProgress() throws Exception {\n+        progress += approximateRecordSize;\n+        if (progress > totalWork) {\n+          throw new IOException(\"Making progress out of range\");\n+        }\n+      }\n+\n+      @Override\n+      // TODO:[BEAM-10842] A more precise progress update", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8c80d30e298f8e1bd2495fa6a9998d36a8ddcd4b"}, "originalPosition": 418}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY0ODM0OQ==", "bodyText": "It seems like this comment is not addressed yet. Can you add related explanation(what's this option for and what will happens if the option is set to true) and code snippet(how to enable this option) into L89-L155? You can also link your design doc there if that helps end users to use this attribute.", "url": "https://github.com/apache/beam/pull/12223#discussion_r480648349", "createdAt": "2020-09-01T02:50:04Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -133,15 +161,18 @@\n    * pattern).\n    */\n   public static Read read(Schema schema) {\n-    return new AutoValue_ParquetIO_Read.Builder().setSchema(schema).build();\n+    return new AutoValue_ParquetIO_Read.Builder().setSchema(schema).setSplittable(false).build();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3OTUxNzk2NA=="}, "originalCommit": {"oid": "1149881eee8d9508f423a8a45306fd826ca8bbbd"}, "originalPosition": 81}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY1NDEzMg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    while ((tracker).tryClaim(currentBlock)) {\n          \n          \n            \n                    while (tracker.tryClaim(currentBlock)) {", "url": "https://github.com/apache/beam/pull/12223#discussion_r480654132", "createdAt": "2020-09-01T02:54:24Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -229,15 +279,259 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n+    /** Enable the Splittable reading. */\n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n \n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.debug(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8c80d30e298f8e1bd2495fa6a9998d36a8ddcd4b"}, "originalPosition": 247}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY2MzM1Nw==", "bodyText": "IIUC, it seems like there is no need to track currentRow, we can do something like\nrecord = recordReader.read();\nwhile(record != null) {\n  // do something\n  record = recordReader.read();\n}\nIf that's true, we can have a follow-up PR to clean up.", "url": "https://github.com/apache/beam/pull/12223#discussion_r480663357", "createdAt": "2020-09-01T03:01:17Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -229,15 +279,259 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n+    /** Enable the Splittable reading. */\n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n \n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.debug(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          LOG.debug(\"block {} read in memory. row count = {}\", currentBlock, pages.getRowCount());\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, options.useRecordFilter() ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8c80d30e298f8e1bd2495fa6a9998d36a8ddcd4b"}, "originalPosition": 256}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY2NzE5NQ==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                      @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n          \n          \n            \n                      FileIO.ReadableFile file,  OffsetRange restriction)\n          \n      \n    \n    \n  \n\nThe annotations like @Element and @Restriction are only for common APIs of a DoFn.", "url": "https://github.com/apache/beam/pull/12223#discussion_r480667195", "createdAt": "2020-09-01T03:04:04Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -229,15 +279,259 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n+    /** Enable the Splittable reading. */\n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n \n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.debug(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          LOG.debug(\"block {} read in memory. row count = {}\", currentBlock, pages.getRowCount());\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, options.useRecordFilter() ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.debug(\"Finish processing \" + currentRow + \" rows from block \" + (currentBlock - 1));\n+        }\n+      }\n+\n+      public Configuration getConfWithModelClass() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        CountAndSize recordCountAndSize = getRecordCountAndSize(file, restriction);\n+        return new BlockTracker(\n+            restriction,\n+            Math.round(recordCountAndSize.getSize()),\n+            Math.round(recordCountAndSize.getCount()));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      @GetSize\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        return getRecordCountAndSize(file, restriction).getSize();\n+      }\n+\n+      public CountAndSize getRecordCountAndSize(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8c80d30e298f8e1bd2495fa6a9998d36a8ddcd4b"}, "originalPosition": 370}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY2Nzk3OA==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  public CountAndSize getRecordCountAndSize(\n          \n          \n            \n                 private CountAndSize getRecordCountAndSize(\n          \n      \n    \n    \n  \n\ngetRecordCountAndSize is only for this DoFn, right?", "url": "https://github.com/apache/beam/pull/12223#discussion_r480667978", "createdAt": "2020-09-01T03:04:40Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -229,15 +279,259 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n+    /** Enable the Splittable reading. */\n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n \n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.debug(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          LOG.debug(\"block {} read in memory. row count = {}\", currentBlock, pages.getRowCount());\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, options.useRecordFilter() ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.debug(\"Finish processing \" + currentRow + \" rows from block \" + (currentBlock - 1));\n+        }\n+      }\n+\n+      public Configuration getConfWithModelClass() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        CountAndSize recordCountAndSize = getRecordCountAndSize(file, restriction);\n+        return new BlockTracker(\n+            restriction,\n+            Math.round(recordCountAndSize.getSize()),\n+            Math.round(recordCountAndSize.getCount()));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      @GetSize\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        return getRecordCountAndSize(file, restriction).getSize();\n+      }\n+\n+      public CountAndSize getRecordCountAndSize(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8c80d30e298f8e1bd2495fa6a9998d36a8ddcd4b"}, "originalPosition": 369}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MDY3MzQ2Mg==", "bodyText": "Any reason to use double for count and size? If that's not preferable, we can have a follow-up PR to clean up.", "url": "https://github.com/apache/beam/pull/12223#discussion_r480673462", "createdAt": "2020-09-01T03:08:37Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -229,15 +279,259 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n+    /** Enable the Splittable reading. */\n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n \n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.debug(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          LOG.debug(\"block {} read in memory. row count = {}\", currentBlock, pages.getRowCount());\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, options.useRecordFilter() ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.debug(\"Finish processing \" + currentRow + \" rows from block \" + (currentBlock - 1));\n+        }\n+      }\n+\n+      public Configuration getConfWithModelClass() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        CountAndSize recordCountAndSize = getRecordCountAndSize(file, restriction);\n+        return new BlockTracker(\n+            restriction,\n+            Math.round(recordCountAndSize.getSize()),\n+            Math.round(recordCountAndSize.getCount()));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      @GetSize\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        return getRecordCountAndSize(file, restriction).getSize();\n+      }\n+\n+      public CountAndSize getRecordCountAndSize(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        double size = 0;\n+        double recordCount = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          BlockMetaData block = reader.getRowGroups().get((int) i);\n+          recordCount += block.getRowCount();\n+          size += block.getTotalByteSize();\n+        }\n+        CountAndSize countAndSize = CountAndSize.create(recordCount, size);\n+        return countAndSize;\n+      }\n+\n+      @AutoValue\n+      abstract static class CountAndSize {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8c80d30e298f8e1bd2495fa6a9998d36a8ddcd4b"}, "originalPosition": 385}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7a0b0c5d8c34b297ceda64f29bc7e60ca8c50152", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/7a0b0c5d8c34b297ceda64f29bc7e60ca8c50152", "committedDate": "2020-09-01T04:12:41Z", "message": "Update sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n\nCo-authored-by: Boyuan Zhang <36090911+boyuanzz@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bc831a7095f4711f7ffbdcfc9915f4f687f60039", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/bc831a7095f4711f7ffbdcfc9915f4f687f60039", "committedDate": "2020-09-01T04:13:05Z", "message": "Update sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n\nCo-authored-by: Boyuan Zhang <36090911+boyuanzz@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "aea4439254fd3e86e45aad6647375cd33d526fca", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/aea4439254fd3e86e45aad6647375cd33d526fca", "committedDate": "2020-09-01T04:14:28Z", "message": "Update sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java\n\nCo-authored-by: Boyuan Zhang <36090911+boyuanzz@users.noreply.github.com>"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "11cc8819a9b21e0cfc25a5f207e50dc0b2e9b746", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/11cc8819a9b21e0cfc25a5f207e50dc0b2e9b746", "committedDate": "2020-09-01T18:10:55Z", "message": "[BEAM-4379] adding javadoc"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgwMTM5ODIx", "url": "https://github.com/apache/beam/pull/12223#pullrequestreview-480139821", "createdAt": "2020-09-01T21:45:38Z", "commit": {"oid": "11cc8819a9b21e0cfc25a5f207e50dc0b2e9b746"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQyMTo0NTozOFrOHLJX8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMVQyMjowNzoyNFrOHLJ7bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ0OTk2OQ==", "bodyText": "Please put a comment then.", "url": "https://github.com/apache/beam/pull/12223#discussion_r481449969", "createdAt": "2020-09-01T21:45:38Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 3)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;\n+          if (totalSize >= limit) {\n+            offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+            rangeStart = rangeEnd;\n+            totalSize = 0;\n+          }\n+        }\n+        if (totalSize != 0) {\n+          offsetList.add(new OffsetRange(rangeStart, rangeEnd));\n+        }\n+        return offsetList;\n+      }\n+\n+      @NewTracker\n+      public RestrictionTracker<OffsetRange, Long> newTracker(\n+          @Restriction OffsetRange restriction, @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        return new BlockTracker(\n+            restriction, (long) getSize(file, restriction), getRecordCount(file, restriction));\n+      }\n+\n+      @GetRestrictionCoder\n+      public OffsetRange.Coder getRestrictionCoder() {\n+        return new OffsetRange.Coder();\n+      }\n+\n+      public long getRecordCount(\n+          @Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        long start = 0;\n+        long end = 0;\n+        start = restriction.getFrom();\n+        end = restriction.getTo();\n+        long recordCount = 0;\n+        for (long i = start; i < end; i++) {\n+          recordCount += reader.getRowGroups().get((int) i).getRowCount();\n+        }\n+        return recordCount;\n+      }\n+\n+      public double getSize(@Element FileIO.ReadableFile file, @Restriction OffsetRange restriction)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        double size = 0;\n+        for (long i = restriction.getFrom(); i < restriction.getTo(); i++) {\n+          size += reader.getRowGroups().get((int) i).getTotalByteSize();\n+        }\n+        return size;\n+      }\n+    }\n+\n+    public static class BlockTracker extends OffsetRangeTracker {\n+      private static final Logger LOG = LoggerFactory.getLogger(BlockTracker.class);\n+      private long totalWork;\n+      private long progress;\n+      private long approximateRecordSize;\n+\n+      public BlockTracker(OffsetRange range, long work, long recordCount) {\n+        super(range);\n+        if (recordCount != 0) {\n+          this.approximateRecordSize = work / recordCount;\n+          this.totalWork = approximateRecordSize * recordCount;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4ODY4OQ=="}, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 421}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1MTE5OA==", "bodyText": "How about renaming i to rangeEnd and removing the separate rangeEnd variable?", "url": "https://github.com/apache/beam/pull/12223#discussion_r481451198", "createdAt": "2020-09-01T21:48:28Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.info(\"finish read \" + currentRow + \" rows\");\n+        }\n+      }\n+\n+      public Configuration setConf() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT / 3)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;\n+        for (long i = start; i < end; i++) {\n+          totalSize += blockList.get((int) i).getTotalByteSize();\n+          rangeEnd += 1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4MjU1Mg=="}, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 353}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1MTU1MQ==", "bodyText": "I think this should be warn not debug. WDYT @chamikaramj ?\nAlso, it would be great if we print out some additional info from the exception like e.getMessage().", "url": "https://github.com/apache/beam/pull/12223#discussion_r481451551", "createdAt": "2020-09-01T21:49:19Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc3ODg5NA=="}, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 273}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1MzEyNg==", "bodyText": "We need to print out the block number with the log message at least. Otherwise it would be hard to debug with this string constant message.", "url": "https://github.com/apache/beam/pull/12223#discussion_r481453126", "createdAt": "2020-09-01T21:53:07Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4MTA0MQ=="}, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 278}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1MzMwMg==", "bodyText": "ditto.", "url": "https://github.com/apache/beam/pull/12223#discussion_r481453302", "createdAt": "2020-09-01T21:53:33Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -230,14 +281,284 @@ public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n \n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplit(true).build();\n+    }\n+\n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (getSplit()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private static final long SPLIT_LIMIT = 64000000;\n+      ReadSupport<GenericRecord> readSupport;\n+\n+      SplitReadFn(GenericData model) {\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      private static <K, V> Map<K, Set<V>> toSetMultiMap(Map<K, V> map) {\n+        Map<K, Set<V>> setMultiMap = new HashMap<K, Set<V>>();\n+        for (Map.Entry<K, V> entry : map.entrySet()) {\n+          Set<V> set = new HashSet<V>();\n+          set.add(entry.getValue());\n+          setMultiMap.put(entry.getKey(), Collections.unmodifiableSet(set));\n+        }\n+        return Collections.unmodifiableMap(setMultiMap);\n+      }\n+\n+      public InputFile getInputFile(FileIO.ReadableFile file) throws IOException {\n+        if (!file.getMetadata().isReadSeekEfficient()) {\n+          throw new RuntimeException(\n+              String.format(\"File has to be seekable: %s\", file.getMetadata().resourceId()));\n+        }\n+        return new BeamParquetInputFile(file.openSeekable());\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        ReadSupport<GenericRecord> readSupport;\n+        InputFile inputFile = getInputFile(file);\n+        Configuration conf = setConf();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        readSupport = new AvroReadSupport<GenericRecord>(model);\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader = ParquetFileReader.open(inputFile, options);\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        conf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(new InitContext(conf, toSetMultiMap(fileMetadata), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(conf, fileMetadata, fileSchema, readContext);\n+        boolean strictTypeChecking = options.isEnabled(STRICT_TYPE_CHECKING, true);\n+        boolean filterRecords = options.useRecordFilter();\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO =\n+            columnIOFactory.getColumnIO(requestedSchema, fileSchema, strictTypeChecking);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while ((tracker).tryClaim(currentBlock)) {\n+          LOG.info(\"reading block\" + currentBlock);\n+          PageReadStore pages = reader.readNextRowGroup();\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, filterRecords ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\"skipping a corrupt record\");\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\"filtered record reader reached end of block\");\n+                break;\n+              }\n+              if (tracker instanceof BlockTracker) {\n+                ((BlockTracker) tracker).makeProgress();\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\"skipping record\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Njc4MDQ5OA=="}, "originalCommit": {"oid": "8b8194f52cc0971ba5f2befcf91aba01a5556485"}, "originalPosition": 286}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MTQ1OTA1NA==", "bodyText": "looks like these two lines could be also replaced with getParquetFileReader().", "url": "https://github.com/apache/beam/pull/12223#discussion_r481459054", "createdAt": "2020-09-01T22:07:24Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -229,15 +290,258 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n+    /** Enable the Splittable reading. */\n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n \n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.debug(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "11cc8819a9b21e0cfc25a5f207e50dc0b2e9b746"}, "originalPosition": 236}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a05bb16fe43f752588e2459bef947e0db89c77d0", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/a05bb16fe43f752588e2459bef947e0db89c77d0", "committedDate": "2020-09-02T21:08:20Z", "message": "[BEAM-4379] Add Debug Info"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgxMzMyMjk3", "url": "https://github.com/apache/beam/pull/12223#pullrequestreview-481332297", "createdAt": "2020-09-02T22:02:41Z", "commit": {"oid": "a05bb16fe43f752588e2459bef947e0db89c77d0"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQyMjowMjo0MVrOHMKlsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wMlQyMjowMjo0MVrOHMKlsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4MjUxODQ0OQ==", "bodyText": "We could remove rangeEnd here (and change rangeEnd at L478 to end.", "url": "https://github.com/apache/beam/pull/12223#discussion_r482518449", "createdAt": "2020-09-02T22:02:41Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -229,15 +290,273 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     public ReadFiles withAvroDataModel(GenericData model) {\n       return toBuilder().setAvroDataModel(model).build();\n     }\n+    /** Enable the Splittable reading. */\n+    public ReadFiles withSplit() {\n+      return toBuilder().setSplittable(true).build();\n+    }\n \n     @Override\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n+      if (isSplittable()) {\n+        return input\n+            .apply(ParDo.of(new SplitReadFn(getAvroDataModel())))\n+            .setCoder(AvroCoder.of(getSchema()));\n+      }\n       return input\n           .apply(ParDo.of(new ReadFn(getAvroDataModel())))\n           .setCoder(AvroCoder.of(getSchema()));\n     }\n \n+    @DoFn.BoundedPerElement\n+    static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n+      private Class<? extends GenericData> modelClass;\n+      private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n+      private static final long SPLIT_LIMIT = 64000000;\n+\n+      SplitReadFn(GenericData model) {\n+\n+        this.modelClass = model != null ? model.getClass() : null;\n+      }\n+\n+      ParquetFileReader getParquetFileReader(FileIO.ReadableFile file) throws Exception {\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        return ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element FileIO.ReadableFile file,\n+          RestrictionTracker<OffsetRange, Long> tracker,\n+          OutputReceiver<GenericRecord> outputReceiver)\n+          throws Exception {\n+        LOG.debug(\n+            \"start \"\n+                + tracker.currentRestriction().getFrom()\n+                + \" to \"\n+                + tracker.currentRestriction().getTo());\n+        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+\n+        Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n+        Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n+        MessageType fileSchema = parquetFileMetadata.getSchema();\n+        Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n+\n+        ReadSupport.ReadContext readContext =\n+            readSupport.init(\n+                new InitContext(\n+                    hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n+        ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n+        MessageType requestedSchema = readContext.getRequestedSchema();\n+        RecordMaterializer<GenericRecord> recordConverter =\n+            readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n+        reader.setRequestedSchema(requestedSchema);\n+        MessageColumnIO columnIO = columnIOFactory.getColumnIO(requestedSchema, fileSchema, true);\n+        long currentBlock = tracker.currentRestriction().getFrom();\n+        for (int i = 0; i < currentBlock; i++) {\n+          reader.skipNextRowGroup();\n+        }\n+\n+        while (tracker.tryClaim(currentBlock)) {\n+          PageReadStore pages = reader.readNextRowGroup();\n+          LOG.debug(\"block {} read in memory. row count = {}\", currentBlock, pages.getRowCount());\n+          currentBlock += 1;\n+          RecordReader<GenericRecord> recordReader =\n+              columnIO.getRecordReader(\n+                  pages, recordConverter, options.useRecordFilter() ? filter : FilterCompat.NOOP);\n+          long currentRow = 0;\n+          long totalRows = pages.getRowCount();\n+          while (currentRow < totalRows) {\n+            try {\n+              GenericRecord record;\n+              currentRow += 1;\n+              try {\n+                record = recordReader.read();\n+              } catch (RecordMaterializer.RecordMaterializationException e) {\n+                LOG.debug(\n+                    \"skipping a corrupt record at {} in block {} in file {}\",\n+                    currentRow,\n+                    currentBlock,\n+                    file.toString());\n+                continue;\n+              }\n+              if (record == null) {\n+                // only happens with FilteredRecordReader at end of block\n+                LOG.debug(\n+                    \"filtered record reader reached end of block in block {} in file {}\",\n+                    currentBlock,\n+                    file.toString());\n+                break;\n+              }\n+              if (recordReader.shouldSkipCurrentRecord()) {\n+                // this record is being filtered via the filter2 package\n+                LOG.debug(\n+                    \"skipping record at {} in block {} in file {}\",\n+                    currentRow,\n+                    currentBlock,\n+                    file.toString());\n+                continue;\n+              }\n+              outputReceiver.output(record);\n+            } catch (RuntimeException e) {\n+\n+              throw new ParquetDecodingException(\n+                  format(\n+                      \"Can not read value at %d in block %d in file %s\",\n+                      currentRow, currentBlock, file.toString()),\n+                  e);\n+            }\n+          }\n+          LOG.debug(\n+              \"Finish processing {} rows from block {} in file {}\",\n+              currentRow,\n+              currentBlock - 1,\n+              file.toString());\n+        }\n+      }\n+\n+      public Configuration getConfWithModelClass() throws Exception {\n+        Configuration conf = new Configuration();\n+        GenericData model = null;\n+        if (modelClass != null) {\n+          model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n+        }\n+        if (model != null\n+            && (model.getClass() == GenericData.class || model.getClass() == SpecificData.class)) {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, true);\n+        } else {\n+          conf.setBoolean(AvroReadSupport.AVRO_COMPATIBILITY, false);\n+        }\n+        return conf;\n+      }\n+\n+      @GetInitialRestriction\n+      public OffsetRange getInitialRestriction(@Element FileIO.ReadableFile file) throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        return new OffsetRange(0, reader.getRowGroups().size());\n+      }\n+\n+      @SplitRestriction\n+      public void split(\n+          @Restriction OffsetRange restriction,\n+          OutputReceiver<OffsetRange> out,\n+          @Element FileIO.ReadableFile file)\n+          throws Exception {\n+        ParquetFileReader reader = getParquetFileReader(file);\n+        List<BlockMetaData> rowGroups = reader.getRowGroups();\n+        for (OffsetRange offsetRange :\n+            splitBlockWithLimit(\n+                restriction.getFrom(), restriction.getTo(), rowGroups, SPLIT_LIMIT)) {\n+          out.output(offsetRange);\n+        }\n+      }\n+\n+      public ArrayList<OffsetRange> splitBlockWithLimit(\n+          long start, long end, List<BlockMetaData> blockList, long limit) {\n+        ArrayList<OffsetRange> offsetList = new ArrayList<OffsetRange>();\n+        long totalSize = 0;\n+        long rangeStart = start;\n+        long rangeEnd = start;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a05bb16fe43f752588e2459bef947e0db89c77d0"}, "originalPosition": 364}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "974ef8e8b817e0b3b03d75ca7504725d044174f3", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/974ef8e8b817e0b3b03d75ca7504725d044174f3", "committedDate": "2020-09-02T22:14:38Z", "message": "[BEAM-4379] change debug to warn"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "abbfd626424f6d2e3168d67ad289f1ec0ec1323d", "author": {"user": {"login": "danielxjd", "name": "Jiadai Xia"}}, "url": "https://github.com/apache/beam/commit/abbfd626424f6d2e3168d67ad289f1ec0ec1323d", "committedDate": "2020-09-02T22:24:59Z", "message": "[BEAM-4379] remove rangeend"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDgxMzU2MTM5", "url": "https://github.com/apache/beam/pull/12223#pullrequestreview-481356139", "createdAt": "2020-09-02T22:25:01Z", "commit": {"oid": "974ef8e8b817e0b3b03d75ca7504725d044174f3"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4048, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}