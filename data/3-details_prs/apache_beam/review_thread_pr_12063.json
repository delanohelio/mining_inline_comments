{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDM4NDI0MjIx", "number": 12063, "reviewThreads": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwODo1OTo0NVrOEH4QqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQxMjo1MTo1N1rOENQ3tQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2Njk3MjU3OnYy", "diffSide": "RIGHT", "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/metrics/MetricsAccumulator.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwODo1OTo0NVrOGnf_iw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwODo1OTo0NVrOGnf_iw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3MTgxOQ==", "bodyText": "I'm not sure about this - whether it's backward compatible", "url": "https://github.com/apache/beam/pull/12063#discussion_r444071819", "createdAt": "2020-06-23T08:59:45Z", "author": {"login": "davidak09"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/metrics/MetricsAccumulator.java", "diffHunk": "@@ -87,13 +86,13 @@ public static MetricsContainerStepMapAccumulator getInstance() {\n     }\n   }\n \n-  private static Optional<MetricsContainerStepMap> recoverValueFromCheckpoint(\n+  private static Optional<SparkMetricsContainerStepMap> recoverValueFromCheckpoint(\n       JavaSparkContext jsc, CheckpointDir checkpointDir) {\n     try {\n       Path beamCheckpointPath = checkpointDir.getBeamCheckpointDir();\n       checkpointFilePath = new Path(beamCheckpointPath, ACCUMULATOR_CHECKPOINT_FILENAME);\n       fileSystem = checkpointFilePath.getFileSystem(jsc.hadoopConfiguration());\n-      MetricsContainerStepMap recoveredValue =\n+      SparkMetricsContainerStepMap recoveredValue =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc731cfb084c27b11449042223223753afd39758"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2Njk4NzM1OnYy", "diffSide": "RIGHT", "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/metrics/SparkBeamMetric.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOTowMzozMFrOGngJHg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwOTowMzozMFrOGngJHg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDA3NDI3MA==", "bodyText": "I'd personally prefer single metric, possibly with .distribution suffix, which could include all 5 stats (count, sum, min, max, mean), it would definitely be more readable in Spark UI", "url": "https://github.com/apache/beam/pull/12063#discussion_r444074270", "createdAt": "2020-06-23T09:03:30Z", "author": {"login": "davidak09"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/metrics/SparkBeamMetric.java", "diffHunk": "@@ -51,11 +51,12 @@\n     }\n     for (MetricResult<DistributionResult> metricResult : metricQueryResults.getDistributions()) {\n       DistributionResult result = metricResult.getAttempted();\n-      metrics.put(renderName(metricResult) + \".count\", result.getCount());\n-      metrics.put(renderName(metricResult) + \".sum\", result.getSum());\n-      metrics.put(renderName(metricResult) + \".min\", result.getMin());\n-      metrics.put(renderName(metricResult) + \".max\", result.getMax());\n-      metrics.put(renderName(metricResult) + \".mean\", result.getMean());\n+      String name = renderName(metricResult);\n+      metrics.put(name + \".count\", result.getCount());\n+      metrics.put(name + \".sum\", result.getSum());\n+      metrics.put(name + \".min\", result.getMin());\n+      metrics.put(name + \".max\", result.getMax());\n+      metrics.put(name + \".mean\", result.getMean());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "cc731cfb084c27b11449042223223753afd39758"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc5ODg3ODI4OnYy", "diffSide": "RIGHT", "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/metrics/MetricsAccumulator.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMlQxNDozMDowMVrOGsPesw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQxMjozODo0NVrOGv1u3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA0NDE0Nw==", "bodyText": "Won't it be enough to just create new instance of SparkMetricsContainerStepMap here but keep the same interface of MetricsContainerStepMap everywhere as it was before get broken? Why do we need to change it everywhere since SparkMetricsContainerStepMap  just overrides toString()?\nAlso, it would be great to add a regression test for this fix.", "url": "https://github.com/apache/beam/pull/12063#discussion_r449044147", "createdAt": "2020-07-02T14:30:01Z", "author": {"login": "aromanenko-dev"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/metrics/MetricsAccumulator.java", "diffHunk": "@@ -58,13 +57,13 @@ public static void init(SparkPipelineOptions opts, JavaSparkContext jsc) {\n               opts.isStreaming()\n                   ? Optional.of(new CheckpointDir(opts.getCheckpointDir()))\n                   : Optional.absent();\n-          MetricsContainerStepMap metricsContainerStepMap = new MetricsContainerStepMap();\n+          SparkMetricsContainerStepMap metricsContainerStepMap = new SparkMetricsContainerStepMap();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c5cd601c445b4d1443d5569c9272c866d7a9eb99"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjgxNjYwNA==", "bodyText": "You're right, it's enough to change it just in initialization. I fixed it.\nI will try to add a regression test.", "url": "https://github.com/apache/beam/pull/12063#discussion_r452816604", "createdAt": "2020-07-10T12:38:45Z", "author": {"login": "davidak09"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/metrics/MetricsAccumulator.java", "diffHunk": "@@ -58,13 +57,13 @@ public static void init(SparkPipelineOptions opts, JavaSparkContext jsc) {\n               opts.isStreaming()\n                   ? Optional.of(new CheckpointDir(opts.getCheckpointDir()))\n                   : Optional.absent();\n-          MetricsContainerStepMap metricsContainerStepMap = new MetricsContainerStepMap();\n+          SparkMetricsContainerStepMap metricsContainerStepMap = new SparkMetricsContainerStepMap();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0OTA0NDE0Nw=="}, "originalCommit": {"oid": "c5cd601c445b4d1443d5569c9272c866d7a9eb99"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyMzQzMzQ5OnYy", "diffSide": "RIGHT", "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/metrics/SparkMetricsContainerStepMap.java", "isResolved": false, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMFQxMjo1MTo1OFrOGv2I2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQxNzowNjozNVrOG23JWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjgyMzI1Nw==", "bodyText": "We ran our application built with Beam 2.18.0 and the metrics are displayed correctly, with this fix Spark displays the same results.\nThe change that broke things up for Spark was part of BEAM-9600 - #11369 .", "url": "https://github.com/apache/beam/pull/12063#discussion_r452823257", "createdAt": "2020-07-10T12:51:58Z", "author": {"login": "davidak09"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/metrics/SparkMetricsContainerStepMap.java", "diffHunk": "@@ -27,7 +27,7 @@\n \n   @Override\n   public String toString() {\n-    return new SparkBeamMetric().renderAll().toString();\n+    return asAttemptedOnlyMetricResults(this).toString();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "68dcc0beb7d46b457947374bb786a24dfa9d2cbd"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzQ0MjA5NQ==", "bodyText": "@ibzib fyi", "url": "https://github.com/apache/beam/pull/12063#discussion_r457442095", "createdAt": "2020-07-20T14:32:46Z", "author": {"login": "aromanenko-dev"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/metrics/SparkMetricsContainerStepMap.java", "diffHunk": "@@ -27,7 +27,7 @@\n \n   @Override\n   public String toString() {\n-    return new SparkBeamMetric().renderAll().toString();\n+    return asAttemptedOnlyMetricResults(this).toString();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjgyMzI1Nw=="}, "originalCommit": {"oid": "68dcc0beb7d46b457947374bb786a24dfa9d2cbd"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NzY0NDc2Ng==", "bodyText": "Thanks for the heads up. My assumption in #11369 was that the result was meant to be consumed by code, not humans. After this change, metrics would remain not too readable for Flink. Ideally we could find a way to make the Flink metrics protos available to the Python client without toString, since toString should generally should be human-readable. The problem is that without a job server, there's nowhere to access job information like metrics outside of what Flink provides natively.", "url": "https://github.com/apache/beam/pull/12063#discussion_r457644766", "createdAt": "2020-07-20T19:35:00Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/metrics/SparkMetricsContainerStepMap.java", "diffHunk": "@@ -27,7 +27,7 @@\n \n   @Override\n   public String toString() {\n-    return new SparkBeamMetric().renderAll().toString();\n+    return asAttemptedOnlyMetricResults(this).toString();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjgyMzI1Nw=="}, "originalCommit": {"oid": "68dcc0beb7d46b457947374bb786a24dfa9d2cbd"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODA3MzAyNw==", "bodyText": "@ibzib Thanks for details. I agree that we should not rely on toString() when it's going to be used by other program.\nI'm sorry but I'm not sure that I fully understand your last sentence - does it seem that it's only specific Flink-related issue?\nRegarding readable Flink metrics - would it be better to fix it on FlinkRunner side, like we do with SparkMetricsContainerStepMap for Spark Runner?", "url": "https://github.com/apache/beam/pull/12063#discussion_r458073027", "createdAt": "2020-07-21T12:55:11Z", "author": {"login": "aromanenko-dev"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/metrics/SparkMetricsContainerStepMap.java", "diffHunk": "@@ -27,7 +27,7 @@\n \n   @Override\n   public String toString() {\n-    return new SparkBeamMetric().renderAll().toString();\n+    return asAttemptedOnlyMetricResults(this).toString();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjgyMzI1Nw=="}, "originalCommit": {"oid": "68dcc0beb7d46b457947374bb786a24dfa9d2cbd"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODI2MzAxOA==", "bodyText": "There are two ways of running Beam Python Flink/Spark. One way involves starting a Java job server, the other (newer) way does not require a Java job server and instead uses only Python. When using a Java job server, it's easy to get metrics, since the job server has access to the Flink/Spark context object. But in Python, we rely on the Flink REST API to get metrics, so the formatting of the results displayed there is important.\nThe reason metrics formatting isn't a problem for Spark is that spark_uber_jar_job_server.py just doesn't implement get_metrics yet.\nThe problem is that if we want to make MetricsContainerStepMap::toString human-readable, we'll need to make the protobuf-formatted metrics accessible somewhere else.\nPerhaps there is a compromise though if we can somehow format metrics so they are readable both by the proto parser and humans.", "url": "https://github.com/apache/beam/pull/12063#discussion_r458263018", "createdAt": "2020-07-21T17:20:59Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/metrics/SparkMetricsContainerStepMap.java", "diffHunk": "@@ -27,7 +27,7 @@\n \n   @Override\n   public String toString() {\n-    return new SparkBeamMetric().renderAll().toString();\n+    return asAttemptedOnlyMetricResults(this).toString();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjgyMzI1Nw=="}, "originalCommit": {"oid": "68dcc0beb7d46b457947374bb786a24dfa9d2cbd"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDEzNDQzMQ==", "bodyText": "@ibzib Thanks for more details. I see the picture bit still not convinced why MetricsContainerStepMap::toString  is used for REST API (if I get it right) - dedicated way  to make the protobuf-formatted metrics accessible, as you said, should be better imho.\nAre you ok to merge this fix for Spark Runner metrics?", "url": "https://github.com/apache/beam/pull/12063#discussion_r460134431", "createdAt": "2020-07-24T15:43:30Z", "author": {"login": "aromanenko-dev"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/metrics/SparkMetricsContainerStepMap.java", "diffHunk": "@@ -27,7 +27,7 @@\n \n   @Override\n   public String toString() {\n-    return new SparkBeamMetric().renderAll().toString();\n+    return asAttemptedOnlyMetricResults(this).toString();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjgyMzI1Nw=="}, "originalCommit": {"oid": "68dcc0beb7d46b457947374bb786a24dfa9d2cbd"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDE3OTgwMA==", "bodyText": "@ibzib Thanks for more details. I see the picture bit still not convinced why MetricsContainerStepMap::toString is used for REST API (if I get it right) - dedicated way to make the protobuf-formatted metrics accessible, as you said, should be better imho.\n\nThe reason I used toString is because the Flink REST API displays accumulator values using toString. I simply wasn't able to come up with a dedicated way to make the protobuf-formatted metrics accessible.\n\nAre you ok to merge this fix for Spark Runner metrics?\n\nYes, this is fine for now. I might need to give Flink runner metrics another look to fix the equivalent problem there.", "url": "https://github.com/apache/beam/pull/12063#discussion_r460179800", "createdAt": "2020-07-24T17:06:35Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/metrics/SparkMetricsContainerStepMap.java", "diffHunk": "@@ -27,7 +27,7 @@\n \n   @Override\n   public String toString() {\n-    return new SparkBeamMetric().renderAll().toString();\n+    return asAttemptedOnlyMetricResults(this).toString();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MjgyMzI1Nw=="}, "originalCommit": {"oid": "68dcc0beb7d46b457947374bb786a24dfa9d2cbd"}, "originalPosition": 5}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3566, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}