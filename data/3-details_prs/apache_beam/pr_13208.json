{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTExMTIyNTIy", "number": 13208, "title": "[BEAM-10703, BEAM-10475] Add an option to GroupIntoBatches to output ShardedKeys. Update Dataflow pipeline translation accordingly.", "bodyText": "Add an option to GroupIntoBatches transform to output batched input elements associated with ShardedKeys. Also adds a default implementation which applies one shard per key.\nUpdate Dataflow pipeline translation accordingly.\n\nThank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:\n\n Choose reviewer(s) and mention them in a comment (R: @username).\n Format the pull request title like [BEAM-XXX] Fixes bug in ApproximateQuantiles, where you replace BEAM-XXX with the appropriate JIRA issue, if applicable. This will automatically link the pull request to the issue.\n Update CHANGES.md with noteworthy changes.\n If this contribution is large, please file an Apache Individual Contributor License Agreement.\n\nSee the Contributor Guide for more tips on how to make review process smoother.\nPost-Commit Tests Status (on master branch)\n\n\n\nLang\nSDK\nDataflow\nFlink\nSamza\nSpark\nTwister2\n\n\n\n\nGo\n\n---\n\n---\n\n---\n\n\nJava\n\n\n\n\n\n\n\n\nPython\n\n\n\n---\n\n---\n\n\nXLang\n\n---\n\n---\n\n---\n\n\n\nPre-Commit Tests Status (on master branch)\n\n\n\n---\nJava\nPython\nGo\nWebsite\nWhitespace\nTypescript\n\n\n\n\nNon-portable\n\n \n\n\n\n\n\n\nPortable\n---\n\n---\n---\n---\n---\n\n\n\nSee .test-infra/jenkins/README for trigger phrase, status and link of all Jenkins jobs.\nGitHub Actions Tests Status (on master branch)\n\n\n\nSee CI.md for more information about GitHub Actions CI.", "createdAt": "2020-10-27T22:11:07Z", "url": "https://github.com/apache/beam/pull/13208", "merged": true, "mergeCommit": {"oid": "f4d889f92fe0041848b2edd625d9b9d1941d9914"}, "closed": true, "closedAt": "2020-11-24T18:27:08Z", "author": {"login": "nehsyc"}, "timelineItems": {"totalCount": 15, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdWwnauAFqTUxODE5NTI1MA==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdfeOM0gBqjQwMzAxNDkxODQ=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE4MTk1MjUw", "url": "https://github.com/apache/beam/pull/13208#pullrequestreview-518195250", "createdAt": "2020-10-27T22:22:03Z", "commit": {"oid": "a50f4b2d7b0d82994c4847ad633bc688a9c92c78"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QyMjoyMjowM1rOHpTLvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QyMjoyMjowM1rOHpTLvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA2Nzk2NQ==", "bodyText": "Non-portable is not ready either. Should we say that it's not supported in general and enable it when the backend is ready?", "url": "https://github.com/apache/beam/pull/13208#discussion_r513067965", "createdAt": "2020-10-27T22:22:03Z", "author": {"login": "nehsyc"}, "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunner.java", "diffHunk": "@@ -1281,8 +1286,12 @@ void addPCollectionRequiringIndexedFormat(PCollection<?> pcol) {\n   }\n \n   void maybeRecordPCollectionWithAutoSharding(PCollection<?> pcol) {\n-    if (hasExperiment(options, \"enable_streaming_auto_sharding\")\n-        && !hasExperiment(options, \"beam_fn_api\")) {\n+    if (hasExperiment(options, \"beam_fn_api\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a50f4b2d7b0d82994c4847ad633bc688a9c92c78"}, "originalPosition": 24}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a50f4b2d7b0d82994c4847ad633bc688a9c92c78", "author": {"user": {"login": "nehsyc", "name": "Siyuan Chen"}}, "url": "https://github.com/apache/beam/commit/a50f4b2d7b0d82994c4847ad633bc688a9c92c78", "committedDate": "2020-10-27T21:30:58Z", "message": "Add an option to GroupIntoBatches to output ShardedKeys. Update Dataflow pipeline translation accordingly."}, "afterCommit": {"oid": "bbf10680a68f36cbd9482f7fb0b4f93907e5b289", "author": {"user": {"login": "nehsyc", "name": "Siyuan Chen"}}, "url": "https://github.com/apache/beam/commit/bbf10680a68f36cbd9482f7fb0b4f93907e5b289", "committedDate": "2020-10-27T22:28:34Z", "message": "Add an option to GroupIntoBatches to output ShardedKeys. Update Dataflow pipeline translation accordingly."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE4MjAwOTQx", "url": "https://github.com/apache/beam/pull/13208#pullrequestreview-518200941", "createdAt": "2020-10-27T22:33:21Z", "commit": {"oid": "bbf10680a68f36cbd9482f7fb0b4f93907e5b289"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QyMjozMzoyMVrOHpTd0w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QyMjozMzoyMVrOHpTd0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA3MjU5NQ==", "bodyText": "Currently WithShardedKey is only applied with withShardedKey option. We could alternatively always apply WithShardedKey and strip the shard id in the no-sharded-output case. Not sure if it's safe to do that since it introduces coder changes for all current uses.", "url": "https://github.com/apache/beam/pull/13208#discussion_r513072595", "createdAt": "2020-10-27T22:33:21Z", "author": {"login": "nehsyc"}, "path": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/GroupIntoBatches.java", "diffHunk": "@@ -105,23 +108,83 @@ public long getBatchSize() {\n   }\n \n   /**\n-   * Set a time limit (in processing time) on how long an incomplete batch of elements is allowed to\n-   * be buffered. Once a batch is flushed to output, the timer is reset.\n+   * Sets a time limit (in processing time) on how long an incomplete batch of elements is allowed\n+   * to be buffered. Once a batch is flushed to output, the timer is reset.\n    */\n   public GroupIntoBatches<K, InputT> withMaxBufferingDuration(Duration duration) {\n     checkArgument(\n         duration.isLongerThan(Duration.ZERO), \"max buffering duration should be a positive value\");\n     return new GroupIntoBatches<>(batchSize, duration);\n   }\n \n+  /**\n+   * Outputs batched elements associated with sharded input keys. The sharding is determined by the\n+   * runner to balance the load during the execution time. By default, apply no sharding so each key\n+   * has one shard.\n+   */\n+  @Experimental\n+  public WithShardedKey withShardedKey() {\n+    return new WithShardedKey();\n+  }\n+\n+  public class WithShardedKey", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "bbf10680a68f36cbd9482f7fb0b4f93907e5b289"}, "originalPosition": 49}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "bbf10680a68f36cbd9482f7fb0b4f93907e5b289", "author": {"user": {"login": "nehsyc", "name": "Siyuan Chen"}}, "url": "https://github.com/apache/beam/commit/bbf10680a68f36cbd9482f7fb0b4f93907e5b289", "committedDate": "2020-10-27T22:28:34Z", "message": "Add an option to GroupIntoBatches to output ShardedKeys. Update Dataflow pipeline translation accordingly."}, "afterCommit": {"oid": "da5ccd0f8680ebd23c228941ae393b4c07f1ed09", "author": {"user": {"login": "nehsyc", "name": "Siyuan Chen"}}, "url": "https://github.com/apache/beam/commit/da5ccd0f8680ebd23c228941ae393b4c07f1ed09", "committedDate": "2020-10-27T22:39:09Z", "message": "Add an option to GroupIntoBatches to output ShardedKeys. Update Dataflow pipeline translation accordingly."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE4MjI2MDg1", "url": "https://github.com/apache/beam/pull/13208#pullrequestreview-518226085", "createdAt": "2020-10-27T23:34:45Z", "commit": {"oid": "da5ccd0f8680ebd23c228941ae393b4c07f1ed09"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QyMzozNDo0NVrOHpUvug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yN1QyMzozNDo0NVrOHpUvug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA5MzU2Mg==", "bodyText": "So far we don't have plans to do anything fancy for batch but added this override to avoid breaking the transform in batch if withShardedKey is specified.", "url": "https://github.com/apache/beam/pull/13208#discussion_r513093562", "createdAt": "2020-10-27T23:34:45Z", "author": {"login": "nehsyc"}, "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/GroupIntoBatchesOverride.java", "diffHunk": "@@ -92,9 +96,58 @@ public void process(ProcessContext c) {\n     }\n   }\n \n+  static class BatchGroupIntoBatchesWithShardedKeyOverrideFactory<K, V>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da5ccd0f8680ebd23c228941ae393b4c07f1ed09"}, "originalPosition": 31}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTE4OTY0NzU2", "url": "https://github.com/apache/beam/pull/13208#pullrequestreview-518964756", "createdAt": "2020-10-28T18:24:38Z", "commit": {"oid": "da5ccd0f8680ebd23c228941ae393b4c07f1ed09"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxODoyNDozOFrOHp34fw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yOFQxODo0NDoxNVrOHp4mGg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY2OTI0Nw==", "bodyText": "I think it makes the most sense here.", "url": "https://github.com/apache/beam/pull/13208#discussion_r513669247", "createdAt": "2020-10-28T18:24:38Z", "author": {"login": "robertwb"}, "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/DataflowRunner.java", "diffHunk": "@@ -1281,8 +1286,12 @@ void addPCollectionRequiringIndexedFormat(PCollection<?> pcol) {\n   }\n \n   void maybeRecordPCollectionWithAutoSharding(PCollection<?> pcol) {\n-    if (hasExperiment(options, \"enable_streaming_auto_sharding\")\n-        && !hasExperiment(options, \"beam_fn_api\")) {\n+    if (hasExperiment(options, \"beam_fn_api\")) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA2Nzk2NQ=="}, "originalCommit": {"oid": "a50f4b2d7b0d82994c4847ad633bc688a9c92c78"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY2OTUzMA==", "bodyText": "Do we need an override or is the default implementation good enough?", "url": "https://github.com/apache/beam/pull/13208#discussion_r513669530", "createdAt": "2020-10-28T18:25:05Z", "author": {"login": "robertwb"}, "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/GroupIntoBatchesOverride.java", "diffHunk": "@@ -92,9 +96,58 @@ public void process(ProcessContext c) {\n     }\n   }\n \n+  static class BatchGroupIntoBatchesWithShardedKeyOverrideFactory<K, V>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA5MzU2Mg=="}, "originalCommit": {"oid": "da5ccd0f8680ebd23c228941ae393b4c07f1ed09"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY3MTQ4Mw==", "bodyText": "This doesn't look like it'll scale if more options are used. Why not just apply original?", "url": "https://github.com/apache/beam/pull/13208#discussion_r513671483", "createdAt": "2020-10-28T18:28:15Z", "author": {"login": "robertwb"}, "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/GroupIntoBatchesOverride.java", "diffHunk": "@@ -103,43 +156,76 @@ public void process(ProcessContext c) {\n     }\n \n     @Override\n-    public PTransformReplacement<PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>>\n+    public PTransformReplacement<PCollection<KV<K, V>>, PCollection<KV<ShardedKey<K>, Iterable<V>>>>\n         getReplacementTransform(\n             AppliedPTransform<\n-                    PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>, GroupIntoBatches<K, V>>\n+                    PCollection<KV<K, V>>,\n+                    PCollection<KV<ShardedKey<K>, Iterable<V>>>,\n+                    GroupIntoBatches<K, V>.WithShardedKey>\n                 transform) {\n       return PTransformReplacement.of(\n           PTransformReplacements.getSingletonMainInput(transform),\n-          new StreamingGroupIntoBatches(runner, transform.getTransform()));\n+          new StreamingGroupIntoBatchesWithShardedKey<>(runner, transform.getTransform()));\n     }\n \n     @Override\n     public Map<PCollection<?>, ReplacementOutput> mapOutputs(\n-        Map<TupleTag<?>, PCollection<?>> outputs, PCollection<KV<K, Iterable<V>>> newOutput) {\n+        Map<TupleTag<?>, PCollection<?>> outputs,\n+        PCollection<KV<ShardedKey<K>, Iterable<V>>> newOutput) {\n       return ReplacementOutputs.singleton(outputs, newOutput);\n     }\n   }\n \n   /**\n-   * Specialized implementation of {@link GroupIntoBatches} for unbounded Dataflow pipelines. The\n-   * override does the same thing as the original transform but additionally record the input to add\n-   * corresponding properties during the graph translation.\n+   * Specialized implementation of {@link GroupIntoBatches.WithShardedKey} for unbounded Dataflow\n+   * pipelines. The override does the same thing as the original transform but additionally records\n+   * the input of {@code GroupIntoBatchesDoFn} in order to append relevant step properties during\n+   * the graph translation.\n    */\n-  static class StreamingGroupIntoBatches<K, V>\n-      extends PTransform<PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>> {\n+  static class StreamingGroupIntoBatchesWithShardedKey<K, V>\n+      extends PTransform<PCollection<KV<K, V>>, PCollection<KV<ShardedKey<K>, Iterable<V>>>> {\n \n     private final transient DataflowRunner runner;\n-    private final GroupIntoBatches<K, V> original;\n+    private final GroupIntoBatches<K, V>.WithShardedKey original;\n \n-    public StreamingGroupIntoBatches(DataflowRunner runner, GroupIntoBatches<K, V> original) {\n+    public StreamingGroupIntoBatchesWithShardedKey(\n+        DataflowRunner runner, GroupIntoBatches<K, V>.WithShardedKey original) {\n       this.runner = runner;\n       this.original = original;\n     }\n \n     @Override\n-    public PCollection<KV<K, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n-      runner.maybeRecordPCollectionWithAutoSharding(input);\n-      return input.apply(original);\n+    public PCollection<KV<ShardedKey<K>, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n+      PCollection<KV<ShardedKey<K>, V>> intermediate_input = ShardKeys(input);\n+\n+      runner.maybeRecordPCollectionWithAutoSharding(intermediate_input);\n+\n+      if (original.getMaxBufferingDuration() != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da5ccd0f8680ebd23c228941ae393b4c07f1ed09"}, "originalPosition": 149}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY3MjY3NA==", "bodyText": "Methods shouldn't be capitalized.", "url": "https://github.com/apache/beam/pull/13208#discussion_r513672674", "createdAt": "2020-10-28T18:30:10Z", "author": {"login": "robertwb"}, "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/GroupIntoBatchesOverride.java", "diffHunk": "@@ -103,43 +156,76 @@ public void process(ProcessContext c) {\n     }\n \n     @Override\n-    public PTransformReplacement<PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>>\n+    public PTransformReplacement<PCollection<KV<K, V>>, PCollection<KV<ShardedKey<K>, Iterable<V>>>>\n         getReplacementTransform(\n             AppliedPTransform<\n-                    PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>, GroupIntoBatches<K, V>>\n+                    PCollection<KV<K, V>>,\n+                    PCollection<KV<ShardedKey<K>, Iterable<V>>>,\n+                    GroupIntoBatches<K, V>.WithShardedKey>\n                 transform) {\n       return PTransformReplacement.of(\n           PTransformReplacements.getSingletonMainInput(transform),\n-          new StreamingGroupIntoBatches(runner, transform.getTransform()));\n+          new StreamingGroupIntoBatchesWithShardedKey<>(runner, transform.getTransform()));\n     }\n \n     @Override\n     public Map<PCollection<?>, ReplacementOutput> mapOutputs(\n-        Map<TupleTag<?>, PCollection<?>> outputs, PCollection<KV<K, Iterable<V>>> newOutput) {\n+        Map<TupleTag<?>, PCollection<?>> outputs,\n+        PCollection<KV<ShardedKey<K>, Iterable<V>>> newOutput) {\n       return ReplacementOutputs.singleton(outputs, newOutput);\n     }\n   }\n \n   /**\n-   * Specialized implementation of {@link GroupIntoBatches} for unbounded Dataflow pipelines. The\n-   * override does the same thing as the original transform but additionally record the input to add\n-   * corresponding properties during the graph translation.\n+   * Specialized implementation of {@link GroupIntoBatches.WithShardedKey} for unbounded Dataflow\n+   * pipelines. The override does the same thing as the original transform but additionally records\n+   * the input of {@code GroupIntoBatchesDoFn} in order to append relevant step properties during\n+   * the graph translation.\n    */\n-  static class StreamingGroupIntoBatches<K, V>\n-      extends PTransform<PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>> {\n+  static class StreamingGroupIntoBatchesWithShardedKey<K, V>\n+      extends PTransform<PCollection<KV<K, V>>, PCollection<KV<ShardedKey<K>, Iterable<V>>>> {\n \n     private final transient DataflowRunner runner;\n-    private final GroupIntoBatches<K, V> original;\n+    private final GroupIntoBatches<K, V>.WithShardedKey original;\n \n-    public StreamingGroupIntoBatches(DataflowRunner runner, GroupIntoBatches<K, V> original) {\n+    public StreamingGroupIntoBatchesWithShardedKey(\n+        DataflowRunner runner, GroupIntoBatches<K, V>.WithShardedKey original) {\n       this.runner = runner;\n       this.original = original;\n     }\n \n     @Override\n-    public PCollection<KV<K, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n-      runner.maybeRecordPCollectionWithAutoSharding(input);\n-      return input.apply(original);\n+    public PCollection<KV<ShardedKey<K>, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n+      PCollection<KV<ShardedKey<K>, V>> intermediate_input = ShardKeys(input);\n+\n+      runner.maybeRecordPCollectionWithAutoSharding(intermediate_input);\n+\n+      if (original.getMaxBufferingDuration() != null) {\n+        return intermediate_input.apply(\n+            GroupIntoBatches.<ShardedKey<K>, V>ofSize(original.getBatchSize())\n+                .withMaxBufferingDuration(original.getMaxBufferingDuration()));\n+      } else {\n+        return intermediate_input.apply(GroupIntoBatches.ofSize(original.getBatchSize()));\n+      }\n     }\n   }\n+\n+  private static <K, V> PCollection<KV<ShardedKey<K>, V>> ShardKeys(PCollection<KV<K, V>> input) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da5ccd0f8680ebd23c228941ae393b4c07f1ed09"}, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY3NDA5Mg==", "bodyText": "Correct, we can't do this by default due to the coder change.", "url": "https://github.com/apache/beam/pull/13208#discussion_r513674092", "createdAt": "2020-10-28T18:32:33Z", "author": {"login": "robertwb"}, "path": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/GroupIntoBatches.java", "diffHunk": "@@ -105,23 +108,83 @@ public long getBatchSize() {\n   }\n \n   /**\n-   * Set a time limit (in processing time) on how long an incomplete batch of elements is allowed to\n-   * be buffered. Once a batch is flushed to output, the timer is reset.\n+   * Sets a time limit (in processing time) on how long an incomplete batch of elements is allowed\n+   * to be buffered. Once a batch is flushed to output, the timer is reset.\n    */\n   public GroupIntoBatches<K, InputT> withMaxBufferingDuration(Duration duration) {\n     checkArgument(\n         duration.isLongerThan(Duration.ZERO), \"max buffering duration should be a positive value\");\n     return new GroupIntoBatches<>(batchSize, duration);\n   }\n \n+  /**\n+   * Outputs batched elements associated with sharded input keys. The sharding is determined by the\n+   * runner to balance the load during the execution time. By default, apply no sharding so each key\n+   * has one shard.\n+   */\n+  @Experimental\n+  public WithShardedKey withShardedKey() {\n+    return new WithShardedKey();\n+  }\n+\n+  public class WithShardedKey", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA3MjU5NQ=="}, "originalCommit": {"oid": "bbf10680a68f36cbd9482f7fb0b4f93907e5b289"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY3NzM4NA==", "bodyText": "Alternatively one could apply the original GroupIntoBatches that this was derived from here.", "url": "https://github.com/apache/beam/pull/13208#discussion_r513677384", "createdAt": "2020-10-28T18:38:11Z", "author": {"login": "robertwb"}, "path": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/GroupIntoBatches.java", "diffHunk": "@@ -105,23 +108,83 @@ public long getBatchSize() {\n   }\n \n   /**\n-   * Set a time limit (in processing time) on how long an incomplete batch of elements is allowed to\n-   * be buffered. Once a batch is flushed to output, the timer is reset.\n+   * Sets a time limit (in processing time) on how long an incomplete batch of elements is allowed\n+   * to be buffered. Once a batch is flushed to output, the timer is reset.\n    */\n   public GroupIntoBatches<K, InputT> withMaxBufferingDuration(Duration duration) {\n     checkArgument(\n         duration.isLongerThan(Duration.ZERO), \"max buffering duration should be a positive value\");\n     return new GroupIntoBatches<>(batchSize, duration);\n   }\n \n+  /**\n+   * Outputs batched elements associated with sharded input keys. The sharding is determined by the\n+   * runner to balance the load during the execution time. By default, apply no sharding so each key\n+   * has one shard.\n+   */\n+  @Experimental\n+  public WithShardedKey withShardedKey() {\n+    return new WithShardedKey();\n+  }\n+\n+  public class WithShardedKey\n+      extends PTransform<\n+          PCollection<KV<K, InputT>>, PCollection<KV<ShardedKey<K>, Iterable<InputT>>>> {\n+\n+    /** Returns the size of the batch. */\n+    public long getBatchSize() {\n+      return batchSize;\n+    }\n+\n+    /** Returns the size of the batch. */\n+    @Nullable\n+    public Duration getMaxBufferingDuration() {\n+      return maxBufferingDuration;\n+    }\n+\n+    @Override\n+    public PCollection<KV<ShardedKey<K>, Iterable<InputT>>> expand(\n+        PCollection<KV<K, InputT>> input) {\n+      Duration allowedLateness = input.getWindowingStrategy().getAllowedLateness();\n+\n+      checkArgument(\n+          input.getCoder() instanceof KvCoder,\n+          \"coder specified in the input PCollection is not a KvCoder\");\n+      KvCoder<K, InputT> inputCoder = (KvCoder<K, InputT>) input.getCoder();\n+      Coder<K> keyCoder = (Coder<K>) inputCoder.getCoderArguments().get(0);\n+      Coder<InputT> valueCoder = (Coder<InputT>) inputCoder.getCoderArguments().get(1);\n+\n+      return input\n+          .apply(\n+              MapElements.via(\n+                  new SimpleFunction<KV<K, InputT>, KV<ShardedKey<K>, InputT>>() {\n+                    @Override\n+                    public KV<ShardedKey<K>, InputT> apply(KV<K, InputT> input) {\n+                      // By default every input key has only one shard.\n+                      return KV.of(\n+                          ShardedKey.of(input.getKey(), DEFAULT_SHARD_ID), input.getValue());\n+                    }\n+                  }))\n+          .setCoder(KvCoder.of(ShardedKey.Coder.of(keyCoder), valueCoder))\n+          .apply(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da5ccd0f8680ebd23c228941ae393b4c07f1ed09"}, "originalPosition": 88}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzY4MDkyMg==", "bodyText": "A single subshard by default will make this virtually unusable for runners that don't implement the optimization (including batch Dataflow). Instead use something like the thread id here, or at the very lease initialize DEFAULT_SHARD_ID to be different for each worker and add a small nonce. We could alternatively take a hint as to the number of subshards that would be nice (but that has its own downsides).", "url": "https://github.com/apache/beam/pull/13208#discussion_r513680922", "createdAt": "2020-10-28T18:44:15Z", "author": {"login": "robertwb"}, "path": "sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/GroupIntoBatches.java", "diffHunk": "@@ -105,23 +108,83 @@ public long getBatchSize() {\n   }\n \n   /**\n-   * Set a time limit (in processing time) on how long an incomplete batch of elements is allowed to\n-   * be buffered. Once a batch is flushed to output, the timer is reset.\n+   * Sets a time limit (in processing time) on how long an incomplete batch of elements is allowed\n+   * to be buffered. Once a batch is flushed to output, the timer is reset.\n    */\n   public GroupIntoBatches<K, InputT> withMaxBufferingDuration(Duration duration) {\n     checkArgument(\n         duration.isLongerThan(Duration.ZERO), \"max buffering duration should be a positive value\");\n     return new GroupIntoBatches<>(batchSize, duration);\n   }\n \n+  /**\n+   * Outputs batched elements associated with sharded input keys. The sharding is determined by the\n+   * runner to balance the load during the execution time. By default, apply no sharding so each key\n+   * has one shard.\n+   */\n+  @Experimental\n+  public WithShardedKey withShardedKey() {\n+    return new WithShardedKey();\n+  }\n+\n+  public class WithShardedKey\n+      extends PTransform<\n+          PCollection<KV<K, InputT>>, PCollection<KV<ShardedKey<K>, Iterable<InputT>>>> {\n+\n+    /** Returns the size of the batch. */\n+    public long getBatchSize() {\n+      return batchSize;\n+    }\n+\n+    /** Returns the size of the batch. */\n+    @Nullable\n+    public Duration getMaxBufferingDuration() {\n+      return maxBufferingDuration;\n+    }\n+\n+    @Override\n+    public PCollection<KV<ShardedKey<K>, Iterable<InputT>>> expand(\n+        PCollection<KV<K, InputT>> input) {\n+      Duration allowedLateness = input.getWindowingStrategy().getAllowedLateness();\n+\n+      checkArgument(\n+          input.getCoder() instanceof KvCoder,\n+          \"coder specified in the input PCollection is not a KvCoder\");\n+      KvCoder<K, InputT> inputCoder = (KvCoder<K, InputT>) input.getCoder();\n+      Coder<K> keyCoder = (Coder<K>) inputCoder.getCoderArguments().get(0);\n+      Coder<InputT> valueCoder = (Coder<InputT>) inputCoder.getCoderArguments().get(1);\n+\n+      return input\n+          .apply(\n+              MapElements.via(\n+                  new SimpleFunction<KV<K, InputT>, KV<ShardedKey<K>, InputT>>() {\n+                    @Override\n+                    public KV<ShardedKey<K>, InputT> apply(KV<K, InputT> input) {\n+                      // By default every input key has only one shard.\n+                      return KV.of(\n+                          ShardedKey.of(input.getKey(), DEFAULT_SHARD_ID), input.getValue());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da5ccd0f8680ebd23c228941ae393b4c07f1ed09"}, "originalPosition": 84}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxODk5MjE3", "url": "https://github.com/apache/beam/pull/13208#pullrequestreview-521899217", "createdAt": "2020-11-02T18:39:24Z", "commit": {"oid": "be34365c6ada47c5ca74dac20a357f1cff5d0b9b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxODozOToyNVrOHsRINg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wMlQxODozOToyNVrOHsRINg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjE4MDAyMg==", "bodyText": "Thanks. These are likely to be re-used for all workers. Can you add in a statically initialized (for each worker) random long as well?", "url": "https://github.com/apache/beam/pull/13208#discussion_r516180022", "createdAt": "2020-11-02T18:39:25Z", "author": {"login": "robertwb"}, "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/GroupIntoBatchesOverride.java", "diffHunk": "@@ -210,20 +211,23 @@ public StreamingGroupIntoBatchesWithShardedKey(\n     }\n   }\n \n-  private static <K, V> PCollection<KV<ShardedKey<K>, V>> ShardKeys(PCollection<KV<K, V>> input) {\n+  private static <K, V> PCollection<KV<ShardedKey<K>, V>> shardKeys(PCollection<KV<K, V>> input) {\n     KvCoder<K, V> inputCoder = (KvCoder<K, V>) input.getCoder();\n     org.apache.beam.sdk.coders.Coder<K> keyCoder =\n         (org.apache.beam.sdk.coders.Coder<K>) inputCoder.getCoderArguments().get(0);\n     org.apache.beam.sdk.coders.Coder<V> valueCoder =\n         (org.apache.beam.sdk.coders.Coder<V>) inputCoder.getCoderArguments().get(1);\n     return input\n         .apply(\n-            \"ShardKeys\",\n+            \"Shard Keys\",\n             MapElements.via(\n                 new SimpleFunction<KV<K, V>, KV<ShardedKey<K>, V>>() {\n                   @Override\n                   public KV<ShardedKey<K>, V> apply(KV<K, V> input) {\n-                    return KV.of(ShardedKey.of(input.getKey(), new byte[0]), input.getValue());\n+                    long tid = Thread.currentThread().getId();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be34365c6ada47c5ca74dac20a357f1cff5d0b9b"}, "originalPosition": 46}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "dec9698d01961ae0f652fa348bd30e0ce86bea34", "author": {"user": {"login": "nehsyc", "name": "Siyuan Chen"}}, "url": "https://github.com/apache/beam/commit/dec9698d01961ae0f652fa348bd30e0ce86bea34", "committedDate": "2020-11-04T23:20:50Z", "message": "record sharded output to avoid duplicating code in the override"}, "afterCommit": {"oid": "8372dd85a2dcd1dd3dbab3d4233a0db3560ed71e", "author": {"user": {"login": "nehsyc", "name": "Siyuan Chen"}}, "url": "https://github.com/apache/beam/commit/8372dd85a2dcd1dd3dbab3d4233a0db3560ed71e", "committedDate": "2020-11-06T17:06:16Z", "message": "record sharded output to avoid duplicating code in the override"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM0NzgyMDk1", "url": "https://github.com/apache/beam/pull/13208#pullrequestreview-534782095", "createdAt": "2020-11-19T19:31:42Z", "commit": {"oid": "5eeb59ceceea58febdc72b2a75cd15fb681b7188"}, "state": "APPROVED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQxOTozMTo0MlrOH2uZBA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQxOTozODoyMVrOH2uoBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0NTIyMA==", "bodyText": "Well, we currently give different overrides in the two cases for Dataflow. Everything works, but there are different performance characteristics that encourage different implementations. (One could argue that this is runner-specific logic.) Fine not to change now.", "url": "https://github.com/apache/beam/pull/13208#discussion_r527145220", "createdAt": "2020-11-19T19:31:42Z", "author": {"login": "robertwb"}, "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/GroupIntoBatchesOverride.java", "diffHunk": "@@ -92,9 +96,58 @@ public void process(ProcessContext c) {\n     }\n   }\n \n+  static class BatchGroupIntoBatchesWithShardedKeyOverrideFactory<K, V>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxMzA5MzU2Mg=="}, "originalCommit": {"oid": "da5ccd0f8680ebd23c228941ae393b4c07f1ed09"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0ODExMw==", "bodyText": "Maybe take the sum of getMostSignificantBits() and getLeastSignificantBits()?", "url": "https://github.com/apache/beam/pull/13208#discussion_r527148113", "createdAt": "2020-11-19T19:36:32Z", "author": {"login": "robertwb"}, "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/GroupIntoBatchesOverride.java", "diffHunk": "@@ -92,54 +98,141 @@ public void process(ProcessContext c) {\n     }\n   }\n \n-  static class StreamingGroupIntoBatchesOverrideFactory<K, V>\n+  static class BatchGroupIntoBatchesWithShardedKeyOverrideFactory<K, V>\n       implements PTransformOverrideFactory<\n-          PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>, GroupIntoBatches<K, V>> {\n+          PCollection<KV<K, V>>,\n+          PCollection<KV<ShardedKey<K>, Iterable<V>>>,\n+          GroupIntoBatches<K, V>.WithShardedKey> {\n+\n+    @Override\n+    public PTransformReplacement<PCollection<KV<K, V>>, PCollection<KV<ShardedKey<K>, Iterable<V>>>>\n+        getReplacementTransform(\n+            AppliedPTransform<\n+                    PCollection<KV<K, V>>,\n+                    PCollection<KV<ShardedKey<K>, Iterable<V>>>,\n+                    GroupIntoBatches<K, V>.WithShardedKey>\n+                transform) {\n+      return PTransformReplacement.of(\n+          PTransformReplacements.getSingletonMainInput(transform),\n+          new BatchGroupIntoBatchesWithShardedKey<>(transform.getTransform().getBatchSize()));\n+    }\n+\n+    @Override\n+    public Map<PCollection<?>, ReplacementOutput> mapOutputs(\n+        Map<TupleTag<?>, PCollection<?>> outputs,\n+        PCollection<KV<ShardedKey<K>, Iterable<V>>> newOutput) {\n+      return ReplacementOutputs.singleton(outputs, newOutput);\n+    }\n+  }\n+\n+  /**\n+   * Specialized implementation of {@link GroupIntoBatches.WithShardedKey} for bounded Dataflow\n+   * pipelines.\n+   */\n+  static class BatchGroupIntoBatchesWithShardedKey<K, V>\n+      extends PTransform<PCollection<KV<K, V>>, PCollection<KV<ShardedKey<K>, Iterable<V>>>> {\n+\n+    private final long batchSize;\n+\n+    private BatchGroupIntoBatchesWithShardedKey(long batchSize) {\n+      this.batchSize = batchSize;\n+    }\n+\n+    @Override\n+    public PCollection<KV<ShardedKey<K>, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n+      PCollection<KV<ShardedKey<K>, V>> intermediate_input = shardKeys(input);\n+      return intermediate_input.apply(new BatchGroupIntoBatches<>(batchSize));\n+    }\n+  }\n+\n+  static class StreamingGroupIntoBatchesWithShardedKeyOverrideFactory<K, V>\n+      implements PTransformOverrideFactory<\n+          PCollection<KV<K, V>>,\n+          PCollection<KV<ShardedKey<K>, Iterable<V>>>,\n+          GroupIntoBatches<K, V>.WithShardedKey> {\n \n     private final DataflowRunner runner;\n \n-    StreamingGroupIntoBatchesOverrideFactory(DataflowRunner runner) {\n+    StreamingGroupIntoBatchesWithShardedKeyOverrideFactory(DataflowRunner runner) {\n       this.runner = runner;\n     }\n \n     @Override\n-    public PTransformReplacement<PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>>\n+    public PTransformReplacement<PCollection<KV<K, V>>, PCollection<KV<ShardedKey<K>, Iterable<V>>>>\n         getReplacementTransform(\n             AppliedPTransform<\n-                    PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>, GroupIntoBatches<K, V>>\n+                    PCollection<KV<K, V>>,\n+                    PCollection<KV<ShardedKey<K>, Iterable<V>>>,\n+                    GroupIntoBatches<K, V>.WithShardedKey>\n                 transform) {\n       return PTransformReplacement.of(\n           PTransformReplacements.getSingletonMainInput(transform),\n-          new StreamingGroupIntoBatches(runner, transform.getTransform()));\n+          new StreamingGroupIntoBatchesWithShardedKey<>(\n+              runner,\n+              transform.getTransform(),\n+              PTransformReplacements.getSingletonMainOutput(transform)));\n     }\n \n     @Override\n     public Map<PCollection<?>, ReplacementOutput> mapOutputs(\n-        Map<TupleTag<?>, PCollection<?>> outputs, PCollection<KV<K, Iterable<V>>> newOutput) {\n+        Map<TupleTag<?>, PCollection<?>> outputs,\n+        PCollection<KV<ShardedKey<K>, Iterable<V>>> newOutput) {\n       return ReplacementOutputs.singleton(outputs, newOutput);\n     }\n   }\n \n   /**\n-   * Specialized implementation of {@link GroupIntoBatches} for unbounded Dataflow pipelines. The\n-   * override does the same thing as the original transform but additionally record the input to add\n-   * corresponding properties during the graph translation.\n+   * Specialized implementation of {@link GroupIntoBatches.WithShardedKey} for unbounded Dataflow\n+   * pipelines. The override does the same thing as the original transform but additionally records\n+   * the output in order to append required step properties during the graph translation.\n    */\n-  static class StreamingGroupIntoBatches<K, V>\n-      extends PTransform<PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>> {\n+  static class StreamingGroupIntoBatchesWithShardedKey<K, V>\n+      extends PTransform<PCollection<KV<K, V>>, PCollection<KV<ShardedKey<K>, Iterable<V>>>> {\n \n     private final transient DataflowRunner runner;\n-    private final GroupIntoBatches<K, V> original;\n+    private final GroupIntoBatches<K, V>.WithShardedKey original_transform;\n+    private final PCollection<KV<ShardedKey<K>, Iterable<V>>> original_output;\n \n-    public StreamingGroupIntoBatches(DataflowRunner runner, GroupIntoBatches<K, V> original) {\n+    public StreamingGroupIntoBatchesWithShardedKey(\n+        DataflowRunner runner,\n+        GroupIntoBatches<K, V>.WithShardedKey original,\n+        PCollection<KV<ShardedKey<K>, Iterable<V>>> output) {\n       this.runner = runner;\n-      this.original = original;\n+      this.original_transform = original;\n+      this.original_output = output;\n     }\n \n     @Override\n-    public PCollection<KV<K, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n-      runner.maybeRecordPCollectionWithAutoSharding(input);\n-      return input.apply(original);\n+    public PCollection<KV<ShardedKey<K>, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n+      // Record the output PCollection of the original transform since the new output will be\n+      // replaced by the original one when the replacement transform is wired to other nodes in the\n+      // graph, although the old and the new outputs are effectively the same.\n+      runner.maybeRecordPCollectionWithAutoSharding(original_output);\n+      return input.apply(original_transform);\n     }\n   }\n+\n+  private static final long uuid = UUID.randomUUID().getMostSignificantBits();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5eeb59ceceea58febdc72b2a75cd15fb681b7188"}, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0ODQ1Mw==", "bodyText": "Call this workerUuid?", "url": "https://github.com/apache/beam/pull/13208#discussion_r527148453", "createdAt": "2020-11-19T19:37:10Z", "author": {"login": "robertwb"}, "path": "runners/google-cloud-dataflow-java/src/main/java/org/apache/beam/runners/dataflow/GroupIntoBatchesOverride.java", "diffHunk": "@@ -92,54 +98,141 @@ public void process(ProcessContext c) {\n     }\n   }\n \n-  static class StreamingGroupIntoBatchesOverrideFactory<K, V>\n+  static class BatchGroupIntoBatchesWithShardedKeyOverrideFactory<K, V>\n       implements PTransformOverrideFactory<\n-          PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>, GroupIntoBatches<K, V>> {\n+          PCollection<KV<K, V>>,\n+          PCollection<KV<ShardedKey<K>, Iterable<V>>>,\n+          GroupIntoBatches<K, V>.WithShardedKey> {\n+\n+    @Override\n+    public PTransformReplacement<PCollection<KV<K, V>>, PCollection<KV<ShardedKey<K>, Iterable<V>>>>\n+        getReplacementTransform(\n+            AppliedPTransform<\n+                    PCollection<KV<K, V>>,\n+                    PCollection<KV<ShardedKey<K>, Iterable<V>>>,\n+                    GroupIntoBatches<K, V>.WithShardedKey>\n+                transform) {\n+      return PTransformReplacement.of(\n+          PTransformReplacements.getSingletonMainInput(transform),\n+          new BatchGroupIntoBatchesWithShardedKey<>(transform.getTransform().getBatchSize()));\n+    }\n+\n+    @Override\n+    public Map<PCollection<?>, ReplacementOutput> mapOutputs(\n+        Map<TupleTag<?>, PCollection<?>> outputs,\n+        PCollection<KV<ShardedKey<K>, Iterable<V>>> newOutput) {\n+      return ReplacementOutputs.singleton(outputs, newOutput);\n+    }\n+  }\n+\n+  /**\n+   * Specialized implementation of {@link GroupIntoBatches.WithShardedKey} for bounded Dataflow\n+   * pipelines.\n+   */\n+  static class BatchGroupIntoBatchesWithShardedKey<K, V>\n+      extends PTransform<PCollection<KV<K, V>>, PCollection<KV<ShardedKey<K>, Iterable<V>>>> {\n+\n+    private final long batchSize;\n+\n+    private BatchGroupIntoBatchesWithShardedKey(long batchSize) {\n+      this.batchSize = batchSize;\n+    }\n+\n+    @Override\n+    public PCollection<KV<ShardedKey<K>, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n+      PCollection<KV<ShardedKey<K>, V>> intermediate_input = shardKeys(input);\n+      return intermediate_input.apply(new BatchGroupIntoBatches<>(batchSize));\n+    }\n+  }\n+\n+  static class StreamingGroupIntoBatchesWithShardedKeyOverrideFactory<K, V>\n+      implements PTransformOverrideFactory<\n+          PCollection<KV<K, V>>,\n+          PCollection<KV<ShardedKey<K>, Iterable<V>>>,\n+          GroupIntoBatches<K, V>.WithShardedKey> {\n \n     private final DataflowRunner runner;\n \n-    StreamingGroupIntoBatchesOverrideFactory(DataflowRunner runner) {\n+    StreamingGroupIntoBatchesWithShardedKeyOverrideFactory(DataflowRunner runner) {\n       this.runner = runner;\n     }\n \n     @Override\n-    public PTransformReplacement<PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>>\n+    public PTransformReplacement<PCollection<KV<K, V>>, PCollection<KV<ShardedKey<K>, Iterable<V>>>>\n         getReplacementTransform(\n             AppliedPTransform<\n-                    PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>, GroupIntoBatches<K, V>>\n+                    PCollection<KV<K, V>>,\n+                    PCollection<KV<ShardedKey<K>, Iterable<V>>>,\n+                    GroupIntoBatches<K, V>.WithShardedKey>\n                 transform) {\n       return PTransformReplacement.of(\n           PTransformReplacements.getSingletonMainInput(transform),\n-          new StreamingGroupIntoBatches(runner, transform.getTransform()));\n+          new StreamingGroupIntoBatchesWithShardedKey<>(\n+              runner,\n+              transform.getTransform(),\n+              PTransformReplacements.getSingletonMainOutput(transform)));\n     }\n \n     @Override\n     public Map<PCollection<?>, ReplacementOutput> mapOutputs(\n-        Map<TupleTag<?>, PCollection<?>> outputs, PCollection<KV<K, Iterable<V>>> newOutput) {\n+        Map<TupleTag<?>, PCollection<?>> outputs,\n+        PCollection<KV<ShardedKey<K>, Iterable<V>>> newOutput) {\n       return ReplacementOutputs.singleton(outputs, newOutput);\n     }\n   }\n \n   /**\n-   * Specialized implementation of {@link GroupIntoBatches} for unbounded Dataflow pipelines. The\n-   * override does the same thing as the original transform but additionally record the input to add\n-   * corresponding properties during the graph translation.\n+   * Specialized implementation of {@link GroupIntoBatches.WithShardedKey} for unbounded Dataflow\n+   * pipelines. The override does the same thing as the original transform but additionally records\n+   * the output in order to append required step properties during the graph translation.\n    */\n-  static class StreamingGroupIntoBatches<K, V>\n-      extends PTransform<PCollection<KV<K, V>>, PCollection<KV<K, Iterable<V>>>> {\n+  static class StreamingGroupIntoBatchesWithShardedKey<K, V>\n+      extends PTransform<PCollection<KV<K, V>>, PCollection<KV<ShardedKey<K>, Iterable<V>>>> {\n \n     private final transient DataflowRunner runner;\n-    private final GroupIntoBatches<K, V> original;\n+    private final GroupIntoBatches<K, V>.WithShardedKey original_transform;\n+    private final PCollection<KV<ShardedKey<K>, Iterable<V>>> original_output;\n \n-    public StreamingGroupIntoBatches(DataflowRunner runner, GroupIntoBatches<K, V> original) {\n+    public StreamingGroupIntoBatchesWithShardedKey(\n+        DataflowRunner runner,\n+        GroupIntoBatches<K, V>.WithShardedKey original,\n+        PCollection<KV<ShardedKey<K>, Iterable<V>>> output) {\n       this.runner = runner;\n-      this.original = original;\n+      this.original_transform = original;\n+      this.original_output = output;\n     }\n \n     @Override\n-    public PCollection<KV<K, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n-      runner.maybeRecordPCollectionWithAutoSharding(input);\n-      return input.apply(original);\n+    public PCollection<KV<ShardedKey<K>, Iterable<V>>> expand(PCollection<KV<K, V>> input) {\n+      // Record the output PCollection of the original transform since the new output will be\n+      // replaced by the original one when the replacement transform is wired to other nodes in the\n+      // graph, although the old and the new outputs are effectively the same.\n+      runner.maybeRecordPCollectionWithAutoSharding(original_output);\n+      return input.apply(original_transform);\n     }\n   }\n+\n+  private static final long uuid = UUID.randomUUID().getMostSignificantBits();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5eeb59ceceea58febdc72b2a75cd15fb681b7188"}, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzE0OTA2Mw==", "bodyText": "Could we go with a larger batch size (say 5 or 10) and also verify that most batches are of the expected size?", "url": "https://github.com/apache/beam/pull/13208#discussion_r527149063", "createdAt": "2020-11-19T19:38:21Z", "author": {"login": "robertwb"}, "path": "sdks/java/core/src/test/java/org/apache/beam/sdk/transforms/GroupIntoBatchesTest.java", "diffHunk": "@@ -125,6 +126,45 @@ public Void apply(Iterable<KV<String, Iterable<String>>> input) {\n     pipeline.run();\n   }\n \n+  @Test\n+  @Category({NeedsRunner.class, UsesTimersInParDo.class, UsesStatefulParDo.class})\n+  public void testWithShardedKeyInGlobalWindow() {\n+    // Since with default sharding, the number of subshards of of a key is nondeterministic, create", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "5eeb59ceceea58febdc72b2a75cd15fb681b7188"}, "originalPosition": 15}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5eeb59ceceea58febdc72b2a75cd15fb681b7188", "author": {"user": {"login": "nehsyc", "name": "Siyuan Chen"}}, "url": "https://github.com/apache/beam/commit/5eeb59ceceea58febdc72b2a75cd15fb681b7188", "committedDate": "2020-11-18T19:04:38Z", "message": "Update documentation; update uuid generation."}, "afterCommit": {"oid": "fea27034372375f7db0f0f3b09a62ac5ac0979cd", "author": {"user": {"login": "nehsyc", "name": "Siyuan Chen"}}, "url": "https://github.com/apache/beam/commit/fea27034372375f7db0f0f3b09a62ac5ac0979cd", "committedDate": "2020-11-20T05:28:26Z", "message": "Add an option to GroupIntoBatches to output ShardedKeys. Update Dataflow pipeline translation accordingly."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fea27034372375f7db0f0f3b09a62ac5ac0979cd", "author": {"user": {"login": "nehsyc", "name": "Siyuan Chen"}}, "url": "https://github.com/apache/beam/commit/fea27034372375f7db0f0f3b09a62ac5ac0979cd", "committedDate": "2020-11-20T05:28:26Z", "message": "Add an option to GroupIntoBatches to output ShardedKeys. Update Dataflow pipeline translation accordingly."}, "afterCommit": {"oid": "797b77d81c4772a7f5711fc5aeab49c4a0eb37ce", "author": {"user": {"login": "nehsyc", "name": "Siyuan Chen"}}, "url": "https://github.com/apache/beam/commit/797b77d81c4772a7f5711fc5aeab49c4a0eb37ce", "committedDate": "2020-11-20T18:20:40Z", "message": "Add an option to GroupIntoBatches to output ShardedKeys. Update Dataflow pipeline translation accordingly."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "797b77d81c4772a7f5711fc5aeab49c4a0eb37ce", "author": {"user": {"login": "nehsyc", "name": "Siyuan Chen"}}, "url": "https://github.com/apache/beam/commit/797b77d81c4772a7f5711fc5aeab49c4a0eb37ce", "committedDate": "2020-11-20T18:20:40Z", "message": "Add an option to GroupIntoBatches to output ShardedKeys. Update Dataflow pipeline translation accordingly."}, "afterCommit": {"oid": "4cc0f688397771adb985b62a6319796ba8c350a5", "author": {"user": {"login": "nehsyc", "name": "Siyuan Chen"}}, "url": "https://github.com/apache/beam/commit/4cc0f688397771adb985b62a6319796ba8c350a5", "committedDate": "2020-11-23T18:38:32Z", "message": "Add an option to GroupIntoBatches to output ShardedKeys. Update Dataflow pipeline translation accordingly."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "4cc0f688397771adb985b62a6319796ba8c350a5", "author": {"user": {"login": "nehsyc", "name": "Siyuan Chen"}}, "url": "https://github.com/apache/beam/commit/4cc0f688397771adb985b62a6319796ba8c350a5", "committedDate": "2020-11-23T18:38:32Z", "message": "Add an option to GroupIntoBatches to output ShardedKeys. Update Dataflow pipeline translation accordingly."}, "afterCommit": {"oid": "1ceeaed84cfcf7b77becabaefe5d69632e294e70", "author": {"user": {"login": "nehsyc", "name": "Siyuan Chen"}}, "url": "https://github.com/apache/beam/commit/1ceeaed84cfcf7b77becabaefe5d69632e294e70", "committedDate": "2020-11-23T20:23:16Z", "message": "Add an option to GroupIntoBatches to output ShardedKeys. Update Dataflow pipeline translation accordingly."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b51d64e0eee662a1cc75f1a558ef99c2e812813e", "author": {"user": {"login": "nehsyc", "name": "Siyuan Chen"}}, "url": "https://github.com/apache/beam/commit/b51d64e0eee662a1cc75f1a558ef99c2e812813e", "committedDate": "2020-11-24T00:01:16Z", "message": "Add an option to GroupIntoBatches to output ShardedKeys. Update Dataflow pipeline translation accordingly."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1ceeaed84cfcf7b77becabaefe5d69632e294e70", "author": {"user": {"login": "nehsyc", "name": "Siyuan Chen"}}, "url": "https://github.com/apache/beam/commit/1ceeaed84cfcf7b77becabaefe5d69632e294e70", "committedDate": "2020-11-23T20:23:16Z", "message": "Add an option to GroupIntoBatches to output ShardedKeys. Update Dataflow pipeline translation accordingly."}, "afterCommit": {"oid": "b51d64e0eee662a1cc75f1a558ef99c2e812813e", "author": {"user": {"login": "nehsyc", "name": "Siyuan Chen"}}, "url": "https://github.com/apache/beam/commit/b51d64e0eee662a1cc75f1a558ef99c2e812813e", "committedDate": "2020-11-24T00:01:16Z", "message": "Add an option to GroupIntoBatches to output ShardedKeys. Update Dataflow pipeline translation accordingly."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1770, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}