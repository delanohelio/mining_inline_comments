{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzYwOTg3NDI0", "number": 10543, "title": "[BEAM-8941] Implement simple DSL for load tests", "bodyText": "Implement simple DSL for load tests. Probably first iteration of this PR.\nChanges:\n\nconfig -- creates configuration map from given closure (example below)\ntemplateConfig -- create reusable LoadTestConfig instance (no need to create whole map for every single test), specific settings can be changed for every configuration map made from template (title, input options etc)\nfromTemplate -- creates configuration map from given template, specific settings can be changed\nvalidation for every returned map is performed (ex Flink runner requires environmentConfig to be set, test can not be launched without runner etc)\n\ndef loadTestConfigurations = { datasetName ->\n    final def template = templateConfig {\n        test 'apache_beam.testing.load_tests.group_by_key_test:GroupByKeyTest.testGroupByKey'\n        dataflow()\n        pipelineOptions {\n            python()\n            project 'apache-beam-testing'\n            tempLocation 'gs://temp-storage-for-perf-tests/loadtests'\n            publishToBigQuery true\n            metricsDataset datasetName\n            numWorkers 5\n            autoscalingAlgorithm \"NONE\"\n            specificParameters([\n                fanout: 1,\n                iterations: 1\n            ]}\n        }\n    }\n    [\n            fromTemplate(template) {\n                title 'GroupByKey Python Load test 2GB of 10B records'\n                pipelineOptions {\n                    jobName 'load-tests-python-dataflow-batch-gbk-1-'\n                    metricsTable 'python_dataflow_batch_gbk_1'\n                    inputOptions {\n                        numRecords 200000000\n                        keySize 1\n                        valueSize 9\n                    }\n\n                }\n            },\n            fromTemplate(template) {\n                title 'GroupByKey Python Load test 2GB of 100kB records'\n                pipelineOptions {\n                    jobName 'load-tests-python-dataflow-batch-gbk-3-'\n                    metricsTable 'python_dataflow_batch_gbk_3'\n                    inputOptions {\n                        numRecords 2000\n                        keySize 100000\n                        valueSize 900000\n                    }\n\n                }\n            },\n            fromTemplate(template) {\n                title 'GroupByKey Python Load test fanout 4 times with 2GB 10-byte records total'\n                pipelineOptions {\n                    jobName 'load-tests-python-dataflow-batch-gbk-4-'\n                    metricsTable 'python_dataflow_batch_gbk_4'\n                    inputOptions {\n                        numRecords 5000000\n                        keySize 10\n                        valueSize 90\n                    }\n                    specificParameters([\n                        fanout: 5\n                    ]}\n                }\n            }\n    ]\n}\n//compatible with current, map based implementation\nloadTestsBuilder.loadTests(delegate, CommonTestProperties.SDK.PYTHON, loadTestConfigurations(datasetName), \"GBK\", \"batch\")\n\n\nThank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:\n\n Choose reviewer(s) and mention them in a comment (R: @username).\n Format the pull request title like [BEAM-XXX] Fixes bug in ApproximateQuantiles, where you replace BEAM-XXX with the appropriate JIRA issue, if applicable. This will automatically link the pull request to the issue.\n If this contribution is large, please file an Apache Individual Contributor License Agreement.\n\nSee the Contributor Guide for more tips on how to make review process smoother.\nPost-Commit Tests Status (on master branch)\n\n\n\nLang\nSDK\nApex\nDataflow\nFlink\nGearpump\nSamza\nSpark\n\n\n\n\nGo\n\n---\n---\n\n---\n---\n\n\n\nJava\n\n\n\n\n\n\n\n\n\nPython\n\n---\n\n\n---\n---\n\n\n\nXLang\n---\n---\n---\n\n---\n---\n---\n\n\n\nPre-Commit Tests Status (on master branch)\n\n\n\n---\nJava\nPython\nGo\nWebsite\n\n\n\n\nNon-portable\n\n\n\n\n\n\nPortable\n---\n\n---\n---\n\n\n\nSee .test-infra/jenkins/README for trigger phrase, status and link of all Jenkins jobs.", "createdAt": "2020-01-09T14:28:10Z", "url": "https://github.com/apache/beam/pull/10543", "merged": true, "mergeCommit": {"oid": "1753b43670c9ceaae524dc7b3a26164bef3efe60"}, "closed": true, "closedAt": "2020-01-27T13:20:15Z", "author": {"login": "pawelpasterz"}, "timelineItems": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABb4q2r9ABqjI5MzUxNTY4ODc=", "endCursor": "Y3Vyc29yOnYyOpPPAAABb-cOGzgBqjI5ODE2NjE2ODA=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "fd93b5a328418dcc79cd9e7b3e676472c59785dd", "author": {"user": {"login": "pawelpasterz", "name": null}}, "url": "https://github.com/apache/beam/commit/fd93b5a328418dcc79cd9e7b3e676472c59785dd", "committedDate": "2020-01-09T14:05:41Z", "message": "[BEAM-8941] Implement simple DSL for load tests"}, "afterCommit": {"oid": "29804b0888dd3940632d6895d176283dbda4cf7d", "author": {"user": {"login": "pawelpasterz", "name": null}}, "url": "https://github.com/apache/beam/commit/29804b0888dd3940632d6895d176283dbda4cf7d", "committedDate": "2020-01-09T14:30:11Z", "message": "[BEAM-8941] Implement simple DSL for load tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "29804b0888dd3940632d6895d176283dbda4cf7d", "author": {"user": {"login": "pawelpasterz", "name": null}}, "url": "https://github.com/apache/beam/commit/29804b0888dd3940632d6895d176283dbda4cf7d", "committedDate": "2020-01-09T14:30:11Z", "message": "[BEAM-8941] Implement simple DSL for load tests"}, "afterCommit": {"oid": "57fd8672c46e4fcb496867da907d1ac4f2676481", "author": {"user": {"login": "pawelpasterz", "name": null}}, "url": "https://github.com/apache/beam/commit/57fd8672c46e4fcb496867da907d1ac4f2676481", "committedDate": "2020-01-10T07:30:44Z", "message": "[BEAM-8941] Implement simple DSL for load tests"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQxMjQ1MTU0", "url": "https://github.com/apache/beam/pull/10543#pullrequestreview-341245154", "createdAt": "2020-01-10T15:40:54Z", "commit": {"oid": "57fd8672c46e4fcb496867da907d1ac4f2676481"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxNTo0MDo1NVrOFcX1sQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0xMFQxNTo0MTowNlrOFcX2Fw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTI5NTAyNQ==", "bodyText": "There is also one more important pipeline option which is consumed by every test (both in Java and Python): streaming. Let's set its default value to False, so we don't have to manually specify it in all batch tests.", "url": "https://github.com/apache/beam/pull/10543#discussion_r365295025", "createdAt": "2020-01-10T15:40:55Z", "author": {"login": "kamilwu"}, "path": ".test-infra/jenkins/LoadTestConfig.groovy", "diffHunk": "@@ -0,0 +1,436 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import CommonTestProperties.Runner\n+import CommonTestProperties.SDK\n+import LoadTestConfig.SerializableOption\n+import groovy.json.JsonBuilder\n+import org.codehaus.groovy.runtime.InvokerHelper\n+\n+import static java.util.Objects.nonNull\n+import static java.util.Objects.requireNonNull\n+\n+/**\n+ * This class contains simple DSL for load tests configuration. Configuration as Map<String, Serializable>\n+ * [{@link LoadTestConfig#config config} -- returns configuration map]\n+ * [{@link LoadTestConfig#templateConfig templateConfig} -- return LoadTestConfig reusable object]\n+ * [{@link LoadTestConfig#fromTemplate fromTemplate} -- returns configuration from given template].<br><br>\n+ *\n+ * Example:\n+ * <blockquote><pre>\n+ * LoadTestConfig template = templateConfig {\n+ *     title 'Load test'\n+ *     test 'Beam Load test'\n+ *     dataflow()\n+ *     pipelineOptions {\n+ *         python()\n+ *         jobName 'Any job name'\n+ *         publishToBigQuery true\n+ *         //other fields\n+ *     }\n+ * }\n+ * Map<String, Serializable> configMap = fromTemplate(template) {\n+ *     //fields can be changed or/and added\n+ *     flink()\n+ *     pipelineOptions {\n+ *         parallelism 5\n+ *         inputOptions {\n+ *             numRecords 20000\n+ *             keySize 1000\n+ *             valueSize 10\n+ *         }\n+ *     }\n+ * }\n+ * </pre></blockquote>\n+ */\n+class LoadTestConfig implements SerializableOption<Map<String, Serializable>> {\n+\n+    private String _title\n+    private String _test\n+    private Runner _runner\n+    private PipelineOptions _pipelineOptions\n+\n+    private LoadTestConfig() {}\n+\n+    void title(final String title) { _title = title }\n+    void test(final String test) { _test = test }\n+\n+    //runners\n+    void dataflow() { setRunnerAndUpdatePipelineOptions(Runner.DATAFLOW)}\n+    void spark() { setRunnerAndUpdatePipelineOptions(Runner.SPARK) }\n+    void flink() { setRunnerAndUpdatePipelineOptions(Runner.FLINK) }\n+    void direct() { setRunnerAndUpdatePipelineOptions(Runner.DIRECT) }\n+    void portable() { setRunnerAndUpdatePipelineOptions(Runner.PORTABLE) }\n+\n+    private void setRunnerAndUpdatePipelineOptions(final Runner runner) {\n+        _runner = runner\n+        final def pipeline = _pipelineOptions ?: new PipelineOptions()\n+        pipeline.i_runner = runner\n+        _pipelineOptions = pipeline\n+    }\n+\n+    void pipelineOptions(final Closure cl = {}) {\n+        final def options = _pipelineOptions ?: new PipelineOptions()\n+        delegateAndInvoke(options, cl)\n+        _pipelineOptions = options\n+    }\n+\n+    /**\n+     * Returns load test config object which can be reusable.</br>\n+     * All possible fields that can be set:\n+     * <blockquote><pre>\n+     * templateConfig {\n+     *     title        [String]\n+     *     test         [String]\n+     *     [dataflow(), spark(), flink(), direct(), portable()] -- runner\n+     *     pipelineOptions {\n+     *         [python(), python37(), java()] -- sdk\n+     *         jobName              [String]\n+     *         project              [String]\n+     *         publishToBigQuery    [boolean]\n+     *         metricsDataset       [String]\n+     *         metricsTable         [String]\n+     *         iterations           [int]\n+     *         fanout               [int]\n+     *         numWorkers           [int]\n+     *         parallelism          [int]\n+     *         tempLocation         [String]\n+     *         autoscalingAlgorithm [String]\n+     *         jobEndpoint          [String]\n+     *         environmentType      [String]\n+     *         environmentConfig    [String]\n+     *         inputOptions {\n+     *             numRecords       [int]\n+     *             keySize          [int]\n+     *             valueSize        [int]\n+     *             numHotKeys       [int]\n+     *             hotKeyFraction   [int]\n+     *         }\n+     *     }\n+     * }\n+     * </pre></blockquote>\n+     * @param cl Closure with fields setting\n+     * @return LoadTestConfig object\n+     */\n+    static LoadTestConfig templateConfig(final Closure cl = {}) {\n+        final def config = new LoadTestConfig()\n+        delegateAndInvoke(config, cl)\n+        return config\n+    }\n+\n+    /**\n+     * Returns configuration map from given template. Any field can be changed or/and added. Validation is performed\n+     * before final map is returned (ex. Flink runner requires <b>environmentConfig</b> to be set). In case of\n+     * validation failure exception is thrown.<br>\n+     * Example result:\n+     *<blockquote><pre>\n+     * [\n+     *  title          : 'any given title',\n+     *  test           : 'any given test',\n+     *  runner         : CommonTestProperties.Runner.DATAFLOW,\n+     *  pipelineOptions: [\n+     *    job_name            : 'any given job name',\n+     *    publish_to_big_query: true,\n+     *    project             : 'apache-beam-testing',\n+     *    metrics_dataset     : 'given_dataset_name',\n+     *    metrics_table       : 'given_table_name',\n+     *    input_options       : '\\'{\"num_records\": 200000000,\"key_size\": 1,\"value_size\":9}\\'',\n+     *    iterations          : 1,\n+     *    fanout              : 1,\n+     *    parallelism         : 5,\n+     *    job_endpoint        : 'localhost:1234',\n+     *    environment_config  : 'given_environment_config',\n+     *    environment_type    : 'given_environment_type'\n+     *  ]\n+     * ]\n+     * </blockquote></pre>\n+     * @param templateConfig LoadTestConfig instance\n+     * @param cl Closure with fields setting\n+     * @return configuration map\n+     * @see LoadTestConfig\n+     * @see LoadTestConfig#templateConfig\n+     */\n+    static Map<String, Serializable> fromTemplate(final LoadTestConfig templateConfig, final Closure cl = {}) {\n+        final def newConfig = of(templateConfig)\n+        delegateAndInvoke(newConfig, cl)\n+        final def properties = newConfig.propertiesMap\n+        verifyProperties(properties)\n+        return ConfigHelper.convertProperties(properties)\n+    }\n+\n+    /**\n+     * Returns configuration map (see {@link LoadTestConfig#fromTemplate}) directly from given settings\n+     * @param cl Closure with settings\n+     * @return configuration map\n+     */\n+    static Map<String, Serializable> config(final Closure cl = {}) {\n+        final def config = new LoadTestConfig()\n+        delegateAndInvoke(config, cl)\n+        final def properties = config.propertiesMap\n+        verifyProperties(properties)\n+        return ConfigHelper.convertProperties(config.propertiesMap)\n+    }\n+\n+    private static void delegateAndInvoke(final delegate, final Closure cl = {}) {\n+        final def code = cl.rehydrate(delegate, this, this)\n+        code.resolveStrategy = Closure.DELEGATE_ONLY\n+        code()\n+    }\n+\n+    private static LoadTestConfig of(final LoadTestConfig oldConfig) {\n+        final def newConfig = new LoadTestConfig()\n+\n+        //primitive values\n+        InvokerHelper.setProperties(newConfig, oldConfig.propertiesMap)\n+\n+        //non-primitive values\n+        newConfig._pipelineOptions = oldConfig._pipelineOptions ? PipelineOptions.of(oldConfig._pipelineOptions) : null\n+\n+        return newConfig\n+    }\n+\n+    @Override\n+    Map<String, Serializable> toPrimitiveValues() {\n+        final def map = propertiesMap\n+        verifyProperties(map)\n+        return ConfigHelper.convertProperties(map)\n+    }\n+\n+    LinkedHashMap<String, Object> getPropertiesMap() {\n+        return [\n+                _title: _title,\n+                _test: _test,\n+                _runner: _runner,\n+                _pipelineOptions: _pipelineOptions\n+        ]\n+    }\n+\n+    private static void verifyProperties(final LinkedHashMap<String, Object> map) {\n+        for (entry in map.entrySet()) {\n+            requireNonNull(entry.value, \"Missing ${entry.key.substring(1)} in configuration\")\n+        }\n+    }\n+\n+    private static class PipelineOptions implements SerializableOption<Map<String, Serializable>> {\n+        private String _jobName\n+        private String _project\n+        private String  _publishToBigQuery\n+        private String  _metricsDataset\n+        private String  _metricsTable\n+        private InputOptions  _inputOptions\n+        private def  _iterations\n+        private def  _fanout\n+\n+        //internal usage\n+        private SDK i_sdk\n+        private Runner i_runner\n+        private static final i_dataflowRequired = [\"_numWorkers\", \"_tempLocation\", \"_autoscalingAlgorithm\"]\n+        private static final i_flinkRequired = [\"_jobEndpoint\", \"_environmentType\", \"_environmentConfig\", \"_parallelism\"]\n+\n+        //dataflow only\n+        private def  _numWorkers\n+        private String  _tempLocation\n+        private String  _autoscalingAlgorithm\n+\n+        //flink only\n+        private String _jobEndpoint\n+        private String _environmentType\n+        private String _environmentConfig\n+        private def _parallelism\n+\n+        void jobName(final String name) { _jobName = name }\n+        void project(final String project) { _project = project }\n+        void tempLocation(final String location) { _tempLocation = location }\n+        void publishToBigQuery(final boolean publish) { _publishToBigQuery = publish }\n+        void metricsDataset(final String dataset) { _metricsDataset = dataset }\n+        void metricsTable(final String table) { _metricsTable = table }\n+        void inputOptions(final InputOptions options) { _inputOptions = options }\n+        void iterations(final int itNumber) { _iterations = itNumber }\n+        void fanout(final int fanout) { _fanout = fanout }\n+        void numWorkers(final int workers) { _numWorkers = workers }\n+        void autoscalingAlgorithm(final String algorithm) { _autoscalingAlgorithm = algorithm }\n+        void jobEndpoint(final String endpoint) { _jobEndpoint = endpoint }\n+        void environmentType(final String type) { _environmentType = type }\n+        void environmentConfig(final String config) { _environmentConfig = config }\n+        void parallelism(final int parallelism) { _parallelism = parallelism }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57fd8672c46e4fcb496867da907d1ac4f2676481"}, "originalPosition": 270}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2NTI5NTEyNw==", "bodyText": "Not every load test requires iterations parameter. There are also tests which require totally different parameters. For example, combine test requires top_count parameter and this parameter is absent here.", "url": "https://github.com/apache/beam/pull/10543#discussion_r365295127", "createdAt": "2020-01-10T15:41:06Z", "author": {"login": "kamilwu"}, "path": ".test-infra/jenkins/LoadTestConfig.groovy", "diffHunk": "@@ -0,0 +1,436 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import CommonTestProperties.Runner\n+import CommonTestProperties.SDK\n+import LoadTestConfig.SerializableOption\n+import groovy.json.JsonBuilder\n+import org.codehaus.groovy.runtime.InvokerHelper\n+\n+import static java.util.Objects.nonNull\n+import static java.util.Objects.requireNonNull\n+\n+/**\n+ * This class contains simple DSL for load tests configuration. Configuration as Map<String, Serializable>\n+ * [{@link LoadTestConfig#config config} -- returns configuration map]\n+ * [{@link LoadTestConfig#templateConfig templateConfig} -- return LoadTestConfig reusable object]\n+ * [{@link LoadTestConfig#fromTemplate fromTemplate} -- returns configuration from given template].<br><br>\n+ *\n+ * Example:\n+ * <blockquote><pre>\n+ * LoadTestConfig template = templateConfig {\n+ *     title 'Load test'\n+ *     test 'Beam Load test'\n+ *     dataflow()\n+ *     pipelineOptions {\n+ *         python()\n+ *         jobName 'Any job name'\n+ *         publishToBigQuery true\n+ *         //other fields\n+ *     }\n+ * }\n+ * Map<String, Serializable> configMap = fromTemplate(template) {\n+ *     //fields can be changed or/and added\n+ *     flink()\n+ *     pipelineOptions {\n+ *         parallelism 5\n+ *         inputOptions {\n+ *             numRecords 20000\n+ *             keySize 1000\n+ *             valueSize 10\n+ *         }\n+ *     }\n+ * }\n+ * </pre></blockquote>\n+ */\n+class LoadTestConfig implements SerializableOption<Map<String, Serializable>> {\n+\n+    private String _title\n+    private String _test\n+    private Runner _runner\n+    private PipelineOptions _pipelineOptions\n+\n+    private LoadTestConfig() {}\n+\n+    void title(final String title) { _title = title }\n+    void test(final String test) { _test = test }\n+\n+    //runners\n+    void dataflow() { setRunnerAndUpdatePipelineOptions(Runner.DATAFLOW)}\n+    void spark() { setRunnerAndUpdatePipelineOptions(Runner.SPARK) }\n+    void flink() { setRunnerAndUpdatePipelineOptions(Runner.FLINK) }\n+    void direct() { setRunnerAndUpdatePipelineOptions(Runner.DIRECT) }\n+    void portable() { setRunnerAndUpdatePipelineOptions(Runner.PORTABLE) }\n+\n+    private void setRunnerAndUpdatePipelineOptions(final Runner runner) {\n+        _runner = runner\n+        final def pipeline = _pipelineOptions ?: new PipelineOptions()\n+        pipeline.i_runner = runner\n+        _pipelineOptions = pipeline\n+    }\n+\n+    void pipelineOptions(final Closure cl = {}) {\n+        final def options = _pipelineOptions ?: new PipelineOptions()\n+        delegateAndInvoke(options, cl)\n+        _pipelineOptions = options\n+    }\n+\n+    /**\n+     * Returns load test config object which can be reusable.</br>\n+     * All possible fields that can be set:\n+     * <blockquote><pre>\n+     * templateConfig {\n+     *     title        [String]\n+     *     test         [String]\n+     *     [dataflow(), spark(), flink(), direct(), portable()] -- runner\n+     *     pipelineOptions {\n+     *         [python(), python37(), java()] -- sdk\n+     *         jobName              [String]\n+     *         project              [String]\n+     *         publishToBigQuery    [boolean]\n+     *         metricsDataset       [String]\n+     *         metricsTable         [String]\n+     *         iterations           [int]\n+     *         fanout               [int]\n+     *         numWorkers           [int]\n+     *         parallelism          [int]\n+     *         tempLocation         [String]\n+     *         autoscalingAlgorithm [String]\n+     *         jobEndpoint          [String]\n+     *         environmentType      [String]\n+     *         environmentConfig    [String]\n+     *         inputOptions {\n+     *             numRecords       [int]\n+     *             keySize          [int]\n+     *             valueSize        [int]\n+     *             numHotKeys       [int]\n+     *             hotKeyFraction   [int]\n+     *         }\n+     *     }\n+     * }\n+     * </pre></blockquote>\n+     * @param cl Closure with fields setting\n+     * @return LoadTestConfig object\n+     */\n+    static LoadTestConfig templateConfig(final Closure cl = {}) {\n+        final def config = new LoadTestConfig()\n+        delegateAndInvoke(config, cl)\n+        return config\n+    }\n+\n+    /**\n+     * Returns configuration map from given template. Any field can be changed or/and added. Validation is performed\n+     * before final map is returned (ex. Flink runner requires <b>environmentConfig</b> to be set). In case of\n+     * validation failure exception is thrown.<br>\n+     * Example result:\n+     *<blockquote><pre>\n+     * [\n+     *  title          : 'any given title',\n+     *  test           : 'any given test',\n+     *  runner         : CommonTestProperties.Runner.DATAFLOW,\n+     *  pipelineOptions: [\n+     *    job_name            : 'any given job name',\n+     *    publish_to_big_query: true,\n+     *    project             : 'apache-beam-testing',\n+     *    metrics_dataset     : 'given_dataset_name',\n+     *    metrics_table       : 'given_table_name',\n+     *    input_options       : '\\'{\"num_records\": 200000000,\"key_size\": 1,\"value_size\":9}\\'',\n+     *    iterations          : 1,\n+     *    fanout              : 1,\n+     *    parallelism         : 5,\n+     *    job_endpoint        : 'localhost:1234',\n+     *    environment_config  : 'given_environment_config',\n+     *    environment_type    : 'given_environment_type'\n+     *  ]\n+     * ]\n+     * </blockquote></pre>\n+     * @param templateConfig LoadTestConfig instance\n+     * @param cl Closure with fields setting\n+     * @return configuration map\n+     * @see LoadTestConfig\n+     * @see LoadTestConfig#templateConfig\n+     */\n+    static Map<String, Serializable> fromTemplate(final LoadTestConfig templateConfig, final Closure cl = {}) {\n+        final def newConfig = of(templateConfig)\n+        delegateAndInvoke(newConfig, cl)\n+        final def properties = newConfig.propertiesMap\n+        verifyProperties(properties)\n+        return ConfigHelper.convertProperties(properties)\n+    }\n+\n+    /**\n+     * Returns configuration map (see {@link LoadTestConfig#fromTemplate}) directly from given settings\n+     * @param cl Closure with settings\n+     * @return configuration map\n+     */\n+    static Map<String, Serializable> config(final Closure cl = {}) {\n+        final def config = new LoadTestConfig()\n+        delegateAndInvoke(config, cl)\n+        final def properties = config.propertiesMap\n+        verifyProperties(properties)\n+        return ConfigHelper.convertProperties(config.propertiesMap)\n+    }\n+\n+    private static void delegateAndInvoke(final delegate, final Closure cl = {}) {\n+        final def code = cl.rehydrate(delegate, this, this)\n+        code.resolveStrategy = Closure.DELEGATE_ONLY\n+        code()\n+    }\n+\n+    private static LoadTestConfig of(final LoadTestConfig oldConfig) {\n+        final def newConfig = new LoadTestConfig()\n+\n+        //primitive values\n+        InvokerHelper.setProperties(newConfig, oldConfig.propertiesMap)\n+\n+        //non-primitive values\n+        newConfig._pipelineOptions = oldConfig._pipelineOptions ? PipelineOptions.of(oldConfig._pipelineOptions) : null\n+\n+        return newConfig\n+    }\n+\n+    @Override\n+    Map<String, Serializable> toPrimitiveValues() {\n+        final def map = propertiesMap\n+        verifyProperties(map)\n+        return ConfigHelper.convertProperties(map)\n+    }\n+\n+    LinkedHashMap<String, Object> getPropertiesMap() {\n+        return [\n+                _title: _title,\n+                _test: _test,\n+                _runner: _runner,\n+                _pipelineOptions: _pipelineOptions\n+        ]\n+    }\n+\n+    private static void verifyProperties(final LinkedHashMap<String, Object> map) {\n+        for (entry in map.entrySet()) {\n+            requireNonNull(entry.value, \"Missing ${entry.key.substring(1)} in configuration\")\n+        }\n+    }\n+\n+    private static class PipelineOptions implements SerializableOption<Map<String, Serializable>> {\n+        private String _jobName\n+        private String _project\n+        private String  _publishToBigQuery\n+        private String  _metricsDataset\n+        private String  _metricsTable\n+        private InputOptions  _inputOptions\n+        private def  _iterations", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "57fd8672c46e4fcb496867da907d1ac4f2676481"}, "originalPosition": 236}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "57fd8672c46e4fcb496867da907d1ac4f2676481", "author": {"user": {"login": "pawelpasterz", "name": null}}, "url": "https://github.com/apache/beam/commit/57fd8672c46e4fcb496867da907d1ac4f2676481", "committedDate": "2020-01-10T07:30:44Z", "message": "[BEAM-8941] Implement simple DSL for load tests"}, "afterCommit": {"oid": "58488af8d3a4855bc0efeb4b1f1fd89ed8f94064", "author": {"user": {"login": "pawelpasterz", "name": null}}, "url": "https://github.com/apache/beam/commit/58488af8d3a4855bc0efeb4b1f1fd89ed8f94064", "committedDate": "2020-01-13T05:47:28Z", "message": "[BEAM-8941] Implement simple DSL for load tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "58488af8d3a4855bc0efeb4b1f1fd89ed8f94064", "author": {"user": {"login": "pawelpasterz", "name": null}}, "url": "https://github.com/apache/beam/commit/58488af8d3a4855bc0efeb4b1f1fd89ed8f94064", "committedDate": "2020-01-13T05:47:28Z", "message": "[BEAM-8941] Implement simple DSL for load tests"}, "afterCommit": {"oid": "2561b48e203579e4e3f58281d327555d50d065b0", "author": {"user": {"login": "pawelpasterz", "name": null}}, "url": "https://github.com/apache/beam/commit/2561b48e203579e4e3f58281d327555d50d065b0", "committedDate": "2020-01-13T12:34:23Z", "message": "[BEAM-8941] Implement simple DSL for load tests"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c08a8d50241f314cfb0d5f2f118aaa2b5f4efa69", "author": {"user": {"login": "pawelpasterz", "name": null}}, "url": "https://github.com/apache/beam/commit/c08a8d50241f314cfb0d5f2f118aaa2b5f4efa69", "committedDate": "2020-01-15T08:54:53Z", "message": "[BEAM-8941] update docs"}, "afterCommit": {"oid": "1c90664cd8dbc8321f8abebe7e58d77bd95d1c3e", "author": {"user": {"login": "pawelpasterz", "name": null}}, "url": "https://github.com/apache/beam/commit/1c90664cd8dbc8321f8abebe7e58d77bd95d1c3e", "committedDate": "2020-01-16T12:27:07Z", "message": "[BEAM-8941] Implement simple DSL for load tests\n\n[BEAM-8941] review changes\n\n[BEAM-8941] add missing non-primitive copies\n\n[BEAM-8941] implement map for non-required parameters\n\n[BEAM-8941] update docs"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzQ1MTUwNDU4", "url": "https://github.com/apache/beam/pull/10543#pullrequestreview-345150458", "createdAt": "2020-01-20T08:49:42Z", "commit": {"oid": "1c90664cd8dbc8321f8abebe7e58d77bd95d1c3e"}, "state": "COMMENTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMFQwODo0OTo0M1rOFfW57Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMS0yMFQwOTozMToxMFrOFfYEyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODQyNTQ1Mw==", "bodyText": "Do you have any load tests in Spark or Flink (non-portable)? I am asking because those two options can be added later. My main concern is here if we are sure if someone will actually run test with Spark/Flink option if it won't fail.", "url": "https://github.com/apache/beam/pull/10543#discussion_r368425453", "createdAt": "2020-01-20T08:49:43Z", "author": {"login": "kkucharc"}, "path": ".test-infra/jenkins/LoadTestConfig.groovy", "diffHunk": "@@ -0,0 +1,621 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import CommonTestProperties.Runner\n+import CommonTestProperties.SDK\n+import LoadTestConfig.SerializableOption\n+import groovy.json.JsonBuilder\n+import org.codehaus.groovy.runtime.InvokerHelper\n+\n+import java.util.function.Predicate\n+\n+import static java.util.Objects.nonNull\n+import static java.util.Objects.requireNonNull\n+\n+/**\n+ * This class contains simple DSL for load tests configuration. Configuration as Map<String, Serializable>\n+ * [{@link LoadTestConfig#config config} -- returns configuration map]\n+ * [{@link LoadTestConfig#templateConfig templateConfig} -- return LoadTestConfig reusable object]\n+ * [{@link LoadTestConfig#fromTemplate fromTemplate} -- returns configuration from given template].<br><br>\n+ *\n+ * Example:\n+ * <blockquote><pre>\n+ * LoadTestConfig template = templateConfig {\n+ *     title 'Load test'\n+ *     test 'Beam Load test'\n+ *     dataflow()\n+ *     pipelineOptions {\n+ *         python()\n+ *         jobName 'Any job name'\n+ *         publishToBigQuery true\n+ *         //other fields\n+ *     }\n+ *     specificParameters([\n+ *          fanout: 4\n+ *     ])\n+ * }\n+ * Map<String, Serializable> configMap = fromTemplate(template) {\n+ *     //fields can be changed or/and added\n+ *     flink()\n+ *     pipelineOptions {\n+ *         parallelism 5\n+ *         inputOptions {\n+ *             numRecords 20000\n+ *             keySize 1000\n+ *             valueSize 10\n+ *         }\n+ *     }\n+ * }\n+ * </pre></blockquote>\n+ */\n+class LoadTestConfig implements SerializableOption<Map<String, Serializable>> {\n+\n+    private String _title\n+    private String _test\n+    private Runner _runner\n+    private PipelineOptions _pipelineOptions\n+\n+    private LoadTestConfig() {}\n+\n+    void title(final String title) { _title = title }\n+    void test(final String test) { _test = test }\n+\n+    //runners\n+    void dataflow() { setRunnerAndUpdatePipelineOptions(Runner.DATAFLOW)}\n+    void spark() { setRunnerAndUpdatePipelineOptions(Runner.SPARK) }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1c90664cd8dbc8321f8abebe7e58d77bd95d1c3e"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODQyNzQ1Ng==", "bodyText": "I am wondering if test if self-explanatory enough. My first guess was this is filename (or path) to test. But here I am not sure and how it's different from title. If it is impossible to change it I would document it better.", "url": "https://github.com/apache/beam/pull/10543#discussion_r368427456", "createdAt": "2020-01-20T08:54:22Z", "author": {"login": "kkucharc"}, "path": ".test-infra/jenkins/LoadTestConfig.groovy", "diffHunk": "@@ -0,0 +1,621 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import CommonTestProperties.Runner\n+import CommonTestProperties.SDK\n+import LoadTestConfig.SerializableOption\n+import groovy.json.JsonBuilder\n+import org.codehaus.groovy.runtime.InvokerHelper\n+\n+import java.util.function.Predicate\n+\n+import static java.util.Objects.nonNull\n+import static java.util.Objects.requireNonNull\n+\n+/**\n+ * This class contains simple DSL for load tests configuration. Configuration as Map<String, Serializable>\n+ * [{@link LoadTestConfig#config config} -- returns configuration map]\n+ * [{@link LoadTestConfig#templateConfig templateConfig} -- return LoadTestConfig reusable object]\n+ * [{@link LoadTestConfig#fromTemplate fromTemplate} -- returns configuration from given template].<br><br>\n+ *\n+ * Example:\n+ * <blockquote><pre>\n+ * LoadTestConfig template = templateConfig {\n+ *     title 'Load test'\n+ *     test 'Beam Load test'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1c90664cd8dbc8321f8abebe7e58d77bd95d1c3e"}, "originalPosition": 40}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODQzOTg5Mg==", "bodyText": "What do you think about changing it into notation \"'${...}'\"?", "url": "https://github.com/apache/beam/pull/10543#discussion_r368439892", "createdAt": "2020-01-20T09:21:45Z", "author": {"login": "kkucharc"}, "path": ".test-infra/jenkins/LoadTestConfig.groovy", "diffHunk": "@@ -0,0 +1,621 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import CommonTestProperties.Runner\n+import CommonTestProperties.SDK\n+import LoadTestConfig.SerializableOption\n+import groovy.json.JsonBuilder\n+import org.codehaus.groovy.runtime.InvokerHelper\n+\n+import java.util.function.Predicate\n+\n+import static java.util.Objects.nonNull\n+import static java.util.Objects.requireNonNull\n+\n+/**\n+ * This class contains simple DSL for load tests configuration. Configuration as Map<String, Serializable>\n+ * [{@link LoadTestConfig#config config} -- returns configuration map]\n+ * [{@link LoadTestConfig#templateConfig templateConfig} -- return LoadTestConfig reusable object]\n+ * [{@link LoadTestConfig#fromTemplate fromTemplate} -- returns configuration from given template].<br><br>\n+ *\n+ * Example:\n+ * <blockquote><pre>\n+ * LoadTestConfig template = templateConfig {\n+ *     title 'Load test'\n+ *     test 'Beam Load test'\n+ *     dataflow()\n+ *     pipelineOptions {\n+ *         python()\n+ *         jobName 'Any job name'\n+ *         publishToBigQuery true\n+ *         //other fields\n+ *     }\n+ *     specificParameters([\n+ *          fanout: 4\n+ *     ])\n+ * }\n+ * Map<String, Serializable> configMap = fromTemplate(template) {\n+ *     //fields can be changed or/and added\n+ *     flink()\n+ *     pipelineOptions {\n+ *         parallelism 5\n+ *         inputOptions {\n+ *             numRecords 20000\n+ *             keySize 1000\n+ *             valueSize 10\n+ *         }\n+ *     }\n+ * }\n+ * </pre></blockquote>\n+ */\n+class LoadTestConfig implements SerializableOption<Map<String, Serializable>> {\n+\n+    private String _title\n+    private String _test\n+    private Runner _runner\n+    private PipelineOptions _pipelineOptions\n+\n+    private LoadTestConfig() {}\n+\n+    void title(final String title) { _title = title }\n+    void test(final String test) { _test = test }\n+\n+    //runners\n+    void dataflow() { setRunnerAndUpdatePipelineOptions(Runner.DATAFLOW)}\n+    void spark() { setRunnerAndUpdatePipelineOptions(Runner.SPARK) }\n+    void flink() { setRunnerAndUpdatePipelineOptions(Runner.FLINK) }\n+    void direct() { setRunnerAndUpdatePipelineOptions(Runner.DIRECT) }\n+    void portable() { setRunnerAndUpdatePipelineOptions(Runner.PORTABLE) }\n+\n+    private void setRunnerAndUpdatePipelineOptions(final Runner runner) {\n+        _runner = runner\n+        final def pipeline = _pipelineOptions ?: new PipelineOptions()\n+        pipeline.i_runner = runner\n+        _pipelineOptions = pipeline\n+    }\n+\n+    void pipelineOptions(final Closure cl = {}) {\n+        final def options = _pipelineOptions ?: new PipelineOptions()\n+        delegateAndInvoke(options, cl)\n+        _pipelineOptions = options\n+    }\n+\n+    /**\n+     * Returns load test config object which can be reusable.</br>\n+     * All possible fields that can be set:\n+     * <blockquote><pre>\n+     * templateConfig {\n+     *     title        [String]\n+     *     test         [String]\n+     *     [dataflow(), spark(), flink(), direct(), portable()] -- runner\n+     *     pipelineOptions {\n+     *         [python(), python37(), java()] -- sdk\n+     *         jobName                  [String]\n+     *         project                  [String]\n+     *         publishToBigQuery        [boolean]\n+     *         metricsDataset (python)  [String]\n+     *         metricsTable (python)    [String]\n+     *         bigQueryDataset (java)   [String]\n+     *         bigQueryTable (java)     [String]\n+     *         numWorkers               [int]\n+     *         parallelism              [int]\n+     *         tempLocation             [String]\n+     *         autoscalingAlgorithm     [String]\n+     *         jobEndpoint              [String]\n+     *         environmentType          [String]\n+     *         environmentConfig        [String]\n+     *         inputOptions/coInputOptions (for python) {\n+     *             numRecords           [int]\n+     *             keySize              [int]\n+     *             valueSize            [int]\n+     *             numHotKeys           [int]\n+     *             hotKeyFraction       [int]\n+     *         }\n+     *         sourceOptions/coSourceOptions (for java) {\n+     *             numRecords           [int]\n+     *             keySizeBytes         [int]\n+     *             valueSizeBytes       [int]\n+     *             numHotKeys           [int]\n+     *             hotKeyFraction       [int]\n+     *             splitPointFrequencyRecords       [int]\n+     *         }\n+     *         stepOptions {\n+     *             outputRecordsPerInputRecord      [int]\n+     *             preservesInputKeyDistribution    [boolean]\n+     *         }\n+     *         specificParameters       [Map<String, Object>]\n+     *     }\n+     * }\n+     * </pre></blockquote>\n+     * @param cl Closure with fields setting\n+     * @return LoadTestConfig object\n+     */\n+    static LoadTestConfig templateConfig(final Closure cl = {}) {\n+        final def config = new LoadTestConfig()\n+        delegateAndInvoke(config, cl)\n+        return config\n+    }\n+\n+    /**\n+     * Returns configuration map from given template. Any field can be changed or/and added. Validation is performed\n+     * before final map is returned (ex. Flink runner requires <b>environmentConfig</b> to be set). In case of\n+     * validation failure exception is thrown.<br>\n+     * Example result:\n+     *<blockquote><pre>\n+     * [\n+     *  title          : 'any given title',\n+     *  test           : 'any given test',\n+     *  runner         : CommonTestProperties.Runner.DATAFLOW,\n+     *  pipelineOptions: [\n+     *    job_name            : 'any given job name',\n+     *    publish_to_big_query: true,\n+     *    project             : 'apache-beam-testing',\n+     *    metrics_dataset     : 'given_dataset_name',\n+     *    metrics_table       : 'given_table_name',\n+     *    input_options       : '\\'{\"num_records\": 200000000,\"key_size\": 1,\"value_size\":9}\\'',\n+     *    iterations          : 1,\n+     *    fanout              : 1,\n+     *    parallelism         : 5,\n+     *    job_endpoint        : 'localhost:1234',\n+     *    environment_config  : 'given_environment_config',\n+     *    environment_type    : 'given_environment_type'\n+     *  ]\n+     * ]\n+     * </blockquote></pre>\n+     * @param templateConfig LoadTestConfig instance\n+     * @param cl Closure with fields setting\n+     * @return configuration map\n+     * @see LoadTestConfig\n+     * @see LoadTestConfig#templateConfig\n+     */\n+    static Map<String, Serializable> fromTemplate(final LoadTestConfig templateConfig, final Closure cl = {}) {\n+        final def newConfig = of(templateConfig)\n+        delegateAndInvoke(newConfig, cl)\n+        final def properties = newConfig.propertiesMap\n+        verifyProperties(properties)\n+        return ConfigHelper.convertProperties(properties)\n+    }\n+\n+    /**\n+     * Returns configuration map (see {@link LoadTestConfig#fromTemplate}) directly from given settings\n+     * @param cl Closure with settings\n+     * @return configuration map\n+     */\n+    static Map<String, Serializable> config(final Closure cl = {}) {\n+        final def config = new LoadTestConfig()\n+        delegateAndInvoke(config, cl)\n+        final def properties = config.propertiesMap\n+        verifyProperties(properties)\n+        return ConfigHelper.convertProperties(config.propertiesMap)\n+    }\n+\n+    private static void delegateAndInvoke(final delegate, final Closure cl = {}) {\n+        final def code = cl.rehydrate(delegate, this, this)\n+        code.resolveStrategy = Closure.DELEGATE_ONLY\n+        code()\n+    }\n+\n+    private static LoadTestConfig of(final LoadTestConfig oldConfig) {\n+        final def newConfig = new LoadTestConfig()\n+\n+        //primitive values\n+        InvokerHelper.setProperties(newConfig, oldConfig.propertiesMap)\n+\n+        //non-primitive values\n+        newConfig._pipelineOptions = oldConfig._pipelineOptions ? PipelineOptions.of(oldConfig._pipelineOptions) : null\n+\n+        return newConfig\n+    }\n+\n+    @Override\n+    Map<String, Serializable> toPrimitiveValues() {\n+        final def map = propertiesMap\n+        verifyProperties(map)\n+        return ConfigHelper.convertProperties(map)\n+    }\n+\n+    LinkedHashMap<String, Object> getPropertiesMap() {\n+        return [\n+                _title: _title,\n+                _test: _test,\n+                _runner: _runner,\n+                _pipelineOptions: _pipelineOptions\n+        ]\n+    }\n+\n+    private static void verifyProperties(final LinkedHashMap<String, Object> map) {\n+        for (entry in map.entrySet()) {\n+            requireNonNull(entry.value, \"Missing ${entry.key.substring(1)} in configuration\")\n+        }\n+    }\n+\n+    private static class PipelineOptions implements SerializableOption<Map<String, Serializable>> {\n+        private Map<String, Object> _specificParameters = new HashMap<>()\n+        private boolean _streaming = false\n+        private SourceOptions _coSourceOptions\n+        private InputOptions _coInputOptions\n+        private StepOptions _stepOptions\n+\n+        //required\n+        private String _jobName\n+        private String _project\n+        private String _publishToBigQuery\n+\n+        //java required\n+        private String _bigQueryDataset\n+        private String _bigQueryTable\n+        private SourceOptions _sourceOptions\n+\n+        //python required\n+        private String _metricsDataset\n+        private String _metricsTable\n+        private InputOptions _inputOptions\n+\n+        //internal usage\n+        private SDK i_sdk\n+        private Runner i_runner\n+        private static final i_required = [\"_jobName\", \"_project\", \"_publishToBigQuery\"]\n+        private static final i_dataflowRequired = [\"_numWorkers\", \"_tempLocation\", \"_autoscalingAlgorithm\"]\n+        private static final i_flinkRequired = [\"_jobEndpoint\", \"_environmentType\", \"_environmentConfig\", \"_parallelism\"]\n+        private static final i_javaRequired = [\"_bigQueryDataset\", \"_bigQueryTable\", \"_sourceOptions\"]\n+        private static final i_pythonRequired = [\"_metricsDataset\", \"_metricsTable\", \"_inputOptions\"]\n+\n+        //dataflow required\n+        private def  _numWorkers\n+        private String  _tempLocation\n+        private String  _autoscalingAlgorithm\n+\n+        //flink required\n+        private String _jobEndpoint\n+        private String _environmentType\n+        private String _environmentConfig\n+        private def _parallelism\n+\n+        void jobName(final String name) { _jobName = name }\n+        void project(final String project) { _project = project }\n+        void tempLocation(final String location) { _tempLocation = location }\n+        void publishToBigQuery(final boolean publish) { _publishToBigQuery = publish }\n+        void metricsDataset(final String dataset) { _metricsDataset = dataset }\n+        void metricsTable(final String table) { _metricsTable = table }\n+        void inputOptions(final InputOptions options) { _inputOptions = options }\n+        void numWorkers(final int workers) { _numWorkers = workers }\n+        void autoscalingAlgorithm(final String algorithm) { _autoscalingAlgorithm = algorithm }\n+        void jobEndpoint(final String endpoint) { _jobEndpoint = endpoint }\n+        void environmentType(final String type) { _environmentType = type }\n+        void environmentConfig(final String config) { _environmentConfig = config }\n+        void parallelism(final int parallelism) { _parallelism = parallelism }\n+        void bigQueryDataset(final String dataset) { _bigQueryDataset = dataset }\n+        void bigQueryTable(final String table) { _bigQueryTable = table }\n+        void streaming(final boolean isStreaming) { _streaming = isStreaming }\n+        void sourceOptions(final Closure cl = {}) { _sourceOptions = makeSourceOptions {cl} }\n+        void coSourceOptions(final Closure cl = {}) { _coSourceOptions = makeSourceOptions(cl) }\n+        void inputOptions(final Closure cl = {}) { _inputOptions = makeInputOptions(cl) }\n+        void coInputOptions(final Closure cl = {}) { _coInputOptions = makeInputOptions(cl) }\n+        void stepOptions(final Closure cl = {}) { _stepOptions = makeStepOptions(cl) }\n+        void specificParameters(final Map<String, Object> map) { _specificParameters.putAll(map) }\n+\n+        //sdk -- snake_case vs camelCase\n+        void python() { i_sdk = SDK.PYTHON }\n+        void python37() { i_sdk = SDK.PYTHON_37 }\n+        void java() { i_sdk = SDK.JAVA }\n+\n+\n+        private InputOptions makeInputOptions(final Closure cl = {}) {\n+            return makeOptions(cl, _inputOptions ?: InputOptions.withSDK(i_sdk))\n+        }\n+\n+        private SourceOptions makeSourceOptions(final Closure cl = {}) {\n+            return makeOptions(cl, _sourceOptions ?: SourceOptions.withSDK(i_sdk))\n+        }\n+\n+        private StepOptions makeStepOptions(final Closure cl = {}) {\n+            return makeOptions(cl, _stepOptions ?: StepOptions.withSDK(i_sdk))\n+        }\n+\n+        private <T> T makeOptions(final Closure cl = {}, final T options) {\n+            final def code = cl.rehydrate(options, this, this)\n+            code.resolveStrategy = Closure.DELEGATE_ONLY\n+            code()\n+            return options\n+        }\n+\n+        @Override\n+        Map<String, Serializable> toPrimitiveValues() {\n+            final def map = propertiesMap\n+            verifyPipelineProperties(map)\n+            return ConfigHelper.convertProperties(map, i_sdk)\n+        }\n+\n+        private void verifyPipelineProperties(final Map<String, Object> map) {\n+            verifyRequired(map)\n+            switch (i_runner) {\n+                case Runner.DATAFLOW:\n+                    verifyDataflowProperties(map)\n+                    break\n+                case Runner.FLINK:\n+                    verifyFlinkProperties(map)\n+                    break\n+                default:\n+                    break\n+            }\n+        }\n+\n+        private void verifyRequired(final Map<String, Object> map) {\n+            verifyCommonRequired(map)\n+            switch (i_sdk) {\n+                case SDK.PYTHON:\n+                case SDK.PYTHON_37:\n+                    verifyPythonRequired(map)\n+                    break\n+                case SDK.JAVA:\n+                    verifyJavaRequired(map)\n+                    break\n+                default:\n+                    break\n+            }\n+        }\n+\n+        private static void verifyCommonRequired(final Map<String, Object> map) {\n+            verify(map, \"\") { i_required.contains(it.key) }\n+        }\n+\n+        private static void verifyPythonRequired(final Map<String, Object> map) {\n+            verify(map, \"for Python SDK\") { i_pythonRequired.contains(it.key) }\n+        }\n+\n+        private static void verifyJavaRequired(final Map<String, Object> map) {\n+            verify(map, \"for Java SDK\") { i_javaRequired.contains(it.key) }\n+        }\n+\n+        private static void verifyDataflowProperties(final Map<String, Object> map) {\n+            verify(map, \"for Dataflow runner\") { i_dataflowRequired.contains(it.key) }\n+        }\n+\n+        private static void verifyFlinkProperties(final Map<String, Object> map) {\n+            verify(map, \"for Flink runner\") { i_flinkRequired.contains(it.key) }\n+        }\n+\n+        private static void verify(final Map<String, Object> map, final String message, final Predicate<Map.Entry<String, Object>> predicate) {\n+            map.entrySet()\n+                    .stream()\n+                    .filter(predicate)\n+                    .forEach{ requireNonNull(it.value, \"${it.key.substring(1)} is required \" + message) }\n+        }\n+\n+        static PipelineOptions of(final PipelineOptions options) {\n+            final def newOptions = new PipelineOptions()\n+\n+            //primitive values\n+            InvokerHelper.setProperties(newOptions, options.propertiesMap)\n+\n+            //non-primitive\n+            newOptions._inputOptions = options._inputOptions ? InputOptions.of(options._inputOptions) : null\n+            newOptions._coInputOptions = options._coInputOptions ? InputOptions.of(options._coInputOptions) : null\n+            newOptions._sourceOptions = options._sourceOptions ? SourceOptions.of(options._sourceOptions) : null\n+            newOptions._coSourceOptions = options._coSourceOptions ? SourceOptions.of(options._coSourceOptions) : null\n+            newOptions._stepOptions = options._stepOptions ? StepOptions.of(options._stepOptions) : null\n+            newOptions._specificParameters = new HashMap<>(options._specificParameters)\n+\n+            return newOptions\n+        }\n+\n+        Map<String, Object> getPropertiesMap() {\n+            return [\n+                    i_sdk: i_sdk,\n+                    i_runner: i_runner,\n+                    _jobName: _jobName,\n+                    _project: _project,\n+                    _tempLocation: _tempLocation,\n+                    _publishToBigQuery: _publishToBigQuery,\n+                    _metricsDataset: _metricsDataset,\n+                    _metricsTable: _metricsTable,\n+                    _numWorkers: _numWorkers,\n+                    _autoscalingAlgorithm: _autoscalingAlgorithm,\n+                    _inputOptions: _inputOptions,\n+                    _coInputOptions: _coInputOptions,\n+                    _jobEndpoint: _jobEndpoint,\n+                    _environmentType: _environmentType,\n+                    _environmentConfig: _environmentConfig,\n+                    _parallelism: _parallelism,\n+                    _bigQueryDataset: _bigQueryDataset,\n+                    _bigQueryTable: _bigQueryTable,\n+                    _streaming: _streaming,\n+                    _sourceOptions: _sourceOptions,\n+                    _coSourceOptions: _coSourceOptions,\n+                    _stepOptions: _stepOptions\n+            ].putAll(_specificParameters.entrySet())\n+        }\n+\n+        private static class InputOptions implements SerializableOption<String> {\n+            private def _numRecords\n+            private def _keySize\n+            private def _valueSize\n+            private def _numHotKeys\n+            private def _hotKeyFraction\n+\n+            //internal usage\n+            private SDK i_sdk\n+\n+            private InputOptions() {}\n+\n+            static withSDK(final SDK sdk) {\n+                final def input = new InputOptions()\n+                input.i_sdk = sdk\n+                return input\n+            }\n+\n+            void numRecords(final int num) { _numRecords = num }\n+            void keySize(final int size) { _keySize = size }\n+            void valueSize(final int size) { _valueSize = size }\n+            void numHotsKeys(final int num) { _numHotKeys = num }\n+            void hotKeyFraction(final int fraction) { _hotKeyFraction = fraction }\n+\n+            @Override\n+            String toPrimitiveValues() {\n+                return '\\'' + new JsonBuilder(ConfigHelper.convertProperties(propertiesMap, i_sdk)).toString() + '\\''\n+            }\n+\n+            static InputOptions of(final InputOptions oldOptions) {\n+                final def newOptions = new InputOptions()\n+                InvokerHelper.setProperties(newOptions, oldOptions.propertiesMap)\n+                return newOptions\n+            }\n+\n+            LinkedHashMap<String, Object> getPropertiesMap() {\n+                return [\n+                        i_sdk: i_sdk,\n+                        _numRecords: _numRecords,\n+                        _keySize: _keySize,\n+                        _valueSize: _valueSize,\n+                        _numHotKeys: _numHotKeys,\n+                        _hotKeyFraction: _hotKeyFraction\n+                ] as LinkedHashMap<String, Object>\n+            }\n+        }\n+\n+        private static class SourceOptions implements SerializableOption<String> {\n+            private def _numRecords\n+            private def _keySizeBytes\n+            private def _valueSizeBytes\n+            private def _numHotKeys\n+            private def _hotKeyFraction\n+            private def _splitPointFrequencyRecords\n+\n+            //internal usage\n+            private SDK i_sdk\n+\n+            private SourceOptions() {}\n+\n+            static withSDK(final SDK sdk) {\n+                final def input = new SourceOptions()\n+                input.i_sdk = sdk\n+                return input\n+            }\n+\n+            void numRecords(final int num) { _numRecords = num }\n+            void keySizeBytes(final int size) { _keySizeBytes = size }\n+            void valueSizeBytes(final int size) { _valueSizeBytes = size }\n+            void numHotsKeys(final int num) { _numHotKeys = num }\n+            void hotKeyFraction(final int fraction) { _hotKeyFraction = fraction }\n+            void splitPointFrequencyRecords(final int splitPoint) { _splitPointFrequencyRecords = splitPoint }\n+\n+            @Override\n+            String toPrimitiveValues() {\n+                return '\\'' + new JsonBuilder(ConfigHelper.convertProperties(propertiesMap, i_sdk)).toString() + '\\''\n+            }\n+\n+            static SourceOptions of(final SourceOptions oldOptions) {\n+                final def newOptions = new SourceOptions()\n+                InvokerHelper.setProperties(newOptions, oldOptions.propertiesMap)\n+                return newOptions\n+            }\n+\n+            Map<String, Object> getPropertiesMap() {\n+                return [\n+                        i_sdk: i_sdk,\n+                        _numRecords: _numRecords,\n+                        _keySizeBytes: _keySizeBytes,\n+                        _valueSizeBytes: _valueSizeBytes,\n+                        _numHotKeys: _numHotKeys,\n+                        _hotKeyFraction: _hotKeyFraction,\n+                        _splitPointFrequencyRecords: _splitPointFrequencyRecords\n+                ]\n+            }\n+        }\n+\n+        private static class StepOptions implements SerializableOption<String> {\n+            private def _outputRecordsPerInputRecord\n+            private boolean  _preservesInputKeyDistribution\n+\n+            //internal usage\n+            private SDK i_sdk\n+\n+            private StepOptions() {}\n+\n+            static withSDK(final SDK sdk) {\n+                final def option = new StepOptions()\n+                option.i_sdk = sdk\n+                return option\n+            }\n+\n+            void outputRecordsPerInputRecord(final int records) { _outputRecordsPerInputRecord = records }\n+            void preservesInputKeyDistribution(final boolean  shouldPreserve) { _preservesInputKeyDistribution = shouldPreserve }\n+\n+            @Override\n+            String toPrimitiveValues() {\n+                return '\\'' + new JsonBuilder(ConfigHelper.convertProperties(propertiesMap, i_sdk)).toString() + '\\''", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1c90664cd8dbc8321f8abebe7e58d77bd95d1c3e"}, "originalPosition": 561}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM2ODQ0NDYxOQ==", "bodyText": "What do you think about changing it into notation \"'${...}'\"?", "url": "https://github.com/apache/beam/pull/10543#discussion_r368444619", "createdAt": "2020-01-20T09:31:10Z", "author": {"login": "kkucharc"}, "path": ".test-infra/jenkins/LoadTestConfig.groovy", "diffHunk": "@@ -0,0 +1,621 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+import CommonTestProperties.Runner\n+import CommonTestProperties.SDK\n+import LoadTestConfig.SerializableOption\n+import groovy.json.JsonBuilder\n+import org.codehaus.groovy.runtime.InvokerHelper\n+\n+import java.util.function.Predicate\n+\n+import static java.util.Objects.nonNull\n+import static java.util.Objects.requireNonNull\n+\n+/**\n+ * This class contains simple DSL for load tests configuration. Configuration as Map<String, Serializable>\n+ * [{@link LoadTestConfig#config config} -- returns configuration map]\n+ * [{@link LoadTestConfig#templateConfig templateConfig} -- return LoadTestConfig reusable object]\n+ * [{@link LoadTestConfig#fromTemplate fromTemplate} -- returns configuration from given template].<br><br>\n+ *\n+ * Example:\n+ * <blockquote><pre>\n+ * LoadTestConfig template = templateConfig {\n+ *     title 'Load test'\n+ *     test 'Beam Load test'\n+ *     dataflow()\n+ *     pipelineOptions {\n+ *         python()\n+ *         jobName 'Any job name'\n+ *         publishToBigQuery true\n+ *         //other fields\n+ *     }\n+ *     specificParameters([\n+ *          fanout: 4\n+ *     ])\n+ * }\n+ * Map<String, Serializable> configMap = fromTemplate(template) {\n+ *     //fields can be changed or/and added\n+ *     flink()\n+ *     pipelineOptions {\n+ *         parallelism 5\n+ *         inputOptions {\n+ *             numRecords 20000\n+ *             keySize 1000\n+ *             valueSize 10\n+ *         }\n+ *     }\n+ * }\n+ * </pre></blockquote>\n+ */\n+class LoadTestConfig implements SerializableOption<Map<String, Serializable>> {\n+\n+    private String _title\n+    private String _test\n+    private Runner _runner\n+    private PipelineOptions _pipelineOptions\n+\n+    private LoadTestConfig() {}\n+\n+    void title(final String title) { _title = title }\n+    void test(final String test) { _test = test }\n+\n+    //runners\n+    void dataflow() { setRunnerAndUpdatePipelineOptions(Runner.DATAFLOW)}\n+    void spark() { setRunnerAndUpdatePipelineOptions(Runner.SPARK) }\n+    void flink() { setRunnerAndUpdatePipelineOptions(Runner.FLINK) }\n+    void direct() { setRunnerAndUpdatePipelineOptions(Runner.DIRECT) }\n+    void portable() { setRunnerAndUpdatePipelineOptions(Runner.PORTABLE) }\n+\n+    private void setRunnerAndUpdatePipelineOptions(final Runner runner) {\n+        _runner = runner\n+        final def pipeline = _pipelineOptions ?: new PipelineOptions()\n+        pipeline.i_runner = runner\n+        _pipelineOptions = pipeline\n+    }\n+\n+    void pipelineOptions(final Closure cl = {}) {\n+        final def options = _pipelineOptions ?: new PipelineOptions()\n+        delegateAndInvoke(options, cl)\n+        _pipelineOptions = options\n+    }\n+\n+    /**\n+     * Returns load test config object which can be reusable.</br>\n+     * All possible fields that can be set:\n+     * <blockquote><pre>\n+     * templateConfig {\n+     *     title        [String]\n+     *     test         [String]\n+     *     [dataflow(), spark(), flink(), direct(), portable()] -- runner\n+     *     pipelineOptions {\n+     *         [python(), python37(), java()] -- sdk\n+     *         jobName                  [String]\n+     *         project                  [String]\n+     *         publishToBigQuery        [boolean]\n+     *         metricsDataset (python)  [String]\n+     *         metricsTable (python)    [String]\n+     *         bigQueryDataset (java)   [String]\n+     *         bigQueryTable (java)     [String]\n+     *         numWorkers               [int]\n+     *         parallelism              [int]\n+     *         tempLocation             [String]\n+     *         autoscalingAlgorithm     [String]\n+     *         jobEndpoint              [String]\n+     *         environmentType          [String]\n+     *         environmentConfig        [String]\n+     *         inputOptions/coInputOptions (for python) {\n+     *             numRecords           [int]\n+     *             keySize              [int]\n+     *             valueSize            [int]\n+     *             numHotKeys           [int]\n+     *             hotKeyFraction       [int]\n+     *         }\n+     *         sourceOptions/coSourceOptions (for java) {\n+     *             numRecords           [int]\n+     *             keySizeBytes         [int]\n+     *             valueSizeBytes       [int]\n+     *             numHotKeys           [int]\n+     *             hotKeyFraction       [int]\n+     *             splitPointFrequencyRecords       [int]\n+     *         }\n+     *         stepOptions {\n+     *             outputRecordsPerInputRecord      [int]\n+     *             preservesInputKeyDistribution    [boolean]\n+     *         }\n+     *         specificParameters       [Map<String, Object>]\n+     *     }\n+     * }\n+     * </pre></blockquote>\n+     * @param cl Closure with fields setting\n+     * @return LoadTestConfig object\n+     */\n+    static LoadTestConfig templateConfig(final Closure cl = {}) {\n+        final def config = new LoadTestConfig()\n+        delegateAndInvoke(config, cl)\n+        return config\n+    }\n+\n+    /**\n+     * Returns configuration map from given template. Any field can be changed or/and added. Validation is performed\n+     * before final map is returned (ex. Flink runner requires <b>environmentConfig</b> to be set). In case of\n+     * validation failure exception is thrown.<br>\n+     * Example result:\n+     *<blockquote><pre>\n+     * [\n+     *  title          : 'any given title',\n+     *  test           : 'any given test',\n+     *  runner         : CommonTestProperties.Runner.DATAFLOW,\n+     *  pipelineOptions: [\n+     *    job_name            : 'any given job name',\n+     *    publish_to_big_query: true,\n+     *    project             : 'apache-beam-testing',\n+     *    metrics_dataset     : 'given_dataset_name',\n+     *    metrics_table       : 'given_table_name',\n+     *    input_options       : '\\'{\"num_records\": 200000000,\"key_size\": 1,\"value_size\":9}\\'',\n+     *    iterations          : 1,\n+     *    fanout              : 1,\n+     *    parallelism         : 5,\n+     *    job_endpoint        : 'localhost:1234',\n+     *    environment_config  : 'given_environment_config',\n+     *    environment_type    : 'given_environment_type'\n+     *  ]\n+     * ]\n+     * </blockquote></pre>\n+     * @param templateConfig LoadTestConfig instance\n+     * @param cl Closure with fields setting\n+     * @return configuration map\n+     * @see LoadTestConfig\n+     * @see LoadTestConfig#templateConfig\n+     */\n+    static Map<String, Serializable> fromTemplate(final LoadTestConfig templateConfig, final Closure cl = {}) {\n+        final def newConfig = of(templateConfig)\n+        delegateAndInvoke(newConfig, cl)\n+        final def properties = newConfig.propertiesMap\n+        verifyProperties(properties)\n+        return ConfigHelper.convertProperties(properties)\n+    }\n+\n+    /**\n+     * Returns configuration map (see {@link LoadTestConfig#fromTemplate}) directly from given settings\n+     * @param cl Closure with settings\n+     * @return configuration map\n+     */\n+    static Map<String, Serializable> config(final Closure cl = {}) {\n+        final def config = new LoadTestConfig()\n+        delegateAndInvoke(config, cl)\n+        final def properties = config.propertiesMap\n+        verifyProperties(properties)\n+        return ConfigHelper.convertProperties(config.propertiesMap)\n+    }\n+\n+    private static void delegateAndInvoke(final delegate, final Closure cl = {}) {\n+        final def code = cl.rehydrate(delegate, this, this)\n+        code.resolveStrategy = Closure.DELEGATE_ONLY\n+        code()\n+    }\n+\n+    private static LoadTestConfig of(final LoadTestConfig oldConfig) {\n+        final def newConfig = new LoadTestConfig()\n+\n+        //primitive values\n+        InvokerHelper.setProperties(newConfig, oldConfig.propertiesMap)\n+\n+        //non-primitive values\n+        newConfig._pipelineOptions = oldConfig._pipelineOptions ? PipelineOptions.of(oldConfig._pipelineOptions) : null\n+\n+        return newConfig\n+    }\n+\n+    @Override\n+    Map<String, Serializable> toPrimitiveValues() {\n+        final def map = propertiesMap\n+        verifyProperties(map)\n+        return ConfigHelper.convertProperties(map)\n+    }\n+\n+    LinkedHashMap<String, Object> getPropertiesMap() {\n+        return [\n+                _title: _title,\n+                _test: _test,\n+                _runner: _runner,\n+                _pipelineOptions: _pipelineOptions\n+        ]\n+    }\n+\n+    private static void verifyProperties(final LinkedHashMap<String, Object> map) {\n+        for (entry in map.entrySet()) {\n+            requireNonNull(entry.value, \"Missing ${entry.key.substring(1)} in configuration\")\n+        }\n+    }\n+\n+    private static class PipelineOptions implements SerializableOption<Map<String, Serializable>> {\n+        private Map<String, Object> _specificParameters = new HashMap<>()\n+        private boolean _streaming = false\n+        private SourceOptions _coSourceOptions\n+        private InputOptions _coInputOptions\n+        private StepOptions _stepOptions\n+\n+        //required\n+        private String _jobName\n+        private String _project\n+        private String _publishToBigQuery\n+\n+        //java required\n+        private String _bigQueryDataset\n+        private String _bigQueryTable\n+        private SourceOptions _sourceOptions\n+\n+        //python required\n+        private String _metricsDataset\n+        private String _metricsTable\n+        private InputOptions _inputOptions\n+\n+        //internal usage\n+        private SDK i_sdk\n+        private Runner i_runner\n+        private static final i_required = [\"_jobName\", \"_project\", \"_publishToBigQuery\"]\n+        private static final i_dataflowRequired = [\"_numWorkers\", \"_tempLocation\", \"_autoscalingAlgorithm\"]\n+        private static final i_flinkRequired = [\"_jobEndpoint\", \"_environmentType\", \"_environmentConfig\", \"_parallelism\"]\n+        private static final i_javaRequired = [\"_bigQueryDataset\", \"_bigQueryTable\", \"_sourceOptions\"]\n+        private static final i_pythonRequired = [\"_metricsDataset\", \"_metricsTable\", \"_inputOptions\"]\n+\n+        //dataflow required\n+        private def  _numWorkers\n+        private String  _tempLocation\n+        private String  _autoscalingAlgorithm\n+\n+        //flink required\n+        private String _jobEndpoint\n+        private String _environmentType\n+        private String _environmentConfig\n+        private def _parallelism\n+\n+        void jobName(final String name) { _jobName = name }\n+        void project(final String project) { _project = project }\n+        void tempLocation(final String location) { _tempLocation = location }\n+        void publishToBigQuery(final boolean publish) { _publishToBigQuery = publish }\n+        void metricsDataset(final String dataset) { _metricsDataset = dataset }\n+        void metricsTable(final String table) { _metricsTable = table }\n+        void inputOptions(final InputOptions options) { _inputOptions = options }\n+        void numWorkers(final int workers) { _numWorkers = workers }\n+        void autoscalingAlgorithm(final String algorithm) { _autoscalingAlgorithm = algorithm }\n+        void jobEndpoint(final String endpoint) { _jobEndpoint = endpoint }\n+        void environmentType(final String type) { _environmentType = type }\n+        void environmentConfig(final String config) { _environmentConfig = config }\n+        void parallelism(final int parallelism) { _parallelism = parallelism }\n+        void bigQueryDataset(final String dataset) { _bigQueryDataset = dataset }\n+        void bigQueryTable(final String table) { _bigQueryTable = table }\n+        void streaming(final boolean isStreaming) { _streaming = isStreaming }\n+        void sourceOptions(final Closure cl = {}) { _sourceOptions = makeSourceOptions {cl} }\n+        void coSourceOptions(final Closure cl = {}) { _coSourceOptions = makeSourceOptions(cl) }\n+        void inputOptions(final Closure cl = {}) { _inputOptions = makeInputOptions(cl) }\n+        void coInputOptions(final Closure cl = {}) { _coInputOptions = makeInputOptions(cl) }\n+        void stepOptions(final Closure cl = {}) { _stepOptions = makeStepOptions(cl) }\n+        void specificParameters(final Map<String, Object> map) { _specificParameters.putAll(map) }\n+\n+        //sdk -- snake_case vs camelCase\n+        void python() { i_sdk = SDK.PYTHON }\n+        void python37() { i_sdk = SDK.PYTHON_37 }\n+        void java() { i_sdk = SDK.JAVA }\n+\n+\n+        private InputOptions makeInputOptions(final Closure cl = {}) {\n+            return makeOptions(cl, _inputOptions ?: InputOptions.withSDK(i_sdk))\n+        }\n+\n+        private SourceOptions makeSourceOptions(final Closure cl = {}) {\n+            return makeOptions(cl, _sourceOptions ?: SourceOptions.withSDK(i_sdk))\n+        }\n+\n+        private StepOptions makeStepOptions(final Closure cl = {}) {\n+            return makeOptions(cl, _stepOptions ?: StepOptions.withSDK(i_sdk))\n+        }\n+\n+        private <T> T makeOptions(final Closure cl = {}, final T options) {\n+            final def code = cl.rehydrate(options, this, this)\n+            code.resolveStrategy = Closure.DELEGATE_ONLY\n+            code()\n+            return options\n+        }\n+\n+        @Override\n+        Map<String, Serializable> toPrimitiveValues() {\n+            final def map = propertiesMap\n+            verifyPipelineProperties(map)\n+            return ConfigHelper.convertProperties(map, i_sdk)\n+        }\n+\n+        private void verifyPipelineProperties(final Map<String, Object> map) {\n+            verifyRequired(map)\n+            switch (i_runner) {\n+                case Runner.DATAFLOW:\n+                    verifyDataflowProperties(map)\n+                    break\n+                case Runner.FLINK:\n+                    verifyFlinkProperties(map)\n+                    break\n+                default:\n+                    break\n+            }\n+        }\n+\n+        private void verifyRequired(final Map<String, Object> map) {\n+            verifyCommonRequired(map)\n+            switch (i_sdk) {\n+                case SDK.PYTHON:\n+                case SDK.PYTHON_37:\n+                    verifyPythonRequired(map)\n+                    break\n+                case SDK.JAVA:\n+                    verifyJavaRequired(map)\n+                    break\n+                default:\n+                    break\n+            }\n+        }\n+\n+        private static void verifyCommonRequired(final Map<String, Object> map) {\n+            verify(map, \"\") { i_required.contains(it.key) }\n+        }\n+\n+        private static void verifyPythonRequired(final Map<String, Object> map) {\n+            verify(map, \"for Python SDK\") { i_pythonRequired.contains(it.key) }\n+        }\n+\n+        private static void verifyJavaRequired(final Map<String, Object> map) {\n+            verify(map, \"for Java SDK\") { i_javaRequired.contains(it.key) }\n+        }\n+\n+        private static void verifyDataflowProperties(final Map<String, Object> map) {\n+            verify(map, \"for Dataflow runner\") { i_dataflowRequired.contains(it.key) }\n+        }\n+\n+        private static void verifyFlinkProperties(final Map<String, Object> map) {\n+            verify(map, \"for Flink runner\") { i_flinkRequired.contains(it.key) }\n+        }\n+\n+        private static void verify(final Map<String, Object> map, final String message, final Predicate<Map.Entry<String, Object>> predicate) {\n+            map.entrySet()\n+                    .stream()\n+                    .filter(predicate)\n+                    .forEach{ requireNonNull(it.value, \"${it.key.substring(1)} is required \" + message) }\n+        }\n+\n+        static PipelineOptions of(final PipelineOptions options) {\n+            final def newOptions = new PipelineOptions()\n+\n+            //primitive values\n+            InvokerHelper.setProperties(newOptions, options.propertiesMap)\n+\n+            //non-primitive\n+            newOptions._inputOptions = options._inputOptions ? InputOptions.of(options._inputOptions) : null\n+            newOptions._coInputOptions = options._coInputOptions ? InputOptions.of(options._coInputOptions) : null\n+            newOptions._sourceOptions = options._sourceOptions ? SourceOptions.of(options._sourceOptions) : null\n+            newOptions._coSourceOptions = options._coSourceOptions ? SourceOptions.of(options._coSourceOptions) : null\n+            newOptions._stepOptions = options._stepOptions ? StepOptions.of(options._stepOptions) : null\n+            newOptions._specificParameters = new HashMap<>(options._specificParameters)\n+\n+            return newOptions\n+        }\n+\n+        Map<String, Object> getPropertiesMap() {\n+            return [\n+                    i_sdk: i_sdk,\n+                    i_runner: i_runner,\n+                    _jobName: _jobName,\n+                    _project: _project,\n+                    _tempLocation: _tempLocation,\n+                    _publishToBigQuery: _publishToBigQuery,\n+                    _metricsDataset: _metricsDataset,\n+                    _metricsTable: _metricsTable,\n+                    _numWorkers: _numWorkers,\n+                    _autoscalingAlgorithm: _autoscalingAlgorithm,\n+                    _inputOptions: _inputOptions,\n+                    _coInputOptions: _coInputOptions,\n+                    _jobEndpoint: _jobEndpoint,\n+                    _environmentType: _environmentType,\n+                    _environmentConfig: _environmentConfig,\n+                    _parallelism: _parallelism,\n+                    _bigQueryDataset: _bigQueryDataset,\n+                    _bigQueryTable: _bigQueryTable,\n+                    _streaming: _streaming,\n+                    _sourceOptions: _sourceOptions,\n+                    _coSourceOptions: _coSourceOptions,\n+                    _stepOptions: _stepOptions\n+            ].putAll(_specificParameters.entrySet())\n+        }\n+\n+        private static class InputOptions implements SerializableOption<String> {\n+            private def _numRecords\n+            private def _keySize\n+            private def _valueSize\n+            private def _numHotKeys\n+            private def _hotKeyFraction\n+\n+            //internal usage\n+            private SDK i_sdk\n+\n+            private InputOptions() {}\n+\n+            static withSDK(final SDK sdk) {\n+                final def input = new InputOptions()\n+                input.i_sdk = sdk\n+                return input\n+            }\n+\n+            void numRecords(final int num) { _numRecords = num }\n+            void keySize(final int size) { _keySize = size }\n+            void valueSize(final int size) { _valueSize = size }\n+            void numHotsKeys(final int num) { _numHotKeys = num }\n+            void hotKeyFraction(final int fraction) { _hotKeyFraction = fraction }\n+\n+            @Override\n+            String toPrimitiveValues() {\n+                return '\\'' + new JsonBuilder(ConfigHelper.convertProperties(propertiesMap, i_sdk)).toString() + '\\''\n+            }\n+\n+            static InputOptions of(final InputOptions oldOptions) {\n+                final def newOptions = new InputOptions()\n+                InvokerHelper.setProperties(newOptions, oldOptions.propertiesMap)\n+                return newOptions\n+            }\n+\n+            LinkedHashMap<String, Object> getPropertiesMap() {\n+                return [\n+                        i_sdk: i_sdk,\n+                        _numRecords: _numRecords,\n+                        _keySize: _keySize,\n+                        _valueSize: _valueSize,\n+                        _numHotKeys: _numHotKeys,\n+                        _hotKeyFraction: _hotKeyFraction\n+                ] as LinkedHashMap<String, Object>\n+            }\n+        }\n+\n+        private static class SourceOptions implements SerializableOption<String> {\n+            private def _numRecords\n+            private def _keySizeBytes\n+            private def _valueSizeBytes\n+            private def _numHotKeys\n+            private def _hotKeyFraction\n+            private def _splitPointFrequencyRecords\n+\n+            //internal usage\n+            private SDK i_sdk\n+\n+            private SourceOptions() {}\n+\n+            static withSDK(final SDK sdk) {\n+                final def input = new SourceOptions()\n+                input.i_sdk = sdk\n+                return input\n+            }\n+\n+            void numRecords(final int num) { _numRecords = num }\n+            void keySizeBytes(final int size) { _keySizeBytes = size }\n+            void valueSizeBytes(final int size) { _valueSizeBytes = size }\n+            void numHotsKeys(final int num) { _numHotKeys = num }\n+            void hotKeyFraction(final int fraction) { _hotKeyFraction = fraction }\n+            void splitPointFrequencyRecords(final int splitPoint) { _splitPointFrequencyRecords = splitPoint }\n+\n+            @Override\n+            String toPrimitiveValues() {\n+                return '\\'' + new JsonBuilder(ConfigHelper.convertProperties(propertiesMap, i_sdk)).toString() + '\\''", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1c90664cd8dbc8321f8abebe7e58d77bd95d1c3e"}, "originalPosition": 519}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "bfcbf75ff85e0de1d8bf50773b8b904d7337ae00", "author": {"user": {"login": "pawelpasterz", "name": null}}, "url": "https://github.com/apache/beam/commit/bfcbf75ff85e0de1d8bf50773b8b904d7337ae00", "committedDate": "2020-01-22T11:42:22Z", "message": "[BEAM-8941] remove redundant single quotation mark"}, "afterCommit": {"oid": "8a5bee2635ca00d200de05b3c86a36de93e74d9c", "author": {"user": {"login": "pawelpasterz", "name": null}}, "url": "https://github.com/apache/beam/commit/8a5bee2635ca00d200de05b3c86a36de93e74d9c", "committedDate": "2020-01-22T13:20:43Z", "message": "[BEAM-8941] Implement simple DSL for load tests\n\n[BEAM-8941] review changes\n\n[BEAM-8941] add missing non-primitive copies\n\n[BEAM-8941] implement map for non-required parameters\n\n[BEAM-8941] update docs"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8a5bee2635ca00d200de05b3c86a36de93e74d9c", "author": {"user": {"login": "pawelpasterz", "name": null}}, "url": "https://github.com/apache/beam/commit/8a5bee2635ca00d200de05b3c86a36de93e74d9c", "committedDate": "2020-01-22T13:20:43Z", "message": "[BEAM-8941] Implement simple DSL for load tests\n\n[BEAM-8941] review changes\n\n[BEAM-8941] add missing non-primitive copies\n\n[BEAM-8941] implement map for non-required parameters\n\n[BEAM-8941] update docs"}, "afterCommit": {"oid": "56644b169c0d5158aa5820694e7916f6a742f7e7", "author": {"user": {"login": "pawelpasterz", "name": null}}, "url": "https://github.com/apache/beam/commit/56644b169c0d5158aa5820694e7916f6a742f7e7", "committedDate": "2020-01-27T08:42:42Z", "message": "[BEAM-8941] Implement simple DSL for load tests\n\n[BEAM-8941] review changes\n\n[BEAM-8941] add missing non-primitive copies\n\n[BEAM-8941] implement map for non-required parameters\n\n[BEAM-8941] update docs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "801a6bb5e4554af43b29dee00a1b7175c326b6be", "author": {"user": {"login": "pawelpasterz", "name": null}}, "url": "https://github.com/apache/beam/commit/801a6bb5e4554af43b29dee00a1b7175c326b6be", "committedDate": "2020-01-27T12:50:48Z", "message": "[BEAM-8941] Implement simple DSL for load tests\n\n[BEAM-8941] review changes\n\n[BEAM-8941] add missing non-primitive copies\n\n[BEAM-8941] implement map for non-required parameters\n\n[BEAM-8941] update docs"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "56644b169c0d5158aa5820694e7916f6a742f7e7", "author": {"user": {"login": "pawelpasterz", "name": null}}, "url": "https://github.com/apache/beam/commit/56644b169c0d5158aa5820694e7916f6a742f7e7", "committedDate": "2020-01-27T08:42:42Z", "message": "[BEAM-8941] Implement simple DSL for load tests\n\n[BEAM-8941] review changes\n\n[BEAM-8941] add missing non-primitive copies\n\n[BEAM-8941] implement map for non-required parameters\n\n[BEAM-8941] update docs"}, "afterCommit": {"oid": "801a6bb5e4554af43b29dee00a1b7175c326b6be", "author": {"user": {"login": "pawelpasterz", "name": null}}, "url": "https://github.com/apache/beam/commit/801a6bb5e4554af43b29dee00a1b7175c326b6be", "committedDate": "2020-01-27T12:50:48Z", "message": "[BEAM-8941] Implement simple DSL for load tests\n\n[BEAM-8941] review changes\n\n[BEAM-8941] add missing non-primitive copies\n\n[BEAM-8941] implement map for non-required parameters\n\n[BEAM-8941] update docs"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3705, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}