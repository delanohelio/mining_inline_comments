{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDU1ODQ2MzM3", "number": 12355, "reviewThreads": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxODoyNDoyOVrOERkLqA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQwMDoxMzoxNFrOERqIKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODU0MDU2OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxODoyNDoyOVrOG2WbkQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxODoyNDoyOVrOG2WbkQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY0Mzc5Mw==", "bodyText": "\".. that use the SQL transform.\"  (to differentiate from wordcount_xlang.py)", "url": "https://github.com/apache/beam/pull/12355#discussion_r459643793", "createdAt": "2020-07-23T18:24:29Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -15,12 +15,16 @@\n # limitations under the License.\n #\n \n-\"\"\"A cross-language word-counting workflow.\"\"\"\n+\"\"\"A cross-language word-counting workflow.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODU0Mzg4OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "isResolved": false, "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxODoyNToyNFrOG2WdvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yN1QxNjo0OTozOFrOG3q78w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY0NDM0OQ==", "bodyText": "I think we need a README file more specific instructions for Dataflow and portable runners for users to easily try this out. I believe @TheNeuralBit is working on it.", "url": "https://github.com/apache/beam/pull/12355#discussion_r459644349", "createdAt": "2020-07-23T18:25:24Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -15,12 +15,16 @@\n # limitations under the License.\n #\n \n-\"\"\"A cross-language word-counting workflow.\"\"\"\n+\"\"\"A cross-language word-counting workflow.\n+\n+Java and docker must be available to run this pipeline.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1ODgzNA==", "bodyText": "Yep - BEAM-10559 is tracking this.", "url": "https://github.com/apache/beam/pull/12355#discussion_r459658834", "createdAt": "2020-07-23T18:51:13Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -15,12 +15,16 @@\n # limitations under the License.\n #\n \n-\"\"\"A cross-language word-counting workflow.\"\"\"\n+\"\"\"A cross-language word-counting workflow.\n+\n+Java and docker must be available to run this pipeline.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY0NDM0OQ=="}, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY2Mjk4NQ==", "bodyText": "Sounds good.", "url": "https://github.com/apache/beam/pull/12355#discussion_r459662985", "createdAt": "2020-07-23T18:58:57Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -15,12 +15,16 @@\n # limitations under the License.\n #\n \n-\"\"\"A cross-language word-counting workflow.\"\"\"\n+\"\"\"A cross-language word-counting workflow.\n+\n+Java and docker must be available to run this pipeline.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY0NDM0OQ=="}, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2ODI2MA==", "bodyText": "I don't know what such a readme would say. You run the pipeline with python wordcount_xlang_sql.py or python -m apache_beam.examples.wordcount_xlang_sql just as with all the other examples here. For Dataflow you set --runner=DataflowRunner (plus all the other parameters as documented elsewhere, but I did add a note about the experiment), for Flink you set --runner=FlinkRunner. Unlike kafka, there's no need for special instructions about setting up or connecting to an external cluster. Pretty much anything I can think of putting in a readme for this pipeline would be equally as well placed in a readme for running pipelines in general.\nThe best readme is a readme that is not needed because it just works.\n(I do think it'd be good to have more docs and other more comprehensive examples, e.g. using Row and joins and more complicated queries.)", "url": "https://github.com/apache/beam/pull/12355#discussion_r459768260", "createdAt": "2020-07-23T22:43:16Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -15,12 +15,16 @@\n # limitations under the License.\n #\n \n-\"\"\"A cross-language word-counting workflow.\"\"\"\n+\"\"\"A cross-language word-counting workflow.\n+\n+Java and docker must be available to run this pipeline.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY0NDM0OQ=="}, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc4NzU4Mg==", "bodyText": "Probably we need to include following information as well (either here on in a README).\n(1) The fact that users have to install Java and which version to install\n(2) Instructions for running from HEAD (for example, you need to push a Docker container to Docker Hub for Dataflow). May be you can just point to kafkataxi README for this.\n(3) Example command for Dataflow with 'use_runner_v2' (and additionally sdk_harness_container_image_overrides for HEAD).", "url": "https://github.com/apache/beam/pull/12355#discussion_r459787582", "createdAt": "2020-07-23T23:44:58Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -15,12 +15,16 @@\n # limitations under the License.\n #\n \n-\"\"\"A cross-language word-counting workflow.\"\"\"\n+\"\"\"A cross-language word-counting workflow.\n+\n+Java and docker must be available to run this pipeline.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY0NDM0OQ=="}, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc5NTExNg==", "bodyText": "That's true if I wrote a README, everything for running on Dataflow would just be copied from the kafkataxi README.", "url": "https://github.com/apache/beam/pull/12355#discussion_r459795116", "createdAt": "2020-07-24T00:12:15Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -15,12 +15,16 @@\n # limitations under the License.\n #\n \n-\"\"\"A cross-language word-counting workflow.\"\"\"\n+\"\"\"A cross-language word-counting workflow.\n+\n+Java and docker must be available to run this pipeline.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY0NDM0OQ=="}, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc5NTE1MQ==", "bodyText": "At the top we note that Java is required. Good point about versions; I updated this to say java 8. I don't think we need instructions for building and running at head (which are not specific to SQL) in the very first introduction to using SqlTransform.", "url": "https://github.com/apache/beam/pull/12355#discussion_r459795151", "createdAt": "2020-07-24T00:12:25Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -15,12 +15,16 @@\n # limitations under the License.\n #\n \n-\"\"\"A cross-language word-counting workflow.\"\"\"\n+\"\"\"A cross-language word-counting workflow.\n+\n+Java and docker must be available to run this pipeline.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY0NDM0OQ=="}, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc5Njg4Mw==", "bodyText": "Note that Docker is only required when running at HEAD (with a custom container).", "url": "https://github.com/apache/beam/pull/12355#discussion_r459796883", "createdAt": "2020-07-24T00:19:21Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -15,12 +15,16 @@\n # limitations under the License.\n #\n \n-\"\"\"A cross-language word-counting workflow.\"\"\"\n+\"\"\"A cross-language word-counting workflow.\n+\n+Java and docker must be available to run this pipeline.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY0NDM0OQ=="}, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTAyODMzOQ==", "bodyText": "Updated the note about docker. It is required to run locally (ULR, local Flink cluster, ...). It's also required to create a custom container, but that's more about custom containers.", "url": "https://github.com/apache/beam/pull/12355#discussion_r461028339", "createdAt": "2020-07-27T16:49:38Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -15,12 +15,16 @@\n # limitations under the License.\n #\n \n-\"\"\"A cross-language word-counting workflow.\"\"\"\n+\"\"\"A cross-language word-counting workflow.\n+\n+Java and docker must be available to run this pipeline.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY0NDM0OQ=="}, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODU0OTg3OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxODoyNzoxNlrOG2Whlw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjo0NzowMFrOG2eGuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY0NTMzNQ==", "bodyText": "Can we generalize this so that users can easily try this out with Dataflow as well ?", "url": "https://github.com/apache/beam/pull/12355#discussion_r459645335", "createdAt": "2020-07-23T18:27:16Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -101,12 +89,33 @@ def main():\n   # workflow rely on global context (e.g., a module imported at module level).\n   pipeline_options.view_as(SetupOptions).save_main_session = True\n \n-  p = beam.Pipeline(options=pipeline_options)\n-  # Preemptively start due to BEAM-6666.\n-  p.runner.create_job_service(pipeline_options)\n+  with beam.Pipeline(options=pipeline_options) as p:\n+    if isinstance(p.runner, portable_runner.PortableRunner):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2OTUyOA==", "bodyText": "This will just work for Dataflow. That's why I added the guard.", "url": "https://github.com/apache/beam/pull/12355#discussion_r459769528", "createdAt": "2020-07-23T22:47:00Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -101,12 +89,33 @@ def main():\n   # workflow rely on global context (e.g., a module imported at module level).\n   pipeline_options.view_as(SetupOptions).save_main_session = True\n \n-  p = beam.Pipeline(options=pipeline_options)\n-  # Preemptively start due to BEAM-6666.\n-  p.runner.create_job_service(pipeline_options)\n+  with beam.Pipeline(options=pipeline_options) as p:\n+    if isinstance(p.runner, portable_runner.PortableRunner):", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY0NTMzNQ=="}, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 91}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODU1MjUyOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxODoyODowNlrOG2WjYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjozNTozN1rOG2d3xA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY0NTc5NA==", "bodyText": "Can we also briefly describe what this pipeline does and what output it produces ?", "url": "https://github.com/apache/beam/pull/12355#discussion_r459645794", "createdAt": "2020-07-23T18:28:06Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -15,12 +15,16 @@\n # limitations under the License.\n #\n \n-\"\"\"A cross-language word-counting workflow.\"\"\"\n+\"\"\"A cross-language word-counting workflow.\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2NTcwMA==", "bodyText": "Wordcount is the quintessential example of all Beam pipelines, and this does exactly the same thing that all the other wordcount variants in this folder do. I think it makes more sense to highlight the differences.", "url": "https://github.com/apache/beam/pull/12355#discussion_r459765700", "createdAt": "2020-07-23T22:35:37Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -15,12 +15,16 @@\n # limitations under the License.\n #\n \n-\"\"\"A cross-language word-counting workflow.\"\"\"\n+\"\"\"A cross-language word-counting workflow.\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY0NTc5NA=="}, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODU1NTc4OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxODoyOTowN1rOG2Wlnw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMjo0NDoxM1rOG2eDBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY0NjM2Nw==", "bodyText": "What are the other ways ? We should document all such possibilities in the Sql transform. Currently sql.py mentions that input PCollection must have a PCollection NamedTuple type.", "url": "https://github.com/apache/beam/pull/12355#discussion_r459646367", "createdAt": "2020-07-23T18:29:07Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -31,51 +35,35 @@\n from apache_beam.io import WriteToText\n from apache_beam.options.pipeline_options import PipelineOptions\n from apache_beam.options.pipeline_options import SetupOptions\n+from apache_beam.runners.portability import portable_runner\n from apache_beam.transforms.sql import SqlTransform\n \n+# The input to SqlTransform must be a PCollection(s) of known schema.\n+# One way to create such a PCollection is to produce a PCollection of\n+# NamedTuple registered with the RowCoder.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2ODU4MA==", "bodyText": "There is the recently added Row type. The docs on SqlTransform should be updated, but that's not a prerequisite for using these transforms.", "url": "https://github.com/apache/beam/pull/12355#discussion_r459768580", "createdAt": "2020-07-23T22:44:13Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -31,51 +35,35 @@\n from apache_beam.io import WriteToText\n from apache_beam.options.pipeline_options import PipelineOptions\n from apache_beam.options.pipeline_options import SetupOptions\n+from apache_beam.runners.portability import portable_runner\n from apache_beam.transforms.sql import SqlTransform\n \n+# The input to SqlTransform must be a PCollection(s) of known schema.\n+# One way to create such a PCollection is to produce a PCollection of\n+# NamedTuple registered with the RowCoder.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY0NjM2Nw=="}, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODU4MjUyOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxODozNjo1N1rOG2W2ww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxODozNjo1N1rOG2W2ww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MDc1NQ==", "bodyText": "We should also document current limitations of sql.py in the pydoc in that file (if any).", "url": "https://github.com/apache/beam/pull/12355#discussion_r459650755", "createdAt": "2020-07-23T18:36:57Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -31,51 +35,35 @@\n from apache_beam.io import WriteToText\n from apache_beam.options.pipeline_options import PipelineOptions\n from apache_beam.options.pipeline_options import SetupOptions\n+from apache_beam.runners.portability import portable_runner\n from apache_beam.transforms.sql import SqlTransform\n \n+# The input to SqlTransform must be a PCollection(s) of known schema.\n+# One way to create such a PCollection is to produce a PCollection of\n+# NamedTuple registered with the RowCoder.\n+#\n+# Here we create and register a simple NamedTuple with a single unicode typed\n+# field named 'word' which we will use below.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 30}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODU5MTQ3OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxODozOTo0MFrOG2W8lw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMzo0NzoyOFrOG2fQKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MjI0Nw==", "bodyText": "key and count properties are already defined ? Let's clarify that here in a comment.", "url": "https://github.com/apache/beam/pull/12355#discussion_r459652247", "createdAt": "2020-07-23T18:39:40Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -31,51 +35,35 @@\n from apache_beam.io import WriteToText\n from apache_beam.options.pipeline_options import PipelineOptions\n from apache_beam.options.pipeline_options import SetupOptions\n+from apache_beam.runners.portability import portable_runner\n from apache_beam.transforms.sql import SqlTransform\n \n+# The input to SqlTransform must be a PCollection(s) of known schema.\n+# One way to create such a PCollection is to produce a PCollection of\n+# NamedTuple registered with the RowCoder.\n+#\n+# Here we create and register a simple NamedTuple with a single unicode typed\n+# field named 'word' which we will use below.\n MyRow = typing.NamedTuple('MyRow', [('word', unicode)])\n coders.registry.register_coder(MyRow, coders.RowCoder)\n \n-# Some more fun queries:\n-# ------\n-# SELECT\n-#   word as key,\n-#   COUNT(*) as `count`\n-# FROM PCOLLECTION\n-# GROUP BY word\n-# ORDER BY `count` DESC\n-# LIMIT 100\n-# ------\n-# SELECT\n-#   len as key,\n-#   COUNT(*) as `count`\n-# FROM (\n-#   SELECT\n-#     LENGTH(word) AS len\n-#   FROM PCOLLECTION\n-# )\n-# GROUP BY len\n-\n \n def run(p, input_file, output_file):\n   #pylint: disable=expression-not-assigned\n   (\n       p\n-      | 'read' >> ReadFromText(input_file)\n-      | 'split' >> beam.FlatMap(str.split)\n-      | 'row' >> beam.Map(MyRow).with_output_types(MyRow)\n-      | 'sql!!' >> SqlTransform(\n+      | 'Read' >> ReadFromText(input_file)\n+      | 'Split' >> beam.FlatMap(lambda line: re.split(r'\\W+', line))\n+      | 'ToRow' >> beam.Map(MyRow).with_output_types(MyRow)\n+      | 'Sql!!' >> SqlTransform(\n           \"\"\"\n                    SELECT\n                      word as key,\n                      COUNT(*) as `count`\n                    FROM PCOLLECTION\n                    GROUP BY word\"\"\")\n-      | 'format' >> beam.Map(lambda row: '{}: {}'.format(row.key, row.count))\n-      | 'write' >> WriteToText(output_file))\n-\n-  result = p.run()\n-  result.wait_until_finish()\n+      | 'Format' >> beam.Map(lambda row: '{}: {}'.format(row.key, row.count))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY2MDE4Mg==", "bodyText": "Perhaps something like:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  | 'Format' >> beam.Map(lambda row: '{}: {}'.format(row.key, row.count))\n          \n          \n            \n                  # SqlTransform yields a PCollection containing elements of a generated NamedTuple type, with attributes based on the output of the query.\n          \n          \n            \n                  | 'Format' >> beam.Map(lambda row: '{}: {}'.format(row.key, row.count))", "url": "https://github.com/apache/beam/pull/12355#discussion_r459660182", "createdAt": "2020-07-23T18:53:53Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -31,51 +35,35 @@\n from apache_beam.io import WriteToText\n from apache_beam.options.pipeline_options import PipelineOptions\n from apache_beam.options.pipeline_options import SetupOptions\n+from apache_beam.runners.portability import portable_runner\n from apache_beam.transforms.sql import SqlTransform\n \n+# The input to SqlTransform must be a PCollection(s) of known schema.\n+# One way to create such a PCollection is to produce a PCollection of\n+# NamedTuple registered with the RowCoder.\n+#\n+# Here we create and register a simple NamedTuple with a single unicode typed\n+# field named 'word' which we will use below.\n MyRow = typing.NamedTuple('MyRow', [('word', unicode)])\n coders.registry.register_coder(MyRow, coders.RowCoder)\n \n-# Some more fun queries:\n-# ------\n-# SELECT\n-#   word as key,\n-#   COUNT(*) as `count`\n-# FROM PCOLLECTION\n-# GROUP BY word\n-# ORDER BY `count` DESC\n-# LIMIT 100\n-# ------\n-# SELECT\n-#   len as key,\n-#   COUNT(*) as `count`\n-# FROM (\n-#   SELECT\n-#     LENGTH(word) AS len\n-#   FROM PCOLLECTION\n-# )\n-# GROUP BY len\n-\n \n def run(p, input_file, output_file):\n   #pylint: disable=expression-not-assigned\n   (\n       p\n-      | 'read' >> ReadFromText(input_file)\n-      | 'split' >> beam.FlatMap(str.split)\n-      | 'row' >> beam.Map(MyRow).with_output_types(MyRow)\n-      | 'sql!!' >> SqlTransform(\n+      | 'Read' >> ReadFromText(input_file)\n+      | 'Split' >> beam.FlatMap(lambda line: re.split(r'\\W+', line))\n+      | 'ToRow' >> beam.Map(MyRow).with_output_types(MyRow)\n+      | 'Sql!!' >> SqlTransform(\n           \"\"\"\n                    SELECT\n                      word as key,\n                      COUNT(*) as `count`\n                    FROM PCOLLECTION\n                    GROUP BY word\"\"\")\n-      | 'format' >> beam.Map(lambda row: '{}: {}'.format(row.key, row.count))\n-      | 'write' >> WriteToText(output_file))\n-\n-  result = p.run()\n-  result.wait_until_finish()\n+      | 'Format' >> beam.Map(lambda row: '{}: {}'.format(row.key, row.count))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MjI0Nw=="}, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc2OTI5Mg==", "bodyText": "Maybe just\nSqlTransform yields a PCollection containing elements with attributes based on the output of the query.\nThe fact that it's a named tuple is not needed for its use (and I'm not sure we want to promise that).", "url": "https://github.com/apache/beam/pull/12355#discussion_r459769292", "createdAt": "2020-07-23T22:46:16Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -31,51 +35,35 @@\n from apache_beam.io import WriteToText\n from apache_beam.options.pipeline_options import PipelineOptions\n from apache_beam.options.pipeline_options import SetupOptions\n+from apache_beam.runners.portability import portable_runner\n from apache_beam.transforms.sql import SqlTransform\n \n+# The input to SqlTransform must be a PCollection(s) of known schema.\n+# One way to create such a PCollection is to produce a PCollection of\n+# NamedTuple registered with the RowCoder.\n+#\n+# Here we create and register a simple NamedTuple with a single unicode typed\n+# field named 'word' which we will use below.\n MyRow = typing.NamedTuple('MyRow', [('word', unicode)])\n coders.registry.register_coder(MyRow, coders.RowCoder)\n \n-# Some more fun queries:\n-# ------\n-# SELECT\n-#   word as key,\n-#   COUNT(*) as `count`\n-# FROM PCOLLECTION\n-# GROUP BY word\n-# ORDER BY `count` DESC\n-# LIMIT 100\n-# ------\n-# SELECT\n-#   len as key,\n-#   COUNT(*) as `count`\n-# FROM (\n-#   SELECT\n-#     LENGTH(word) AS len\n-#   FROM PCOLLECTION\n-# )\n-# GROUP BY len\n-\n \n def run(p, input_file, output_file):\n   #pylint: disable=expression-not-assigned\n   (\n       p\n-      | 'read' >> ReadFromText(input_file)\n-      | 'split' >> beam.FlatMap(str.split)\n-      | 'row' >> beam.Map(MyRow).with_output_types(MyRow)\n-      | 'sql!!' >> SqlTransform(\n+      | 'Read' >> ReadFromText(input_file)\n+      | 'Split' >> beam.FlatMap(lambda line: re.split(r'\\W+', line))\n+      | 'ToRow' >> beam.Map(MyRow).with_output_types(MyRow)\n+      | 'Sql!!' >> SqlTransform(\n           \"\"\"\n                    SELECT\n                      word as key,\n                      COUNT(*) as `count`\n                    FROM PCOLLECTION\n                    GROUP BY word\"\"\")\n-      | 'format' >> beam.Map(lambda row: '{}: {}'.format(row.key, row.count))\n-      | 'write' >> WriteToText(output_file))\n-\n-  result = p.run()\n-  result.wait_until_finish()\n+      | 'Format' >> beam.Map(lambda row: '{}: {}'.format(row.key, row.count))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MjI0Nw=="}, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc3NTI5Mg==", "bodyText": "SGTM \ud83d\udc4d", "url": "https://github.com/apache/beam/pull/12355#discussion_r459775292", "createdAt": "2020-07-23T23:04:58Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -31,51 +35,35 @@\n from apache_beam.io import WriteToText\n from apache_beam.options.pipeline_options import PipelineOptions\n from apache_beam.options.pipeline_options import SetupOptions\n+from apache_beam.runners.portability import portable_runner\n from apache_beam.transforms.sql import SqlTransform\n \n+# The input to SqlTransform must be a PCollection(s) of known schema.\n+# One way to create such a PCollection is to produce a PCollection of\n+# NamedTuple registered with the RowCoder.\n+#\n+# Here we create and register a simple NamedTuple with a single unicode typed\n+# field named 'word' which we will use below.\n MyRow = typing.NamedTuple('MyRow', [('word', unicode)])\n coders.registry.register_coder(MyRow, coders.RowCoder)\n \n-# Some more fun queries:\n-# ------\n-# SELECT\n-#   word as key,\n-#   COUNT(*) as `count`\n-# FROM PCOLLECTION\n-# GROUP BY word\n-# ORDER BY `count` DESC\n-# LIMIT 100\n-# ------\n-# SELECT\n-#   len as key,\n-#   COUNT(*) as `count`\n-# FROM (\n-#   SELECT\n-#     LENGTH(word) AS len\n-#   FROM PCOLLECTION\n-# )\n-# GROUP BY len\n-\n \n def run(p, input_file, output_file):\n   #pylint: disable=expression-not-assigned\n   (\n       p\n-      | 'read' >> ReadFromText(input_file)\n-      | 'split' >> beam.FlatMap(str.split)\n-      | 'row' >> beam.Map(MyRow).with_output_types(MyRow)\n-      | 'sql!!' >> SqlTransform(\n+      | 'Read' >> ReadFromText(input_file)\n+      | 'Split' >> beam.FlatMap(lambda line: re.split(r'\\W+', line))\n+      | 'ToRow' >> beam.Map(MyRow).with_output_types(MyRow)\n+      | 'Sql!!' >> SqlTransform(\n           \"\"\"\n                    SELECT\n                      word as key,\n                      COUNT(*) as `count`\n                    FROM PCOLLECTION\n                    GROUP BY word\"\"\")\n-      | 'format' >> beam.Map(lambda row: '{}: {}'.format(row.key, row.count))\n-      | 'write' >> WriteToText(output_file))\n-\n-  result = p.run()\n-  result.wait_until_finish()\n+      | 'Format' >> beam.Map(lambda row: '{}: {}'.format(row.key, row.count))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MjI0Nw=="}, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 78}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc4ODMyOA==", "bodyText": "SGTM. Probably we need to update sql.py to match this: https://github.com/apache/beam/blob/master/sdks/python/apache_beam/transforms/sql.py#L51", "url": "https://github.com/apache/beam/pull/12355#discussion_r459788328", "createdAt": "2020-07-23T23:47:28Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -31,51 +35,35 @@\n from apache_beam.io import WriteToText\n from apache_beam.options.pipeline_options import PipelineOptions\n from apache_beam.options.pipeline_options import SetupOptions\n+from apache_beam.runners.portability import portable_runner\n from apache_beam.transforms.sql import SqlTransform\n \n+# The input to SqlTransform must be a PCollection(s) of known schema.\n+# One way to create such a PCollection is to produce a PCollection of\n+# NamedTuple registered with the RowCoder.\n+#\n+# Here we create and register a simple NamedTuple with a single unicode typed\n+# field named 'word' which we will use below.\n MyRow = typing.NamedTuple('MyRow', [('word', unicode)])\n coders.registry.register_coder(MyRow, coders.RowCoder)\n \n-# Some more fun queries:\n-# ------\n-# SELECT\n-#   word as key,\n-#   COUNT(*) as `count`\n-# FROM PCOLLECTION\n-# GROUP BY word\n-# ORDER BY `count` DESC\n-# LIMIT 100\n-# ------\n-# SELECT\n-#   len as key,\n-#   COUNT(*) as `count`\n-# FROM (\n-#   SELECT\n-#     LENGTH(word) AS len\n-#   FROM PCOLLECTION\n-# )\n-# GROUP BY len\n-\n \n def run(p, input_file, output_file):\n   #pylint: disable=expression-not-assigned\n   (\n       p\n-      | 'read' >> ReadFromText(input_file)\n-      | 'split' >> beam.FlatMap(str.split)\n-      | 'row' >> beam.Map(MyRow).with_output_types(MyRow)\n-      | 'sql!!' >> SqlTransform(\n+      | 'Read' >> ReadFromText(input_file)\n+      | 'Split' >> beam.FlatMap(lambda line: re.split(r'\\W+', line))\n+      | 'ToRow' >> beam.Map(MyRow).with_output_types(MyRow)\n+      | 'Sql!!' >> SqlTransform(\n           \"\"\"\n                    SELECT\n                      word as key,\n                      COUNT(*) as `count`\n                    FROM PCOLLECTION\n                    GROUP BY word\"\"\")\n-      | 'format' >> beam.Map(lambda row: '{}: {}'.format(row.key, row.count))\n-      | 'write' >> WriteToText(output_file))\n-\n-  result = p.run()\n-  result.wait_until_finish()\n+      | 'Format' >> beam.Map(lambda row: '{}: {}'.format(row.key, row.count))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MjI0Nw=="}, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 78}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODU5MzEzOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxODo0MDowOFrOG2W9lw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxODo1OToyMlrOG2Xnaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MjUwMw==", "bodyText": "This should not be done for Dataflow.", "url": "https://github.com/apache/beam/pull/12355#discussion_r459652503", "createdAt": "2020-07-23T18:40:08Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -101,12 +89,33 @@ def main():\n   # workflow rely on global context (e.g., a module imported at module level).\n   pipeline_options.view_as(SetupOptions).save_main_session = True\n \n-  p = beam.Pipeline(options=pipeline_options)\n-  # Preemptively start due to BEAM-6666.\n-  p.runner.create_job_service(pipeline_options)\n+  with beam.Pipeline(options=pipeline_options) as p:\n+    if isinstance(p.runner, portable_runner.PortableRunner):\n+      # Preemptively start due to BEAM-6666.\n+      p.runner.create_job_service(pipeline_options)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY2MTU5OQ==", "bodyText": "won't the isinstance check ensure it's not?", "url": "https://github.com/apache/beam/pull/12355#discussion_r459661599", "createdAt": "2020-07-23T18:56:22Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -101,12 +89,33 @@ def main():\n   # workflow rely on global context (e.g., a module imported at module level).\n   pipeline_options.view_as(SetupOptions).save_main_session = True\n \n-  p = beam.Pipeline(options=pipeline_options)\n-  # Preemptively start due to BEAM-6666.\n-  p.runner.create_job_service(pipeline_options)\n+  with beam.Pipeline(options=pipeline_options) as p:\n+    if isinstance(p.runner, portable_runner.PortableRunner):\n+      # Preemptively start due to BEAM-6666.\n+      p.runner.create_job_service(pipeline_options)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MjUwMw=="}, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 93}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY2MzIxMQ==", "bodyText": "Yeah, you are right.", "url": "https://github.com/apache/beam/pull/12355#discussion_r459663211", "createdAt": "2020-07-23T18:59:22Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -101,12 +89,33 @@ def main():\n   # workflow rely on global context (e.g., a module imported at module level).\n   pipeline_options.view_as(SetupOptions).save_main_session = True\n \n-  p = beam.Pipeline(options=pipeline_options)\n-  # Preemptively start due to BEAM-6666.\n-  p.runner.create_job_service(pipeline_options)\n+  with beam.Pipeline(options=pipeline_options) as p:\n+    if isinstance(p.runner, portable_runner.PortableRunner):\n+      # Preemptively start due to BEAM-6666.\n+      p.runner.create_job_service(pipeline_options)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MjUwMw=="}, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 93}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2ODU5ODYzOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QxODo0MTo0NlrOG2XBGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yM1QyMzozOTozM1rOG2fGlg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MzQwMA==", "bodyText": "Did we confirm that these are working for Daataflow and portable runners ?", "url": "https://github.com/apache/beam/pull/12355#discussion_r459653400", "createdAt": "2020-07-23T18:41:46Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -101,12 +89,33 @@ def main():\n   # workflow rely on global context (e.g., a module imported at module level).\n   pipeline_options.view_as(SetupOptions).save_main_session = True\n \n-  p = beam.Pipeline(options=pipeline_options)\n-  # Preemptively start due to BEAM-6666.\n-  p.runner.create_job_service(pipeline_options)\n+  with beam.Pipeline(options=pipeline_options) as p:\n+    if isinstance(p.runner, portable_runner.PortableRunner):\n+      # Preemptively start due to BEAM-6666.\n+      p.runner.create_job_service(pipeline_options)\n+\n+    run(p, known_args.input, known_args.output)\n \n-  run(p, known_args.input, known_args.output)\n \n+# Some more fun queries:\n+# ------\n+# SELECT\n+#   word as key,\n+#   COUNT(*) as `count`\n+# FROM PCOLLECTION\n+# GROUP BY word", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY2MTgwNg==", "bodyText": "I ran all these queries when I was demoing on Flink with the prototype last Fall, but haven't done it for a while.", "url": "https://github.com/apache/beam/pull/12355#discussion_r459661806", "createdAt": "2020-07-23T18:56:45Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -101,12 +89,33 @@ def main():\n   # workflow rely on global context (e.g., a module imported at module level).\n   pipeline_options.view_as(SetupOptions).save_main_session = True\n \n-  p = beam.Pipeline(options=pipeline_options)\n-  # Preemptively start due to BEAM-6666.\n-  p.runner.create_job_service(pipeline_options)\n+  with beam.Pipeline(options=pipeline_options) as p:\n+    if isinstance(p.runner, portable_runner.PortableRunner):\n+      # Preemptively start due to BEAM-6666.\n+      p.runner.create_job_service(pipeline_options)\n+\n+    run(p, known_args.input, known_args.output)\n \n-  run(p, known_args.input, known_args.output)\n \n+# Some more fun queries:\n+# ------\n+# SELECT\n+#   word as key,\n+#   COUNT(*) as `count`\n+# FROM PCOLLECTION\n+# GROUP BY word", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MzQwMA=="}, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY2MzQxMg==", "bodyText": "Let's try them again and try with Dataflow as well.", "url": "https://github.com/apache/beam/pull/12355#discussion_r459663412", "createdAt": "2020-07-23T18:59:44Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -101,12 +89,33 @@ def main():\n   # workflow rely on global context (e.g., a module imported at module level).\n   pipeline_options.view_as(SetupOptions).save_main_session = True\n \n-  p = beam.Pipeline(options=pipeline_options)\n-  # Preemptively start due to BEAM-6666.\n-  p.runner.create_job_service(pipeline_options)\n+  with beam.Pipeline(options=pipeline_options) as p:\n+    if isinstance(p.runner, portable_runner.PortableRunner):\n+      # Preemptively start due to BEAM-6666.\n+      p.runner.create_job_service(pipeline_options)\n+\n+    run(p, known_args.input, known_args.output)\n \n-  run(p, known_args.input, known_args.output)\n \n+# Some more fun queries:\n+# ------\n+# SELECT\n+#   word as key,\n+#   COUNT(*) as `count`\n+# FROM PCOLLECTION\n+# GROUP BY word", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MzQwMA=="}, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc4NTg3OA==", "bodyText": "Good idea. I ran all of them on FnApiRunner and Dataflow on 2.23.0-RC2 (with the job service line commented out) and they worked as intended \ud83c\udf89\nJob ids for the Dataflow runs:\n\n2020-07-23_16_00_29-9161755024487428146\n2020-07-23_16_11_11-9635287738762184253\n2020-07-23_16_19_49-9899271126466068112", "url": "https://github.com/apache/beam/pull/12355#discussion_r459785878", "createdAt": "2020-07-23T23:39:33Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -101,12 +89,33 @@ def main():\n   # workflow rely on global context (e.g., a module imported at module level).\n   pipeline_options.view_as(SetupOptions).save_main_session = True\n \n-  p = beam.Pipeline(options=pipeline_options)\n-  # Preemptively start due to BEAM-6666.\n-  p.runner.create_job_service(pipeline_options)\n+  with beam.Pipeline(options=pipeline_options) as p:\n+    if isinstance(p.runner, portable_runner.PortableRunner):\n+      # Preemptively start due to BEAM-6666.\n+      p.runner.create_job_service(pipeline_options)\n+\n+    run(p, known_args.input, known_args.output)\n \n-  run(p, known_args.input, known_args.output)\n \n+# Some more fun queries:\n+# ------\n+# SELECT\n+#   word as key,\n+#   COUNT(*) as `count`\n+# FROM PCOLLECTION\n+# GROUP BY word", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTY1MzQwMA=="}, "originalCommit": {"oid": "ad943a7f113af0c2764111004647462bf12210c1"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg2OTUxNDY3OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQwMDoxMzoxNFrOG2frkw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNFQwMDoxMzoxNFrOG2frkw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1OTc5NTM0Nw==", "bodyText": "How about:\nJava 8 must be available to run this pipeline and --experiments=use_runner_v2 flag must be passed for running on Dataflow.\nAdditionally you need to install Docker to run this pipeline from HEAD with a custom Java container. See `here <https://github.com/apache/beam/tree/master/sdks/python/apache_beam/examples/kafkataxi/>` for more details.", "url": "https://github.com/apache/beam/pull/12355#discussion_r459795347", "createdAt": "2020-07-24T00:13:14Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/examples/wordcount_xlang_sql.py", "diffHunk": "@@ -15,12 +15,17 @@\n # limitations under the License.\n #\n \n-\"\"\"A cross-language word-counting workflow.\"\"\"\n+\"\"\"A word-counting workflow that uses the SQL transform.\n+\n+Java 8 and docker must be available to run this pipeline, and the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "da6612a2bd9e59ab1e8240700e5121c2a7daf5e0"}, "originalPosition": 7}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 938, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}