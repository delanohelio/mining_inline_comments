{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY0MjYxNjQ0", "number": 12485, "reviewThreads": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQwMTo0NTo1MlrOEXBflg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQwMTo0Nzo1MFrOEXBg9A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyNTc3MTc0OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/io/gcp/bigquery.py", "isResolved": true, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQwMTo0NTo1MlrOG-lfhA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMDoyNDowOFrOG_H5ig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI3OTE3Mg==", "bodyText": "I believe Java uses 50 shards. Do we need a larger default for Python ?", "url": "https://github.com/apache/beam/pull/12485#discussion_r468279172", "createdAt": "2020-08-11T01:45:52Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/io/gcp/bigquery.py", "diffHunk": "@@ -304,6 +308,8 @@ def compute_table_name(row):\n NOTE: This job name template does not have backwards compatibility guarantees.\n \"\"\"\n BQ_JOB_NAME_TEMPLATE = \"beam_bq_job_{job_type}_{job_id}_{step_id}{random}\"\n+\"\"\"The number of shards per destination when writing via streaming inserts.\"\"\"\n+DEFAULT_SHARDS_PER_DESTINATION = 500", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1e075755b871ee99454580fc17d0ff9b89c42dc9"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgyNTY3Nw==", "bodyText": "we've been able to reach ~1k EPS per worker in Python. If we have 50 shards, we'll only reach ~50k maximum. I'd like to have a larger default, so we don't automatically cap EPS at a very low rate.", "url": "https://github.com/apache/beam/pull/12485#discussion_r468825677", "createdAt": "2020-08-11T19:50:05Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/io/gcp/bigquery.py", "diffHunk": "@@ -304,6 +308,8 @@ def compute_table_name(row):\n NOTE: This job name template does not have backwards compatibility guarantees.\n \"\"\"\n BQ_JOB_NAME_TEMPLATE = \"beam_bq_job_{job_type}_{job_id}_{step_id}{random}\"\n+\"\"\"The number of shards per destination when writing via streaming inserts.\"\"\"\n+DEFAULT_SHARDS_PER_DESTINATION = 500", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI3OTE3Mg=="}, "originalCommit": {"oid": "1e075755b871ee99454580fc17d0ff9b89c42dc9"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgyOTg1Ng==", "bodyText": "Note that Java also allows users control this through a pipeline option and Dataflow support ask users to update this parameter as they fit.", "url": "https://github.com/apache/beam/pull/12485#discussion_r468829856", "createdAt": "2020-08-11T19:57:59Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/io/gcp/bigquery.py", "diffHunk": "@@ -304,6 +308,8 @@ def compute_table_name(row):\n NOTE: This job name template does not have backwards compatibility guarantees.\n \"\"\"\n BQ_JOB_NAME_TEMPLATE = \"beam_bq_job_{job_type}_{job_id}_{step_id}{random}\"\n+\"\"\"The number of shards per destination when writing via streaming inserts.\"\"\"\n+DEFAULT_SHARDS_PER_DESTINATION = 500", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI3OTE3Mg=="}, "originalCommit": {"oid": "1e075755b871ee99454580fc17d0ff9b89c42dc9"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgzMDExNg==", "bodyText": "https://github.com/apache/beam/blob/master/sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryOptions.java#L58", "url": "https://github.com/apache/beam/pull/12485#discussion_r468830116", "createdAt": "2020-08-11T19:58:34Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/io/gcp/bigquery.py", "diffHunk": "@@ -304,6 +308,8 @@ def compute_table_name(row):\n NOTE: This job name template does not have backwards compatibility guarantees.\n \"\"\"\n BQ_JOB_NAME_TEMPLATE = \"beam_bq_job_{job_type}_{job_id}_{step_id}{random}\"\n+\"\"\"The number of shards per destination when writing via streaming inserts.\"\"\"\n+DEFAULT_SHARDS_PER_DESTINATION = 500", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI3OTE3Mg=="}, "originalCommit": {"oid": "1e075755b871ee99454580fc17d0ff9b89c42dc9"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgzMDQ1MA==", "bodyText": "These flags for streaming inserts can be added in a separate PR though.", "url": "https://github.com/apache/beam/pull/12485#discussion_r468830450", "createdAt": "2020-08-11T19:59:12Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/io/gcp/bigquery.py", "diffHunk": "@@ -304,6 +308,8 @@ def compute_table_name(row):\n NOTE: This job name template does not have backwards compatibility guarantees.\n \"\"\"\n BQ_JOB_NAME_TEMPLATE = \"beam_bq_job_{job_type}_{job_id}_{step_id}{random}\"\n+\"\"\"The number of shards per destination when writing via streaming inserts.\"\"\"\n+DEFAULT_SHARDS_PER_DESTINATION = 500", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI3OTE3Mg=="}, "originalCommit": {"oid": "1e075755b871ee99454580fc17d0ff9b89c42dc9"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg0Mjg5MA==", "bodyText": "Filed https://issues.apache.org/jira/browse/BEAM-10680 - thanks Cham!", "url": "https://github.com/apache/beam/pull/12485#discussion_r468842890", "createdAt": "2020-08-11T20:24:08Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/io/gcp/bigquery.py", "diffHunk": "@@ -304,6 +308,8 @@ def compute_table_name(row):\n NOTE: This job name template does not have backwards compatibility guarantees.\n \"\"\"\n BQ_JOB_NAME_TEMPLATE = \"beam_bq_job_{job_type}_{job_id}_{step_id}{random}\"\n+\"\"\"The number of shards per destination when writing via streaming inserts.\"\"\"\n+DEFAULT_SHARDS_PER_DESTINATION = 500", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI3OTE3Mg=="}, "originalCommit": {"oid": "1e075755b871ee99454580fc17d0ff9b89c42dc9"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyNTc3NTI0OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/io/gcp/bigquery.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQwMTo0Nzo1MFrOG-lhgQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQxOTo0ODo1OVrOG_G0Cw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI3OTY4MQ==", "bodyText": "This is just a documentation update for a already available (and verified) feature ?", "url": "https://github.com/apache/beam/pull/12485#discussion_r468279681", "createdAt": "2020-08-11T01:47:50Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/io/gcp/bigquery.py", "diffHunk": "@@ -1419,7 +1448,18 @@ def __init__(\n         Default is to retry always. This means that whenever there are rows\n         that fail to be inserted to BigQuery, they will be retried indefinitely.\n         Other retry strategy settings will produce a deadletter PCollection\n-        as output.\n+        as output. Appropriate values are:\n+\n+        * `RetryStrategy.RETRY_ALWAYS`: retry all rows if\n+          there are any kind of errors. Note that this will hold your pipeline\n+          back if there are errors until you cancel or update it.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1e075755b871ee99454580fc17d0ff9b89c42dc9"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODgyNTA5OQ==", "bodyText": "that's correct.", "url": "https://github.com/apache/beam/pull/12485#discussion_r468825099", "createdAt": "2020-08-11T19:48:59Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/io/gcp/bigquery.py", "diffHunk": "@@ -1419,7 +1448,18 @@ def __init__(\n         Default is to retry always. This means that whenever there are rows\n         that fail to be inserted to BigQuery, they will be retried indefinitely.\n         Other retry strategy settings will produce a deadletter PCollection\n-        as output.\n+        as output. Appropriate values are:\n+\n+        * `RetryStrategy.RETRY_ALWAYS`: retry all rows if\n+          there are any kind of errors. Note that this will hold your pipeline\n+          back if there are errors until you cancel or update it.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODI3OTY4MQ=="}, "originalCommit": {"oid": "1e075755b871ee99454580fc17d0ff9b89c42dc9"}, "originalPosition": 152}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 852, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}