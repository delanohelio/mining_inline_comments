{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDgzMDc0Njky", "number": 12799, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQxNDo0NDoxNVrOEkFvOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQxNToxODozNFrOEkGvhQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA2Mjc4MjAxOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/interactive/utils.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQxNDo0NDoxNVrOHS0XPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQyMjoxNTo0NlrOHTGD8w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQ5NDMzNQ==", "bodyText": "In this case, 'teststream events' seem t refer to watermark/processing time events only. Perhaps you can call it 'include_time_events' or something like that? (since Data events also come form the teststream but are ont affected by this flag)", "url": "https://github.com/apache/beam/pull/12799#discussion_r489494335", "createdAt": "2020-09-16T14:44:15Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/runners/interactive/utils.py", "diffHunk": "@@ -34,7 +34,8 @@ def to_element_list(\n     reader,  # type: Generator[Union[TestStreamPayload.Event, WindowedValueHolder]]\n     coder,  # type: Coder\n     include_window_info,  # type: bool\n-    n=None  # type: int\n+    n=None,  # type: int\n+    include_teststream_events=False, # type: bool", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ba76c58ff93b70bb55d2cbc5a1d02d46257d2c4"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTc4NDMwNw==", "bodyText": "Gotcha, changed to include_time_events", "url": "https://github.com/apache/beam/pull/12799#discussion_r489784307", "createdAt": "2020-09-16T22:15:46Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/interactive/utils.py", "diffHunk": "@@ -34,7 +34,8 @@ def to_element_list(\n     reader,  # type: Generator[Union[TestStreamPayload.Event, WindowedValueHolder]]\n     coder,  # type: Coder\n     include_window_info,  # type: bool\n-    n=None  # type: int\n+    n=None,  # type: int\n+    include_teststream_events=False, # type: bool", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQ5NDMzNQ=="}, "originalCommit": {"oid": "9ba76c58ff93b70bb55d2cbc5a1d02d46257d2c4"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA2MjgzNjk0OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/interactive/recording_manager.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQxNDo1NToyM1rOHS05mg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQyMjoxNTo0OVrOHTGEAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTUwMzEzMA==", "bodyText": "Are Data elements sent decoded, and that's why we have this if/else?", "url": "https://github.com/apache/beam/pull/12799#discussion_r489503130", "createdAt": "2020-09-16T14:55:23Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/runners/interactive/recording_manager.py", "diffHunk": "@@ -114,14 +113,19 @@ def read(self, tail=True):\n     # all elements from the cache were read. In the latter situation, it may be\n     # the case that the pipeline was still running. Thus, another invocation of\n     # `read` will yield new elements.\n+    count_limiter = CountLimiter(self._n)\n+    time_limiter = ProcessingTimeLimiter(self._duration_secs)\n+    limiters = (count_limiter, time_limiter)\n     for e in utils.to_element_list(reader,\n                                    coder,\n                                    include_window_info=True,\n-                                   n=self._n):\n-      for l in limiters:\n-        l.update(e)\n-\n-      yield e\n+                                   n=self._n,\n+                                   include_teststream_events=True):\n+      if isinstance(e, TestStreamPayload.Event):\n+        time_limiter.update(e)\n+      else:\n+        count_limiter.update(e)\n+        yield e", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ba76c58ff93b70bb55d2cbc5a1d02d46257d2c4"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTc4NDMyMg==", "bodyText": "Yep, it's to make sure we only count decoded elements. I added a comment to make it more clear.", "url": "https://github.com/apache/beam/pull/12799#discussion_r489784322", "createdAt": "2020-09-16T22:15:49Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/interactive/recording_manager.py", "diffHunk": "@@ -114,14 +113,19 @@ def read(self, tail=True):\n     # all elements from the cache were read. In the latter situation, it may be\n     # the case that the pipeline was still running. Thus, another invocation of\n     # `read` will yield new elements.\n+    count_limiter = CountLimiter(self._n)\n+    time_limiter = ProcessingTimeLimiter(self._duration_secs)\n+    limiters = (count_limiter, time_limiter)\n     for e in utils.to_element_list(reader,\n                                    coder,\n                                    include_window_info=True,\n-                                   n=self._n):\n-      for l in limiters:\n-        l.update(e)\n-\n-      yield e\n+                                   n=self._n,\n+                                   include_teststream_events=True):\n+      if isinstance(e, TestStreamPayload.Event):\n+        time_limiter.update(e)\n+      else:\n+        count_limiter.update(e)\n+        yield e", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTUwMzEzMA=="}, "originalCommit": {"oid": "9ba76c58ff93b70bb55d2cbc5a1d02d46257d2c4"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA2MjgzOTA1OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/interactive/recording_manager.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQxNDo1NTo0N1rOHS064g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQyMjoxNTo1MlrOHTGEDw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTUwMzQ1OA==", "bodyText": "Why are you removing start from here?", "url": "https://github.com/apache/beam/pull/12799#discussion_r489503458", "createdAt": "2020-09-16T14:55:47Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/runners/interactive/recording_manager.py", "diffHunk": "@@ -256,7 +259,7 @@ def describe(self):\n \n     size = sum(\n         cache_manager.size('full', s.cache_key) for s in self._streams.values())\n-    return {'size': size, 'start': self._start}\n+    return {'size': size}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ba76c58ff93b70bb55d2cbc5a1d02d46257d2c4"}, "originalPosition": 76}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTc4NDMzNQ==", "bodyText": "Because the start time wasn't correct if we only start a background caching job. In that case there wouldn't be a new Recording so the start time would be 0. I think this also cleans up the logic a bit (no more min-ing over all the start times).", "url": "https://github.com/apache/beam/pull/12799#discussion_r489784335", "createdAt": "2020-09-16T22:15:52Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/interactive/recording_manager.py", "diffHunk": "@@ -256,7 +259,7 @@ def describe(self):\n \n     size = sum(\n         cache_manager.size('full', s.cache_key) for s in self._streams.values())\n-    return {'size': size, 'start': self._start}\n+    return {'size': size}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTUwMzQ1OA=="}, "originalCommit": {"oid": "9ba76c58ff93b70bb55d2cbc5a1d02d46257d2c4"}, "originalPosition": 76}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA2MjkwNzE1OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/interactive/recording_manager.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQxNToxMDowNVrOHS1mDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQyMjozMzoxOVrOHTGc2g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTUxNDUwOA==", "bodyText": "You can just add line 347 without the if/else. sum(..) over an empty list will return 0.", "url": "https://github.com/apache/beam/pull/12799#discussion_r489514508", "createdAt": "2020-09-16T15:10:05Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/runners/interactive/recording_manager.py", "diffHunk": "@@ -314,15 +330,55 @@ def cancel(self):\n       r.wait_until_finish()\n     self._recordings = set()\n \n+    # The recordings rely on a reference to the BCJ to correctly finish. So we\n+    # evict the BCJ after they complete.\n+    ie.current_env().evict_background_caching_job(self.user_pipeline)\n+\n   def describe(self):\n     # type: () -> dict[str, int]\n \n     \"\"\"Returns a dictionary describing the cache and recording.\"\"\"\n \n+    cache_manager = ie.current_env().get_cache_manager(self.user_pipeline)\n+    capture_size = getattr(cache_manager, 'capture_size', 0)\n+\n     descriptions = [r.describe() for r in self._recordings]\n-    size = sum(d['size'] for d in descriptions)\n-    start = min(d['start'] for d in descriptions)\n-    return {'size': size, 'start': start}\n+    if descriptions:\n+      size = sum(d['size'] for d in descriptions) + capture_size\n+    else:\n+      size = capture_size", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ba76c58ff93b70bb55d2cbc5a1d02d46257d2c4"}, "originalPosition": 142}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTc5MDY4Mg==", "bodyText": "Done", "url": "https://github.com/apache/beam/pull/12799#discussion_r489790682", "createdAt": "2020-09-16T22:33:19Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/interactive/recording_manager.py", "diffHunk": "@@ -314,15 +330,55 @@ def cancel(self):\n       r.wait_until_finish()\n     self._recordings = set()\n \n+    # The recordings rely on a reference to the BCJ to correctly finish. So we\n+    # evict the BCJ after they complete.\n+    ie.current_env().evict_background_caching_job(self.user_pipeline)\n+\n   def describe(self):\n     # type: () -> dict[str, int]\n \n     \"\"\"Returns a dictionary describing the cache and recording.\"\"\"\n \n+    cache_manager = ie.current_env().get_cache_manager(self.user_pipeline)\n+    capture_size = getattr(cache_manager, 'capture_size', 0)\n+\n     descriptions = [r.describe() for r in self._recordings]\n-    size = sum(d['size'] for d in descriptions)\n-    start = min(d['start'] for d in descriptions)\n-    return {'size': size, 'start': start}\n+    if descriptions:\n+      size = sum(d['size'] for d in descriptions) + capture_size\n+    else:\n+      size = capture_size", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTUxNDUwOA=="}, "originalCommit": {"oid": "9ba76c58ff93b70bb55d2cbc5a1d02d46257d2c4"}, "originalPosition": 142}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA2MjkzOTQwOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/interactive/recording_manager.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQxNToxNzoxNVrOHS16iw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQyMjozNzo0MlrOHTGiuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTUxOTc1NQ==", "bodyText": "Would this failure be logged / handled somewhere? Maybe we should log the fialure to start the BCJ?", "url": "https://github.com/apache/beam/pull/12799#discussion_r489519755", "createdAt": "2020-09-16T15:17:15Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/runners/interactive/recording_manager.py", "diffHunk": "@@ -314,15 +330,55 @@ def cancel(self):\n       r.wait_until_finish()\n     self._recordings = set()\n \n+    # The recordings rely on a reference to the BCJ to correctly finish. So we\n+    # evict the BCJ after they complete.\n+    ie.current_env().evict_background_caching_job(self.user_pipeline)\n+\n   def describe(self):\n     # type: () -> dict[str, int]\n \n     \"\"\"Returns a dictionary describing the cache and recording.\"\"\"\n \n+    cache_manager = ie.current_env().get_cache_manager(self.user_pipeline)\n+    capture_size = getattr(cache_manager, 'capture_size', 0)\n+\n     descriptions = [r.describe() for r in self._recordings]\n-    size = sum(d['size'] for d in descriptions)\n-    start = min(d['start'] for d in descriptions)\n-    return {'size': size, 'start': start}\n+    if descriptions:\n+      size = sum(d['size'] for d in descriptions) + capture_size\n+    else:\n+      size = capture_size\n+    start = self._start_time_sec\n+    bcj = ie.current_env().get_background_caching_job(self.user_pipeline)\n+    if bcj:\n+      state = bcj.state\n+    else:\n+      state = PipelineState.STOPPED\n+    return {'size': size, 'start': start, 'state': state}\n+\n+  def record_pipeline(self):\n+    # type: () -> bool\n+\n+    \"\"\"Starts a background caching job for this RecordingManager's pipeline.\"\"\"\n+\n+    runner = self.user_pipeline.runner\n+    if isinstance(runner, ir.InteractiveRunner):\n+      runner = runner._underlying_runner\n+\n+    # Make sure that sources without a user reference are still cached.\n+    pi.watch_sources(self.user_pipeline)\n+\n+    # Attempt to run background caching job to record any sources.\n+    if ie.current_env().is_in_ipython:\n+      warnings.filterwarnings(\n+          'ignore',\n+          'options is deprecated since First stable release. References to '\n+          '<pipeline>.options will not be supported',\n+          category=DeprecationWarning)\n+    if bcj.attempt_to_run_background_caching_job(\n+        runner, self.user_pipeline, options=self.user_pipeline.options):\n+      self._start_time_sec = time.time()\n+      return True\n+    return False", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ba76c58ff93b70bb55d2cbc5a1d02d46257d2c4"}, "originalPosition": 174}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTc5MjE4NA==", "bodyText": "I don't think we should because it's not necessarily an error that the BCJ didn't start. There's logic inside the BCJ that only starts if it needs to, i.e. the list unbounded sources changed or there is nothing in the cache. In the next PR, there is extra more logging around when it is OK to try and start a new recording.", "url": "https://github.com/apache/beam/pull/12799#discussion_r489792184", "createdAt": "2020-09-16T22:37:42Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/interactive/recording_manager.py", "diffHunk": "@@ -314,15 +330,55 @@ def cancel(self):\n       r.wait_until_finish()\n     self._recordings = set()\n \n+    # The recordings rely on a reference to the BCJ to correctly finish. So we\n+    # evict the BCJ after they complete.\n+    ie.current_env().evict_background_caching_job(self.user_pipeline)\n+\n   def describe(self):\n     # type: () -> dict[str, int]\n \n     \"\"\"Returns a dictionary describing the cache and recording.\"\"\"\n \n+    cache_manager = ie.current_env().get_cache_manager(self.user_pipeline)\n+    capture_size = getattr(cache_manager, 'capture_size', 0)\n+\n     descriptions = [r.describe() for r in self._recordings]\n-    size = sum(d['size'] for d in descriptions)\n-    start = min(d['start'] for d in descriptions)\n-    return {'size': size, 'start': start}\n+    if descriptions:\n+      size = sum(d['size'] for d in descriptions) + capture_size\n+    else:\n+      size = capture_size\n+    start = self._start_time_sec\n+    bcj = ie.current_env().get_background_caching_job(self.user_pipeline)\n+    if bcj:\n+      state = bcj.state\n+    else:\n+      state = PipelineState.STOPPED\n+    return {'size': size, 'start': start, 'state': state}\n+\n+  def record_pipeline(self):\n+    # type: () -> bool\n+\n+    \"\"\"Starts a background caching job for this RecordingManager's pipeline.\"\"\"\n+\n+    runner = self.user_pipeline.runner\n+    if isinstance(runner, ir.InteractiveRunner):\n+      runner = runner._underlying_runner\n+\n+    # Make sure that sources without a user reference are still cached.\n+    pi.watch_sources(self.user_pipeline)\n+\n+    # Attempt to run background caching job to record any sources.\n+    if ie.current_env().is_in_ipython:\n+      warnings.filterwarnings(\n+          'ignore',\n+          'options is deprecated since First stable release. References to '\n+          '<pipeline>.options will not be supported',\n+          category=DeprecationWarning)\n+    if bcj.attempt_to_run_background_caching_job(\n+        runner, self.user_pipeline, options=self.user_pipeline.options):\n+      self._start_time_sec = time.time()\n+      return True\n+    return False", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTUxOTc1NQ=="}, "originalCommit": {"oid": "9ba76c58ff93b70bb55d2cbc5a1d02d46257d2c4"}, "originalPosition": 174}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA2Mjk0NjYxOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/interactive/recording_manager_test.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQxNToxODozNFrOHS1-zw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNlQyMjo0MjowMlrOHTGolg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTUyMDg0Nw==", "bodyText": "I am trying to figure out - where did we stop passing encoded data events?", "url": "https://github.com/apache/beam/pull/12799#discussion_r489520847", "createdAt": "2020-09-16T15:18:34Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/runners/interactive/recording_manager_test.py", "diffHunk": "@@ -149,43 +152,37 @@ def test_read_n(self):\n \n   def test_read_duration(self):\n     \"\"\"Test that the stream only reads a 'duration' of elements.\"\"\"\n+    def as_windowed_value(element):\n+      return WindowedValueHolder(WindowedValue(element, 0, []))\n \n     values = (FileRecordsBuilder(tag=self.cache_key)\n               .advance_processing_time(1)\n-              .add_element(element=0, event_time_secs=0)\n+              .add_element(element=as_windowed_value(0), event_time_secs=0)\n               .advance_processing_time(1)\n-              .add_element(element=1, event_time_secs=1)\n+              .add_element(element=as_windowed_value(1), event_time_secs=1)\n               .advance_processing_time(1)\n-              .add_element(element=2, event_time_secs=3)\n+              .add_element(element=as_windowed_value(2), event_time_secs=3)\n               .advance_processing_time(1)\n-              .add_element(element=3, event_time_secs=4)\n+              .add_element(element=as_windowed_value(3), event_time_secs=4)\n               .advance_processing_time(1)\n-              .add_element(element=4, event_time_secs=5)\n+              .add_element(element=as_windowed_value(4), event_time_secs=5)\n               .build()) # yapf: disable\n \n+    values = [\n+        v.recorded_event for v in values if isinstance(v, TestStreamFileRecord)\n+    ]\n+\n     self.mock_result.set_state(PipelineState.DONE)\n     self.cache.write(values, 'full', self.cache_key)\n-    self.cache.save_pcoder(None, 'full', self.cache_key)\n-\n-    # The elements read from the cache are TestStreamFileRecord instances and\n-    # have the underlying elements encoded. This method decodes the elements\n-    # from the TestStreamFileRecord.\n-    def get_elements(events):\n-      coder = coders.FastPrimitivesCoder()\n-      elements = []\n-      for e in events:\n-        if not isinstance(e, TestStreamFileRecord):\n-          continue\n-\n-        if e.recorded_event.element_event:\n-          elements += ([\n-              coder.decode(el.encoded_element)\n-              for el in e.recorded_event.element_event.elements\n-          ])\n-      return elements\n+    self.cache.save_pcoder(coders.FastPrimitivesCoder(), 'full', self.cache_key)\n \n     # The following tests a progression of reading different durations from the\n     # cache.\n+\n+    # Small convienence function for getting the values.\n+    def get_elements(events):\n+      return [e.value for e in events]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9ba76c58ff93b70bb55d2cbc5a1d02d46257d2c4"}, "originalPosition": 80}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTc5MzY4Ng==", "bodyText": "The test uses the InMemoryCache set in the setUp method that directly writes values to an in-memory map verbatim.", "url": "https://github.com/apache/beam/pull/12799#discussion_r489793686", "createdAt": "2020-09-16T22:42:02Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/interactive/recording_manager_test.py", "diffHunk": "@@ -149,43 +152,37 @@ def test_read_n(self):\n \n   def test_read_duration(self):\n     \"\"\"Test that the stream only reads a 'duration' of elements.\"\"\"\n+    def as_windowed_value(element):\n+      return WindowedValueHolder(WindowedValue(element, 0, []))\n \n     values = (FileRecordsBuilder(tag=self.cache_key)\n               .advance_processing_time(1)\n-              .add_element(element=0, event_time_secs=0)\n+              .add_element(element=as_windowed_value(0), event_time_secs=0)\n               .advance_processing_time(1)\n-              .add_element(element=1, event_time_secs=1)\n+              .add_element(element=as_windowed_value(1), event_time_secs=1)\n               .advance_processing_time(1)\n-              .add_element(element=2, event_time_secs=3)\n+              .add_element(element=as_windowed_value(2), event_time_secs=3)\n               .advance_processing_time(1)\n-              .add_element(element=3, event_time_secs=4)\n+              .add_element(element=as_windowed_value(3), event_time_secs=4)\n               .advance_processing_time(1)\n-              .add_element(element=4, event_time_secs=5)\n+              .add_element(element=as_windowed_value(4), event_time_secs=5)\n               .build()) # yapf: disable\n \n+    values = [\n+        v.recorded_event for v in values if isinstance(v, TestStreamFileRecord)\n+    ]\n+\n     self.mock_result.set_state(PipelineState.DONE)\n     self.cache.write(values, 'full', self.cache_key)\n-    self.cache.save_pcoder(None, 'full', self.cache_key)\n-\n-    # The elements read from the cache are TestStreamFileRecord instances and\n-    # have the underlying elements encoded. This method decodes the elements\n-    # from the TestStreamFileRecord.\n-    def get_elements(events):\n-      coder = coders.FastPrimitivesCoder()\n-      elements = []\n-      for e in events:\n-        if not isinstance(e, TestStreamFileRecord):\n-          continue\n-\n-        if e.recorded_event.element_event:\n-          elements += ([\n-              coder.decode(el.encoded_element)\n-              for el in e.recorded_event.element_event.elements\n-          ])\n-      return elements\n+    self.cache.save_pcoder(coders.FastPrimitivesCoder(), 'full', self.cache_key)\n \n     # The following tests a progression of reading different durations from the\n     # cache.\n+\n+    # Small convienence function for getting the values.\n+    def get_elements(events):\n+      return [e.value for e in events]", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTUyMDg0Nw=="}, "originalCommit": {"oid": "9ba76c58ff93b70bb55d2cbc5a1d02d46257d2c4"}, "originalPosition": 80}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 488, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}