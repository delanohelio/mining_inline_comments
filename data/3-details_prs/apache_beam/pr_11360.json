{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDAxMzIzMTI1", "number": 11360, "title": "[BEAM-9722] added SnowflakeIO with Read operation", "bodyText": "This PR is adding SnowflakeIO with Read operation as part of BEAM-9722\nSnowflake is an analytic data warehouse provided as Software-as-a-Service (SaaS). It uses a new SQL database engine with a unique architecture designed for the cloud. To read more details please check here and here.\nThe SnowflakeIO.Read uses Snowflake's JDBC driver to run COPY INTO  statement to move data on GCS as CSV files that are then read via FileIO.\nThe SnowflakeIO allows to use three authentication methods against Snowflake:\n\nusername and password\nkey-pair\npre-obtained OAuth token\n\nThis PR is first of series, once merged, subsequent PRs will come - with Write, integration tests etc. We're also working on cross-language to support Python SDK.\n\nThank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:\n\n Choose reviewer(s) and mention them in a comment (R: @username).\n Format the pull request title like [BEAM-XXX] Fixes bug in ApproximateQuantiles, where you replace BEAM-XXX with the appropriate JIRA issue, if applicable. This will automatically link the pull request to the issue.\n Update CHANGES.md with noteworthy changes.\n If this contribution is large, please file an Apache Individual Contributor License Agreement.\n\nSee the Contributor Guide for more tips on how to make review process smoother.\nPost-Commit Tests Status (on master branch)\n\n\n\nLang\nSDK\nApex\nDataflow\nFlink\nGearpump\nSamza\nSpark\n\n\n\n\nGo\n\n---\n---\n\n---\n---\n\n\n\nJava\n\n\n\n\n\n\n\n\n\nPython\n\n---\n\n\n---\n---\n\n\n\nXLang\n---\n---\n---\n\n---\n---\n\n\n\n\nPre-Commit Tests Status (on master branch)\n\n\n\n---\nJava\nPython\nGo\nWebsite\n\n\n\n\nNon-portable\n\n\n\n\n\n\nPortable\n---\n\n---\n---\n\n\n\nSee .test-infra/jenkins/README for trigger phrase, status and link of all Jenkins jobs.", "createdAt": "2020-04-09T09:49:11Z", "url": "https://github.com/apache/beam/pull/11360", "merged": true, "mergeCommit": {"oid": "73fa1356e4bc2d04e51c06cbf7c3aa860264f9ee"}, "closed": true, "closedAt": "2020-05-21T22:39:17Z", "author": {"login": "DariuszAniszewski"}, "timelineItems": {"totalCount": 30, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcWDgF1gFqTM5MTE0OTMyMQ==", "endCursor": "Y3Vyc29yOnYyOpPPAAABcjY7TuABqjMzNTk2NTc0NDE=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzkxMTQ5MzIx", "url": "https://github.com/apache/beam/pull/11360#pullrequestreview-391149321", "createdAt": "2020-04-09T21:37:10Z", "commit": {"oid": "0bc349c20463acbe6250d2fa8ed22bb86e003572"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQyMTozNzoxMVrOGDqSxQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wOVQyMTozNzoxMVrOGDqSxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNjQ5MTg0NQ==", "bodyText": "options.getSchema()", "url": "https://github.com/apache/beam/pull/11360#discussion_r406491845", "createdAt": "2020-04-09T21:37:11Z", "author": {"login": "takidau"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,691 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.options.ValueProvider;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(int)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0bc349c20463acbe6250d2fa8ed22bb86e003572"}, "originalPosition": 93}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0bc349c20463acbe6250d2fa8ed22bb86e003572", "author": {"user": {"login": "DariuszAniszewski", "name": "Dariusz Aniszewski"}}, "url": "https://github.com/apache/beam/commit/0bc349c20463acbe6250d2fa8ed22bb86e003572", "committedDate": "2020-04-08T12:11:23Z", "message": "added SnowflakeIO with Read operation"}, "afterCommit": {"oid": "0fa99f92f220f5f88f4debbf0f69170e7fad01bf", "author": {"user": {"login": "DariuszAniszewski", "name": "Dariusz Aniszewski"}}, "url": "https://github.com/apache/beam/commit/0fa99f92f220f5f88f4debbf0f69170e7fad01bf", "committedDate": "2020-04-10T14:31:26Z", "message": "added SnowflakeIO with Read operation"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0fa99f92f220f5f88f4debbf0f69170e7fad01bf", "author": {"user": {"login": "DariuszAniszewski", "name": "Dariusz Aniszewski"}}, "url": "https://github.com/apache/beam/commit/0fa99f92f220f5f88f4debbf0f69170e7fad01bf", "committedDate": "2020-04-10T14:31:26Z", "message": "added SnowflakeIO with Read operation"}, "afterCommit": {"oid": "6365c6e7030c861b0db8ad9a82ccd719d34365bb", "author": {"user": {"login": "DariuszAniszewski", "name": "Dariusz Aniszewski"}}, "url": "https://github.com/apache/beam/commit/6365c6e7030c861b0db8ad9a82ccd719d34365bb", "committedDate": "2020-04-10T14:33:34Z", "message": "added SnowflakeIO with Read operation"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6365c6e7030c861b0db8ad9a82ccd719d34365bb", "author": {"user": {"login": "DariuszAniszewski", "name": "Dariusz Aniszewski"}}, "url": "https://github.com/apache/beam/commit/6365c6e7030c861b0db8ad9a82ccd719d34365bb", "committedDate": "2020-04-10T14:33:34Z", "message": "added SnowflakeIO with Read operation"}, "afterCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c", "author": {"user": {"login": "DariuszAniszewski", "name": "Dariusz Aniszewski"}}, "url": "https://github.com/apache/beam/commit/70518c24c760f97670d381b638b0b708978fe94c", "committedDate": "2020-04-10T14:35:01Z", "message": "added SnowflakeIO with Read operation"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAwMTAwMjk4", "url": "https://github.com/apache/beam/pull/11360#pullrequestreview-400100298", "createdAt": "2020-04-24T16:33:52Z", "commit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxNjozMzo1MlrOGLf03g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxNzoxODo0NlrOGLhhCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDcwODk1OA==", "bodyText": "Please, move it to [2.22.0] - Unreleased section (I guess you need to rebase to have it).", "url": "https://github.com/apache/beam/pull/11360#discussion_r414708958", "createdAt": "2020-04-24T16:33:52Z", "author": {"login": "aromanenko-dev"}, "path": "CHANGES.md", "diffHunk": "@@ -28,7 +28,8 @@\n \n ## I/Os\n \n-* Support for X source added (Java/Python) ([BEAM-X](https://issues.apache.org/jira/browse/BEAM-X)).\n+* Support for reading from Snowflake added (Java) ([BEAM-9722](https://issues.apache.org/jira/browse/BEAM-9722)).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDcxNTY1MQ==", "bodyText": "Does it establish a connection over a network?", "url": "https://github.com/apache/beam/pull/11360#discussion_r414715651", "createdAt": "2020-04-24T16:44:31Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,643 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDcxNjQ1Ng==", "bodyText": "Please, add a Javadoc for all \"with...\" exposed methods, since it's a part of public API of this IO", "url": "https://github.com/apache/beam/pull/11360#discussion_r414716456", "createdAt": "2020-04-24T16:45:50Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,643 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 210}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDcxODQyMA==", "bodyText": "nit \"are not allowed\"", "url": "https://github.com/apache/beam/pull/11360#discussion_r414718420", "createdAt": "2020-04-24T16:49:01Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,643 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() is not allowed together\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 259}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDcyMDQ5OQ==", "bodyText": "Will the temp directory be cleaned if pipeline was failed before?", "url": "https://github.com/apache/beam/pull/11360#discussion_r414720499", "createdAt": "2020-04-24T16:52:20Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,643 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() is not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(\n+                          getDataSourceProviderFn(),\n+                          getQuery(),\n+                          getTable(),\n+                          getIntegrationName(),\n+                          getStagingBucketName(),\n+                          gcpTmpDirName,\n+                          getSnowflakeService())))\n+              .apply(FileIO.matchAll())\n+              .apply(FileIO.readMatches())\n+              .apply(readFiles())\n+              .apply(ParDo.of(new MapCsvToStringArrayFn()))\n+              .apply(ParDo.of(new MapStringArrayToUserDataFn<>(getCsvMapper())));\n+\n+      output.setCoder(getCoder());\n+\n+      emptyCollection\n+          .apply(Wait.on(output))\n+          .apply(\n+              ParDo.of(\n+                  new CleanTmpFilesFromGcsFn(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 295}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDcyNzM2OQ==", "bodyText": "I believe, in general, COPY INTO can use the different external locations, like Amazon S3, Google Cloud Storage, or Microsoft Azure. So, please make it configurable for users even if this IO now  supports only GCS.", "url": "https://github.com/apache/beam/pull/11360#discussion_r414727369", "createdAt": "2020-04-24T17:03:26Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,643 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() is not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 268}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDczMjUyMA==", "bodyText": "Is it GCS specific implementation? I think we need to make it configurable which type of external location to use.", "url": "https://github.com/apache/beam/pull/11360#discussion_r414732520", "createdAt": "2020-04-24T17:12:07Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeServiceImpl.java", "diffHunk": "@@ -0,0 +1,83 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import java.sql.Connection;\n+import java.sql.PreparedStatement;\n+import java.sql.ResultSet;\n+import java.sql.SQLException;\n+import java.util.function.Consumer;\n+import javax.sql.DataSource;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+\n+/**\n+ * Implemenation of {@link org.apache.beam.sdk.io.snowflake.SnowflakeService} used in production.\n+ */\n+public class SnowflakeServiceImpl implements SnowflakeService {\n+\n+  @Override\n+  public String copyIntoStage(\n+      SerializableFunction<Void, DataSource> dataSourceProviderFn,\n+      String query,\n+      String table,\n+      String integrationName,\n+      String stagingBucketName,\n+      String tmpDirName)\n+      throws SQLException {\n+\n+    String from;\n+    if (query != null) {\n+      // Query must be surrounded with brackets\n+      from = String.format(\"(%s)\", query);\n+    } else {\n+      from = table;\n+    }\n+\n+    String externalLocation = String.format(\"gcs://%s/%s/\", stagingBucketName, tmpDirName);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDczNDEzNQ==", "bodyText": "Add ASF license header", "url": "https://github.com/apache/beam/pull/11360#discussion_r414734135", "createdAt": "2020-04-24T17:14:58Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/credentials/package-info.java", "diffHunk": "@@ -0,0 +1,2 @@\n+/** Credentials for SnowflakeIO. */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDczNDIxNw==", "bodyText": "Add ASF license header", "url": "https://github.com/apache/beam/pull/11360#discussion_r414734217", "createdAt": "2020-04-24T17:15:05Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/package-info.java", "diffHunk": "@@ -0,0 +1,2 @@\n+/** Snowflake IO transforms. */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 1}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDczNTg2MQ==", "bodyText": "Are there any mock/fake already implemented snowflake databases that can be used for testing?", "url": "https://github.com/apache/beam/pull/11360#discussion_r414735861", "createdAt": "2020-04-24T17:17:34Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/snowflake/src/test/java/org/apache/beam/sdk/io/snowflake/test/FakeSnowflakeDatabase.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake.test;\n+\n+import java.io.Serializable;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.stream.Collectors;\n+import net.snowflake.client.jdbc.SnowflakeSQLException;\n+\n+/** Fake implementation of SnowFlake warehouse used in test code. */\n+public class FakeSnowflakeDatabase implements Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 29}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDczNjY1MA==", "bodyText": "Add ASF license header", "url": "https://github.com/apache/beam/pull/11360#discussion_r414736650", "createdAt": "2020-04-24T17:18:46Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/snowflake/src/test/java/org/apache/beam/sdk/io/snowflake/test/package-info.java", "diffHunk": "@@ -0,0 +1,2 @@\n+/** Snowflake IO tests. */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 1}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDAwMjIxNzkx", "url": "https://github.com/apache/beam/pull/11360#pullrequestreview-400221791", "createdAt": "2020-04-24T19:39:44Z", "commit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxOTozOTo0NFrOGLme0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0yNFQxOTo0MDowM1rOGLmfmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDgxODAwMA==", "bodyText": "Why not use Apache Beam's notion of a FileSystem?", "url": "https://github.com/apache/beam/pull/11360#discussion_r414818000", "createdAt": "2020-04-24T19:39:44Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeCloudProvider.java", "diffHunk": "@@ -0,0 +1,23 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+/** Interface which defines common methods for cloud providers. */\n+public interface SnowflakeCloudProvider {\n+  void removeFiles(String bucketName, String pathOnBucket);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDgxODIwMg==", "bodyText": "Why not use the Apache Beam GCS filesystem?", "url": "https://github.com/apache/beam/pull/11360#discussion_r414818202", "createdAt": "2020-04-24T19:40:03Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/GCSProvider.java", "diffHunk": "@@ -0,0 +1,36 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import com.google.api.gax.paging.Page;\n+import com.google.cloud.storage.Blob;\n+import com.google.cloud.storage.Storage;\n+import com.google.cloud.storage.StorageOptions;\n+import java.io.Serializable;\n+\n+public class GCSProvider implements SnowflakeCloudProvider, Serializable {\n+\n+  @Override\n+  public void removeFiles(String bucketName, String pathOnBucket) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 29}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c", "author": {"user": {"login": "DariuszAniszewski", "name": "Dariusz Aniszewski"}}, "url": "https://github.com/apache/beam/commit/70518c24c760f97670d381b638b0b708978fe94c", "committedDate": "2020-04-10T14:35:01Z", "message": "added SnowflakeIO with Read operation"}, "afterCommit": {"oid": "c981338979fb6532fee81bf76200457f817d40f0", "author": {"user": {"login": "DariuszAniszewski", "name": "Dariusz Aniszewski"}}, "url": "https://github.com/apache/beam/commit/c981338979fb6532fee81bf76200457f817d40f0", "committedDate": "2020-04-26T15:52:05Z", "message": "[BEAM-9722] added SnowflakeIO with Read operation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "189d45d7ba69a1d90b20a2c3ded8650d9d38be07", "author": {"user": {"login": "DariuszAniszewski", "name": "Dariusz Aniszewski"}}, "url": "https://github.com/apache/beam/commit/189d45d7ba69a1d90b20a2c3ded8650d9d38be07", "committedDate": "2020-04-26T15:55:19Z", "message": "[BEAM-9722] added SnowflakeIO with Read operation"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c981338979fb6532fee81bf76200457f817d40f0", "author": {"user": {"login": "DariuszAniszewski", "name": "Dariusz Aniszewski"}}, "url": "https://github.com/apache/beam/commit/c981338979fb6532fee81bf76200457f817d40f0", "committedDate": "2020-04-26T15:52:05Z", "message": "[BEAM-9722] added SnowflakeIO with Read operation"}, "afterCommit": {"oid": "189d45d7ba69a1d90b20a2c3ded8650d9d38be07", "author": {"user": {"login": "DariuszAniszewski", "name": "Dariusz Aniszewski"}}, "url": "https://github.com/apache/beam/commit/189d45d7ba69a1d90b20a2c3ded8650d9d38be07", "committedDate": "2020-04-26T15:55:19Z", "message": "[BEAM-9722] added SnowflakeIO with Read operation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5f50e463b61d46750e24af7144704035d5d0b0b3", "author": {"user": {"login": "kkucharc", "name": "Kasia Kucharczyk"}}, "url": "https://github.com/apache/beam/commit/5f50e463b61d46750e24af7144704035d5d0b0b3", "committedDate": "2020-04-30T15:16:16Z", "message": "[BEAM-9722] Added SnowflakeCloudProvider to enable use various clouds with Snowflake"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2ecedb14e5be233025fbcc075eadb896642b624e", "author": {"user": {"login": "DariuszAniszewski", "name": "Dariusz Aniszewski"}}, "url": "https://github.com/apache/beam/commit/2ecedb14e5be233025fbcc075eadb896642b624e", "committedDate": "2020-05-06T12:06:04Z", "message": "[BEAM-9722] added docstrings for public methods"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "00b0ba57aae1243d0b778fb43002254e896ffddf", "author": {"user": {"login": "kkucharc", "name": "Kasia Kucharczyk"}}, "url": "https://github.com/apache/beam/commit/00b0ba57aae1243d0b778fb43002254e896ffddf", "committedDate": "2020-05-06T12:32:32Z", "message": "[BEAM-9722] Added changed cleanup staged GCS files to Beam FileSystems"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887", "author": {"user": {"login": "DariuszAniszewski", "name": "Dariusz Aniszewski"}}, "url": "https://github.com/apache/beam/commit/3830fb7b116f1e056347babf228b292803a0d887", "committedDate": "2020-05-06T13:16:25Z", "message": "[BEAM-9722] Added javadocs for public methods in DataSourceConfiguration"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDA4NTgwNDgx", "url": "https://github.com/apache/beam/pull/11360#pullrequestreview-408580481", "createdAt": "2020-05-08T23:40:05Z", "commit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOFQyMzo0NToxM1rOGS2kaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0wOVQwMTo0MTo1N1rOGS3npA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQyMTYwOA==", "bodyText": "Seems like all these methods are related to file handling. Can't we just use functionality offered by FileSystems class here ? If that is inadequate for some reason we can consider adding to that instead of introducing a new abstraction here as well.", "url": "https://github.com/apache/beam/pull/11360#discussion_r422421608", "createdAt": "2020-05-08T23:45:13Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeCloudProvider.java", "diffHunk": "@@ -0,0 +1,29 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import java.io.IOException;\n+\n+/** Interface which defines common methods for cloud providers. */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQyMTg0Ng==", "bodyText": "Is it always Google Could Storage or are other Beam FileSystem implementations supported as well ? (or can be supported in the future).", "url": "https://github.com/apache/beam/pull/11360#discussion_r422421846", "createdAt": "2020-05-08T23:46:34Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,768 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 62}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQyMjUzNg==", "bodyText": "I don't think implementation details are useful here (and probably detrimental here since we might change that at any point).", "url": "https://github.com/apache/beam/pull/11360#discussion_r422422536", "createdAt": "2020-05-08T23:50:31Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,768 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 135}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQyMzE3OA==", "bodyText": "GCS bucket name or is it something more generic ?", "url": "https://github.com/apache/beam/pull/11360#discussion_r422423178", "createdAt": "2020-05-08T23:53:40Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,768 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    /**\n+     * Setting information about Snowflake server.\n+     *\n+     * @param config - An instance of {@link DataSourceConfiguration}.\n+     */\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    /**\n+     * Setting function that will provide {@link DataSourceConfiguration} in runtime.\n+     *\n+     * @param dataSourceProviderFn a {@link SerializableFunction}.\n+     */\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    /**\n+     * A query to be executed in Snowflake.\n+     *\n+     * @param query - String with query.\n+     */\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * A table name to be read in Snowflake.\n+     *\n+     * @param table - String with the name of the table.\n+     */\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    /**\n+     * Name of the bucket to use as tmp location of CSVs during COPY statement.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 256}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQyMzI5NA==", "bodyText": "Probably clarify what you mean by \"Storage Integration\" here.", "url": "https://github.com/apache/beam/pull/11360#discussion_r422423294", "createdAt": "2020-05-08T23:54:12Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,768 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    /**\n+     * Setting information about Snowflake server.\n+     *\n+     * @param config - An instance of {@link DataSourceConfiguration}.\n+     */\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    /**\n+     * Setting function that will provide {@link DataSourceConfiguration} in runtime.\n+     *\n+     * @param dataSourceProviderFn a {@link SerializableFunction}.\n+     */\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    /**\n+     * A query to be executed in Snowflake.\n+     *\n+     * @param query - String with query.\n+     */\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * A table name to be read in Snowflake.\n+     *\n+     * @param table - String with the name of the table.\n+     */\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    /**\n+     * Name of the bucket to use as tmp location of CSVs during COPY statement.\n+     *\n+     * @param stagingBucketName - String with the name of the bucket.\n+     */\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    /**\n+     * Name of the Storage Integration in Snowflake to be used.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 265}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQzNjAzNg==", "bodyText": "Coder to be used by the output PCollection generated by the source.\nBTW that's the default if a coder is not provided ? This does not have to be always configurable.", "url": "https://github.com/apache/beam/pull/11360#discussion_r422436036", "createdAt": "2020-05-09T01:13:49Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,768 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    /**\n+     * Setting information about Snowflake server.\n+     *\n+     * @param config - An instance of {@link DataSourceConfiguration}.\n+     */\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    /**\n+     * Setting function that will provide {@link DataSourceConfiguration} in runtime.\n+     *\n+     * @param dataSourceProviderFn a {@link SerializableFunction}.\n+     */\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    /**\n+     * A query to be executed in Snowflake.\n+     *\n+     * @param query - String with query.\n+     */\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * A table name to be read in Snowflake.\n+     *\n+     * @param table - String with the name of the table.\n+     */\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    /**\n+     * Name of the bucket to use as tmp location of CSVs during COPY statement.\n+     *\n+     * @param stagingBucketName - String with the name of the bucket.\n+     */\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    /**\n+     * Name of the Storage Integration in Snowflake to be used.\n+     *\n+     * @param integrationName - String with the name of the Storage Integration.\n+     */\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    /**\n+     * User-defined function mapping CSV lines into user data.\n+     *\n+     * @param csvMapper - an instance of {@link CsvMapper}.\n+     */\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    /**\n+     * A coder to use.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 283}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQzNjQxMw==", "bodyText": "This is a known issue with BQ source as well. Failed pipelines can leave temporary files behind. I'm afraid there is no good solution today. I think we need to introduce some sort of a generalized cleanup step to address this.", "url": "https://github.com/apache/beam/pull/11360#discussion_r422436413", "createdAt": "2020-05-09T01:17:31Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,643 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() is not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(\n+                          getDataSourceProviderFn(),\n+                          getQuery(),\n+                          getTable(),\n+                          getIntegrationName(),\n+                          getStagingBucketName(),\n+                          gcpTmpDirName,\n+                          getSnowflakeService())))\n+              .apply(FileIO.matchAll())\n+              .apply(FileIO.readMatches())\n+              .apply(readFiles())\n+              .apply(ParDo.of(new MapCsvToStringArrayFn()))\n+              .apply(ParDo.of(new MapStringArrayToUserDataFn<>(getCsvMapper())));\n+\n+      output.setCoder(getCoder());\n+\n+      emptyCollection\n+          .apply(Wait.on(output))\n+          .apply(\n+              ParDo.of(\n+                  new CleanTmpFilesFromGcsFn(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDcyMDQ5OQ=="}, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 295}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQzNjYwNw==", "bodyText": "Is there a reason to introduce two transforms here instead of having a single transform that converts Strings to type T ?", "url": "https://github.com/apache/beam/pull/11360#discussion_r422436607", "createdAt": "2020-05-09T01:19:42Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,768 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    /**\n+     * Setting information about Snowflake server.\n+     *\n+     * @param config - An instance of {@link DataSourceConfiguration}.\n+     */\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    /**\n+     * Setting function that will provide {@link DataSourceConfiguration} in runtime.\n+     *\n+     * @param dataSourceProviderFn a {@link SerializableFunction}.\n+     */\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    /**\n+     * A query to be executed in Snowflake.\n+     *\n+     * @param query - String with query.\n+     */\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * A table name to be read in Snowflake.\n+     *\n+     * @param table - String with the name of the table.\n+     */\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    /**\n+     * Name of the bucket to use as tmp location of CSVs during COPY statement.\n+     *\n+     * @param stagingBucketName - String with the name of the bucket.\n+     */\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    /**\n+     * Name of the Storage Integration in Snowflake to be used.\n+     *\n+     * @param integrationName - String with the name of the Storage Integration.\n+     */\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    /**\n+     * User-defined function mapping CSV lines into user data.\n+     *\n+     * @param csvMapper - an instance of {@link CsvMapper}.\n+     */\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    /**\n+     * A coder to use.\n+     *\n+     * @param coder - an instance of {@link Coder}.\n+     */\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() are not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(\n+                          getDataSourceProviderFn(),\n+                          getQuery(),\n+                          getTable(),\n+                          getIntegrationName(),\n+                          getStagingBucketName(),\n+                          gcpTmpDirName,\n+                          getSnowflakeService(),\n+                          getSnowflakeCloudProvider())))\n+              .apply(FileIO.matchAll())\n+              .apply(FileIO.readMatches())\n+              .apply(readFiles())\n+              .apply(ParDo.of(new MapCsvToStringArrayFn()))\n+              .apply(ParDo.of(new MapStringArrayToUserDataFn<>(getCsvMapper())));\n+\n+      output.setCoder(getCoder());\n+\n+      emptyCollection\n+          .apply(Wait.on(output))\n+          .apply(\n+              ParDo.of(\n+                  new CleanTmpFilesFromGcsFn(\n+                      getStagingBucketName(), gcpTmpDirName, getSnowflakeCloudProvider())));\n+\n+      return output;\n+    }\n+\n+    private String makeTmpDirName() {\n+      return String.format(\n+          \"sf_copy_csv_%s_%s\",\n+          new SimpleDateFormat(\"yyyyMMdd_HHmmss\").format(new Date()),\n+          UUID.randomUUID().toString().subSequence(0, 8) // first 8 chars of UUID should be enough\n+          );\n+    }\n+\n+    private static class CopyIntoStageFn extends DoFn<Object, String> {\n+      private final SerializableFunction<Void, DataSource> dataSourceProviderFn;\n+      private final String query;\n+      private final String table;\n+      private final String integrationName;\n+      private final String stagingBucketName;\n+      private final String tmpDirName;\n+      private final SnowflakeService snowflakeService;\n+      private final SnowflakeCloudProvider cloudProvider;\n+\n+      private CopyIntoStageFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn,\n+          String query,\n+          String table,\n+          String integrationName,\n+          String stagingBucketName,\n+          String tmpDirName,\n+          SnowflakeService snowflakeService,\n+          SnowflakeCloudProvider cloudProvider) {\n+        this.dataSourceProviderFn = dataSourceProviderFn;\n+        this.query = query;\n+        this.table = table;\n+        this.integrationName = integrationName;\n+        this.stagingBucketName = stagingBucketName;\n+        this.tmpDirName = tmpDirName;\n+        this.snowflakeService = snowflakeService;\n+        this.cloudProvider = cloudProvider;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(ProcessContext context) throws Exception {\n+        String stagingBucketDir = this.cloudProvider.formatCloudPath(stagingBucketName, tmpDirName);\n+        String output =\n+            snowflakeService.copyIntoStage(\n+                dataSourceProviderFn,\n+                query,\n+                table,\n+                integrationName,\n+                stagingBucketDir,\n+                tmpDirName,\n+                this.cloudProvider);\n+\n+        context.output(output);\n+      }\n+    }\n+\n+    public static class MapCsvToStringArrayFn extends DoFn<String, String[]> {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 396}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQzNzU3OQ==", "bodyText": "Can we introduce just one create(SnowflakeCredentials credentials) method and fork based on the type of object passed in ?", "url": "https://github.com/apache/beam/pull/11360#discussion_r422437579", "createdAt": "2020-05-09T01:29:30Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,768 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    /**\n+     * Setting information about Snowflake server.\n+     *\n+     * @param config - An instance of {@link DataSourceConfiguration}.\n+     */\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    /**\n+     * Setting function that will provide {@link DataSourceConfiguration} in runtime.\n+     *\n+     * @param dataSourceProviderFn a {@link SerializableFunction}.\n+     */\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    /**\n+     * A query to be executed in Snowflake.\n+     *\n+     * @param query - String with query.\n+     */\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * A table name to be read in Snowflake.\n+     *\n+     * @param table - String with the name of the table.\n+     */\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    /**\n+     * Name of the bucket to use as tmp location of CSVs during COPY statement.\n+     *\n+     * @param stagingBucketName - String with the name of the bucket.\n+     */\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    /**\n+     * Name of the Storage Integration in Snowflake to be used.\n+     *\n+     * @param integrationName - String with the name of the Storage Integration.\n+     */\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    /**\n+     * User-defined function mapping CSV lines into user data.\n+     *\n+     * @param csvMapper - an instance of {@link CsvMapper}.\n+     */\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    /**\n+     * A coder to use.\n+     *\n+     * @param coder - an instance of {@link Coder}.\n+     */\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() are not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(\n+                          getDataSourceProviderFn(),\n+                          getQuery(),\n+                          getTable(),\n+                          getIntegrationName(),\n+                          getStagingBucketName(),\n+                          gcpTmpDirName,\n+                          getSnowflakeService(),\n+                          getSnowflakeCloudProvider())))\n+              .apply(FileIO.matchAll())\n+              .apply(FileIO.readMatches())\n+              .apply(readFiles())\n+              .apply(ParDo.of(new MapCsvToStringArrayFn()))\n+              .apply(ParDo.of(new MapStringArrayToUserDataFn<>(getCsvMapper())));\n+\n+      output.setCoder(getCoder());\n+\n+      emptyCollection\n+          .apply(Wait.on(output))\n+          .apply(\n+              ParDo.of(\n+                  new CleanTmpFilesFromGcsFn(\n+                      getStagingBucketName(), gcpTmpDirName, getSnowflakeCloudProvider())));\n+\n+      return output;\n+    }\n+\n+    private String makeTmpDirName() {\n+      return String.format(\n+          \"sf_copy_csv_%s_%s\",\n+          new SimpleDateFormat(\"yyyyMMdd_HHmmss\").format(new Date()),\n+          UUID.randomUUID().toString().subSequence(0, 8) // first 8 chars of UUID should be enough\n+          );\n+    }\n+\n+    private static class CopyIntoStageFn extends DoFn<Object, String> {\n+      private final SerializableFunction<Void, DataSource> dataSourceProviderFn;\n+      private final String query;\n+      private final String table;\n+      private final String integrationName;\n+      private final String stagingBucketName;\n+      private final String tmpDirName;\n+      private final SnowflakeService snowflakeService;\n+      private final SnowflakeCloudProvider cloudProvider;\n+\n+      private CopyIntoStageFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn,\n+          String query,\n+          String table,\n+          String integrationName,\n+          String stagingBucketName,\n+          String tmpDirName,\n+          SnowflakeService snowflakeService,\n+          SnowflakeCloudProvider cloudProvider) {\n+        this.dataSourceProviderFn = dataSourceProviderFn;\n+        this.query = query;\n+        this.table = table;\n+        this.integrationName = integrationName;\n+        this.stagingBucketName = stagingBucketName;\n+        this.tmpDirName = tmpDirName;\n+        this.snowflakeService = snowflakeService;\n+        this.cloudProvider = cloudProvider;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(ProcessContext context) throws Exception {\n+        String stagingBucketDir = this.cloudProvider.formatCloudPath(stagingBucketName, tmpDirName);\n+        String output =\n+            snowflakeService.copyIntoStage(\n+                dataSourceProviderFn,\n+                query,\n+                table,\n+                integrationName,\n+                stagingBucketDir,\n+                tmpDirName,\n+                this.cloudProvider);\n+\n+        context.output(output);\n+      }\n+    }\n+\n+    public static class MapCsvToStringArrayFn extends DoFn<String, String[]> {\n+      @ProcessElement\n+      public void processElement(ProcessContext c) throws IOException {\n+        String csvLine = c.element();\n+        CSVParser parser = new CSVParserBuilder().withQuoteChar(CSV_QUOTE_CHAR.charAt(0)).build();\n+        String[] parts = parser.parseLine(csvLine);\n+        c.output(parts);\n+      }\n+    }\n+\n+    private static class MapStringArrayToUserDataFn<T> extends DoFn<String[], T> {\n+      private final CsvMapper<T> csvMapper;\n+\n+      public MapStringArrayToUserDataFn(CsvMapper<T> csvMapper) {\n+        this.csvMapper = csvMapper;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(ProcessContext context) throws Exception {\n+        context.output(csvMapper.mapRow(context.element()));\n+      }\n+    }\n+\n+    public static class CleanTmpFilesFromGcsFn extends DoFn<Object, Object> {\n+      private final String bucketName;\n+      private final String bucketPath;\n+      private final SnowflakeCloudProvider snowflakeCloudProvider;\n+\n+      public CleanTmpFilesFromGcsFn(\n+          String bucketName, String bucketPath, SnowflakeCloudProvider snowflakeCloudProvider) {\n+        this.bucketName = bucketName;\n+        this.bucketPath = bucketPath;\n+        this.snowflakeCloudProvider = snowflakeCloudProvider;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(ProcessContext c) throws IOException {\n+        snowflakeCloudProvider.removeFiles(bucketName, bucketPath);\n+      }\n+    }\n+\n+    @Override\n+    public void populateDisplayData(DisplayData.Builder builder) {\n+      super.populateDisplayData(builder);\n+      if (getQuery() != null) {\n+        builder.add(DisplayData.item(\"query\", getQuery()));\n+      }\n+      if (getTable() != null) {\n+        builder.add(DisplayData.item(\"table\", getTable()));\n+      }\n+      builder.add(DisplayData.item(\"integrationName\", getIntegrationName()));\n+      builder.add(DisplayData.item(\"stagingBucketName\", getStagingBucketName()));\n+      builder.add(DisplayData.item(\"csvMapper\", getCsvMapper().getClass().getName()));\n+      builder.add(DisplayData.item(\"coder\", getCoder().getClass().getName()));\n+      if (getDataSourceProviderFn() instanceof HasDisplayData) {\n+        ((HasDisplayData) getDataSourceProviderFn()).populateDisplayData(builder);\n+      }\n+    }\n+  }\n+\n+  /**\n+   * A POJO describing a {@link DataSource}, providing all properties allowing to create a {@link\n+   * DataSource}.\n+   */\n+  @AutoValue\n+  public abstract static class DataSourceConfiguration implements Serializable {\n+    @Nullable\n+    public abstract String getUrl();\n+\n+    @Nullable\n+    public abstract String getUsername();\n+\n+    @Nullable\n+    public abstract String getPassword();\n+\n+    @Nullable\n+    public abstract PrivateKey getPrivateKey();\n+\n+    @Nullable\n+    public abstract String getOauthToken();\n+\n+    @Nullable\n+    public abstract String getDatabase();\n+\n+    @Nullable\n+    public abstract String getWarehouse();\n+\n+    @Nullable\n+    public abstract String getSchema();\n+\n+    @Nullable\n+    public abstract String getServerName();\n+\n+    @Nullable\n+    public abstract Integer getPortNumber();\n+\n+    @Nullable\n+    public abstract String getRole();\n+\n+    @Nullable\n+    public abstract Integer getLoginTimeout();\n+\n+    @Nullable\n+    public abstract Boolean getSsl();\n+\n+    @Nullable\n+    public abstract DataSource getDataSource();\n+\n+    abstract Builder builder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder {\n+      abstract Builder setUrl(String url);\n+\n+      abstract Builder setUsername(String username);\n+\n+      abstract Builder setPassword(String password);\n+\n+      abstract Builder setPrivateKey(PrivateKey privateKey);\n+\n+      abstract Builder setOauthToken(String oauthToken);\n+\n+      abstract Builder setDatabase(String database);\n+\n+      abstract Builder setWarehouse(String warehouse);\n+\n+      abstract Builder setSchema(String schema);\n+\n+      abstract Builder setServerName(String serverName);\n+\n+      abstract Builder setPortNumber(Integer portNumber);\n+\n+      abstract Builder setRole(String role);\n+\n+      abstract Builder setLoginTimeout(Integer loginTimeout);\n+\n+      abstract Builder setSsl(Boolean ssl);\n+\n+      abstract Builder setDataSource(DataSource dataSource);\n+\n+      abstract DataSourceConfiguration build();\n+    }\n+\n+    /**\n+     * Creates {@link DataSourceConfiguration} from existing instance of {@link DataSource}.\n+     *\n+     * @param dataSource - an instance of {@link DataSource}.\n+     */\n+    public static DataSourceConfiguration create(DataSource dataSource) {\n+      checkArgument(dataSource instanceof Serializable, \"dataSource must be Serializable\");\n+      return new AutoValue_SnowflakeIO_DataSourceConfiguration.Builder()\n+          .setDataSource(dataSource)\n+          .build();\n+    }\n+\n+    /**\n+     * Creates {@link DataSourceConfiguration} from instance of {@link SnowflakeCredentials}.\n+     *\n+     * @param credentials - an instance of {@link SnowflakeCredentials}.\n+     */\n+    public static DataSourceConfiguration create(SnowflakeCredentials credentials) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 556}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQzODgyMA==", "bodyText": "Can we introduce tests to actually read data (from SnowFlakeFakeServiceImpl) and verify results. Please see DirectRunner-based BQ pipeline read tests.\nhttps://github.com/apache/beam/blob/master/sdks/java/io/google-cloud-platform/src/test/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIOReadTest.java#L285\nAlso there's no need to manually call 'expand' in tests. We can just execute regular Beam pipelines that will perform the expansion.", "url": "https://github.com/apache/beam/pull/11360#discussion_r422438820", "createdAt": "2020-05-09T01:41:57Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/test/java/org/apache/beam/sdk/io/snowflake/test/unit/read/SnowflakeIOReadTest.java", "diffHunk": "@@ -0,0 +1,251 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake.test.unit.read;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.beam.sdk.Pipeline.PipelineExecutionException;\n+import org.apache.beam.sdk.coders.AvroCoder;\n+import org.apache.beam.sdk.io.AvroGeneratedUser;\n+import org.apache.beam.sdk.io.snowflake.SnowflakeCloudProvider;\n+import org.apache.beam.sdk.io.snowflake.SnowflakeIO;\n+import org.apache.beam.sdk.io.snowflake.SnowflakeService;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeBasicDataSource;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeCloudProvider;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeDatabase;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeServiceImpl;\n+import org.apache.beam.sdk.io.snowflake.test.unit.BatchTestPipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+@RunWith(JUnit4.class)\n+public class SnowflakeIOReadTest {\n+  public static final String FAKE_TABLE = \"FAKE_TABLE\";\n+\n+  @Rule public final transient TestPipeline pipeline = TestPipeline.create();\n+  @Rule public ExpectedException exceptionRule = ExpectedException.none();\n+\n+  private static SnowflakeIO.DataSourceConfiguration dataSourceConfiguration;\n+  private static BatchTestPipelineOptions options;\n+\n+  private static SnowflakeService snowflakeService;\n+  private static SnowflakeCloudProvider snowflakeCloudProvider;\n+\n+  private static String stagingBucketName;\n+  private static String integrationName;\n+\n+  private static List<GenericRecord> avroTestData;\n+\n+  @BeforeClass\n+  public static void setup() {\n+\n+    List<String> testData = Arrays.asList(\"Paul,51,red\", \"Jackson,41,green\");\n+\n+    avroTestData =\n+        ImmutableList.of(\n+            new AvroGeneratedUser(\"Paul\", 51, \"red\"),\n+            new AvroGeneratedUser(\"Jackson\", 41, \"green\"));\n+\n+    FakeSnowflakeDatabase.createTableWithElements(FAKE_TABLE, testData);\n+\n+    PipelineOptionsFactory.register(BatchTestPipelineOptions.class);\n+    options = TestPipeline.testingPipelineOptions().as(BatchTestPipelineOptions.class);\n+    options.setServerName(\"NULL.snowflakecomputing.com\");\n+    options.setStorageIntegration(\"STORAGE_INTEGRATION\");\n+    options.setStagingBucketName(\"BUCKET\");\n+\n+    stagingBucketName = options.getStagingBucketName();\n+    integrationName = options.getStorageIntegration();\n+\n+    dataSourceConfiguration =\n+        SnowflakeIO.DataSourceConfiguration.create(new FakeSnowflakeBasicDataSource())\n+            .withServerName(options.getServerName());\n+\n+    snowflakeService = new FakeSnowflakeServiceImpl();\n+    snowflakeCloudProvider = new FakeSnowflakeCloudProvider();\n+  }\n+\n+  @Test\n+  public void testConfigIsMissingStagingBucketName() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\"withStagingBucketName() is required\");\n+\n+    SnowflakeIO.Read<GenericRecord> read =\n+        SnowflakeIO.<GenericRecord>read(snowflakeService, snowflakeCloudProvider)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .fromTable(FAKE_TABLE)\n+            .withIntegrationName(integrationName)\n+            .withCsvMapper(getCsvMapper())\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema()));\n+\n+    read.expand(null);\n+  }\n+\n+  @Test\n+  public void testConfigIsMissingIntegrationName() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\"withIntegrationName() is required\");\n+\n+    SnowflakeIO.Read<GenericRecord> read =\n+        SnowflakeIO.<GenericRecord>read(snowflakeService, snowflakeCloudProvider)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .fromTable(FAKE_TABLE)\n+            .withStagingBucketName(stagingBucketName)\n+            .withCsvMapper(getCsvMapper())\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema()));\n+\n+    read.expand(null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 123}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6f3ff11d758a45619ff25b6e5f270c2c970fce05", "author": {"user": {"login": "DariuszAniszewski", "name": "Dariusz Aniszewski"}}, "url": "https://github.com/apache/beam/commit/6f3ff11d758a45619ff25b6e5f270c2c970fce05", "committedDate": "2020-05-12T10:23:37Z", "message": "add testing p8 file to RAT exclude\nrefactor SnowflakeCredentials\nadd information about possibly left files on cloud storage\nsmall docs changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7caa5dfd8b26b8c1c0a28a6ffcc503a5d462d156", "author": {"user": {"login": "DariuszAniszewski", "name": "Dariusz Aniszewski"}}, "url": "https://github.com/apache/beam/commit/7caa5dfd8b26b8c1c0a28a6ffcc503a5d462d156", "committedDate": "2020-05-12T10:50:20Z", "message": "documentation changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6fc1c483fbed6f4f6d91aafae7945aace9d2c1af", "author": {"user": {"login": "kkucharc", "name": "Kasia Kucharczyk"}}, "url": "https://github.com/apache/beam/commit/6fc1c483fbed6f4f6d91aafae7945aace9d2c1af", "committedDate": "2020-05-13T13:17:44Z", "message": "[BEAM-9722] Added TestRule and changed Unit tests to use pipeline.run"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c5abe5ab21d040e45ab75f95dd93cfa1490b4bf9", "author": {"user": {"login": "kkucharc", "name": "Kasia Kucharczyk"}}, "url": "https://github.com/apache/beam/commit/c5abe5ab21d040e45ab75f95dd93cfa1490b4bf9", "committedDate": "2020-05-13T14:19:48Z", "message": "[BEAM-9722] Renamed Snowflake Read unit test and applied spotless"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "4641631c602969794b13a70309d31b01f90cd388", "author": {"user": {"login": "purbanow", "name": null}}, "url": "https://github.com/apache/beam/commit/4641631c602969794b13a70309d31b01f90cd388", "committedDate": "2020-05-14T05:51:30Z", "message": "[BEAM-9722] remove SnowflakeCloudProvider interface"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDEzMTA4MjAz", "url": "https://github.com/apache/beam/pull/11360#pullrequestreview-413108203", "createdAt": "2020-05-16T20:59:04Z", "commit": {"oid": "4641631c602969794b13a70309d31b01f90cd388"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQyMDo1OTowNFrOGWcnqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0xNlQyMToyMTowMFrOGWctpg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE5MDc2MQ==", "bodyText": "s/deleted/cleaned up", "url": "https://github.com/apache/beam/pull/11360#discussion_r426190761", "createdAt": "2020-05-16T20:59:04Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,735 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.stream.Collectors;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to a cloud bucket. By\n+ * now only Google Cloud Storage is supported.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ *\n+ * <p><b>Important</b> When reading data from Snowflake, temporary CSV files are created on the\n+ * specified stagingBucketName in directory named `sf_copy_csv_[RANDOM CHARS]_[TIMESTAMP]`. This\n+ * directory and all the files are deleted automatically by default, but in case of failed pipeline", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4641631c602969794b13a70309d31b01f90cd388"}, "originalPosition": 121}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE5MDkyOQ==", "bodyText": "they may remain and will have to ...", "url": "https://github.com/apache/beam/pull/11360#discussion_r426190929", "createdAt": "2020-05-16T21:01:12Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,735 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.stream.Collectors;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to a cloud bucket. By\n+ * now only Google Cloud Storage is supported.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ *\n+ * <p><b>Important</b> When reading data from Snowflake, temporary CSV files are created on the\n+ * specified stagingBucketName in directory named `sf_copy_csv_[RANDOM CHARS]_[TIMESTAMP]`. This\n+ * directory and all the files are deleted automatically by default, but in case of failed pipeline\n+ * they will remain and have to be removed manually.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4641631c602969794b13a70309d31b01f90cd388"}, "originalPosition": 122}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE5MTA1OA==", "bodyText": "Probably you should provide a withoutValidation() option so that users who submit a pipeline form a node that do not have access to the server can disable this.", "url": "https://github.com/apache/beam/pull/11360#discussion_r426191058", "createdAt": "2020-05-16T21:02:54Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,643 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQxNDcxNTY1MQ=="}, "originalCommit": {"oid": "70518c24c760f97670d381b638b0b708978fe94c"}, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE5MTM4Nw==", "bodyText": "What will happen in case of a failure and a retry of a bundle ?\nI think you should add a Reshuffle step after CopyIntoStageFn() step to checkpoint the set of files (supported by most runners).\nAlso, make sure you have considered what would happen if CopyIntoStageFn() itself fails and retries. For example, BQ source tries to look for an already started export job before starting a new one.", "url": "https://github.com/apache/beam/pull/11360#discussion_r426191387", "createdAt": "2020-05-16T21:07:44Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,735 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.stream.Collectors;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to a cloud bucket. By\n+ * now only Google Cloud Storage is supported.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ *\n+ * <p><b>Important</b> When reading data from Snowflake, temporary CSV files are created on the\n+ * specified stagingBucketName in directory named `sf_copy_csv_[RANDOM CHARS]_[TIMESTAMP]`. This\n+ * directory and all the files are deleted automatically by default, but in case of failed pipeline\n+ * they will remain and have to be removed manually.\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+  /**\n+   * Read data from Snowflake.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(SnowflakeService snowflakeService) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    /**\n+     * Setting information about Snowflake server.\n+     *\n+     * @param config - An instance of {@link DataSourceConfiguration}.\n+     */\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    /**\n+     * Setting function that will provide {@link DataSourceConfiguration} in runtime.\n+     *\n+     * @param dataSourceProviderFn a {@link SerializableFunction}.\n+     */\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    /**\n+     * A query to be executed in Snowflake.\n+     *\n+     * @param query - String with query.\n+     */\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * A table name to be read in Snowflake.\n+     *\n+     * @param table - String with the name of the table.\n+     */\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    /**\n+     * Name of the cloud bucket (GCS by now) to use as tmp location of CSVs during COPY statement.\n+     *\n+     * @param stagingBucketName - String with the name of the bucket.\n+     */\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    /**\n+     * Name of the Storage Integration in Snowflake to be used. See\n+     * https://docs.snowflake.com/en/sql-reference/sql/create-storage-integration.html for\n+     * reference.\n+     *\n+     * @param integrationName - String with the name of the Storage Integration.\n+     */\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    /**\n+     * User-defined function mapping CSV lines into user data.\n+     *\n+     * @param csvMapper - an instance of {@link CsvMapper}.\n+     */\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    /**\n+     * A Coder to be used by the output PCollection generated by the source.\n+     *\n+     * @param coder - an instance of {@link Coder}.\n+     */\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() are not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      String stagingBucketDir = String.format(\"%s/%s/\", getStagingBucketName(), gcpTmpDirName);\n+\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4641631c602969794b13a70309d31b01f90cd388"}, "originalPosition": 320}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE5MTg1NQ==", "bodyText": "Sounds good.", "url": "https://github.com/apache/beam/pull/11360#discussion_r426191855", "createdAt": "2020-05-16T21:14:57Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,768 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to Google Cloud\n+ * Storage.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via user-defined {@link SnowflakeService}.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(\n+      SnowflakeService snowflakeService, SnowflakeCloudProvider snowflakeCloudProvider) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .setSnowflakeCloudProvider(snowflakeCloudProvider)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake via COPY statement via default {@link SnowflakeServiceImpl}.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl(), new GCSProvider());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    @Nullable\n+    abstract SnowflakeCloudProvider getSnowflakeCloudProvider();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Builder<T> setSnowflakeCloudProvider(SnowflakeCloudProvider snowflakeCloudProvider);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    /**\n+     * Setting information about Snowflake server.\n+     *\n+     * @param config - An instance of {@link DataSourceConfiguration}.\n+     */\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    /**\n+     * Setting function that will provide {@link DataSourceConfiguration} in runtime.\n+     *\n+     * @param dataSourceProviderFn a {@link SerializableFunction}.\n+     */\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    /**\n+     * A query to be executed in Snowflake.\n+     *\n+     * @param query - String with query.\n+     */\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * A table name to be read in Snowflake.\n+     *\n+     * @param table - String with the name of the table.\n+     */\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    /**\n+     * Name of the bucket to use as tmp location of CSVs during COPY statement.\n+     *\n+     * @param stagingBucketName - String with the name of the bucket.\n+     */\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    /**\n+     * Name of the Storage Integration in Snowflake to be used.\n+     *\n+     * @param integrationName - String with the name of the Storage Integration.\n+     */\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    /**\n+     * User-defined function mapping CSV lines into user data.\n+     *\n+     * @param csvMapper - an instance of {@link CsvMapper}.\n+     */\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    /**\n+     * A coder to use.\n+     *\n+     * @param coder - an instance of {@link Coder}.\n+     */\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() are not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(\n+                          getDataSourceProviderFn(),\n+                          getQuery(),\n+                          getTable(),\n+                          getIntegrationName(),\n+                          getStagingBucketName(),\n+                          gcpTmpDirName,\n+                          getSnowflakeService(),\n+                          getSnowflakeCloudProvider())))\n+              .apply(FileIO.matchAll())\n+              .apply(FileIO.readMatches())\n+              .apply(readFiles())\n+              .apply(ParDo.of(new MapCsvToStringArrayFn()))\n+              .apply(ParDo.of(new MapStringArrayToUserDataFn<>(getCsvMapper())));\n+\n+      output.setCoder(getCoder());\n+\n+      emptyCollection\n+          .apply(Wait.on(output))\n+          .apply(\n+              ParDo.of(\n+                  new CleanTmpFilesFromGcsFn(\n+                      getStagingBucketName(), gcpTmpDirName, getSnowflakeCloudProvider())));\n+\n+      return output;\n+    }\n+\n+    private String makeTmpDirName() {\n+      return String.format(\n+          \"sf_copy_csv_%s_%s\",\n+          new SimpleDateFormat(\"yyyyMMdd_HHmmss\").format(new Date()),\n+          UUID.randomUUID().toString().subSequence(0, 8) // first 8 chars of UUID should be enough\n+          );\n+    }\n+\n+    private static class CopyIntoStageFn extends DoFn<Object, String> {\n+      private final SerializableFunction<Void, DataSource> dataSourceProviderFn;\n+      private final String query;\n+      private final String table;\n+      private final String integrationName;\n+      private final String stagingBucketName;\n+      private final String tmpDirName;\n+      private final SnowflakeService snowflakeService;\n+      private final SnowflakeCloudProvider cloudProvider;\n+\n+      private CopyIntoStageFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn,\n+          String query,\n+          String table,\n+          String integrationName,\n+          String stagingBucketName,\n+          String tmpDirName,\n+          SnowflakeService snowflakeService,\n+          SnowflakeCloudProvider cloudProvider) {\n+        this.dataSourceProviderFn = dataSourceProviderFn;\n+        this.query = query;\n+        this.table = table;\n+        this.integrationName = integrationName;\n+        this.stagingBucketName = stagingBucketName;\n+        this.tmpDirName = tmpDirName;\n+        this.snowflakeService = snowflakeService;\n+        this.cloudProvider = cloudProvider;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(ProcessContext context) throws Exception {\n+        String stagingBucketDir = this.cloudProvider.formatCloudPath(stagingBucketName, tmpDirName);\n+        String output =\n+            snowflakeService.copyIntoStage(\n+                dataSourceProviderFn,\n+                query,\n+                table,\n+                integrationName,\n+                stagingBucketDir,\n+                tmpDirName,\n+                this.cloudProvider);\n+\n+        context.output(output);\n+      }\n+    }\n+\n+    public static class MapCsvToStringArrayFn extends DoFn<String, String[]> {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyMjQzNjYwNw=="}, "originalCommit": {"oid": "3830fb7b116f1e056347babf228b292803a0d887"}, "originalPosition": 396}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE5MTk4OQ==", "bodyText": "Probably specify MoveOptions.IGNORE_MISSING_FILES to not fail incase this bundle is retries after a failure (either the source or the destination should exist).\nhttps://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/io/fs/MoveOptions.java#L30", "url": "https://github.com/apache/beam/pull/11360#discussion_r426191989", "createdAt": "2020-05-16T21:16:42Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,735 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.stream.Collectors;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to a cloud bucket. By\n+ * now only Google Cloud Storage is supported.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ *\n+ * <p><b>Important</b> When reading data from Snowflake, temporary CSV files are created on the\n+ * specified stagingBucketName in directory named `sf_copy_csv_[RANDOM CHARS]_[TIMESTAMP]`. This\n+ * directory and all the files are deleted automatically by default, but in case of failed pipeline\n+ * they will remain and have to be removed manually.\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+  /**\n+   * Read data from Snowflake.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(SnowflakeService snowflakeService) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    /**\n+     * Setting information about Snowflake server.\n+     *\n+     * @param config - An instance of {@link DataSourceConfiguration}.\n+     */\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    /**\n+     * Setting function that will provide {@link DataSourceConfiguration} in runtime.\n+     *\n+     * @param dataSourceProviderFn a {@link SerializableFunction}.\n+     */\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    /**\n+     * A query to be executed in Snowflake.\n+     *\n+     * @param query - String with query.\n+     */\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * A table name to be read in Snowflake.\n+     *\n+     * @param table - String with the name of the table.\n+     */\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    /**\n+     * Name of the cloud bucket (GCS by now) to use as tmp location of CSVs during COPY statement.\n+     *\n+     * @param stagingBucketName - String with the name of the bucket.\n+     */\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    /**\n+     * Name of the Storage Integration in Snowflake to be used. See\n+     * https://docs.snowflake.com/en/sql-reference/sql/create-storage-integration.html for\n+     * reference.\n+     *\n+     * @param integrationName - String with the name of the Storage Integration.\n+     */\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    /**\n+     * User-defined function mapping CSV lines into user data.\n+     *\n+     * @param csvMapper - an instance of {@link CsvMapper}.\n+     */\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    /**\n+     * A Coder to be used by the output PCollection generated by the source.\n+     *\n+     * @param coder - an instance of {@link Coder}.\n+     */\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() are not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      String stagingBucketDir = String.format(\"%s/%s/\", getStagingBucketName(), gcpTmpDirName);\n+\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(\n+                          getDataSourceProviderFn(),\n+                          getQuery(),\n+                          getTable(),\n+                          getIntegrationName(),\n+                          stagingBucketDir,\n+                          getSnowflakeService())))\n+              .apply(FileIO.matchAll())\n+              .apply(FileIO.readMatches())\n+              .apply(readFiles())\n+              .apply(ParDo.of(new MapCsvToStringArrayFn()))\n+              .apply(ParDo.of(new MapStringArrayToUserDataFn<>(getCsvMapper())));\n+\n+      output.setCoder(getCoder());\n+\n+      emptyCollection\n+          .apply(Wait.on(output))\n+          .apply(ParDo.of(new CleanTmpFilesFromGcsFn(stagingBucketDir)));\n+\n+      return output;\n+    }\n+\n+    private String makeTmpDirName() {\n+      return String.format(\n+          \"sf_copy_csv_%s_%s\",\n+          new SimpleDateFormat(\"yyyyMMdd_HHmmss\").format(new Date()),\n+          UUID.randomUUID().toString().subSequence(0, 8) // first 8 chars of UUID should be enough\n+          );\n+    }\n+\n+    private static class CopyIntoStageFn extends DoFn<Object, String> {\n+      private final SerializableFunction<Void, DataSource> dataSourceProviderFn;\n+      private final String query;\n+      private final String table;\n+      private final String integrationName;\n+      private final String stagingBucketDir;\n+      private final SnowflakeService snowflakeService;\n+\n+      private CopyIntoStageFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn,\n+          String query,\n+          String table,\n+          String integrationName,\n+          String stagingBucketDir,\n+          SnowflakeService snowflakeService) {\n+        this.dataSourceProviderFn = dataSourceProviderFn;\n+        this.query = query;\n+        this.table = table;\n+        this.integrationName = integrationName;\n+        this.stagingBucketDir = stagingBucketDir;\n+        this.snowflakeService = snowflakeService;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(ProcessContext context) throws Exception {\n+        String output =\n+            snowflakeService.copyIntoStage(\n+                dataSourceProviderFn, query, table, integrationName, stagingBucketDir);\n+\n+        context.output(output);\n+      }\n+    }\n+\n+    public static class MapCsvToStringArrayFn extends DoFn<String, String[]> {\n+      @ProcessElement\n+      public void processElement(ProcessContext c) throws IOException {\n+        String csvLine = c.element();\n+        CSVParser parser = new CSVParserBuilder().withQuoteChar(CSV_QUOTE_CHAR.charAt(0)).build();\n+        String[] parts = parser.parseLine(csvLine);\n+        c.output(parts);\n+      }\n+    }\n+\n+    private static class MapStringArrayToUserDataFn<T> extends DoFn<String[], T> {\n+      private final CsvMapper<T> csvMapper;\n+\n+      public MapStringArrayToUserDataFn(CsvMapper<T> csvMapper) {\n+        this.csvMapper = csvMapper;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(ProcessContext context) throws Exception {\n+        context.output(csvMapper.mapRow(context.element()));\n+      }\n+    }\n+\n+    public static class CleanTmpFilesFromGcsFn extends DoFn<Object, Object> {\n+      private final String stagingBucketDir;\n+\n+      public CleanTmpFilesFromGcsFn(String stagingBucketDir) {\n+        this.stagingBucketDir = stagingBucketDir;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(ProcessContext c) throws IOException {\n+        String combinedPath = stagingBucketDir + \"*\";\n+        List<ResourceId> paths =\n+            FileSystems.match(combinedPath).metadata().stream()\n+                .map(metadata -> metadata.resourceId())\n+                .collect(Collectors.toList());\n+\n+        FileSystems.delete(paths);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4641631c602969794b13a70309d31b01f90cd388"}, "originalPosition": 421}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE5MjI5NA==", "bodyText": "Please make sure that all read paths are covered by tests.", "url": "https://github.com/apache/beam/pull/11360#discussion_r426192294", "createdAt": "2020-05-16T21:21:00Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/test/java/org/apache/beam/sdk/io/snowflake/test/unit/read/SnowflakeIOReadTest.java", "diffHunk": "@@ -0,0 +1,268 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake.test.unit.read;\n+\n+import java.util.Arrays;\n+import java.util.List;\n+import org.apache.avro.generic.GenericRecord;\n+import org.apache.avro.generic.GenericRecordBuilder;\n+import org.apache.beam.sdk.Pipeline.PipelineExecutionException;\n+import org.apache.beam.sdk.coders.AvroCoder;\n+import org.apache.beam.sdk.io.AvroGeneratedUser;\n+import org.apache.beam.sdk.io.snowflake.SnowflakeIO;\n+import org.apache.beam.sdk.io.snowflake.SnowflakeService;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeBasicDataSource;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeDatabase;\n+import org.apache.beam.sdk.io.snowflake.test.FakeSnowflakeServiceImpl;\n+import org.apache.beam.sdk.io.snowflake.test.unit.BatchTestPipelineOptions;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.junit.BeforeClass;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.rules.TemporaryFolder;\n+import org.junit.rules.TestRule;\n+import org.junit.runner.Description;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+import org.junit.runners.model.Statement;\n+\n+@RunWith(JUnit4.class)\n+public class SnowflakeIOReadTest {\n+  public static final String FAKE_TABLE = \"FAKE_TABLE\";\n+\n+  @Rule public transient TestPipeline pipeline = TestPipeline.create();\n+  @Rule public ExpectedException exceptionRule = ExpectedException.none();\n+\n+  private static SnowflakeIO.DataSourceConfiguration dataSourceConfiguration;\n+  private static BatchTestPipelineOptions options;\n+\n+  private static SnowflakeService snowflakeService;\n+\n+  private static String stagingBucketName;\n+  private static String integrationName;\n+\n+  private static List<GenericRecord> avroTestData;\n+\n+  private transient TemporaryFolder testFolder = new TemporaryFolder();\n+\n+  @BeforeClass\n+  public static void setup() {\n+\n+    List<String> testData = Arrays.asList(\"Paul,51,red\", \"Jackson,41,green\");\n+\n+    avroTestData =\n+        ImmutableList.of(\n+            new AvroGeneratedUser(\"Paul\", 51, \"red\"),\n+            new AvroGeneratedUser(\"Jackson\", 41, \"green\"));\n+\n+    FakeSnowflakeDatabase.createTableWithElements(FAKE_TABLE, testData);\n+    PipelineOptionsFactory.register(BatchTestPipelineOptions.class);\n+    options = TestPipeline.testingPipelineOptions().as(BatchTestPipelineOptions.class);\n+    options.setServerName(\"NULL.snowflakecomputing.com\");\n+    options.setStorageIntegration(\"STORAGE_INTEGRATION\");\n+    options.setStagingBucketName(\"BUCKET\");\n+\n+    stagingBucketName = options.getStagingBucketName();\n+    integrationName = options.getStorageIntegration();\n+\n+    dataSourceConfiguration =\n+        SnowflakeIO.DataSourceConfiguration.create(new FakeSnowflakeBasicDataSource())\n+            .withServerName(options.getServerName());\n+\n+    snowflakeService = new FakeSnowflakeServiceImpl();\n+  }\n+\n+  @Rule\n+  public final transient TestRule folderThenPipeline =\n+      new TestRule() {\n+        @Override\n+        public Statement apply(final Statement base, final Description description) {\n+          Statement withPipeline =\n+              new Statement() {\n+                @Override\n+                public void evaluate() {\n+                  pipeline = TestPipeline.fromOptions(options);\n+                }\n+              };\n+          return testFolder.apply(withPipeline, description);\n+        }\n+      };\n+\n+  @Test\n+  public void testConfigIsMissingStagingBucketName() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\"withStagingBucketName() is required\");\n+\n+    pipeline.apply(\n+        SnowflakeIO.<GenericRecord>read(snowflakeService)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .fromTable(FAKE_TABLE)\n+            .withIntegrationName(integrationName)\n+            .withCsvMapper(getCsvMapper())\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema())));\n+\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void testConfigIsMissingIntegrationName() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\"withIntegrationName() is required\");\n+\n+    pipeline.apply(\n+        SnowflakeIO.<GenericRecord>read(snowflakeService)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .fromTable(FAKE_TABLE)\n+            .withStagingBucketName(stagingBucketName)\n+            .withCsvMapper(getCsvMapper())\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema())));\n+\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void testConfigIsMissingCsvMapper() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\"withCsvMapper() is required\");\n+\n+    pipeline.apply(\n+        SnowflakeIO.<GenericRecord>read(snowflakeService)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .fromTable(FAKE_TABLE)\n+            .withStagingBucketName(stagingBucketName)\n+            .withIntegrationName(integrationName)\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema())));\n+\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void testConfigIsMissingCoder() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\"withCoder() is required\");\n+\n+    pipeline.apply(\n+        SnowflakeIO.<GenericRecord>read(snowflakeService)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .fromTable(FAKE_TABLE)\n+            .withStagingBucketName(stagingBucketName)\n+            .withIntegrationName(integrationName)\n+            .withCsvMapper(getCsvMapper()));\n+\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void testConfigIsMissingFromTableOrFromQuery() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\"fromTable() or fromQuery() is required\");\n+\n+    pipeline.apply(\n+        SnowflakeIO.<GenericRecord>read(snowflakeService)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .withStagingBucketName(stagingBucketName)\n+            .withIntegrationName(integrationName)\n+            .withCsvMapper(getCsvMapper())\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema())));\n+\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void testConfigIsMissingDataSourceConfiguration() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\n+        \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+    pipeline.apply(\n+        SnowflakeIO.<GenericRecord>read(snowflakeService)\n+            .fromTable(FAKE_TABLE)\n+            .withStagingBucketName(stagingBucketName)\n+            .withIntegrationName(integrationName)\n+            .withCsvMapper(getCsvMapper())\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema())));\n+\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void testConfigContainsFromQueryAndFromTable() {\n+    exceptionRule.expect(IllegalArgumentException.class);\n+    exceptionRule.expectMessage(\"fromTable() and fromQuery() are not allowed together\");\n+\n+    pipeline.apply(\n+        SnowflakeIO.<GenericRecord>read(snowflakeService)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .fromQuery(\"\")\n+            .fromTable(FAKE_TABLE)\n+            .withStagingBucketName(stagingBucketName)\n+            .withIntegrationName(integrationName)\n+            .withCsvMapper(getCsvMapper())\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema())));\n+\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void testTableDoesntExist() {\n+    exceptionRule.expect(PipelineExecutionException.class);\n+    exceptionRule.expectMessage(\"SQL compilation error: Table does not exist\");\n+\n+    pipeline.apply(\n+        SnowflakeIO.<GenericRecord>read(snowflakeService)\n+            .withDataSourceConfiguration(dataSourceConfiguration)\n+            .fromTable(\"NON_EXIST\")\n+            .withStagingBucketName(stagingBucketName)\n+            .withIntegrationName(integrationName)\n+            .withCsvMapper(getCsvMapper())\n+            .withCoder(AvroCoder.of(AvroGeneratedUser.getClassSchema())));\n+\n+    pipeline.run();\n+  }\n+\n+  @Test\n+  public void testReadWithConfigIsProper() {\n+    PCollection<GenericRecord> items =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4641631c602969794b13a70309d31b01f90cd388"}, "originalPosition": 245}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3369ded192ca082adcf7fb0600343b94595b8d29", "author": {"user": {"login": "DariuszAniszewski", "name": "Dariusz Aniszewski"}}, "url": "https://github.com/apache/beam/commit/3369ded192ca082adcf7fb0600343b94595b8d29", "committedDate": "2020-05-18T08:12:46Z", "message": "[BEAM-9722] doc changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "7b487520b243de29796d619df8d5d47befe6f52a", "author": {"user": {"login": "DariuszAniszewski", "name": "Dariusz Aniszewski"}}, "url": "https://github.com/apache/beam/commit/7b487520b243de29796d619df8d5d47befe6f52a", "committedDate": "2020-05-18T10:55:13Z", "message": "[BEAM-9722] add `withoutValidation` to disable verifying connection to Snowflake during pipeline construction"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ca9a7f7982cf69e60cc483b052af1c3d83e20524", "author": {"user": {"login": "DariuszAniszewski", "name": "Dariusz Aniszewski"}}, "url": "https://github.com/apache/beam/commit/ca9a7f7982cf69e60cc483b052af1c3d83e20524", "committedDate": "2020-05-19T10:22:56Z", "message": "[BEAM-9722] added MoveOption and removed leftover file"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e49a0fac40ab177d561ebe7976f69aa6ef9cd6fd", "author": {"user": {"login": "DariuszAniszewski", "name": "Dariusz Aniszewski"}}, "url": "https://github.com/apache/beam/commit/e49a0fac40ab177d561ebe7976f69aa6ef9cd6fd", "committedDate": "2020-05-19T20:23:34Z", "message": "[BEAM-9722] fixed tests. Add tests for `withQuery`"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE0OTY0NzYz", "url": "https://github.com/apache/beam/pull/11360#pullrequestreview-414964763", "createdAt": "2020-05-20T03:18:30Z", "commit": {"oid": "e49a0fac40ab177d561ebe7976f69aa6ef9cd6fd"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwMzoxODozMFrOGX54dQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNS0yMFQwMzoxODozMFrOGX54dQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNzcxODc3Mw==", "bodyText": "It's better but I don't think it's a good user experience to fail the pipeline whenever a bundle is retried. Bundle retries are a usual part of the runner execution and source/sink transforms should be able to handle that.\nWhat is usually done in this case is to write data to a new temporary location each time the copy step is run. Then you add a Reshuffle write after the copy operation to checkpoint the results. This way you can guarantee that the set of files that are output to the next step are from a single execution and the job does not have to fail simply because a bundle is retried.\nWhen you cleanup you have to clean all temporary locations including retries.\nI'm fine if you don't want to handle this in the first try but let's at least add a TODO and a JIRA to fix this later.", "url": "https://github.com/apache/beam/pull/11360#discussion_r427718773", "createdAt": "2020-05-20T03:18:30Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -0,0 +1,735 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake;\n+\n+import static org.apache.beam.sdk.io.TextIO.readFiles;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import com.opencsv.CSVParser;\n+import com.opencsv.CSVParserBuilder;\n+import java.io.IOException;\n+import java.io.Serializable;\n+import java.security.PrivateKey;\n+import java.sql.Connection;\n+import java.sql.SQLException;\n+import java.text.SimpleDateFormat;\n+import java.util.Date;\n+import java.util.List;\n+import java.util.UUID;\n+import java.util.concurrent.ConcurrentHashMap;\n+import java.util.stream.Collectors;\n+import javax.annotation.Nullable;\n+import javax.sql.DataSource;\n+import net.snowflake.client.jdbc.SnowflakeBasicDataSource;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.FileIO;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.io.fs.ResourceId;\n+import org.apache.beam.sdk.io.snowflake.credentials.KeyPairSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.OAuthTokenSnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.UsernamePasswordSnowflakeCredentials;\n+import org.apache.beam.sdk.transforms.Create;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.Wait;\n+import org.apache.beam.sdk.transforms.display.DisplayData;\n+import org.apache.beam.sdk.transforms.display.HasDisplayData;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * IO to read and write data on Snowflake.\n+ *\n+ * <p>SnowflakeIO uses <a href=\"https://docs.snowflake.net/manuals/user-guide/jdbc.html\">Snowflake\n+ * JDBC</a> driver under the hood, but data isn't read/written using JDBC directly. Instead,\n+ * SnowflakeIO uses dedicated <b>COPY</b> operations to read/write data from/to a cloud bucket. By\n+ * now only Google Cloud Storage is supported.\n+ *\n+ * <p>To configure SnowflakeIO to read/write from your Snowflake instance, you have to provide a\n+ * {@link DataSourceConfiguration} using {@link\n+ * DataSourceConfiguration#create(SnowflakeCredentials)}, where {@link SnowflakeCredentials might be\n+ * created using {@link org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory}}.\n+ * Additionally one of {@link DataSourceConfiguration#withServerName(String)} or {@link\n+ * DataSourceConfiguration#withUrl(String)} must be used to tell SnowflakeIO which instance to use.\n+ * <br>\n+ * There are also other options available to configure connection to Snowflake:\n+ *\n+ * <ul>\n+ *   <li>{@link DataSourceConfiguration#withWarehouse(String)} to specify which Warehouse to use\n+ *   <li>{@link DataSourceConfiguration#withDatabase(String)} to specify which Database to connect\n+ *       to\n+ *   <li>{@link DataSourceConfiguration#withSchema(String)} to specify which schema to use\n+ *   <li>{@link DataSourceConfiguration#withRole(String)} to specify which role to use\n+ *   <li>{@link DataSourceConfiguration#withLoginTimeout(Integer)} to specify the timeout for the\n+ *       login\n+ *   <li>{@link DataSourceConfiguration#withPortNumber(Integer)} to specify custom port of Snowflake\n+ *       instance\n+ * </ul>\n+ *\n+ * <p>For example:\n+ *\n+ * <pre>{@code\n+ * SnowflakeIO.DataSourceConfiguration dataSourceConfiguration =\n+ *     SnowflakeIO.DataSourceConfiguration.create(SnowflakeCredentialsFactory.of(options))\n+ *         .withServerName(options.getServerName())\n+ *         .withWarehouse(options.getWarehouse())\n+ *         .withDatabase(options.getDatabase())\n+ *         .withSchema(options.getSchema());\n+ * }</pre>\n+ *\n+ * <h3>Reading from Snowflake</h3>\n+ *\n+ * <p>SnowflakeIO.Read returns a bounded collection of {@code T} as a {@code PCollection<T>}. T is\n+ * the type returned by the provided {@link CsvMapper}.\n+ *\n+ * <p>For example\n+ *\n+ * <pre>{@code\n+ * PCollection<GenericRecord> items = pipeline.apply(\n+ *  SnowflakeIO.<GenericRecord>read()\n+ *    .withDataSourceConfiguration(dataSourceConfiguration)\n+ *    .fromQuery(QUERY)\n+ *    .withStagingBucketName(stagingBucketName)\n+ *    .withIntegrationName(integrationName)\n+ *    .withCsvMapper(...)\n+ *    .withCoder(...));\n+ * }</pre>\n+ *\n+ * <p><b>Important</b> When reading data from Snowflake, temporary CSV files are created on the\n+ * specified stagingBucketName in directory named `sf_copy_csv_[RANDOM CHARS]_[TIMESTAMP]`. This\n+ * directory and all the files are deleted automatically by default, but in case of failed pipeline\n+ * they will remain and have to be removed manually.\n+ */\n+public class SnowflakeIO {\n+  private static final Logger LOG = LoggerFactory.getLogger(SnowflakeIO.class);\n+\n+  private static final String CSV_QUOTE_CHAR = \"'\";\n+  /**\n+   * Read data from Snowflake.\n+   *\n+   * @param snowflakeService user-defined {@link SnowflakeService}\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read(SnowflakeService snowflakeService) {\n+    return new AutoValue_SnowflakeIO_Read.Builder<T>()\n+        .setSnowflakeService(snowflakeService)\n+        .build();\n+  }\n+\n+  /**\n+   * Read data from Snowflake.\n+   *\n+   * @param <T> Type of the data to be read.\n+   */\n+  public static <T> Read<T> read() {\n+    return read(new SnowflakeServiceImpl());\n+  }\n+\n+  /**\n+   * Interface for user-defined function mapping parts of CSV line into T. Used for\n+   * SnowflakeIO.Read.\n+   *\n+   * @param <T> Type of data to be read.\n+   */\n+  @FunctionalInterface\n+  public interface CsvMapper<T> extends Serializable {\n+    T mapRow(String[] parts) throws Exception;\n+  }\n+\n+  /** Implementation of {@link #read()}. */\n+  @AutoValue\n+  public abstract static class Read<T> extends PTransform<PBegin, PCollection<T>> {\n+    @Nullable\n+    abstract SerializableFunction<Void, DataSource> getDataSourceProviderFn();\n+\n+    @Nullable\n+    abstract String getQuery();\n+\n+    @Nullable\n+    abstract String getTable();\n+\n+    @Nullable\n+    abstract String getIntegrationName();\n+\n+    @Nullable\n+    abstract String getStagingBucketName();\n+\n+    @Nullable\n+    abstract CsvMapper<T> getCsvMapper();\n+\n+    @Nullable\n+    abstract Coder<T> getCoder();\n+\n+    @Nullable\n+    abstract SnowflakeService getSnowflakeService();\n+\n+    abstract Builder<T> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<T> {\n+      abstract Builder<T> setDataSourceProviderFn(\n+          SerializableFunction<Void, DataSource> dataSourceProviderFn);\n+\n+      abstract Builder<T> setQuery(String query);\n+\n+      abstract Builder<T> setTable(String table);\n+\n+      abstract Builder<T> setIntegrationName(String integrationName);\n+\n+      abstract Builder<T> setStagingBucketName(String stagingBucketName);\n+\n+      abstract Builder<T> setCsvMapper(CsvMapper<T> csvMapper);\n+\n+      abstract Builder<T> setCoder(Coder<T> coder);\n+\n+      abstract Builder<T> setSnowflakeService(SnowflakeService snowflakeService);\n+\n+      abstract Read<T> build();\n+    }\n+\n+    /**\n+     * Setting information about Snowflake server.\n+     *\n+     * @param config - An instance of {@link DataSourceConfiguration}.\n+     */\n+    public Read<T> withDataSourceConfiguration(final DataSourceConfiguration config) {\n+\n+      try {\n+        Connection connection = config.buildDatasource().getConnection();\n+        connection.close();\n+      } catch (SQLException e) {\n+        throw new IllegalArgumentException(\n+            \"Invalid DataSourceConfiguration. Underlying cause: \" + e);\n+      }\n+      return withDataSourceProviderFn(new DataSourceProviderFromDataSourceConfiguration(config));\n+    }\n+\n+    /**\n+     * Setting function that will provide {@link DataSourceConfiguration} in runtime.\n+     *\n+     * @param dataSourceProviderFn a {@link SerializableFunction}.\n+     */\n+    public Read<T> withDataSourceProviderFn(\n+        SerializableFunction<Void, DataSource> dataSourceProviderFn) {\n+      return toBuilder().setDataSourceProviderFn(dataSourceProviderFn).build();\n+    }\n+\n+    /**\n+     * A query to be executed in Snowflake.\n+     *\n+     * @param query - String with query.\n+     */\n+    public Read<T> fromQuery(String query) {\n+      return toBuilder().setQuery(query).build();\n+    }\n+\n+    /**\n+     * A table name to be read in Snowflake.\n+     *\n+     * @param table - String with the name of the table.\n+     */\n+    public Read<T> fromTable(String table) {\n+      return toBuilder().setTable(table).build();\n+    }\n+\n+    /**\n+     * Name of the cloud bucket (GCS by now) to use as tmp location of CSVs during COPY statement.\n+     *\n+     * @param stagingBucketName - String with the name of the bucket.\n+     */\n+    public Read<T> withStagingBucketName(String stagingBucketName) {\n+      return toBuilder().setStagingBucketName(stagingBucketName).build();\n+    }\n+\n+    /**\n+     * Name of the Storage Integration in Snowflake to be used. See\n+     * https://docs.snowflake.com/en/sql-reference/sql/create-storage-integration.html for\n+     * reference.\n+     *\n+     * @param integrationName - String with the name of the Storage Integration.\n+     */\n+    public Read<T> withIntegrationName(String integrationName) {\n+      return toBuilder().setIntegrationName(integrationName).build();\n+    }\n+\n+    /**\n+     * User-defined function mapping CSV lines into user data.\n+     *\n+     * @param csvMapper - an instance of {@link CsvMapper}.\n+     */\n+    public Read<T> withCsvMapper(CsvMapper<T> csvMapper) {\n+      return toBuilder().setCsvMapper(csvMapper).build();\n+    }\n+\n+    /**\n+     * A Coder to be used by the output PCollection generated by the source.\n+     *\n+     * @param coder - an instance of {@link Coder}.\n+     */\n+    public Read<T> withCoder(Coder<T> coder) {\n+      return toBuilder().setCoder(coder).build();\n+    }\n+\n+    @Override\n+    public PCollection<T> expand(PBegin input) {\n+      // Either table or query is required. If query is present, it's being used, table is used\n+      // otherwise\n+      checkArgument(\n+          getQuery() != null || getTable() != null, \"fromTable() or fromQuery() is required\");\n+      checkArgument(\n+          !(getQuery() != null && getTable() != null),\n+          \"fromTable() and fromQuery() are not allowed together\");\n+      checkArgument(getCsvMapper() != null, \"withCsvMapper() is required\");\n+      checkArgument(getCoder() != null, \"withCoder() is required\");\n+      checkArgument(getIntegrationName() != null, \"withIntegrationName() is required\");\n+      checkArgument(getStagingBucketName() != null, \"withStagingBucketName() is required\");\n+      checkArgument(\n+          (getDataSourceProviderFn() != null),\n+          \"withDataSourceConfiguration() or withDataSourceProviderFn() is required\");\n+\n+      String gcpTmpDirName = makeTmpDirName();\n+      String stagingBucketDir = String.format(\"%s/%s/\", getStagingBucketName(), gcpTmpDirName);\n+\n+      PCollection<Void> emptyCollection = input.apply(Create.of((Void) null));\n+\n+      PCollection<T> output =\n+          emptyCollection\n+              .apply(\n+                  ParDo.of(\n+                      new CopyIntoStageFn(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQyNjE5MTM4Nw=="}, "originalCommit": {"oid": "4641631c602969794b13a70309d31b01f90cd388"}, "originalPosition": 320}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3f7f3bd037dc44f58d2630b142aba213f52f755b", "author": {"user": {"login": "DariuszAniszewski", "name": "Dariusz Aniszewski"}}, "url": "https://github.com/apache/beam/commit/3f7f3bd037dc44f58d2630b142aba213f52f755b", "committedDate": "2020-05-20T13:23:15Z", "message": "[BEAM-9722] make `CopyIntoStageFn` retryable"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ac38c8bf21ee80452cb87282453fe22c74f7af84", "author": {"user": {"login": "DariuszAniszewski", "name": "Dariusz Aniszewski"}}, "url": "https://github.com/apache/beam/commit/ac38c8bf21ee80452cb87282453fe22c74f7af84", "committedDate": "2020-05-20T12:57:25Z", "message": "[BEAM-9722] make `CopyIntoStageFn` retryable"}, "afterCommit": {"oid": "3f7f3bd037dc44f58d2630b142aba213f52f755b", "author": {"user": {"login": "DariuszAniszewski", "name": "Dariusz Aniszewski"}}, "url": "https://github.com/apache/beam/commit/3f7f3bd037dc44f58d2630b142aba213f52f755b", "committedDate": "2020-05-20T13:23:15Z", "message": "[BEAM-9722] make `CopyIntoStageFn` retryable"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "3ba192a57217a9b251f71f77771b68ceb2882b02", "author": {"user": {"login": "DariuszAniszewski", "name": "Dariusz Aniszewski"}}, "url": "https://github.com/apache/beam/commit/3ba192a57217a9b251f71f77771b68ceb2882b02", "committedDate": "2020-05-20T19:02:18Z", "message": "[BEAM-9722] added `Reshuffle` step after `CopyIntoStageFn`"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDE1NjM3NDYz", "url": "https://github.com/apache/beam/pull/11360#pullrequestreview-415637463", "createdAt": "2020-05-20T19:10:15Z", "commit": {"oid": "3ba192a57217a9b251f71f77771b68ceb2882b02"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3ba192a57217a9b251f71f77771b68ceb2882b02", "author": {"user": {"login": "DariuszAniszewski", "name": "Dariusz Aniszewski"}}, "url": "https://github.com/apache/beam/commit/3ba192a57217a9b251f71f77771b68ceb2882b02", "committedDate": "2020-05-20T19:02:18Z", "message": "[BEAM-9722] added `Reshuffle` step after `CopyIntoStageFn`"}, "afterCommit": {"oid": "4641631c602969794b13a70309d31b01f90cd388", "author": {"user": {"login": "purbanow", "name": null}}, "url": "https://github.com/apache/beam/commit/4641631c602969794b13a70309d31b01f90cd388", "committedDate": "2020-05-14T05:51:30Z", "message": "[BEAM-9722] remove SnowflakeCloudProvider interface"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4568, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}