{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY3NjYwMDY3", "number": 12572, "title": "[BEAM-10123] Add Kafka Commit transform.", "bodyText": "Changes include:\n\n\nAdd KafkaCommitOffset transform, which expands to\nPCollection<KV<KafkaSourceDescriptor, KafkaRecord>> -> Map to KV<KafkaSourceDescriptor, (Long)offset> -> WindowInto(5 min FixWindow) -> Max.longsPerKey() -> CommitOffsetDoFn\n\n\nChange ReadFromKafkaDoFn to output KV<KafkaSourceDescriptor, KafkaRecord> instead of KafkaRecord only.\n\n\nAdd commit offset expansion to KafkaIO.ReadSourceDescriptors:\nParDo(ReadFromKafkaDoFn) -->\nReshuffle() --> Map(output KafkaRecord) --> output\n|\n- -> KafkaCommitOffset\nNote that this expansion is not supported when using x-lang on Dataflow.\n\n\nr: @aromanenko-dev\ncc: @lukecwik\n\nThank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:\n\n Choose reviewer(s) and mention them in a comment (R: @username).\n Format the pull request title like [BEAM-XXX] Fixes bug in ApproximateQuantiles, where you replace BEAM-XXX with the appropriate JIRA issue, if applicable. This will automatically link the pull request to the issue.\n Update CHANGES.md with noteworthy changes.\n If this contribution is large, please file an Apache Individual Contributor License Agreement.\n\nSee the Contributor Guide for more tips on how to make review process smoother.\nPost-Commit Tests Status (on master branch)\n\n\n\nLang\nSDK\nDataflow\nFlink\nSamza\nSpark\nTwister2\n\n\n\n\nGo\n\n---\n\n---\n\n---\n\n\nJava\n\n\n\n\n\n\n\n\nPython\n\n\n\n---\n\n---\n\n\nXLang\n\n---\n\n---\n\n---\n\n\n\nPre-Commit Tests Status (on master branch)\n\n\n\n---\nJava\nPython\nGo\nWebsite\n\n\n\n\nNon-portable\n\n\n\n\n\n\nPortable\n---\n\n---\n---\n\n\n\nSee .test-infra/jenkins/README for trigger phrase, status and link of all Jenkins jobs.\nGitHub Actions Tests Status (on master branch)\n\nSee CI.md for more information about GitHub Actions CI.", "createdAt": "2020-08-13T22:16:56Z", "url": "https://github.com/apache/beam/pull/12572", "merged": true, "mergeCommit": {"oid": "7507f8c893c2d93b9e2cf9a2bca94d406f95d081"}, "closed": true, "closedAt": "2020-11-03T15:43:19Z", "author": {"login": "boyuanzz"}, "timelineItems": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdDEDbDABqjM3MDA3MzYwMDM=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdY7GNeAFqTUyMjYyNjUyMA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d12f3d2cb56eb51d1eebb61ca1c20df72a9b8f52", "author": {"user": null}, "url": "https://github.com/apache/beam/commit/d12f3d2cb56eb51d1eebb61ca1c20df72a9b8f52", "committedDate": "2020-08-13T22:16:11Z", "message": "Add commit transform."}, "afterCommit": {"oid": "47b4588e40495c00b10ec48d0ee2266efca513d2", "author": {"user": null}, "url": "https://github.com/apache/beam/commit/47b4588e40495c00b10ec48d0ee2266efca513d2", "committedDate": "2020-08-27T17:42:08Z", "message": "Add commit transform."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "47b4588e40495c00b10ec48d0ee2266efca513d2", "author": {"user": null}, "url": "https://github.com/apache/beam/commit/47b4588e40495c00b10ec48d0ee2266efca513d2", "committedDate": "2020-08-27T17:42:08Z", "message": "Add commit transform."}, "afterCommit": {"oid": "1e91d6344a1b91505b581c2ecb80679848f057f4", "author": {"user": null}, "url": "https://github.com/apache/beam/commit/1e91d6344a1b91505b581c2ecb80679848f057f4", "committedDate": "2020-09-11T18:05:16Z", "message": "Add commit transform."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk3NzI3Mzk4", "url": "https://github.com/apache/beam/pull/12572#pullrequestreview-497727398", "createdAt": "2020-09-28T16:59:33Z", "commit": {"oid": "1e91d6344a1b91505b581c2ecb80679848f057f4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxNjo1OTozNFrOHZHi4g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQxNjo1OTozNFrOHZHi4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjEwMDA2Ng==", "bodyText": "Why do we need Reshuffle here?", "url": "https://github.com/apache/beam/pull/12572#discussion_r496100066", "createdAt": "2020-09-28T16:59:34Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1620,14 +1635,43 @@ public void processElement(\n       CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n       Coder<K> keyCoder = getKeyCoder(coderRegistry);\n       Coder<V> valueCoder = getValueCoder(coderRegistry);\n-      Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n-      PCollection<KafkaRecord<K, V>> output =\n-          input.apply(ParDo.of(new ReadFromKafkaDoFn<K, V>(this))).setCoder(outputCoder);\n-      // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n-      if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n-        throw new IllegalStateException(\"Offset committed is not supported yet\");\n+      Coder<KafkaRecord<K, V>> recordCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+\n+      try {\n+        PCollection<KV<KafkaSourceDescriptor, KafkaRecord<K, V>>> outputWithDescriptor =\n+            input\n+                .apply(ParDo.of(new ReadFromKafkaDoFn<K, V>(this)))\n+                .setCoder(\n+                    KvCoder.of(\n+                        input\n+                            .getPipeline()\n+                            .getSchemaRegistry()\n+                            .getSchemaCoder(KafkaSourceDescriptor.class),\n+                        recordCoder));\n+        if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+          outputWithDescriptor =\n+              outputWithDescriptor\n+                  .apply(Reshuffle.viaRandomKey())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1e91d6344a1b91505b581c2ecb80679848f057f4"}, "originalPosition": 76}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk4NjgyMTkx", "url": "https://github.com/apache/beam/pull/12572#pullrequestreview-498682191", "createdAt": "2020-09-29T16:41:16Z", "commit": {"oid": "1e91d6344a1b91505b581c2ecb80679848f057f4"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxNjo0MToxN1rOHZ3e-g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQxNjo0NzoxM1rOHZ3t9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njg4NTQ5OA==", "bodyText": "Is it a possible state that there are no bootstrap servers defined?", "url": "https://github.com/apache/beam/pull/12572#discussion_r496885498", "createdAt": "2020-09-29T16:41:17Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaCommitOffset.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.VarLongCoder;\n+import org.apache.beam.sdk.coders.VoidCoder;\n+import org.apache.beam.sdk.schemas.NoSuchSchemaException;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.transforms.Max;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** A {@link PTransform} that commits offsets of {@link KafkaRecord}. */\n+public class KafkaCommitOffset<K, V>\n+    extends PTransform<\n+        PCollection<KV<KafkaSourceDescriptor, KafkaRecord<K, V>>>, PCollection<Void>> {\n+  private final KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors;\n+\n+  KafkaCommitOffset(KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors) {\n+    this.readSourceDescriptors = readSourceDescriptors;\n+  }\n+\n+  static class CommitOffsetDoFn extends DoFn<KV<KafkaSourceDescriptor, Long>, Void> {\n+    private static final Logger LOG = LoggerFactory.getLogger(CommitOffsetDoFn.class);\n+    private final Map<String, Object> offsetConsumerConfig;\n+    private final Map<String, Object> consumerConfig;\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+\n+    private transient ConsumerSpEL consumerSpEL = null;\n+\n+    CommitOffsetDoFn(KafkaIO.ReadSourceDescriptors readSourceDescriptors) {\n+      offsetConsumerConfig = readSourceDescriptors.getOffsetConsumerConfig();\n+      consumerConfig = readSourceDescriptors.getConsumerConfig();\n+      consumerFactoryFn = readSourceDescriptors.getConsumerFactoryFn();\n+    }\n+\n+    @ProcessElement\n+    public void processElement(@Element KV<KafkaSourceDescriptor, Long> element) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, element.getKey());\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"commitOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        try {\n+          offsetConsumer.commitSync(\n+              Collections.singletonMap(\n+                  element.getKey().getTopicPartition(),\n+                  new OffsetAndMetadata(element.getValue() + 1)));\n+        } catch (Exception e) {\n+          // TODO: consider retrying.\n+          LOG.warn(\"Getting exception when committing offset: {}\", e.getMessage());\n+        }\n+      }\n+    }\n+\n+    private Map<String, Object> overrideBootstrapServersConfig(\n+        Map<String, Object> currentConfig, KafkaSourceDescriptor description) {\n+      checkState(\n+          currentConfig.containsKey(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG)\n+              || description.getBootStrapServers() != null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1e91d6344a1b91505b581c2ecb80679848f057f4"}, "originalPosition": 96}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njg4NzA3OA==", "bodyText": "How many uniq keys per bundle are expected? Only one (because of Max.longsPerKey()) on previous step?", "url": "https://github.com/apache/beam/pull/12572#discussion_r496887078", "createdAt": "2020-09-29T16:43:50Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaCommitOffset.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.VarLongCoder;\n+import org.apache.beam.sdk.coders.VoidCoder;\n+import org.apache.beam.sdk.schemas.NoSuchSchemaException;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.transforms.Max;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** A {@link PTransform} that commits offsets of {@link KafkaRecord}. */\n+public class KafkaCommitOffset<K, V>\n+    extends PTransform<\n+        PCollection<KV<KafkaSourceDescriptor, KafkaRecord<K, V>>>, PCollection<Void>> {\n+  private final KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors;\n+\n+  KafkaCommitOffset(KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors) {\n+    this.readSourceDescriptors = readSourceDescriptors;\n+  }\n+\n+  static class CommitOffsetDoFn extends DoFn<KV<KafkaSourceDescriptor, Long>, Void> {\n+    private static final Logger LOG = LoggerFactory.getLogger(CommitOffsetDoFn.class);\n+    private final Map<String, Object> offsetConsumerConfig;\n+    private final Map<String, Object> consumerConfig;\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+\n+    private transient ConsumerSpEL consumerSpEL = null;\n+\n+    CommitOffsetDoFn(KafkaIO.ReadSourceDescriptors readSourceDescriptors) {\n+      offsetConsumerConfig = readSourceDescriptors.getOffsetConsumerConfig();\n+      consumerConfig = readSourceDescriptors.getConsumerConfig();\n+      consumerFactoryFn = readSourceDescriptors.getConsumerFactoryFn();\n+    }\n+\n+    @ProcessElement\n+    public void processElement(@Element KV<KafkaSourceDescriptor, Long> element) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, element.getKey());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1e91d6344a1b91505b581c2ecb80679848f057f4"}, "originalPosition": 75}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5Njg4OTMzNA==", "bodyText": "Could you elaborate a bit why Window is hardcoded to 5 mins?", "url": "https://github.com/apache/beam/pull/12572#discussion_r496889334", "createdAt": "2020-09-29T16:47:13Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaCommitOffset.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.VarLongCoder;\n+import org.apache.beam.sdk.coders.VoidCoder;\n+import org.apache.beam.sdk.schemas.NoSuchSchemaException;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.MapElements;\n+import org.apache.beam.sdk.transforms.Max;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.windowing.FixedWindows;\n+import org.apache.beam.sdk.transforms.windowing.Window;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.OffsetAndMetadata;\n+import org.joda.time.Duration;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** A {@link PTransform} that commits offsets of {@link KafkaRecord}. */\n+public class KafkaCommitOffset<K, V>\n+    extends PTransform<\n+        PCollection<KV<KafkaSourceDescriptor, KafkaRecord<K, V>>>, PCollection<Void>> {\n+  private final KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors;\n+\n+  KafkaCommitOffset(KafkaIO.ReadSourceDescriptors<K, V> readSourceDescriptors) {\n+    this.readSourceDescriptors = readSourceDescriptors;\n+  }\n+\n+  static class CommitOffsetDoFn extends DoFn<KV<KafkaSourceDescriptor, Long>, Void> {\n+    private static final Logger LOG = LoggerFactory.getLogger(CommitOffsetDoFn.class);\n+    private final Map<String, Object> offsetConsumerConfig;\n+    private final Map<String, Object> consumerConfig;\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+\n+    private transient ConsumerSpEL consumerSpEL = null;\n+\n+    CommitOffsetDoFn(KafkaIO.ReadSourceDescriptors readSourceDescriptors) {\n+      offsetConsumerConfig = readSourceDescriptors.getOffsetConsumerConfig();\n+      consumerConfig = readSourceDescriptors.getConsumerConfig();\n+      consumerFactoryFn = readSourceDescriptors.getConsumerFactoryFn();\n+    }\n+\n+    @ProcessElement\n+    public void processElement(@Element KV<KafkaSourceDescriptor, Long> element) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, element.getKey());\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"commitOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        try {\n+          offsetConsumer.commitSync(\n+              Collections.singletonMap(\n+                  element.getKey().getTopicPartition(),\n+                  new OffsetAndMetadata(element.getValue() + 1)));\n+        } catch (Exception e) {\n+          // TODO: consider retrying.\n+          LOG.warn(\"Getting exception when committing offset: {}\", e.getMessage());\n+        }\n+      }\n+    }\n+\n+    private Map<String, Object> overrideBootstrapServersConfig(\n+        Map<String, Object> currentConfig, KafkaSourceDescriptor description) {\n+      checkState(\n+          currentConfig.containsKey(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG)\n+              || description.getBootStrapServers() != null);\n+      Map<String, Object> config = new HashMap<>(currentConfig);\n+      if (description.getBootStrapServers() != null\n+          && description.getBootStrapServers().size() > 0) {\n+        config.put(\n+            ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG,\n+            String.join(\",\", description.getBootStrapServers()));\n+      }\n+      return config;\n+    }\n+  }\n+\n+  @Override\n+  public PCollection<Void> expand(PCollection<KV<KafkaSourceDescriptor, KafkaRecord<K, V>>> input) {\n+    try {\n+      return input\n+          .apply(\n+              MapElements.into(new TypeDescriptor<KV<KafkaSourceDescriptor, Long>>() {})\n+                  .via(element -> KV.of(element.getKey(), element.getValue().getOffset())))\n+          .setCoder(\n+              KvCoder.of(\n+                  input\n+                      .getPipeline()\n+                      .getSchemaRegistry()\n+                      .getSchemaCoder(KafkaSourceDescriptor.class),\n+                  VarLongCoder.of()))\n+          .apply(Window.into(FixedWindows.of(Duration.standardMinutes(5))))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1e91d6344a1b91505b581c2ecb80679848f057f4"}, "originalPosition": 122}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "1e91d6344a1b91505b581c2ecb80679848f057f4", "author": {"user": null}, "url": "https://github.com/apache/beam/commit/1e91d6344a1b91505b581c2ecb80679848f057f4", "committedDate": "2020-09-11T18:05:16Z", "message": "Add commit transform."}, "afterCommit": {"oid": "a087685d0cee3cf940cf819e254b52e4a0aa7ae5", "author": {"user": null}, "url": "https://github.com/apache/beam/commit/a087685d0cee3cf940cf819e254b52e4a0aa7ae5", "committedDate": "2020-10-30T19:03:10Z", "message": "Add commit transform."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c1fbe423c667d68bd4821a919bc7d8fcfc14c9d5", "author": {"user": null}, "url": "https://github.com/apache/beam/commit/c1fbe423c667d68bd4821a919bc7d8fcfc14c9d5", "committedDate": "2020-11-02T18:19:35Z", "message": "Add commit transform."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9e2d5cfa24642bbbef075f6f8a438bbeee8ee45c", "author": {"user": null}, "url": "https://github.com/apache/beam/commit/9e2d5cfa24642bbbef075f6f8a438bbeee8ee45c", "committedDate": "2020-10-30T22:35:43Z", "message": "test - remove global combine"}, "afterCommit": {"oid": "c1fbe423c667d68bd4821a919bc7d8fcfc14c9d5", "author": {"user": null}, "url": "https://github.com/apache/beam/commit/c1fbe423c667d68bd4821a919bc7d8fcfc14c9d5", "committedDate": "2020-11-02T18:19:35Z", "message": "Add commit transform."}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIyNjI2NTIw", "url": "https://github.com/apache/beam/pull/12572#pullrequestreview-522626520", "createdAt": "2020-11-03T15:42:36Z", "commit": {"oid": "c1fbe423c667d68bd4821a919bc7d8fcfc14c9d5"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3507, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}