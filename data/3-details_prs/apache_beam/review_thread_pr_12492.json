{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY0MzUwMTE4", "number": 12492, "reviewThreads": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMTozMjo0M1rOEXY62Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQyMTo1Nzo0M1rOEcRDVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyOTYwOTg1OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/io/azure/blobstoragefilesystem_test.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMTozMjo0M1rOG_KAAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMTozMjo0M1rOG_KAAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg3NzMxNA==", "bodyText": "the import error occurs here, so you should move this import to line 40", "url": "https://github.com/apache/beam/pull/12492#discussion_r468877314", "createdAt": "2020-08-11T21:32:43Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/io/azure/blobstoragefilesystem_test.py", "diffHunk": "@@ -0,0 +1,315 @@\n+# -*- coding: utf-8 -*-\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"Unit tests for Azure Blob Storage File System.\"\"\"\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import logging\n+import unittest\n+\n+# patches unittest.TestCase to be python3 compatible.\n+import future.tests.base  # pylint: disable=unused-import\n+import mock\n+\n+from apache_beam.io.azure import blobstorageio", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b2e0ac429c83d65ded3a680e327940f3ecc51a0f"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyOTYyMDA2OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/io/azure/blobstorageio_test.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMTozNjozMlrOG_KGUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMVQyMTozNjozMlrOG_KGUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2ODg3ODkzMA==", "bodyText": "there's also an import error happening here. you need to catch it and skip the test", "url": "https://github.com/apache/beam/pull/12492#discussion_r468878930", "createdAt": "2020-08-11T21:36:32Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/io/azure/blobstorageio_test.py", "diffHunk": "@@ -0,0 +1,86 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"Tests for Azure Blob Storage client.\n+\"\"\"\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import logging\n+import unittest\n+\n+from apache_beam.io.azure import blobstorageio", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b2e0ac429c83d65ded3a680e327940f3ecc51a0f"}, "originalPosition": 27}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4MDY0NTU5OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/io/azure/blobstorageio.py", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQyMTozNzoxOVrOHGrHjg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQyMjoxNDo0M1rOHHfkMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njc1OTk1MA==", "bodyText": "I recall an issue related to very large files. What happens when we're trying to upload a large file?", "url": "https://github.com/apache/beam/pull/12492#discussion_r476759950", "createdAt": "2020-08-25T21:37:19Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/io/azure/blobstorageio.py", "diffHunk": "@@ -0,0 +1,664 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"Azure Blob Storage client.\n+\"\"\"\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import errno\n+import io\n+import logging\n+import os\n+import re\n+import tempfile\n+import time\n+from builtins import object\n+\n+from apache_beam.io.filesystemio import Downloader\n+from apache_beam.io.filesystemio import DownloaderStream\n+from apache_beam.io.filesystemio import Uploader\n+from apache_beam.io.filesystemio import UploaderStream\n+from apache_beam.utils import retry\n+\n+_LOGGER = logging.getLogger(__name__)\n+\n+try:\n+  # pylint: disable=wrong-import-order, wrong-import-position\n+  # pylint: disable=ungrouped-imports\n+  from azure.core.exceptions import ResourceNotFoundError\n+  from azure.storage.blob import (\n+      BlobServiceClient,\n+      ContentSettings,\n+  )\n+  AZURE_DEPS_INSTALLED = True\n+except ImportError:\n+  AZURE_DEPS_INSTALLED = False\n+\n+DEFAULT_READ_BUFFER_SIZE = 16 * 1024 * 1024\n+\n+MAX_BATCH_OPERATION_SIZE = 100\n+\n+\n+def parse_azfs_path(azfs_path, blob_optional=False, get_account=False):\n+  \"\"\"Return the storage account, the container and\n+  blob names of the given azfs:// path.\n+  \"\"\"\n+  match = re.match(\n+      '^azfs://([a-z0-9]{3,24})/([a-z0-9](?![a-z0-9-]*--[a-z0-9-]*)'\n+      '[a-z0-9-]{1,61}[a-z0-9])/(.*)$',\n+      azfs_path)\n+  if match is None or (match.group(3) == '' and not blob_optional):\n+    raise ValueError(\n+        'Azure Blob Storage path must be in the form '\n+        'azfs://<storage-account>/<container>/<path>.')\n+  result = None\n+  if get_account:\n+    result = match.group(1), match.group(2), match.group(3)\n+  else:\n+    result = match.group(2), match.group(3)\n+  return result\n+\n+\n+def get_azfs_url(storage_account, container, blob=''):\n+  \"\"\"Returns the url in the form of\n+   https://account.blob.core.windows.net/container/blob-name\n+  \"\"\"\n+  return 'https://' + storage_account + '.blob.core.windows.net/' + \\\n+          container + '/' + blob\n+\n+\n+class Blob():\n+  \"\"\"A Blob in Azure Blob Storage.\"\"\"\n+  def __init__(self, etag, name, last_updated, size, mime_type):\n+    self.etag = etag\n+    self.name = name\n+    self.last_updated = last_updated\n+    self.size = size\n+    self.mime_type = mime_type\n+\n+\n+class BlobStorageIOError(IOError, retry.PermanentException):\n+  \"\"\"Blob Strorage IO error that should not be retried.\"\"\"\n+  pass\n+\n+\n+class BlobStorageError(Exception):\n+  \"\"\"Blob Storage client error.\"\"\"\n+  def __init__(self, message=None, code=None):\n+    self.message = message\n+    self.code = code\n+\n+\n+class BlobStorageIO(object):\n+  \"\"\"Azure Blob Storage I/O client.\"\"\"\n+  def __init__(self, client=None):\n+    connect_str = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n+    if client is None:\n+      self.client = BlobServiceClient.from_connection_string(connect_str)\n+    else:\n+      self.client = client\n+    if not AZURE_DEPS_INSTALLED:\n+      raise RuntimeError('Azure dependencies are not installed. Unable to run.')\n+\n+  def open(\n+      self,\n+      filename,\n+      mode='r',\n+      read_buffer_size=DEFAULT_READ_BUFFER_SIZE,\n+      mime_type='application/octet-stream'):\n+    \"\"\"Open an Azure Blob Storage file path for reading or writing.\n+\n+    Args:\n+      filename (str): Azure Blob Storage file path in the form\n+                      ``azfs://<storage-account>/<container>/<path>``.\n+      mode (str): ``'r'`` for reading or ``'w'`` for writing.\n+      read_buffer_size (int): Buffer size to use during read operations.\n+      mime_type (str): Mime type to set for write operations.\n+\n+    Returns:\n+      Azure Blob Storage file object.\n+    Raises:\n+      ValueError: Invalid open file mode.\n+    \"\"\"\n+    if mode == 'r' or mode == 'rb':\n+      downloader = BlobStorageDownloader(\n+          self.client, filename, buffer_size=read_buffer_size)\n+      return io.BufferedReader(\n+          DownloaderStream(\n+              downloader, read_buffer_size=read_buffer_size, mode=mode),\n+          buffer_size=read_buffer_size)\n+    elif mode == 'w' or mode == 'wb':\n+      uploader = BlobStorageUploader(self.client, filename, mime_type)\n+      return io.BufferedWriter(\n+          UploaderStream(uploader, mode=mode), buffer_size=128 * 1024)\n+    else:\n+      raise ValueError('Invalid file open mode: %s.' % mode)\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def copy(self, src, dest):\n+    \"\"\"Copies a single Azure Blob Storage blob from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+\n+    Raises:\n+      TimeoutError: on timeout.\n+    \"\"\"\n+    src_storage_account, src_container, src_blob = parse_azfs_path(\n+        src, get_account=True)\n+    dest_container, dest_blob = parse_azfs_path(dest)\n+\n+    source_blob = get_azfs_url(src_storage_account, src_container, src_blob)\n+    copied_blob = self.client.get_blob_client(dest_container, dest_blob)\n+\n+    try:\n+      copied_blob.start_copy_from_url(source_blob)\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy operation is already an idempotent operation protected\n+  # by retry decorators.\n+  def copy_tree(self, src, dest):\n+    \"\"\"Renames the given Azure Blob storage directory and its contents\n+    recursively from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) where exception is None if the\n+      operation succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    assert src.endswith('/')\n+    assert dest.endswith('/')\n+\n+    results = []\n+    for entry in self.list_prefix(src):\n+      rel_path = entry[len(src):]\n+      try:\n+        self.copy(entry, dest + rel_path)\n+        results.append((entry, dest + rel_path, None))\n+      except BlobStorageError as e:\n+        results.append((entry, dest + rel_path, e))\n+\n+    return results\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy operation is already an idempotent operation protected\n+  # by retry decorators.\n+  def copy_paths(self, src_dest_pairs):\n+    \"\"\"Copies the given Azure Blob Storage blobs from src to dest. This can\n+    handle directory or file paths.\n+\n+    Args:\n+      src_dest_pairs: List of (src, dest) tuples of\n+                      azfs://<storage-account>/<container>/[name] file paths\n+                      to copy from src to dest.\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) in the same order as the\n+      src_dest_pairs argument, where exception is None if the operation\n+      succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    if not src_dest_pairs:\n+      return []\n+\n+    results = []\n+\n+    for src_path, dest_path in src_dest_pairs:\n+      # Case 1. They are directories.\n+      if src_path.endswith('/') and dest_path.endswith('/'):\n+        try:\n+          results += self.copy_tree(src_path, dest_path)\n+        except BlobStorageError as e:\n+          results.append((src_path, dest_path, e))\n+\n+      # Case 2. They are individual blobs.\n+      elif not src_path.endswith('/') and not dest_path.endswith('/'):\n+        try:\n+          self.copy(src_path, dest_path)\n+          results.append((src_path, dest_path, None))\n+        except BlobStorageError as e:\n+          results.append((src_path, dest_path, e))\n+\n+      # Mismatched paths (one directory, one non-directory) get an error.\n+      else:\n+        e = BlobStorageError(\n+            \"Unable to copy mismatched paths\" +\n+            \"(directory, non-directory): %s, %s\" % (src_path, dest_path),\n+            400)\n+        results.append((src_path, dest_path, e))\n+\n+    return results\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def rename(self, src, dest):\n+    \"\"\"Renames the given Azure Blob Storage blob from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    self.copy(src, dest)\n+    self.delete(src)\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def rename_files(self, src_dest_pairs):\n+    \"\"\"Renames the given Azure Blob Storage blobs from src to dest.\n+\n+    Args:\n+      src_dest_pairs: List of (src, dest) tuples of\n+                      azfs://<storage-account>/<container>/[name]\n+                      file paths to rename from src to dest.\n+    Returns: List of tuples of (src, dest, exception) in the same order as the\n+             src_dest_pairs argument, where exception is None if the operation\n+             succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    if not src_dest_pairs:\n+      return []\n+\n+    for src, dest in src_dest_pairs:\n+      if src.endswith('/') or dest.endswith('/'):\n+        raise ValueError('Unable to rename a directory.')\n+\n+    # Results from copy operation.\n+    copy_results = self.copy_paths(src_dest_pairs)\n+    paths_to_delete = \\\n+        [src for (src, _, error) in copy_results if error is None]\n+    # Results from delete operation.\n+    delete_results = self.delete_files(paths_to_delete)\n+\n+    # Get rename file results (list of tuples).\n+    results = []\n+\n+    # Using a dictionary will make the operation faster.\n+    delete_results_dict = {src: error for (src, error) in delete_results}\n+\n+    for src, dest, error in copy_results:\n+      # If there was an error in the copy operation.\n+      if error is not None:\n+        results.append((src, dest, error))\n+      # If there was an error in the delete operation.\n+      elif delete_results_dict[src] is not None:\n+        results.append((src, dest, delete_results_dict[src]))\n+      # If there was no error in the operations.\n+      else:\n+        results.append((src, dest, None))\n+\n+    return results\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def exists(self, path):\n+    \"\"\"Returns whether the given Azure Blob Storage blob exists.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      blob_to_check.get_blob_properties()\n+      return True\n+    except ResourceNotFoundError as e:\n+      if e.status_code == 404:\n+        # HTTP 404 indicates that the file did not exist.\n+        return False\n+      else:\n+        # We re-raise all other exceptions.\n+        raise\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def size(self, path):\n+    \"\"\"Returns the size of a single Blob Storage blob.\n+\n+    This method does not perform glob expansion. Hence the\n+    given path must be for a single Blob Storage blob.\n+\n+    Returns: size of the Blob Storage blob in bytes.\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    return properties.size\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def last_updated(self, path):\n+    \"\"\"Returns the last updated epoch time of a single\n+    Azure Blob Storage blob.\n+\n+    This method does not perform glob expansion. Hence the\n+    given path must be for a single Azure Blob Storage blob.\n+\n+    Returns: last updated time of the Azure Blob Storage blob\n+    in seconds.\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    datatime = properties.last_modified\n+    return (\n+        time.mktime(datatime.timetuple()) - time.timezone +\n+        datatime.microsecond / 1000000.0)\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def checksum(self, path):\n+    \"\"\"Looks up the checksum of an Azure Blob Storage blob.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    return properties.etag\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def delete(self, path):\n+    \"\"\"Deletes a single blob at the given Azure Blob Storage path.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_delete = self.client.get_blob_client(container, blob)\n+    try:\n+      blob_to_delete.delete_blob()\n+    except ResourceNotFoundError as e:\n+      if e.status_code == 404:\n+        # Return success when the file doesn't exist anymore for idempotency.\n+        return\n+      else:\n+        logging.error('HTTP error while deleting file %s', path)\n+        raise e\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def delete_paths(self, paths):\n+    \"\"\"Deletes the given Azure Blob Storage blobs from src to dest.\n+    This can handle directory or file paths.\n+\n+    Args:\n+      paths: list of Azure Blob Storage paths in the form\n+             azfs://<storage-account>/<container>/[name] that give the\n+             file blobs to be deleted.\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) in the same order as the\n+      src_dest_pairs argument, where exception is 202 if the operation\n+      succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    directories, blobs = [], []\n+\n+    # Retrieve directories and not directories.\n+    for path in paths:\n+      if path.endswith('/'):\n+        directories.append(path)\n+      else:\n+        blobs.append(path)\n+\n+    results = {}\n+\n+    for directory in directories:\n+      directory_result = dict(self.delete_tree(directory))\n+      results.update(directory_result)\n+\n+    blobs_results = dict(self.delete_files(blobs))\n+    results.update(blobs_results)\n+\n+    return results\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def delete_tree(self, root):\n+    \"\"\"Deletes all blobs under the given Azure BlobStorage virtual\n+    directory.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name]\n+            (ending with a \"/\").\n+\n+    Returns:\n+      List of tuples of (path, exception), where each path is a blob\n+      under the given root. exception is 202 if the operation succeeded\n+      or the relevant exception if the operation failed.\n+    \"\"\"\n+    assert root.endswith('/')\n+\n+    # Get the blob under the root directory.\n+    paths_to_delete = self.list_prefix(root)\n+\n+    return self.delete_files(paths_to_delete)\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def delete_files(self, paths):\n+    \"\"\"Deletes the given Azure Blob Storage blobs from src to dest.\n+\n+    Args:\n+      paths: list of Azure Blob Storage paths in the form\n+             azfs://<storage-account>/<container>/[name] that give the\n+             file blobs to be deleted.\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) in the same order as the\n+      src_dest_pairs argument, where exception is 202 if the operation\n+      succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    if not paths:\n+      return []\n+\n+    # Group blobs into containers.\n+    containers, blobs = zip(*[parse_azfs_path(path, get_account=False) \\\n+        for path in paths])\n+\n+    grouped_blobs = {container: [] for container in containers}\n+\n+    # Fill dictionary.\n+    for container, blob in zip(containers, blobs):\n+      grouped_blobs[container].append(blob)\n+\n+    results = {}\n+\n+    # Delete minibatches of blobs for each container.\n+    for container, blobs in grouped_blobs.items():\n+      for i in range(0, len(blobs), MAX_BATCH_OPERATION_SIZE):\n+        blobs_to_delete = blobs[i:i + MAX_BATCH_OPERATION_SIZE]\n+        results.update(self._delete_batch(container, blobs_to_delete))\n+\n+    final_results = \\\n+        [(path, results[parse_azfs_path(path, get_account=False)]) \\\n+        for path in paths]\n+\n+    return final_results\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def _delete_batch(self, container, blobs):\n+    \"\"\"A helper method. Azure Blob Storage Python Client allows batch\n+    deletions for blobs within the same container.\n+\n+    Args:\n+      container: container name.\n+      blobs: list of blobs to be deleted.\n+\n+    Returns:\n+      Dictionary of the form {(container, blob): error}, where error is\n+      202 if the operation succeeded.\n+    \"\"\"\n+    container_client = self.client.get_container_client(container)\n+    results = {}\n+\n+    try:\n+      response = container_client.delete_blobs(\n+          *blobs, raise_on_any_failure=False)\n+\n+      for blob, error in zip(blobs, response):\n+        results[(container, blob)] = error.status_code\n+\n+    except BlobStorageError as e:\n+      for blob in blobs:\n+        results[(container, blob)] = e.message\n+\n+    return results\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def list_prefix(self, path):\n+    \"\"\"Lists files matching the prefix.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+\n+    Returns:\n+      Dictionary of file name -> size.\n+    \"\"\"\n+    storage_account, container, blob = parse_azfs_path(\n+        path, blob_optional=True, get_account=True)\n+    file_sizes = {}\n+    counter = 0\n+    start_time = time.time()\n+\n+    logging.info(\"Starting the size estimation of the input\")\n+    container_client = self.client.get_container_client(container)\n+\n+    while True:\n+      response = container_client.list_blobs(name_starts_with=blob)\n+      for item in response:\n+        file_name = \"azfs://%s/%s/%s\" % (storage_account, container, item.name)\n+        file_sizes[file_name] = item.size\n+        counter += 1\n+        if counter % 10000 == 0:\n+          logging.info(\"Finished computing size of: %s files\", len(file_sizes))\n+      break\n+\n+    logging.info(\n+        \"Finished listing %s files in %s seconds.\",\n+        counter,\n+        time.time() - start_time)\n+    return file_sizes\n+\n+\n+class BlobStorageDownloader(Downloader):\n+  def __init__(self, client, path, buffer_size):\n+    self._client = client\n+    self._path = path\n+    self._container, self._blob = parse_azfs_path(path)\n+    self._buffer_size = buffer_size\n+\n+    self._blob_to_download = self._client.get_blob_client(\n+        self._container, self._blob)\n+\n+    try:\n+      properties = self._get_object_properties()\n+    except ResourceNotFoundError as http_error:\n+      if http_error.status_code == 404:\n+        raise IOError(errno.ENOENT, 'Not found: %s' % self._path)\n+      else:\n+        _LOGGER.error(\n+            'HTTP error while requesting file %s: %s', self._path, http_error)\n+        raise\n+\n+    self._size = properties.size\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def _get_object_properties(self):\n+    return self._blob_to_download.get_blob_properties()\n+\n+  @property\n+  def size(self):\n+    return self._size\n+\n+  def get_range(self, start, end):\n+    # Download_blob first parameter is offset and second is length (exclusive).\n+    blob_data = self._blob_to_download.download_blob(start, end - start)\n+    # Returns the content as bytes.\n+    return blob_data.readall()\n+\n+\n+class BlobStorageUploader(Uploader):\n+  def __init__(self, client, path, mime_type='application/octet-stream'):\n+    self._client = client\n+    self._path = path\n+    self._container, self._blob = parse_azfs_path(path)\n+    self._content_settings = ContentSettings(mime_type)\n+\n+    self._blob_to_upload = self._client.get_blob_client(\n+        self._container, self._blob)\n+\n+    # Temporary file.\n+    self._temporary_file = tempfile.NamedTemporaryFile()\n+\n+  def put(self, data):\n+    self._temporary_file.write(data.tobytes())\n+\n+  def finish(self):\n+    self._temporary_file.seek(0)\n+    # The temporary file is deleted immediately after the operation.\n+    with open(self._temporary_file.name, \"rb\") as f:\n+      self._blob_to_upload.upload_blob(\n+          f.read(), overwrite=True, content_settings=self._content_settings)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4c5ab4ca7b4f137dce5f65a6349e0f8f1aefecb5"}, "originalPosition": 664}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAwMDY0Mw==", "bodyText": "@pabloem Let's see:\n\nAuthentication. At the moment the only way to authenticate is with a connection string obtained from environment variables.\nIntegration tests with Azurite. Integration tests with Azurite are practically ready. The only thing left is to define a function in build.gradle that runs and stops Azurite. (You can find the branch here: https://github.com/AldairCoronel/beam/commits/azurite).", "url": "https://github.com/apache/beam/pull/12492#discussion_r477000643", "createdAt": "2020-08-26T02:49:04Z", "author": {"login": "AldairCoronel"}, "path": "sdks/python/apache_beam/io/azure/blobstorageio.py", "diffHunk": "@@ -0,0 +1,664 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"Azure Blob Storage client.\n+\"\"\"\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import errno\n+import io\n+import logging\n+import os\n+import re\n+import tempfile\n+import time\n+from builtins import object\n+\n+from apache_beam.io.filesystemio import Downloader\n+from apache_beam.io.filesystemio import DownloaderStream\n+from apache_beam.io.filesystemio import Uploader\n+from apache_beam.io.filesystemio import UploaderStream\n+from apache_beam.utils import retry\n+\n+_LOGGER = logging.getLogger(__name__)\n+\n+try:\n+  # pylint: disable=wrong-import-order, wrong-import-position\n+  # pylint: disable=ungrouped-imports\n+  from azure.core.exceptions import ResourceNotFoundError\n+  from azure.storage.blob import (\n+      BlobServiceClient,\n+      ContentSettings,\n+  )\n+  AZURE_DEPS_INSTALLED = True\n+except ImportError:\n+  AZURE_DEPS_INSTALLED = False\n+\n+DEFAULT_READ_BUFFER_SIZE = 16 * 1024 * 1024\n+\n+MAX_BATCH_OPERATION_SIZE = 100\n+\n+\n+def parse_azfs_path(azfs_path, blob_optional=False, get_account=False):\n+  \"\"\"Return the storage account, the container and\n+  blob names of the given azfs:// path.\n+  \"\"\"\n+  match = re.match(\n+      '^azfs://([a-z0-9]{3,24})/([a-z0-9](?![a-z0-9-]*--[a-z0-9-]*)'\n+      '[a-z0-9-]{1,61}[a-z0-9])/(.*)$',\n+      azfs_path)\n+  if match is None or (match.group(3) == '' and not blob_optional):\n+    raise ValueError(\n+        'Azure Blob Storage path must be in the form '\n+        'azfs://<storage-account>/<container>/<path>.')\n+  result = None\n+  if get_account:\n+    result = match.group(1), match.group(2), match.group(3)\n+  else:\n+    result = match.group(2), match.group(3)\n+  return result\n+\n+\n+def get_azfs_url(storage_account, container, blob=''):\n+  \"\"\"Returns the url in the form of\n+   https://account.blob.core.windows.net/container/blob-name\n+  \"\"\"\n+  return 'https://' + storage_account + '.blob.core.windows.net/' + \\\n+          container + '/' + blob\n+\n+\n+class Blob():\n+  \"\"\"A Blob in Azure Blob Storage.\"\"\"\n+  def __init__(self, etag, name, last_updated, size, mime_type):\n+    self.etag = etag\n+    self.name = name\n+    self.last_updated = last_updated\n+    self.size = size\n+    self.mime_type = mime_type\n+\n+\n+class BlobStorageIOError(IOError, retry.PermanentException):\n+  \"\"\"Blob Strorage IO error that should not be retried.\"\"\"\n+  pass\n+\n+\n+class BlobStorageError(Exception):\n+  \"\"\"Blob Storage client error.\"\"\"\n+  def __init__(self, message=None, code=None):\n+    self.message = message\n+    self.code = code\n+\n+\n+class BlobStorageIO(object):\n+  \"\"\"Azure Blob Storage I/O client.\"\"\"\n+  def __init__(self, client=None):\n+    connect_str = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n+    if client is None:\n+      self.client = BlobServiceClient.from_connection_string(connect_str)\n+    else:\n+      self.client = client\n+    if not AZURE_DEPS_INSTALLED:\n+      raise RuntimeError('Azure dependencies are not installed. Unable to run.')\n+\n+  def open(\n+      self,\n+      filename,\n+      mode='r',\n+      read_buffer_size=DEFAULT_READ_BUFFER_SIZE,\n+      mime_type='application/octet-stream'):\n+    \"\"\"Open an Azure Blob Storage file path for reading or writing.\n+\n+    Args:\n+      filename (str): Azure Blob Storage file path in the form\n+                      ``azfs://<storage-account>/<container>/<path>``.\n+      mode (str): ``'r'`` for reading or ``'w'`` for writing.\n+      read_buffer_size (int): Buffer size to use during read operations.\n+      mime_type (str): Mime type to set for write operations.\n+\n+    Returns:\n+      Azure Blob Storage file object.\n+    Raises:\n+      ValueError: Invalid open file mode.\n+    \"\"\"\n+    if mode == 'r' or mode == 'rb':\n+      downloader = BlobStorageDownloader(\n+          self.client, filename, buffer_size=read_buffer_size)\n+      return io.BufferedReader(\n+          DownloaderStream(\n+              downloader, read_buffer_size=read_buffer_size, mode=mode),\n+          buffer_size=read_buffer_size)\n+    elif mode == 'w' or mode == 'wb':\n+      uploader = BlobStorageUploader(self.client, filename, mime_type)\n+      return io.BufferedWriter(\n+          UploaderStream(uploader, mode=mode), buffer_size=128 * 1024)\n+    else:\n+      raise ValueError('Invalid file open mode: %s.' % mode)\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def copy(self, src, dest):\n+    \"\"\"Copies a single Azure Blob Storage blob from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+\n+    Raises:\n+      TimeoutError: on timeout.\n+    \"\"\"\n+    src_storage_account, src_container, src_blob = parse_azfs_path(\n+        src, get_account=True)\n+    dest_container, dest_blob = parse_azfs_path(dest)\n+\n+    source_blob = get_azfs_url(src_storage_account, src_container, src_blob)\n+    copied_blob = self.client.get_blob_client(dest_container, dest_blob)\n+\n+    try:\n+      copied_blob.start_copy_from_url(source_blob)\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy operation is already an idempotent operation protected\n+  # by retry decorators.\n+  def copy_tree(self, src, dest):\n+    \"\"\"Renames the given Azure Blob storage directory and its contents\n+    recursively from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) where exception is None if the\n+      operation succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    assert src.endswith('/')\n+    assert dest.endswith('/')\n+\n+    results = []\n+    for entry in self.list_prefix(src):\n+      rel_path = entry[len(src):]\n+      try:\n+        self.copy(entry, dest + rel_path)\n+        results.append((entry, dest + rel_path, None))\n+      except BlobStorageError as e:\n+        results.append((entry, dest + rel_path, e))\n+\n+    return results\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy operation is already an idempotent operation protected\n+  # by retry decorators.\n+  def copy_paths(self, src_dest_pairs):\n+    \"\"\"Copies the given Azure Blob Storage blobs from src to dest. This can\n+    handle directory or file paths.\n+\n+    Args:\n+      src_dest_pairs: List of (src, dest) tuples of\n+                      azfs://<storage-account>/<container>/[name] file paths\n+                      to copy from src to dest.\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) in the same order as the\n+      src_dest_pairs argument, where exception is None if the operation\n+      succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    if not src_dest_pairs:\n+      return []\n+\n+    results = []\n+\n+    for src_path, dest_path in src_dest_pairs:\n+      # Case 1. They are directories.\n+      if src_path.endswith('/') and dest_path.endswith('/'):\n+        try:\n+          results += self.copy_tree(src_path, dest_path)\n+        except BlobStorageError as e:\n+          results.append((src_path, dest_path, e))\n+\n+      # Case 2. They are individual blobs.\n+      elif not src_path.endswith('/') and not dest_path.endswith('/'):\n+        try:\n+          self.copy(src_path, dest_path)\n+          results.append((src_path, dest_path, None))\n+        except BlobStorageError as e:\n+          results.append((src_path, dest_path, e))\n+\n+      # Mismatched paths (one directory, one non-directory) get an error.\n+      else:\n+        e = BlobStorageError(\n+            \"Unable to copy mismatched paths\" +\n+            \"(directory, non-directory): %s, %s\" % (src_path, dest_path),\n+            400)\n+        results.append((src_path, dest_path, e))\n+\n+    return results\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def rename(self, src, dest):\n+    \"\"\"Renames the given Azure Blob Storage blob from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    self.copy(src, dest)\n+    self.delete(src)\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def rename_files(self, src_dest_pairs):\n+    \"\"\"Renames the given Azure Blob Storage blobs from src to dest.\n+\n+    Args:\n+      src_dest_pairs: List of (src, dest) tuples of\n+                      azfs://<storage-account>/<container>/[name]\n+                      file paths to rename from src to dest.\n+    Returns: List of tuples of (src, dest, exception) in the same order as the\n+             src_dest_pairs argument, where exception is None if the operation\n+             succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    if not src_dest_pairs:\n+      return []\n+\n+    for src, dest in src_dest_pairs:\n+      if src.endswith('/') or dest.endswith('/'):\n+        raise ValueError('Unable to rename a directory.')\n+\n+    # Results from copy operation.\n+    copy_results = self.copy_paths(src_dest_pairs)\n+    paths_to_delete = \\\n+        [src for (src, _, error) in copy_results if error is None]\n+    # Results from delete operation.\n+    delete_results = self.delete_files(paths_to_delete)\n+\n+    # Get rename file results (list of tuples).\n+    results = []\n+\n+    # Using a dictionary will make the operation faster.\n+    delete_results_dict = {src: error for (src, error) in delete_results}\n+\n+    for src, dest, error in copy_results:\n+      # If there was an error in the copy operation.\n+      if error is not None:\n+        results.append((src, dest, error))\n+      # If there was an error in the delete operation.\n+      elif delete_results_dict[src] is not None:\n+        results.append((src, dest, delete_results_dict[src]))\n+      # If there was no error in the operations.\n+      else:\n+        results.append((src, dest, None))\n+\n+    return results\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def exists(self, path):\n+    \"\"\"Returns whether the given Azure Blob Storage blob exists.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      blob_to_check.get_blob_properties()\n+      return True\n+    except ResourceNotFoundError as e:\n+      if e.status_code == 404:\n+        # HTTP 404 indicates that the file did not exist.\n+        return False\n+      else:\n+        # We re-raise all other exceptions.\n+        raise\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def size(self, path):\n+    \"\"\"Returns the size of a single Blob Storage blob.\n+\n+    This method does not perform glob expansion. Hence the\n+    given path must be for a single Blob Storage blob.\n+\n+    Returns: size of the Blob Storage blob in bytes.\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    return properties.size\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def last_updated(self, path):\n+    \"\"\"Returns the last updated epoch time of a single\n+    Azure Blob Storage blob.\n+\n+    This method does not perform glob expansion. Hence the\n+    given path must be for a single Azure Blob Storage blob.\n+\n+    Returns: last updated time of the Azure Blob Storage blob\n+    in seconds.\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    datatime = properties.last_modified\n+    return (\n+        time.mktime(datatime.timetuple()) - time.timezone +\n+        datatime.microsecond / 1000000.0)\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def checksum(self, path):\n+    \"\"\"Looks up the checksum of an Azure Blob Storage blob.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    return properties.etag\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def delete(self, path):\n+    \"\"\"Deletes a single blob at the given Azure Blob Storage path.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_delete = self.client.get_blob_client(container, blob)\n+    try:\n+      blob_to_delete.delete_blob()\n+    except ResourceNotFoundError as e:\n+      if e.status_code == 404:\n+        # Return success when the file doesn't exist anymore for idempotency.\n+        return\n+      else:\n+        logging.error('HTTP error while deleting file %s', path)\n+        raise e\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def delete_paths(self, paths):\n+    \"\"\"Deletes the given Azure Blob Storage blobs from src to dest.\n+    This can handle directory or file paths.\n+\n+    Args:\n+      paths: list of Azure Blob Storage paths in the form\n+             azfs://<storage-account>/<container>/[name] that give the\n+             file blobs to be deleted.\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) in the same order as the\n+      src_dest_pairs argument, where exception is 202 if the operation\n+      succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    directories, blobs = [], []\n+\n+    # Retrieve directories and not directories.\n+    for path in paths:\n+      if path.endswith('/'):\n+        directories.append(path)\n+      else:\n+        blobs.append(path)\n+\n+    results = {}\n+\n+    for directory in directories:\n+      directory_result = dict(self.delete_tree(directory))\n+      results.update(directory_result)\n+\n+    blobs_results = dict(self.delete_files(blobs))\n+    results.update(blobs_results)\n+\n+    return results\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def delete_tree(self, root):\n+    \"\"\"Deletes all blobs under the given Azure BlobStorage virtual\n+    directory.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name]\n+            (ending with a \"/\").\n+\n+    Returns:\n+      List of tuples of (path, exception), where each path is a blob\n+      under the given root. exception is 202 if the operation succeeded\n+      or the relevant exception if the operation failed.\n+    \"\"\"\n+    assert root.endswith('/')\n+\n+    # Get the blob under the root directory.\n+    paths_to_delete = self.list_prefix(root)\n+\n+    return self.delete_files(paths_to_delete)\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def delete_files(self, paths):\n+    \"\"\"Deletes the given Azure Blob Storage blobs from src to dest.\n+\n+    Args:\n+      paths: list of Azure Blob Storage paths in the form\n+             azfs://<storage-account>/<container>/[name] that give the\n+             file blobs to be deleted.\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) in the same order as the\n+      src_dest_pairs argument, where exception is 202 if the operation\n+      succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    if not paths:\n+      return []\n+\n+    # Group blobs into containers.\n+    containers, blobs = zip(*[parse_azfs_path(path, get_account=False) \\\n+        for path in paths])\n+\n+    grouped_blobs = {container: [] for container in containers}\n+\n+    # Fill dictionary.\n+    for container, blob in zip(containers, blobs):\n+      grouped_blobs[container].append(blob)\n+\n+    results = {}\n+\n+    # Delete minibatches of blobs for each container.\n+    for container, blobs in grouped_blobs.items():\n+      for i in range(0, len(blobs), MAX_BATCH_OPERATION_SIZE):\n+        blobs_to_delete = blobs[i:i + MAX_BATCH_OPERATION_SIZE]\n+        results.update(self._delete_batch(container, blobs_to_delete))\n+\n+    final_results = \\\n+        [(path, results[parse_azfs_path(path, get_account=False)]) \\\n+        for path in paths]\n+\n+    return final_results\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def _delete_batch(self, container, blobs):\n+    \"\"\"A helper method. Azure Blob Storage Python Client allows batch\n+    deletions for blobs within the same container.\n+\n+    Args:\n+      container: container name.\n+      blobs: list of blobs to be deleted.\n+\n+    Returns:\n+      Dictionary of the form {(container, blob): error}, where error is\n+      202 if the operation succeeded.\n+    \"\"\"\n+    container_client = self.client.get_container_client(container)\n+    results = {}\n+\n+    try:\n+      response = container_client.delete_blobs(\n+          *blobs, raise_on_any_failure=False)\n+\n+      for blob, error in zip(blobs, response):\n+        results[(container, blob)] = error.status_code\n+\n+    except BlobStorageError as e:\n+      for blob in blobs:\n+        results[(container, blob)] = e.message\n+\n+    return results\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def list_prefix(self, path):\n+    \"\"\"Lists files matching the prefix.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+\n+    Returns:\n+      Dictionary of file name -> size.\n+    \"\"\"\n+    storage_account, container, blob = parse_azfs_path(\n+        path, blob_optional=True, get_account=True)\n+    file_sizes = {}\n+    counter = 0\n+    start_time = time.time()\n+\n+    logging.info(\"Starting the size estimation of the input\")\n+    container_client = self.client.get_container_client(container)\n+\n+    while True:\n+      response = container_client.list_blobs(name_starts_with=blob)\n+      for item in response:\n+        file_name = \"azfs://%s/%s/%s\" % (storage_account, container, item.name)\n+        file_sizes[file_name] = item.size\n+        counter += 1\n+        if counter % 10000 == 0:\n+          logging.info(\"Finished computing size of: %s files\", len(file_sizes))\n+      break\n+\n+    logging.info(\n+        \"Finished listing %s files in %s seconds.\",\n+        counter,\n+        time.time() - start_time)\n+    return file_sizes\n+\n+\n+class BlobStorageDownloader(Downloader):\n+  def __init__(self, client, path, buffer_size):\n+    self._client = client\n+    self._path = path\n+    self._container, self._blob = parse_azfs_path(path)\n+    self._buffer_size = buffer_size\n+\n+    self._blob_to_download = self._client.get_blob_client(\n+        self._container, self._blob)\n+\n+    try:\n+      properties = self._get_object_properties()\n+    except ResourceNotFoundError as http_error:\n+      if http_error.status_code == 404:\n+        raise IOError(errno.ENOENT, 'Not found: %s' % self._path)\n+      else:\n+        _LOGGER.error(\n+            'HTTP error while requesting file %s: %s', self._path, http_error)\n+        raise\n+\n+    self._size = properties.size\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def _get_object_properties(self):\n+    return self._blob_to_download.get_blob_properties()\n+\n+  @property\n+  def size(self):\n+    return self._size\n+\n+  def get_range(self, start, end):\n+    # Download_blob first parameter is offset and second is length (exclusive).\n+    blob_data = self._blob_to_download.download_blob(start, end - start)\n+    # Returns the content as bytes.\n+    return blob_data.readall()\n+\n+\n+class BlobStorageUploader(Uploader):\n+  def __init__(self, client, path, mime_type='application/octet-stream'):\n+    self._client = client\n+    self._path = path\n+    self._container, self._blob = parse_azfs_path(path)\n+    self._content_settings = ContentSettings(mime_type)\n+\n+    self._blob_to_upload = self._client.get_blob_client(\n+        self._container, self._blob)\n+\n+    # Temporary file.\n+    self._temporary_file = tempfile.NamedTemporaryFile()\n+\n+  def put(self, data):\n+    self._temporary_file.write(data.tobytes())\n+\n+  def finish(self):\n+    self._temporary_file.seek(0)\n+    # The temporary file is deleted immediately after the operation.\n+    with open(self._temporary_file.name, \"rb\") as f:\n+      self._blob_to_upload.upload_blob(\n+          f.read(), overwrite=True, content_settings=self._content_settings)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njc1OTk1MA=="}, "originalCommit": {"oid": "4c5ab4ca7b4f137dce5f65a6349e0f8f1aefecb5"}, "originalPosition": 664}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzAwNDA5OQ==", "bodyText": "@pabloem What happens when we're trying to upload a large file?\nAzure complains when you try to upload large files although in the documentation it states: Calls to write a blob, write a block, or write a page are permitted 10 minutes per megabyte to complete. If an operation is taking longer than 10 minutes per megabyte on average, it will time out.\nRefer to this issue as well: https://github.com/Azure/azure-sdk-for-python/issues/12166", "url": "https://github.com/apache/beam/pull/12492#discussion_r477004099", "createdAt": "2020-08-26T03:01:55Z", "author": {"login": "AldairCoronel"}, "path": "sdks/python/apache_beam/io/azure/blobstorageio.py", "diffHunk": "@@ -0,0 +1,664 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"Azure Blob Storage client.\n+\"\"\"\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import errno\n+import io\n+import logging\n+import os\n+import re\n+import tempfile\n+import time\n+from builtins import object\n+\n+from apache_beam.io.filesystemio import Downloader\n+from apache_beam.io.filesystemio import DownloaderStream\n+from apache_beam.io.filesystemio import Uploader\n+from apache_beam.io.filesystemio import UploaderStream\n+from apache_beam.utils import retry\n+\n+_LOGGER = logging.getLogger(__name__)\n+\n+try:\n+  # pylint: disable=wrong-import-order, wrong-import-position\n+  # pylint: disable=ungrouped-imports\n+  from azure.core.exceptions import ResourceNotFoundError\n+  from azure.storage.blob import (\n+      BlobServiceClient,\n+      ContentSettings,\n+  )\n+  AZURE_DEPS_INSTALLED = True\n+except ImportError:\n+  AZURE_DEPS_INSTALLED = False\n+\n+DEFAULT_READ_BUFFER_SIZE = 16 * 1024 * 1024\n+\n+MAX_BATCH_OPERATION_SIZE = 100\n+\n+\n+def parse_azfs_path(azfs_path, blob_optional=False, get_account=False):\n+  \"\"\"Return the storage account, the container and\n+  blob names of the given azfs:// path.\n+  \"\"\"\n+  match = re.match(\n+      '^azfs://([a-z0-9]{3,24})/([a-z0-9](?![a-z0-9-]*--[a-z0-9-]*)'\n+      '[a-z0-9-]{1,61}[a-z0-9])/(.*)$',\n+      azfs_path)\n+  if match is None or (match.group(3) == '' and not blob_optional):\n+    raise ValueError(\n+        'Azure Blob Storage path must be in the form '\n+        'azfs://<storage-account>/<container>/<path>.')\n+  result = None\n+  if get_account:\n+    result = match.group(1), match.group(2), match.group(3)\n+  else:\n+    result = match.group(2), match.group(3)\n+  return result\n+\n+\n+def get_azfs_url(storage_account, container, blob=''):\n+  \"\"\"Returns the url in the form of\n+   https://account.blob.core.windows.net/container/blob-name\n+  \"\"\"\n+  return 'https://' + storage_account + '.blob.core.windows.net/' + \\\n+          container + '/' + blob\n+\n+\n+class Blob():\n+  \"\"\"A Blob in Azure Blob Storage.\"\"\"\n+  def __init__(self, etag, name, last_updated, size, mime_type):\n+    self.etag = etag\n+    self.name = name\n+    self.last_updated = last_updated\n+    self.size = size\n+    self.mime_type = mime_type\n+\n+\n+class BlobStorageIOError(IOError, retry.PermanentException):\n+  \"\"\"Blob Strorage IO error that should not be retried.\"\"\"\n+  pass\n+\n+\n+class BlobStorageError(Exception):\n+  \"\"\"Blob Storage client error.\"\"\"\n+  def __init__(self, message=None, code=None):\n+    self.message = message\n+    self.code = code\n+\n+\n+class BlobStorageIO(object):\n+  \"\"\"Azure Blob Storage I/O client.\"\"\"\n+  def __init__(self, client=None):\n+    connect_str = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n+    if client is None:\n+      self.client = BlobServiceClient.from_connection_string(connect_str)\n+    else:\n+      self.client = client\n+    if not AZURE_DEPS_INSTALLED:\n+      raise RuntimeError('Azure dependencies are not installed. Unable to run.')\n+\n+  def open(\n+      self,\n+      filename,\n+      mode='r',\n+      read_buffer_size=DEFAULT_READ_BUFFER_SIZE,\n+      mime_type='application/octet-stream'):\n+    \"\"\"Open an Azure Blob Storage file path for reading or writing.\n+\n+    Args:\n+      filename (str): Azure Blob Storage file path in the form\n+                      ``azfs://<storage-account>/<container>/<path>``.\n+      mode (str): ``'r'`` for reading or ``'w'`` for writing.\n+      read_buffer_size (int): Buffer size to use during read operations.\n+      mime_type (str): Mime type to set for write operations.\n+\n+    Returns:\n+      Azure Blob Storage file object.\n+    Raises:\n+      ValueError: Invalid open file mode.\n+    \"\"\"\n+    if mode == 'r' or mode == 'rb':\n+      downloader = BlobStorageDownloader(\n+          self.client, filename, buffer_size=read_buffer_size)\n+      return io.BufferedReader(\n+          DownloaderStream(\n+              downloader, read_buffer_size=read_buffer_size, mode=mode),\n+          buffer_size=read_buffer_size)\n+    elif mode == 'w' or mode == 'wb':\n+      uploader = BlobStorageUploader(self.client, filename, mime_type)\n+      return io.BufferedWriter(\n+          UploaderStream(uploader, mode=mode), buffer_size=128 * 1024)\n+    else:\n+      raise ValueError('Invalid file open mode: %s.' % mode)\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def copy(self, src, dest):\n+    \"\"\"Copies a single Azure Blob Storage blob from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+\n+    Raises:\n+      TimeoutError: on timeout.\n+    \"\"\"\n+    src_storage_account, src_container, src_blob = parse_azfs_path(\n+        src, get_account=True)\n+    dest_container, dest_blob = parse_azfs_path(dest)\n+\n+    source_blob = get_azfs_url(src_storage_account, src_container, src_blob)\n+    copied_blob = self.client.get_blob_client(dest_container, dest_blob)\n+\n+    try:\n+      copied_blob.start_copy_from_url(source_blob)\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy operation is already an idempotent operation protected\n+  # by retry decorators.\n+  def copy_tree(self, src, dest):\n+    \"\"\"Renames the given Azure Blob storage directory and its contents\n+    recursively from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) where exception is None if the\n+      operation succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    assert src.endswith('/')\n+    assert dest.endswith('/')\n+\n+    results = []\n+    for entry in self.list_prefix(src):\n+      rel_path = entry[len(src):]\n+      try:\n+        self.copy(entry, dest + rel_path)\n+        results.append((entry, dest + rel_path, None))\n+      except BlobStorageError as e:\n+        results.append((entry, dest + rel_path, e))\n+\n+    return results\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy operation is already an idempotent operation protected\n+  # by retry decorators.\n+  def copy_paths(self, src_dest_pairs):\n+    \"\"\"Copies the given Azure Blob Storage blobs from src to dest. This can\n+    handle directory or file paths.\n+\n+    Args:\n+      src_dest_pairs: List of (src, dest) tuples of\n+                      azfs://<storage-account>/<container>/[name] file paths\n+                      to copy from src to dest.\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) in the same order as the\n+      src_dest_pairs argument, where exception is None if the operation\n+      succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    if not src_dest_pairs:\n+      return []\n+\n+    results = []\n+\n+    for src_path, dest_path in src_dest_pairs:\n+      # Case 1. They are directories.\n+      if src_path.endswith('/') and dest_path.endswith('/'):\n+        try:\n+          results += self.copy_tree(src_path, dest_path)\n+        except BlobStorageError as e:\n+          results.append((src_path, dest_path, e))\n+\n+      # Case 2. They are individual blobs.\n+      elif not src_path.endswith('/') and not dest_path.endswith('/'):\n+        try:\n+          self.copy(src_path, dest_path)\n+          results.append((src_path, dest_path, None))\n+        except BlobStorageError as e:\n+          results.append((src_path, dest_path, e))\n+\n+      # Mismatched paths (one directory, one non-directory) get an error.\n+      else:\n+        e = BlobStorageError(\n+            \"Unable to copy mismatched paths\" +\n+            \"(directory, non-directory): %s, %s\" % (src_path, dest_path),\n+            400)\n+        results.append((src_path, dest_path, e))\n+\n+    return results\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def rename(self, src, dest):\n+    \"\"\"Renames the given Azure Blob Storage blob from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    self.copy(src, dest)\n+    self.delete(src)\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def rename_files(self, src_dest_pairs):\n+    \"\"\"Renames the given Azure Blob Storage blobs from src to dest.\n+\n+    Args:\n+      src_dest_pairs: List of (src, dest) tuples of\n+                      azfs://<storage-account>/<container>/[name]\n+                      file paths to rename from src to dest.\n+    Returns: List of tuples of (src, dest, exception) in the same order as the\n+             src_dest_pairs argument, where exception is None if the operation\n+             succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    if not src_dest_pairs:\n+      return []\n+\n+    for src, dest in src_dest_pairs:\n+      if src.endswith('/') or dest.endswith('/'):\n+        raise ValueError('Unable to rename a directory.')\n+\n+    # Results from copy operation.\n+    copy_results = self.copy_paths(src_dest_pairs)\n+    paths_to_delete = \\\n+        [src for (src, _, error) in copy_results if error is None]\n+    # Results from delete operation.\n+    delete_results = self.delete_files(paths_to_delete)\n+\n+    # Get rename file results (list of tuples).\n+    results = []\n+\n+    # Using a dictionary will make the operation faster.\n+    delete_results_dict = {src: error for (src, error) in delete_results}\n+\n+    for src, dest, error in copy_results:\n+      # If there was an error in the copy operation.\n+      if error is not None:\n+        results.append((src, dest, error))\n+      # If there was an error in the delete operation.\n+      elif delete_results_dict[src] is not None:\n+        results.append((src, dest, delete_results_dict[src]))\n+      # If there was no error in the operations.\n+      else:\n+        results.append((src, dest, None))\n+\n+    return results\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def exists(self, path):\n+    \"\"\"Returns whether the given Azure Blob Storage blob exists.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      blob_to_check.get_blob_properties()\n+      return True\n+    except ResourceNotFoundError as e:\n+      if e.status_code == 404:\n+        # HTTP 404 indicates that the file did not exist.\n+        return False\n+      else:\n+        # We re-raise all other exceptions.\n+        raise\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def size(self, path):\n+    \"\"\"Returns the size of a single Blob Storage blob.\n+\n+    This method does not perform glob expansion. Hence the\n+    given path must be for a single Blob Storage blob.\n+\n+    Returns: size of the Blob Storage blob in bytes.\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    return properties.size\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def last_updated(self, path):\n+    \"\"\"Returns the last updated epoch time of a single\n+    Azure Blob Storage blob.\n+\n+    This method does not perform glob expansion. Hence the\n+    given path must be for a single Azure Blob Storage blob.\n+\n+    Returns: last updated time of the Azure Blob Storage blob\n+    in seconds.\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    datatime = properties.last_modified\n+    return (\n+        time.mktime(datatime.timetuple()) - time.timezone +\n+        datatime.microsecond / 1000000.0)\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def checksum(self, path):\n+    \"\"\"Looks up the checksum of an Azure Blob Storage blob.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    return properties.etag\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def delete(self, path):\n+    \"\"\"Deletes a single blob at the given Azure Blob Storage path.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_delete = self.client.get_blob_client(container, blob)\n+    try:\n+      blob_to_delete.delete_blob()\n+    except ResourceNotFoundError as e:\n+      if e.status_code == 404:\n+        # Return success when the file doesn't exist anymore for idempotency.\n+        return\n+      else:\n+        logging.error('HTTP error while deleting file %s', path)\n+        raise e\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def delete_paths(self, paths):\n+    \"\"\"Deletes the given Azure Blob Storage blobs from src to dest.\n+    This can handle directory or file paths.\n+\n+    Args:\n+      paths: list of Azure Blob Storage paths in the form\n+             azfs://<storage-account>/<container>/[name] that give the\n+             file blobs to be deleted.\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) in the same order as the\n+      src_dest_pairs argument, where exception is 202 if the operation\n+      succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    directories, blobs = [], []\n+\n+    # Retrieve directories and not directories.\n+    for path in paths:\n+      if path.endswith('/'):\n+        directories.append(path)\n+      else:\n+        blobs.append(path)\n+\n+    results = {}\n+\n+    for directory in directories:\n+      directory_result = dict(self.delete_tree(directory))\n+      results.update(directory_result)\n+\n+    blobs_results = dict(self.delete_files(blobs))\n+    results.update(blobs_results)\n+\n+    return results\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def delete_tree(self, root):\n+    \"\"\"Deletes all blobs under the given Azure BlobStorage virtual\n+    directory.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name]\n+            (ending with a \"/\").\n+\n+    Returns:\n+      List of tuples of (path, exception), where each path is a blob\n+      under the given root. exception is 202 if the operation succeeded\n+      or the relevant exception if the operation failed.\n+    \"\"\"\n+    assert root.endswith('/')\n+\n+    # Get the blob under the root directory.\n+    paths_to_delete = self.list_prefix(root)\n+\n+    return self.delete_files(paths_to_delete)\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def delete_files(self, paths):\n+    \"\"\"Deletes the given Azure Blob Storage blobs from src to dest.\n+\n+    Args:\n+      paths: list of Azure Blob Storage paths in the form\n+             azfs://<storage-account>/<container>/[name] that give the\n+             file blobs to be deleted.\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) in the same order as the\n+      src_dest_pairs argument, where exception is 202 if the operation\n+      succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    if not paths:\n+      return []\n+\n+    # Group blobs into containers.\n+    containers, blobs = zip(*[parse_azfs_path(path, get_account=False) \\\n+        for path in paths])\n+\n+    grouped_blobs = {container: [] for container in containers}\n+\n+    # Fill dictionary.\n+    for container, blob in zip(containers, blobs):\n+      grouped_blobs[container].append(blob)\n+\n+    results = {}\n+\n+    # Delete minibatches of blobs for each container.\n+    for container, blobs in grouped_blobs.items():\n+      for i in range(0, len(blobs), MAX_BATCH_OPERATION_SIZE):\n+        blobs_to_delete = blobs[i:i + MAX_BATCH_OPERATION_SIZE]\n+        results.update(self._delete_batch(container, blobs_to_delete))\n+\n+    final_results = \\\n+        [(path, results[parse_azfs_path(path, get_account=False)]) \\\n+        for path in paths]\n+\n+    return final_results\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def _delete_batch(self, container, blobs):\n+    \"\"\"A helper method. Azure Blob Storage Python Client allows batch\n+    deletions for blobs within the same container.\n+\n+    Args:\n+      container: container name.\n+      blobs: list of blobs to be deleted.\n+\n+    Returns:\n+      Dictionary of the form {(container, blob): error}, where error is\n+      202 if the operation succeeded.\n+    \"\"\"\n+    container_client = self.client.get_container_client(container)\n+    results = {}\n+\n+    try:\n+      response = container_client.delete_blobs(\n+          *blobs, raise_on_any_failure=False)\n+\n+      for blob, error in zip(blobs, response):\n+        results[(container, blob)] = error.status_code\n+\n+    except BlobStorageError as e:\n+      for blob in blobs:\n+        results[(container, blob)] = e.message\n+\n+    return results\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def list_prefix(self, path):\n+    \"\"\"Lists files matching the prefix.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+\n+    Returns:\n+      Dictionary of file name -> size.\n+    \"\"\"\n+    storage_account, container, blob = parse_azfs_path(\n+        path, blob_optional=True, get_account=True)\n+    file_sizes = {}\n+    counter = 0\n+    start_time = time.time()\n+\n+    logging.info(\"Starting the size estimation of the input\")\n+    container_client = self.client.get_container_client(container)\n+\n+    while True:\n+      response = container_client.list_blobs(name_starts_with=blob)\n+      for item in response:\n+        file_name = \"azfs://%s/%s/%s\" % (storage_account, container, item.name)\n+        file_sizes[file_name] = item.size\n+        counter += 1\n+        if counter % 10000 == 0:\n+          logging.info(\"Finished computing size of: %s files\", len(file_sizes))\n+      break\n+\n+    logging.info(\n+        \"Finished listing %s files in %s seconds.\",\n+        counter,\n+        time.time() - start_time)\n+    return file_sizes\n+\n+\n+class BlobStorageDownloader(Downloader):\n+  def __init__(self, client, path, buffer_size):\n+    self._client = client\n+    self._path = path\n+    self._container, self._blob = parse_azfs_path(path)\n+    self._buffer_size = buffer_size\n+\n+    self._blob_to_download = self._client.get_blob_client(\n+        self._container, self._blob)\n+\n+    try:\n+      properties = self._get_object_properties()\n+    except ResourceNotFoundError as http_error:\n+      if http_error.status_code == 404:\n+        raise IOError(errno.ENOENT, 'Not found: %s' % self._path)\n+      else:\n+        _LOGGER.error(\n+            'HTTP error while requesting file %s: %s', self._path, http_error)\n+        raise\n+\n+    self._size = properties.size\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def _get_object_properties(self):\n+    return self._blob_to_download.get_blob_properties()\n+\n+  @property\n+  def size(self):\n+    return self._size\n+\n+  def get_range(self, start, end):\n+    # Download_blob first parameter is offset and second is length (exclusive).\n+    blob_data = self._blob_to_download.download_blob(start, end - start)\n+    # Returns the content as bytes.\n+    return blob_data.readall()\n+\n+\n+class BlobStorageUploader(Uploader):\n+  def __init__(self, client, path, mime_type='application/octet-stream'):\n+    self._client = client\n+    self._path = path\n+    self._container, self._blob = parse_azfs_path(path)\n+    self._content_settings = ContentSettings(mime_type)\n+\n+    self._blob_to_upload = self._client.get_blob_client(\n+        self._container, self._blob)\n+\n+    # Temporary file.\n+    self._temporary_file = tempfile.NamedTemporaryFile()\n+\n+  def put(self, data):\n+    self._temporary_file.write(data.tobytes())\n+\n+  def finish(self):\n+    self._temporary_file.seek(0)\n+    # The temporary file is deleted immediately after the operation.\n+    with open(self._temporary_file.name, \"rb\") as f:\n+      self._blob_to_upload.upload_blob(\n+          f.read(), overwrite=True, content_settings=self._content_settings)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njc1OTk1MA=="}, "originalCommit": {"oid": "4c5ab4ca7b4f137dce5f65a6349e0f8f1aefecb5"}, "originalPosition": 664}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzYxOTI1MA==", "bodyText": "Authentication. At the moment the only way to authenticate is with a connection string obtained from environment variables.\n\nOkay this is not acceptable. We need to enable authentication via pipeline options as we already discussed privately. This PR is ready to go, but we need to enable pipelineoptions-based authentication in a follow up, okay?\nAlso, please address comments by @epicfaace to catch PartialBatchErrorException, and then we can move forward to merge this change.", "url": "https://github.com/apache/beam/pull/12492#discussion_r477619250", "createdAt": "2020-08-26T22:14:43Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/io/azure/blobstorageio.py", "diffHunk": "@@ -0,0 +1,664 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"Azure Blob Storage client.\n+\"\"\"\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import errno\n+import io\n+import logging\n+import os\n+import re\n+import tempfile\n+import time\n+from builtins import object\n+\n+from apache_beam.io.filesystemio import Downloader\n+from apache_beam.io.filesystemio import DownloaderStream\n+from apache_beam.io.filesystemio import Uploader\n+from apache_beam.io.filesystemio import UploaderStream\n+from apache_beam.utils import retry\n+\n+_LOGGER = logging.getLogger(__name__)\n+\n+try:\n+  # pylint: disable=wrong-import-order, wrong-import-position\n+  # pylint: disable=ungrouped-imports\n+  from azure.core.exceptions import ResourceNotFoundError\n+  from azure.storage.blob import (\n+      BlobServiceClient,\n+      ContentSettings,\n+  )\n+  AZURE_DEPS_INSTALLED = True\n+except ImportError:\n+  AZURE_DEPS_INSTALLED = False\n+\n+DEFAULT_READ_BUFFER_SIZE = 16 * 1024 * 1024\n+\n+MAX_BATCH_OPERATION_SIZE = 100\n+\n+\n+def parse_azfs_path(azfs_path, blob_optional=False, get_account=False):\n+  \"\"\"Return the storage account, the container and\n+  blob names of the given azfs:// path.\n+  \"\"\"\n+  match = re.match(\n+      '^azfs://([a-z0-9]{3,24})/([a-z0-9](?![a-z0-9-]*--[a-z0-9-]*)'\n+      '[a-z0-9-]{1,61}[a-z0-9])/(.*)$',\n+      azfs_path)\n+  if match is None or (match.group(3) == '' and not blob_optional):\n+    raise ValueError(\n+        'Azure Blob Storage path must be in the form '\n+        'azfs://<storage-account>/<container>/<path>.')\n+  result = None\n+  if get_account:\n+    result = match.group(1), match.group(2), match.group(3)\n+  else:\n+    result = match.group(2), match.group(3)\n+  return result\n+\n+\n+def get_azfs_url(storage_account, container, blob=''):\n+  \"\"\"Returns the url in the form of\n+   https://account.blob.core.windows.net/container/blob-name\n+  \"\"\"\n+  return 'https://' + storage_account + '.blob.core.windows.net/' + \\\n+          container + '/' + blob\n+\n+\n+class Blob():\n+  \"\"\"A Blob in Azure Blob Storage.\"\"\"\n+  def __init__(self, etag, name, last_updated, size, mime_type):\n+    self.etag = etag\n+    self.name = name\n+    self.last_updated = last_updated\n+    self.size = size\n+    self.mime_type = mime_type\n+\n+\n+class BlobStorageIOError(IOError, retry.PermanentException):\n+  \"\"\"Blob Strorage IO error that should not be retried.\"\"\"\n+  pass\n+\n+\n+class BlobStorageError(Exception):\n+  \"\"\"Blob Storage client error.\"\"\"\n+  def __init__(self, message=None, code=None):\n+    self.message = message\n+    self.code = code\n+\n+\n+class BlobStorageIO(object):\n+  \"\"\"Azure Blob Storage I/O client.\"\"\"\n+  def __init__(self, client=None):\n+    connect_str = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n+    if client is None:\n+      self.client = BlobServiceClient.from_connection_string(connect_str)\n+    else:\n+      self.client = client\n+    if not AZURE_DEPS_INSTALLED:\n+      raise RuntimeError('Azure dependencies are not installed. Unable to run.')\n+\n+  def open(\n+      self,\n+      filename,\n+      mode='r',\n+      read_buffer_size=DEFAULT_READ_BUFFER_SIZE,\n+      mime_type='application/octet-stream'):\n+    \"\"\"Open an Azure Blob Storage file path for reading or writing.\n+\n+    Args:\n+      filename (str): Azure Blob Storage file path in the form\n+                      ``azfs://<storage-account>/<container>/<path>``.\n+      mode (str): ``'r'`` for reading or ``'w'`` for writing.\n+      read_buffer_size (int): Buffer size to use during read operations.\n+      mime_type (str): Mime type to set for write operations.\n+\n+    Returns:\n+      Azure Blob Storage file object.\n+    Raises:\n+      ValueError: Invalid open file mode.\n+    \"\"\"\n+    if mode == 'r' or mode == 'rb':\n+      downloader = BlobStorageDownloader(\n+          self.client, filename, buffer_size=read_buffer_size)\n+      return io.BufferedReader(\n+          DownloaderStream(\n+              downloader, read_buffer_size=read_buffer_size, mode=mode),\n+          buffer_size=read_buffer_size)\n+    elif mode == 'w' or mode == 'wb':\n+      uploader = BlobStorageUploader(self.client, filename, mime_type)\n+      return io.BufferedWriter(\n+          UploaderStream(uploader, mode=mode), buffer_size=128 * 1024)\n+    else:\n+      raise ValueError('Invalid file open mode: %s.' % mode)\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def copy(self, src, dest):\n+    \"\"\"Copies a single Azure Blob Storage blob from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+\n+    Raises:\n+      TimeoutError: on timeout.\n+    \"\"\"\n+    src_storage_account, src_container, src_blob = parse_azfs_path(\n+        src, get_account=True)\n+    dest_container, dest_blob = parse_azfs_path(dest)\n+\n+    source_blob = get_azfs_url(src_storage_account, src_container, src_blob)\n+    copied_blob = self.client.get_blob_client(dest_container, dest_blob)\n+\n+    try:\n+      copied_blob.start_copy_from_url(source_blob)\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy operation is already an idempotent operation protected\n+  # by retry decorators.\n+  def copy_tree(self, src, dest):\n+    \"\"\"Renames the given Azure Blob storage directory and its contents\n+    recursively from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) where exception is None if the\n+      operation succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    assert src.endswith('/')\n+    assert dest.endswith('/')\n+\n+    results = []\n+    for entry in self.list_prefix(src):\n+      rel_path = entry[len(src):]\n+      try:\n+        self.copy(entry, dest + rel_path)\n+        results.append((entry, dest + rel_path, None))\n+      except BlobStorageError as e:\n+        results.append((entry, dest + rel_path, e))\n+\n+    return results\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy operation is already an idempotent operation protected\n+  # by retry decorators.\n+  def copy_paths(self, src_dest_pairs):\n+    \"\"\"Copies the given Azure Blob Storage blobs from src to dest. This can\n+    handle directory or file paths.\n+\n+    Args:\n+      src_dest_pairs: List of (src, dest) tuples of\n+                      azfs://<storage-account>/<container>/[name] file paths\n+                      to copy from src to dest.\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) in the same order as the\n+      src_dest_pairs argument, where exception is None if the operation\n+      succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    if not src_dest_pairs:\n+      return []\n+\n+    results = []\n+\n+    for src_path, dest_path in src_dest_pairs:\n+      # Case 1. They are directories.\n+      if src_path.endswith('/') and dest_path.endswith('/'):\n+        try:\n+          results += self.copy_tree(src_path, dest_path)\n+        except BlobStorageError as e:\n+          results.append((src_path, dest_path, e))\n+\n+      # Case 2. They are individual blobs.\n+      elif not src_path.endswith('/') and not dest_path.endswith('/'):\n+        try:\n+          self.copy(src_path, dest_path)\n+          results.append((src_path, dest_path, None))\n+        except BlobStorageError as e:\n+          results.append((src_path, dest_path, e))\n+\n+      # Mismatched paths (one directory, one non-directory) get an error.\n+      else:\n+        e = BlobStorageError(\n+            \"Unable to copy mismatched paths\" +\n+            \"(directory, non-directory): %s, %s\" % (src_path, dest_path),\n+            400)\n+        results.append((src_path, dest_path, e))\n+\n+    return results\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def rename(self, src, dest):\n+    \"\"\"Renames the given Azure Blob Storage blob from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    self.copy(src, dest)\n+    self.delete(src)\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def rename_files(self, src_dest_pairs):\n+    \"\"\"Renames the given Azure Blob Storage blobs from src to dest.\n+\n+    Args:\n+      src_dest_pairs: List of (src, dest) tuples of\n+                      azfs://<storage-account>/<container>/[name]\n+                      file paths to rename from src to dest.\n+    Returns: List of tuples of (src, dest, exception) in the same order as the\n+             src_dest_pairs argument, where exception is None if the operation\n+             succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    if not src_dest_pairs:\n+      return []\n+\n+    for src, dest in src_dest_pairs:\n+      if src.endswith('/') or dest.endswith('/'):\n+        raise ValueError('Unable to rename a directory.')\n+\n+    # Results from copy operation.\n+    copy_results = self.copy_paths(src_dest_pairs)\n+    paths_to_delete = \\\n+        [src for (src, _, error) in copy_results if error is None]\n+    # Results from delete operation.\n+    delete_results = self.delete_files(paths_to_delete)\n+\n+    # Get rename file results (list of tuples).\n+    results = []\n+\n+    # Using a dictionary will make the operation faster.\n+    delete_results_dict = {src: error for (src, error) in delete_results}\n+\n+    for src, dest, error in copy_results:\n+      # If there was an error in the copy operation.\n+      if error is not None:\n+        results.append((src, dest, error))\n+      # If there was an error in the delete operation.\n+      elif delete_results_dict[src] is not None:\n+        results.append((src, dest, delete_results_dict[src]))\n+      # If there was no error in the operations.\n+      else:\n+        results.append((src, dest, None))\n+\n+    return results\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def exists(self, path):\n+    \"\"\"Returns whether the given Azure Blob Storage blob exists.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      blob_to_check.get_blob_properties()\n+      return True\n+    except ResourceNotFoundError as e:\n+      if e.status_code == 404:\n+        # HTTP 404 indicates that the file did not exist.\n+        return False\n+      else:\n+        # We re-raise all other exceptions.\n+        raise\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def size(self, path):\n+    \"\"\"Returns the size of a single Blob Storage blob.\n+\n+    This method does not perform glob expansion. Hence the\n+    given path must be for a single Blob Storage blob.\n+\n+    Returns: size of the Blob Storage blob in bytes.\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    return properties.size\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def last_updated(self, path):\n+    \"\"\"Returns the last updated epoch time of a single\n+    Azure Blob Storage blob.\n+\n+    This method does not perform glob expansion. Hence the\n+    given path must be for a single Azure Blob Storage blob.\n+\n+    Returns: last updated time of the Azure Blob Storage blob\n+    in seconds.\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    datatime = properties.last_modified\n+    return (\n+        time.mktime(datatime.timetuple()) - time.timezone +\n+        datatime.microsecond / 1000000.0)\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def checksum(self, path):\n+    \"\"\"Looks up the checksum of an Azure Blob Storage blob.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    return properties.etag\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def delete(self, path):\n+    \"\"\"Deletes a single blob at the given Azure Blob Storage path.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_delete = self.client.get_blob_client(container, blob)\n+    try:\n+      blob_to_delete.delete_blob()\n+    except ResourceNotFoundError as e:\n+      if e.status_code == 404:\n+        # Return success when the file doesn't exist anymore for idempotency.\n+        return\n+      else:\n+        logging.error('HTTP error while deleting file %s', path)\n+        raise e\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def delete_paths(self, paths):\n+    \"\"\"Deletes the given Azure Blob Storage blobs from src to dest.\n+    This can handle directory or file paths.\n+\n+    Args:\n+      paths: list of Azure Blob Storage paths in the form\n+             azfs://<storage-account>/<container>/[name] that give the\n+             file blobs to be deleted.\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) in the same order as the\n+      src_dest_pairs argument, where exception is 202 if the operation\n+      succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    directories, blobs = [], []\n+\n+    # Retrieve directories and not directories.\n+    for path in paths:\n+      if path.endswith('/'):\n+        directories.append(path)\n+      else:\n+        blobs.append(path)\n+\n+    results = {}\n+\n+    for directory in directories:\n+      directory_result = dict(self.delete_tree(directory))\n+      results.update(directory_result)\n+\n+    blobs_results = dict(self.delete_files(blobs))\n+    results.update(blobs_results)\n+\n+    return results\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def delete_tree(self, root):\n+    \"\"\"Deletes all blobs under the given Azure BlobStorage virtual\n+    directory.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name]\n+            (ending with a \"/\").\n+\n+    Returns:\n+      List of tuples of (path, exception), where each path is a blob\n+      under the given root. exception is 202 if the operation succeeded\n+      or the relevant exception if the operation failed.\n+    \"\"\"\n+    assert root.endswith('/')\n+\n+    # Get the blob under the root directory.\n+    paths_to_delete = self.list_prefix(root)\n+\n+    return self.delete_files(paths_to_delete)\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def delete_files(self, paths):\n+    \"\"\"Deletes the given Azure Blob Storage blobs from src to dest.\n+\n+    Args:\n+      paths: list of Azure Blob Storage paths in the form\n+             azfs://<storage-account>/<container>/[name] that give the\n+             file blobs to be deleted.\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) in the same order as the\n+      src_dest_pairs argument, where exception is 202 if the operation\n+      succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    if not paths:\n+      return []\n+\n+    # Group blobs into containers.\n+    containers, blobs = zip(*[parse_azfs_path(path, get_account=False) \\\n+        for path in paths])\n+\n+    grouped_blobs = {container: [] for container in containers}\n+\n+    # Fill dictionary.\n+    for container, blob in zip(containers, blobs):\n+      grouped_blobs[container].append(blob)\n+\n+    results = {}\n+\n+    # Delete minibatches of blobs for each container.\n+    for container, blobs in grouped_blobs.items():\n+      for i in range(0, len(blobs), MAX_BATCH_OPERATION_SIZE):\n+        blobs_to_delete = blobs[i:i + MAX_BATCH_OPERATION_SIZE]\n+        results.update(self._delete_batch(container, blobs_to_delete))\n+\n+    final_results = \\\n+        [(path, results[parse_azfs_path(path, get_account=False)]) \\\n+        for path in paths]\n+\n+    return final_results\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def _delete_batch(self, container, blobs):\n+    \"\"\"A helper method. Azure Blob Storage Python Client allows batch\n+    deletions for blobs within the same container.\n+\n+    Args:\n+      container: container name.\n+      blobs: list of blobs to be deleted.\n+\n+    Returns:\n+      Dictionary of the form {(container, blob): error}, where error is\n+      202 if the operation succeeded.\n+    \"\"\"\n+    container_client = self.client.get_container_client(container)\n+    results = {}\n+\n+    try:\n+      response = container_client.delete_blobs(\n+          *blobs, raise_on_any_failure=False)\n+\n+      for blob, error in zip(blobs, response):\n+        results[(container, blob)] = error.status_code\n+\n+    except BlobStorageError as e:\n+      for blob in blobs:\n+        results[(container, blob)] = e.message\n+\n+    return results\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def list_prefix(self, path):\n+    \"\"\"Lists files matching the prefix.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+\n+    Returns:\n+      Dictionary of file name -> size.\n+    \"\"\"\n+    storage_account, container, blob = parse_azfs_path(\n+        path, blob_optional=True, get_account=True)\n+    file_sizes = {}\n+    counter = 0\n+    start_time = time.time()\n+\n+    logging.info(\"Starting the size estimation of the input\")\n+    container_client = self.client.get_container_client(container)\n+\n+    while True:\n+      response = container_client.list_blobs(name_starts_with=blob)\n+      for item in response:\n+        file_name = \"azfs://%s/%s/%s\" % (storage_account, container, item.name)\n+        file_sizes[file_name] = item.size\n+        counter += 1\n+        if counter % 10000 == 0:\n+          logging.info(\"Finished computing size of: %s files\", len(file_sizes))\n+      break\n+\n+    logging.info(\n+        \"Finished listing %s files in %s seconds.\",\n+        counter,\n+        time.time() - start_time)\n+    return file_sizes\n+\n+\n+class BlobStorageDownloader(Downloader):\n+  def __init__(self, client, path, buffer_size):\n+    self._client = client\n+    self._path = path\n+    self._container, self._blob = parse_azfs_path(path)\n+    self._buffer_size = buffer_size\n+\n+    self._blob_to_download = self._client.get_blob_client(\n+        self._container, self._blob)\n+\n+    try:\n+      properties = self._get_object_properties()\n+    except ResourceNotFoundError as http_error:\n+      if http_error.status_code == 404:\n+        raise IOError(errno.ENOENT, 'Not found: %s' % self._path)\n+      else:\n+        _LOGGER.error(\n+            'HTTP error while requesting file %s: %s', self._path, http_error)\n+        raise\n+\n+    self._size = properties.size\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def _get_object_properties(self):\n+    return self._blob_to_download.get_blob_properties()\n+\n+  @property\n+  def size(self):\n+    return self._size\n+\n+  def get_range(self, start, end):\n+    # Download_blob first parameter is offset and second is length (exclusive).\n+    blob_data = self._blob_to_download.download_blob(start, end - start)\n+    # Returns the content as bytes.\n+    return blob_data.readall()\n+\n+\n+class BlobStorageUploader(Uploader):\n+  def __init__(self, client, path, mime_type='application/octet-stream'):\n+    self._client = client\n+    self._path = path\n+    self._container, self._blob = parse_azfs_path(path)\n+    self._content_settings = ContentSettings(mime_type)\n+\n+    self._blob_to_upload = self._client.get_blob_client(\n+        self._container, self._blob)\n+\n+    # Temporary file.\n+    self._temporary_file = tempfile.NamedTemporaryFile()\n+\n+  def put(self, data):\n+    self._temporary_file.write(data.tobytes())\n+\n+  def finish(self):\n+    self._temporary_file.seek(0)\n+    # The temporary file is deleted immediately after the operation.\n+    with open(self._temporary_file.name, \"rb\") as f:\n+      self._blob_to_upload.upload_blob(\n+          f.read(), overwrite=True, content_settings=self._content_settings)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njc1OTk1MA=="}, "originalCommit": {"oid": "4c5ab4ca7b4f137dce5f65a6349e0f8f1aefecb5"}, "originalPosition": 664}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4MDczNDI0OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/io/azure/blobstorageio.py", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQyMTo1NDozOVrOHGsAJg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yOFQwMjo0MDoxOVrOHInkTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njc3NDQzOA==", "bodyText": "@AldairCoronel not sure if you've faced this issue when testing yourself, but when I tried using this code in my own project, I ran into this error: Azure/azure-sdk-for-python#13183\nI had to work around it by calling delete_blob instead of delete_blobs: codalab/codalab-worksheets@1e3dd30.\nNot sure if you faced a similar issue, but adding this here in case it's helpful.", "url": "https://github.com/apache/beam/pull/12492#discussion_r476774438", "createdAt": "2020-08-25T21:54:39Z", "author": {"login": "epicfaace"}, "path": "sdks/python/apache_beam/io/azure/blobstorageio.py", "diffHunk": "@@ -0,0 +1,664 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"Azure Blob Storage client.\n+\"\"\"\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import errno\n+import io\n+import logging\n+import os\n+import re\n+import tempfile\n+import time\n+from builtins import object\n+\n+from apache_beam.io.filesystemio import Downloader\n+from apache_beam.io.filesystemio import DownloaderStream\n+from apache_beam.io.filesystemio import Uploader\n+from apache_beam.io.filesystemio import UploaderStream\n+from apache_beam.utils import retry\n+\n+_LOGGER = logging.getLogger(__name__)\n+\n+try:\n+  # pylint: disable=wrong-import-order, wrong-import-position\n+  # pylint: disable=ungrouped-imports\n+  from azure.core.exceptions import ResourceNotFoundError\n+  from azure.storage.blob import (\n+      BlobServiceClient,\n+      ContentSettings,\n+  )\n+  AZURE_DEPS_INSTALLED = True\n+except ImportError:\n+  AZURE_DEPS_INSTALLED = False\n+\n+DEFAULT_READ_BUFFER_SIZE = 16 * 1024 * 1024\n+\n+MAX_BATCH_OPERATION_SIZE = 100\n+\n+\n+def parse_azfs_path(azfs_path, blob_optional=False, get_account=False):\n+  \"\"\"Return the storage account, the container and\n+  blob names of the given azfs:// path.\n+  \"\"\"\n+  match = re.match(\n+      '^azfs://([a-z0-9]{3,24})/([a-z0-9](?![a-z0-9-]*--[a-z0-9-]*)'\n+      '[a-z0-9-]{1,61}[a-z0-9])/(.*)$',\n+      azfs_path)\n+  if match is None or (match.group(3) == '' and not blob_optional):\n+    raise ValueError(\n+        'Azure Blob Storage path must be in the form '\n+        'azfs://<storage-account>/<container>/<path>.')\n+  result = None\n+  if get_account:\n+    result = match.group(1), match.group(2), match.group(3)\n+  else:\n+    result = match.group(2), match.group(3)\n+  return result\n+\n+\n+def get_azfs_url(storage_account, container, blob=''):\n+  \"\"\"Returns the url in the form of\n+   https://account.blob.core.windows.net/container/blob-name\n+  \"\"\"\n+  return 'https://' + storage_account + '.blob.core.windows.net/' + \\\n+          container + '/' + blob\n+\n+\n+class Blob():\n+  \"\"\"A Blob in Azure Blob Storage.\"\"\"\n+  def __init__(self, etag, name, last_updated, size, mime_type):\n+    self.etag = etag\n+    self.name = name\n+    self.last_updated = last_updated\n+    self.size = size\n+    self.mime_type = mime_type\n+\n+\n+class BlobStorageIOError(IOError, retry.PermanentException):\n+  \"\"\"Blob Strorage IO error that should not be retried.\"\"\"\n+  pass\n+\n+\n+class BlobStorageError(Exception):\n+  \"\"\"Blob Storage client error.\"\"\"\n+  def __init__(self, message=None, code=None):\n+    self.message = message\n+    self.code = code\n+\n+\n+class BlobStorageIO(object):\n+  \"\"\"Azure Blob Storage I/O client.\"\"\"\n+  def __init__(self, client=None):\n+    connect_str = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n+    if client is None:\n+      self.client = BlobServiceClient.from_connection_string(connect_str)\n+    else:\n+      self.client = client\n+    if not AZURE_DEPS_INSTALLED:\n+      raise RuntimeError('Azure dependencies are not installed. Unable to run.')\n+\n+  def open(\n+      self,\n+      filename,\n+      mode='r',\n+      read_buffer_size=DEFAULT_READ_BUFFER_SIZE,\n+      mime_type='application/octet-stream'):\n+    \"\"\"Open an Azure Blob Storage file path for reading or writing.\n+\n+    Args:\n+      filename (str): Azure Blob Storage file path in the form\n+                      ``azfs://<storage-account>/<container>/<path>``.\n+      mode (str): ``'r'`` for reading or ``'w'`` for writing.\n+      read_buffer_size (int): Buffer size to use during read operations.\n+      mime_type (str): Mime type to set for write operations.\n+\n+    Returns:\n+      Azure Blob Storage file object.\n+    Raises:\n+      ValueError: Invalid open file mode.\n+    \"\"\"\n+    if mode == 'r' or mode == 'rb':\n+      downloader = BlobStorageDownloader(\n+          self.client, filename, buffer_size=read_buffer_size)\n+      return io.BufferedReader(\n+          DownloaderStream(\n+              downloader, read_buffer_size=read_buffer_size, mode=mode),\n+          buffer_size=read_buffer_size)\n+    elif mode == 'w' or mode == 'wb':\n+      uploader = BlobStorageUploader(self.client, filename, mime_type)\n+      return io.BufferedWriter(\n+          UploaderStream(uploader, mode=mode), buffer_size=128 * 1024)\n+    else:\n+      raise ValueError('Invalid file open mode: %s.' % mode)\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def copy(self, src, dest):\n+    \"\"\"Copies a single Azure Blob Storage blob from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+\n+    Raises:\n+      TimeoutError: on timeout.\n+    \"\"\"\n+    src_storage_account, src_container, src_blob = parse_azfs_path(\n+        src, get_account=True)\n+    dest_container, dest_blob = parse_azfs_path(dest)\n+\n+    source_blob = get_azfs_url(src_storage_account, src_container, src_blob)\n+    copied_blob = self.client.get_blob_client(dest_container, dest_blob)\n+\n+    try:\n+      copied_blob.start_copy_from_url(source_blob)\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy operation is already an idempotent operation protected\n+  # by retry decorators.\n+  def copy_tree(self, src, dest):\n+    \"\"\"Renames the given Azure Blob storage directory and its contents\n+    recursively from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) where exception is None if the\n+      operation succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    assert src.endswith('/')\n+    assert dest.endswith('/')\n+\n+    results = []\n+    for entry in self.list_prefix(src):\n+      rel_path = entry[len(src):]\n+      try:\n+        self.copy(entry, dest + rel_path)\n+        results.append((entry, dest + rel_path, None))\n+      except BlobStorageError as e:\n+        results.append((entry, dest + rel_path, e))\n+\n+    return results\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy operation is already an idempotent operation protected\n+  # by retry decorators.\n+  def copy_paths(self, src_dest_pairs):\n+    \"\"\"Copies the given Azure Blob Storage blobs from src to dest. This can\n+    handle directory or file paths.\n+\n+    Args:\n+      src_dest_pairs: List of (src, dest) tuples of\n+                      azfs://<storage-account>/<container>/[name] file paths\n+                      to copy from src to dest.\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) in the same order as the\n+      src_dest_pairs argument, where exception is None if the operation\n+      succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    if not src_dest_pairs:\n+      return []\n+\n+    results = []\n+\n+    for src_path, dest_path in src_dest_pairs:\n+      # Case 1. They are directories.\n+      if src_path.endswith('/') and dest_path.endswith('/'):\n+        try:\n+          results += self.copy_tree(src_path, dest_path)\n+        except BlobStorageError as e:\n+          results.append((src_path, dest_path, e))\n+\n+      # Case 2. They are individual blobs.\n+      elif not src_path.endswith('/') and not dest_path.endswith('/'):\n+        try:\n+          self.copy(src_path, dest_path)\n+          results.append((src_path, dest_path, None))\n+        except BlobStorageError as e:\n+          results.append((src_path, dest_path, e))\n+\n+      # Mismatched paths (one directory, one non-directory) get an error.\n+      else:\n+        e = BlobStorageError(\n+            \"Unable to copy mismatched paths\" +\n+            \"(directory, non-directory): %s, %s\" % (src_path, dest_path),\n+            400)\n+        results.append((src_path, dest_path, e))\n+\n+    return results\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def rename(self, src, dest):\n+    \"\"\"Renames the given Azure Blob Storage blob from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    self.copy(src, dest)\n+    self.delete(src)\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def rename_files(self, src_dest_pairs):\n+    \"\"\"Renames the given Azure Blob Storage blobs from src to dest.\n+\n+    Args:\n+      src_dest_pairs: List of (src, dest) tuples of\n+                      azfs://<storage-account>/<container>/[name]\n+                      file paths to rename from src to dest.\n+    Returns: List of tuples of (src, dest, exception) in the same order as the\n+             src_dest_pairs argument, where exception is None if the operation\n+             succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    if not src_dest_pairs:\n+      return []\n+\n+    for src, dest in src_dest_pairs:\n+      if src.endswith('/') or dest.endswith('/'):\n+        raise ValueError('Unable to rename a directory.')\n+\n+    # Results from copy operation.\n+    copy_results = self.copy_paths(src_dest_pairs)\n+    paths_to_delete = \\\n+        [src for (src, _, error) in copy_results if error is None]\n+    # Results from delete operation.\n+    delete_results = self.delete_files(paths_to_delete)\n+\n+    # Get rename file results (list of tuples).\n+    results = []\n+\n+    # Using a dictionary will make the operation faster.\n+    delete_results_dict = {src: error for (src, error) in delete_results}\n+\n+    for src, dest, error in copy_results:\n+      # If there was an error in the copy operation.\n+      if error is not None:\n+        results.append((src, dest, error))\n+      # If there was an error in the delete operation.\n+      elif delete_results_dict[src] is not None:\n+        results.append((src, dest, delete_results_dict[src]))\n+      # If there was no error in the operations.\n+      else:\n+        results.append((src, dest, None))\n+\n+    return results\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def exists(self, path):\n+    \"\"\"Returns whether the given Azure Blob Storage blob exists.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      blob_to_check.get_blob_properties()\n+      return True\n+    except ResourceNotFoundError as e:\n+      if e.status_code == 404:\n+        # HTTP 404 indicates that the file did not exist.\n+        return False\n+      else:\n+        # We re-raise all other exceptions.\n+        raise\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def size(self, path):\n+    \"\"\"Returns the size of a single Blob Storage blob.\n+\n+    This method does not perform glob expansion. Hence the\n+    given path must be for a single Blob Storage blob.\n+\n+    Returns: size of the Blob Storage blob in bytes.\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    return properties.size\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def last_updated(self, path):\n+    \"\"\"Returns the last updated epoch time of a single\n+    Azure Blob Storage blob.\n+\n+    This method does not perform glob expansion. Hence the\n+    given path must be for a single Azure Blob Storage blob.\n+\n+    Returns: last updated time of the Azure Blob Storage blob\n+    in seconds.\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    datatime = properties.last_modified\n+    return (\n+        time.mktime(datatime.timetuple()) - time.timezone +\n+        datatime.microsecond / 1000000.0)\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def checksum(self, path):\n+    \"\"\"Looks up the checksum of an Azure Blob Storage blob.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    return properties.etag\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def delete(self, path):\n+    \"\"\"Deletes a single blob at the given Azure Blob Storage path.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_delete = self.client.get_blob_client(container, blob)\n+    try:\n+      blob_to_delete.delete_blob()\n+    except ResourceNotFoundError as e:\n+      if e.status_code == 404:\n+        # Return success when the file doesn't exist anymore for idempotency.\n+        return\n+      else:\n+        logging.error('HTTP error while deleting file %s', path)\n+        raise e\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def delete_paths(self, paths):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4c5ab4ca7b4f137dce5f65a6349e0f8f1aefecb5"}, "originalPosition": 436}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njk3NjU2Ng==", "bodyText": "@epicfaace It did not give me problems when testing with my Azure account. The only drawback was when testing with Azurite (emulator) because delete_blobs is not implemented yet.\nI will make the changes from delete_blobs to delete_blob in another PR when I add the tests with Azurite.\nThank you very much!", "url": "https://github.com/apache/beam/pull/12492#discussion_r476976566", "createdAt": "2020-08-26T02:05:15Z", "author": {"login": "AldairCoronel"}, "path": "sdks/python/apache_beam/io/azure/blobstorageio.py", "diffHunk": "@@ -0,0 +1,664 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"Azure Blob Storage client.\n+\"\"\"\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import errno\n+import io\n+import logging\n+import os\n+import re\n+import tempfile\n+import time\n+from builtins import object\n+\n+from apache_beam.io.filesystemio import Downloader\n+from apache_beam.io.filesystemio import DownloaderStream\n+from apache_beam.io.filesystemio import Uploader\n+from apache_beam.io.filesystemio import UploaderStream\n+from apache_beam.utils import retry\n+\n+_LOGGER = logging.getLogger(__name__)\n+\n+try:\n+  # pylint: disable=wrong-import-order, wrong-import-position\n+  # pylint: disable=ungrouped-imports\n+  from azure.core.exceptions import ResourceNotFoundError\n+  from azure.storage.blob import (\n+      BlobServiceClient,\n+      ContentSettings,\n+  )\n+  AZURE_DEPS_INSTALLED = True\n+except ImportError:\n+  AZURE_DEPS_INSTALLED = False\n+\n+DEFAULT_READ_BUFFER_SIZE = 16 * 1024 * 1024\n+\n+MAX_BATCH_OPERATION_SIZE = 100\n+\n+\n+def parse_azfs_path(azfs_path, blob_optional=False, get_account=False):\n+  \"\"\"Return the storage account, the container and\n+  blob names of the given azfs:// path.\n+  \"\"\"\n+  match = re.match(\n+      '^azfs://([a-z0-9]{3,24})/([a-z0-9](?![a-z0-9-]*--[a-z0-9-]*)'\n+      '[a-z0-9-]{1,61}[a-z0-9])/(.*)$',\n+      azfs_path)\n+  if match is None or (match.group(3) == '' and not blob_optional):\n+    raise ValueError(\n+        'Azure Blob Storage path must be in the form '\n+        'azfs://<storage-account>/<container>/<path>.')\n+  result = None\n+  if get_account:\n+    result = match.group(1), match.group(2), match.group(3)\n+  else:\n+    result = match.group(2), match.group(3)\n+  return result\n+\n+\n+def get_azfs_url(storage_account, container, blob=''):\n+  \"\"\"Returns the url in the form of\n+   https://account.blob.core.windows.net/container/blob-name\n+  \"\"\"\n+  return 'https://' + storage_account + '.blob.core.windows.net/' + \\\n+          container + '/' + blob\n+\n+\n+class Blob():\n+  \"\"\"A Blob in Azure Blob Storage.\"\"\"\n+  def __init__(self, etag, name, last_updated, size, mime_type):\n+    self.etag = etag\n+    self.name = name\n+    self.last_updated = last_updated\n+    self.size = size\n+    self.mime_type = mime_type\n+\n+\n+class BlobStorageIOError(IOError, retry.PermanentException):\n+  \"\"\"Blob Strorage IO error that should not be retried.\"\"\"\n+  pass\n+\n+\n+class BlobStorageError(Exception):\n+  \"\"\"Blob Storage client error.\"\"\"\n+  def __init__(self, message=None, code=None):\n+    self.message = message\n+    self.code = code\n+\n+\n+class BlobStorageIO(object):\n+  \"\"\"Azure Blob Storage I/O client.\"\"\"\n+  def __init__(self, client=None):\n+    connect_str = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n+    if client is None:\n+      self.client = BlobServiceClient.from_connection_string(connect_str)\n+    else:\n+      self.client = client\n+    if not AZURE_DEPS_INSTALLED:\n+      raise RuntimeError('Azure dependencies are not installed. Unable to run.')\n+\n+  def open(\n+      self,\n+      filename,\n+      mode='r',\n+      read_buffer_size=DEFAULT_READ_BUFFER_SIZE,\n+      mime_type='application/octet-stream'):\n+    \"\"\"Open an Azure Blob Storage file path for reading or writing.\n+\n+    Args:\n+      filename (str): Azure Blob Storage file path in the form\n+                      ``azfs://<storage-account>/<container>/<path>``.\n+      mode (str): ``'r'`` for reading or ``'w'`` for writing.\n+      read_buffer_size (int): Buffer size to use during read operations.\n+      mime_type (str): Mime type to set for write operations.\n+\n+    Returns:\n+      Azure Blob Storage file object.\n+    Raises:\n+      ValueError: Invalid open file mode.\n+    \"\"\"\n+    if mode == 'r' or mode == 'rb':\n+      downloader = BlobStorageDownloader(\n+          self.client, filename, buffer_size=read_buffer_size)\n+      return io.BufferedReader(\n+          DownloaderStream(\n+              downloader, read_buffer_size=read_buffer_size, mode=mode),\n+          buffer_size=read_buffer_size)\n+    elif mode == 'w' or mode == 'wb':\n+      uploader = BlobStorageUploader(self.client, filename, mime_type)\n+      return io.BufferedWriter(\n+          UploaderStream(uploader, mode=mode), buffer_size=128 * 1024)\n+    else:\n+      raise ValueError('Invalid file open mode: %s.' % mode)\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def copy(self, src, dest):\n+    \"\"\"Copies a single Azure Blob Storage blob from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+\n+    Raises:\n+      TimeoutError: on timeout.\n+    \"\"\"\n+    src_storage_account, src_container, src_blob = parse_azfs_path(\n+        src, get_account=True)\n+    dest_container, dest_blob = parse_azfs_path(dest)\n+\n+    source_blob = get_azfs_url(src_storage_account, src_container, src_blob)\n+    copied_blob = self.client.get_blob_client(dest_container, dest_blob)\n+\n+    try:\n+      copied_blob.start_copy_from_url(source_blob)\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy operation is already an idempotent operation protected\n+  # by retry decorators.\n+  def copy_tree(self, src, dest):\n+    \"\"\"Renames the given Azure Blob storage directory and its contents\n+    recursively from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) where exception is None if the\n+      operation succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    assert src.endswith('/')\n+    assert dest.endswith('/')\n+\n+    results = []\n+    for entry in self.list_prefix(src):\n+      rel_path = entry[len(src):]\n+      try:\n+        self.copy(entry, dest + rel_path)\n+        results.append((entry, dest + rel_path, None))\n+      except BlobStorageError as e:\n+        results.append((entry, dest + rel_path, e))\n+\n+    return results\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy operation is already an idempotent operation protected\n+  # by retry decorators.\n+  def copy_paths(self, src_dest_pairs):\n+    \"\"\"Copies the given Azure Blob Storage blobs from src to dest. This can\n+    handle directory or file paths.\n+\n+    Args:\n+      src_dest_pairs: List of (src, dest) tuples of\n+                      azfs://<storage-account>/<container>/[name] file paths\n+                      to copy from src to dest.\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) in the same order as the\n+      src_dest_pairs argument, where exception is None if the operation\n+      succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    if not src_dest_pairs:\n+      return []\n+\n+    results = []\n+\n+    for src_path, dest_path in src_dest_pairs:\n+      # Case 1. They are directories.\n+      if src_path.endswith('/') and dest_path.endswith('/'):\n+        try:\n+          results += self.copy_tree(src_path, dest_path)\n+        except BlobStorageError as e:\n+          results.append((src_path, dest_path, e))\n+\n+      # Case 2. They are individual blobs.\n+      elif not src_path.endswith('/') and not dest_path.endswith('/'):\n+        try:\n+          self.copy(src_path, dest_path)\n+          results.append((src_path, dest_path, None))\n+        except BlobStorageError as e:\n+          results.append((src_path, dest_path, e))\n+\n+      # Mismatched paths (one directory, one non-directory) get an error.\n+      else:\n+        e = BlobStorageError(\n+            \"Unable to copy mismatched paths\" +\n+            \"(directory, non-directory): %s, %s\" % (src_path, dest_path),\n+            400)\n+        results.append((src_path, dest_path, e))\n+\n+    return results\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def rename(self, src, dest):\n+    \"\"\"Renames the given Azure Blob Storage blob from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    self.copy(src, dest)\n+    self.delete(src)\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def rename_files(self, src_dest_pairs):\n+    \"\"\"Renames the given Azure Blob Storage blobs from src to dest.\n+\n+    Args:\n+      src_dest_pairs: List of (src, dest) tuples of\n+                      azfs://<storage-account>/<container>/[name]\n+                      file paths to rename from src to dest.\n+    Returns: List of tuples of (src, dest, exception) in the same order as the\n+             src_dest_pairs argument, where exception is None if the operation\n+             succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    if not src_dest_pairs:\n+      return []\n+\n+    for src, dest in src_dest_pairs:\n+      if src.endswith('/') or dest.endswith('/'):\n+        raise ValueError('Unable to rename a directory.')\n+\n+    # Results from copy operation.\n+    copy_results = self.copy_paths(src_dest_pairs)\n+    paths_to_delete = \\\n+        [src for (src, _, error) in copy_results if error is None]\n+    # Results from delete operation.\n+    delete_results = self.delete_files(paths_to_delete)\n+\n+    # Get rename file results (list of tuples).\n+    results = []\n+\n+    # Using a dictionary will make the operation faster.\n+    delete_results_dict = {src: error for (src, error) in delete_results}\n+\n+    for src, dest, error in copy_results:\n+      # If there was an error in the copy operation.\n+      if error is not None:\n+        results.append((src, dest, error))\n+      # If there was an error in the delete operation.\n+      elif delete_results_dict[src] is not None:\n+        results.append((src, dest, delete_results_dict[src]))\n+      # If there was no error in the operations.\n+      else:\n+        results.append((src, dest, None))\n+\n+    return results\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def exists(self, path):\n+    \"\"\"Returns whether the given Azure Blob Storage blob exists.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      blob_to_check.get_blob_properties()\n+      return True\n+    except ResourceNotFoundError as e:\n+      if e.status_code == 404:\n+        # HTTP 404 indicates that the file did not exist.\n+        return False\n+      else:\n+        # We re-raise all other exceptions.\n+        raise\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def size(self, path):\n+    \"\"\"Returns the size of a single Blob Storage blob.\n+\n+    This method does not perform glob expansion. Hence the\n+    given path must be for a single Blob Storage blob.\n+\n+    Returns: size of the Blob Storage blob in bytes.\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    return properties.size\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def last_updated(self, path):\n+    \"\"\"Returns the last updated epoch time of a single\n+    Azure Blob Storage blob.\n+\n+    This method does not perform glob expansion. Hence the\n+    given path must be for a single Azure Blob Storage blob.\n+\n+    Returns: last updated time of the Azure Blob Storage blob\n+    in seconds.\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    datatime = properties.last_modified\n+    return (\n+        time.mktime(datatime.timetuple()) - time.timezone +\n+        datatime.microsecond / 1000000.0)\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def checksum(self, path):\n+    \"\"\"Looks up the checksum of an Azure Blob Storage blob.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    return properties.etag\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def delete(self, path):\n+    \"\"\"Deletes a single blob at the given Azure Blob Storage path.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_delete = self.client.get_blob_client(container, blob)\n+    try:\n+      blob_to_delete.delete_blob()\n+    except ResourceNotFoundError as e:\n+      if e.status_code == 404:\n+        # Return success when the file doesn't exist anymore for idempotency.\n+        return\n+      else:\n+        logging.error('HTTP error while deleting file %s', path)\n+        raise e\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def delete_paths(self, paths):", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njc3NDQzOA=="}, "originalCommit": {"oid": "4c5ab4ca7b4f137dce5f65a6349e0f8f1aefecb5"}, "originalPosition": 436}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzI4NTk0MQ==", "bodyText": "Interesting. To be clear, I think using delete_blobs would be ideal, since we would only require a single batch request, rather than having to call delete_blob over and over again (which is just a workaround for the error I mentioned above). If it's not supported by Azurite, though, it might be fine to just change it to use the delete_blob workaround.", "url": "https://github.com/apache/beam/pull/12492#discussion_r477285941", "createdAt": "2020-08-26T13:08:23Z", "author": {"login": "epicfaace"}, "path": "sdks/python/apache_beam/io/azure/blobstorageio.py", "diffHunk": "@@ -0,0 +1,664 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"Azure Blob Storage client.\n+\"\"\"\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import errno\n+import io\n+import logging\n+import os\n+import re\n+import tempfile\n+import time\n+from builtins import object\n+\n+from apache_beam.io.filesystemio import Downloader\n+from apache_beam.io.filesystemio import DownloaderStream\n+from apache_beam.io.filesystemio import Uploader\n+from apache_beam.io.filesystemio import UploaderStream\n+from apache_beam.utils import retry\n+\n+_LOGGER = logging.getLogger(__name__)\n+\n+try:\n+  # pylint: disable=wrong-import-order, wrong-import-position\n+  # pylint: disable=ungrouped-imports\n+  from azure.core.exceptions import ResourceNotFoundError\n+  from azure.storage.blob import (\n+      BlobServiceClient,\n+      ContentSettings,\n+  )\n+  AZURE_DEPS_INSTALLED = True\n+except ImportError:\n+  AZURE_DEPS_INSTALLED = False\n+\n+DEFAULT_READ_BUFFER_SIZE = 16 * 1024 * 1024\n+\n+MAX_BATCH_OPERATION_SIZE = 100\n+\n+\n+def parse_azfs_path(azfs_path, blob_optional=False, get_account=False):\n+  \"\"\"Return the storage account, the container and\n+  blob names of the given azfs:// path.\n+  \"\"\"\n+  match = re.match(\n+      '^azfs://([a-z0-9]{3,24})/([a-z0-9](?![a-z0-9-]*--[a-z0-9-]*)'\n+      '[a-z0-9-]{1,61}[a-z0-9])/(.*)$',\n+      azfs_path)\n+  if match is None or (match.group(3) == '' and not blob_optional):\n+    raise ValueError(\n+        'Azure Blob Storage path must be in the form '\n+        'azfs://<storage-account>/<container>/<path>.')\n+  result = None\n+  if get_account:\n+    result = match.group(1), match.group(2), match.group(3)\n+  else:\n+    result = match.group(2), match.group(3)\n+  return result\n+\n+\n+def get_azfs_url(storage_account, container, blob=''):\n+  \"\"\"Returns the url in the form of\n+   https://account.blob.core.windows.net/container/blob-name\n+  \"\"\"\n+  return 'https://' + storage_account + '.blob.core.windows.net/' + \\\n+          container + '/' + blob\n+\n+\n+class Blob():\n+  \"\"\"A Blob in Azure Blob Storage.\"\"\"\n+  def __init__(self, etag, name, last_updated, size, mime_type):\n+    self.etag = etag\n+    self.name = name\n+    self.last_updated = last_updated\n+    self.size = size\n+    self.mime_type = mime_type\n+\n+\n+class BlobStorageIOError(IOError, retry.PermanentException):\n+  \"\"\"Blob Strorage IO error that should not be retried.\"\"\"\n+  pass\n+\n+\n+class BlobStorageError(Exception):\n+  \"\"\"Blob Storage client error.\"\"\"\n+  def __init__(self, message=None, code=None):\n+    self.message = message\n+    self.code = code\n+\n+\n+class BlobStorageIO(object):\n+  \"\"\"Azure Blob Storage I/O client.\"\"\"\n+  def __init__(self, client=None):\n+    connect_str = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n+    if client is None:\n+      self.client = BlobServiceClient.from_connection_string(connect_str)\n+    else:\n+      self.client = client\n+    if not AZURE_DEPS_INSTALLED:\n+      raise RuntimeError('Azure dependencies are not installed. Unable to run.')\n+\n+  def open(\n+      self,\n+      filename,\n+      mode='r',\n+      read_buffer_size=DEFAULT_READ_BUFFER_SIZE,\n+      mime_type='application/octet-stream'):\n+    \"\"\"Open an Azure Blob Storage file path for reading or writing.\n+\n+    Args:\n+      filename (str): Azure Blob Storage file path in the form\n+                      ``azfs://<storage-account>/<container>/<path>``.\n+      mode (str): ``'r'`` for reading or ``'w'`` for writing.\n+      read_buffer_size (int): Buffer size to use during read operations.\n+      mime_type (str): Mime type to set for write operations.\n+\n+    Returns:\n+      Azure Blob Storage file object.\n+    Raises:\n+      ValueError: Invalid open file mode.\n+    \"\"\"\n+    if mode == 'r' or mode == 'rb':\n+      downloader = BlobStorageDownloader(\n+          self.client, filename, buffer_size=read_buffer_size)\n+      return io.BufferedReader(\n+          DownloaderStream(\n+              downloader, read_buffer_size=read_buffer_size, mode=mode),\n+          buffer_size=read_buffer_size)\n+    elif mode == 'w' or mode == 'wb':\n+      uploader = BlobStorageUploader(self.client, filename, mime_type)\n+      return io.BufferedWriter(\n+          UploaderStream(uploader, mode=mode), buffer_size=128 * 1024)\n+    else:\n+      raise ValueError('Invalid file open mode: %s.' % mode)\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def copy(self, src, dest):\n+    \"\"\"Copies a single Azure Blob Storage blob from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+\n+    Raises:\n+      TimeoutError: on timeout.\n+    \"\"\"\n+    src_storage_account, src_container, src_blob = parse_azfs_path(\n+        src, get_account=True)\n+    dest_container, dest_blob = parse_azfs_path(dest)\n+\n+    source_blob = get_azfs_url(src_storage_account, src_container, src_blob)\n+    copied_blob = self.client.get_blob_client(dest_container, dest_blob)\n+\n+    try:\n+      copied_blob.start_copy_from_url(source_blob)\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy operation is already an idempotent operation protected\n+  # by retry decorators.\n+  def copy_tree(self, src, dest):\n+    \"\"\"Renames the given Azure Blob storage directory and its contents\n+    recursively from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) where exception is None if the\n+      operation succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    assert src.endswith('/')\n+    assert dest.endswith('/')\n+\n+    results = []\n+    for entry in self.list_prefix(src):\n+      rel_path = entry[len(src):]\n+      try:\n+        self.copy(entry, dest + rel_path)\n+        results.append((entry, dest + rel_path, None))\n+      except BlobStorageError as e:\n+        results.append((entry, dest + rel_path, e))\n+\n+    return results\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy operation is already an idempotent operation protected\n+  # by retry decorators.\n+  def copy_paths(self, src_dest_pairs):\n+    \"\"\"Copies the given Azure Blob Storage blobs from src to dest. This can\n+    handle directory or file paths.\n+\n+    Args:\n+      src_dest_pairs: List of (src, dest) tuples of\n+                      azfs://<storage-account>/<container>/[name] file paths\n+                      to copy from src to dest.\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) in the same order as the\n+      src_dest_pairs argument, where exception is None if the operation\n+      succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    if not src_dest_pairs:\n+      return []\n+\n+    results = []\n+\n+    for src_path, dest_path in src_dest_pairs:\n+      # Case 1. They are directories.\n+      if src_path.endswith('/') and dest_path.endswith('/'):\n+        try:\n+          results += self.copy_tree(src_path, dest_path)\n+        except BlobStorageError as e:\n+          results.append((src_path, dest_path, e))\n+\n+      # Case 2. They are individual blobs.\n+      elif not src_path.endswith('/') and not dest_path.endswith('/'):\n+        try:\n+          self.copy(src_path, dest_path)\n+          results.append((src_path, dest_path, None))\n+        except BlobStorageError as e:\n+          results.append((src_path, dest_path, e))\n+\n+      # Mismatched paths (one directory, one non-directory) get an error.\n+      else:\n+        e = BlobStorageError(\n+            \"Unable to copy mismatched paths\" +\n+            \"(directory, non-directory): %s, %s\" % (src_path, dest_path),\n+            400)\n+        results.append((src_path, dest_path, e))\n+\n+    return results\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def rename(self, src, dest):\n+    \"\"\"Renames the given Azure Blob Storage blob from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    self.copy(src, dest)\n+    self.delete(src)\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def rename_files(self, src_dest_pairs):\n+    \"\"\"Renames the given Azure Blob Storage blobs from src to dest.\n+\n+    Args:\n+      src_dest_pairs: List of (src, dest) tuples of\n+                      azfs://<storage-account>/<container>/[name]\n+                      file paths to rename from src to dest.\n+    Returns: List of tuples of (src, dest, exception) in the same order as the\n+             src_dest_pairs argument, where exception is None if the operation\n+             succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    if not src_dest_pairs:\n+      return []\n+\n+    for src, dest in src_dest_pairs:\n+      if src.endswith('/') or dest.endswith('/'):\n+        raise ValueError('Unable to rename a directory.')\n+\n+    # Results from copy operation.\n+    copy_results = self.copy_paths(src_dest_pairs)\n+    paths_to_delete = \\\n+        [src for (src, _, error) in copy_results if error is None]\n+    # Results from delete operation.\n+    delete_results = self.delete_files(paths_to_delete)\n+\n+    # Get rename file results (list of tuples).\n+    results = []\n+\n+    # Using a dictionary will make the operation faster.\n+    delete_results_dict = {src: error for (src, error) in delete_results}\n+\n+    for src, dest, error in copy_results:\n+      # If there was an error in the copy operation.\n+      if error is not None:\n+        results.append((src, dest, error))\n+      # If there was an error in the delete operation.\n+      elif delete_results_dict[src] is not None:\n+        results.append((src, dest, delete_results_dict[src]))\n+      # If there was no error in the operations.\n+      else:\n+        results.append((src, dest, None))\n+\n+    return results\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def exists(self, path):\n+    \"\"\"Returns whether the given Azure Blob Storage blob exists.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      blob_to_check.get_blob_properties()\n+      return True\n+    except ResourceNotFoundError as e:\n+      if e.status_code == 404:\n+        # HTTP 404 indicates that the file did not exist.\n+        return False\n+      else:\n+        # We re-raise all other exceptions.\n+        raise\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def size(self, path):\n+    \"\"\"Returns the size of a single Blob Storage blob.\n+\n+    This method does not perform glob expansion. Hence the\n+    given path must be for a single Blob Storage blob.\n+\n+    Returns: size of the Blob Storage blob in bytes.\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    return properties.size\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def last_updated(self, path):\n+    \"\"\"Returns the last updated epoch time of a single\n+    Azure Blob Storage blob.\n+\n+    This method does not perform glob expansion. Hence the\n+    given path must be for a single Azure Blob Storage blob.\n+\n+    Returns: last updated time of the Azure Blob Storage blob\n+    in seconds.\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    datatime = properties.last_modified\n+    return (\n+        time.mktime(datatime.timetuple()) - time.timezone +\n+        datatime.microsecond / 1000000.0)\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def checksum(self, path):\n+    \"\"\"Looks up the checksum of an Azure Blob Storage blob.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    return properties.etag\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def delete(self, path):\n+    \"\"\"Deletes a single blob at the given Azure Blob Storage path.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_delete = self.client.get_blob_client(container, blob)\n+    try:\n+      blob_to_delete.delete_blob()\n+    except ResourceNotFoundError as e:\n+      if e.status_code == 404:\n+        # Return success when the file doesn't exist anymore for idempotency.\n+        return\n+      else:\n+        logging.error('HTTP error while deleting file %s', path)\n+        raise e\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def delete_paths(self, paths):", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njc3NDQzOA=="}, "originalCommit": {"oid": "4c5ab4ca7b4f137dce5f65a6349e0f8f1aefecb5"}, "originalPosition": 436}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3ODc5ODkyNw==", "bodyText": "FYI, it appears that Microsoft might have fixed the delete_blobs issue: Azure/azure-sdk-for-python#13183", "url": "https://github.com/apache/beam/pull/12492#discussion_r478798927", "createdAt": "2020-08-28T02:40:19Z", "author": {"login": "epicfaace"}, "path": "sdks/python/apache_beam/io/azure/blobstorageio.py", "diffHunk": "@@ -0,0 +1,664 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"Azure Blob Storage client.\n+\"\"\"\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import errno\n+import io\n+import logging\n+import os\n+import re\n+import tempfile\n+import time\n+from builtins import object\n+\n+from apache_beam.io.filesystemio import Downloader\n+from apache_beam.io.filesystemio import DownloaderStream\n+from apache_beam.io.filesystemio import Uploader\n+from apache_beam.io.filesystemio import UploaderStream\n+from apache_beam.utils import retry\n+\n+_LOGGER = logging.getLogger(__name__)\n+\n+try:\n+  # pylint: disable=wrong-import-order, wrong-import-position\n+  # pylint: disable=ungrouped-imports\n+  from azure.core.exceptions import ResourceNotFoundError\n+  from azure.storage.blob import (\n+      BlobServiceClient,\n+      ContentSettings,\n+  )\n+  AZURE_DEPS_INSTALLED = True\n+except ImportError:\n+  AZURE_DEPS_INSTALLED = False\n+\n+DEFAULT_READ_BUFFER_SIZE = 16 * 1024 * 1024\n+\n+MAX_BATCH_OPERATION_SIZE = 100\n+\n+\n+def parse_azfs_path(azfs_path, blob_optional=False, get_account=False):\n+  \"\"\"Return the storage account, the container and\n+  blob names of the given azfs:// path.\n+  \"\"\"\n+  match = re.match(\n+      '^azfs://([a-z0-9]{3,24})/([a-z0-9](?![a-z0-9-]*--[a-z0-9-]*)'\n+      '[a-z0-9-]{1,61}[a-z0-9])/(.*)$',\n+      azfs_path)\n+  if match is None or (match.group(3) == '' and not blob_optional):\n+    raise ValueError(\n+        'Azure Blob Storage path must be in the form '\n+        'azfs://<storage-account>/<container>/<path>.')\n+  result = None\n+  if get_account:\n+    result = match.group(1), match.group(2), match.group(3)\n+  else:\n+    result = match.group(2), match.group(3)\n+  return result\n+\n+\n+def get_azfs_url(storage_account, container, blob=''):\n+  \"\"\"Returns the url in the form of\n+   https://account.blob.core.windows.net/container/blob-name\n+  \"\"\"\n+  return 'https://' + storage_account + '.blob.core.windows.net/' + \\\n+          container + '/' + blob\n+\n+\n+class Blob():\n+  \"\"\"A Blob in Azure Blob Storage.\"\"\"\n+  def __init__(self, etag, name, last_updated, size, mime_type):\n+    self.etag = etag\n+    self.name = name\n+    self.last_updated = last_updated\n+    self.size = size\n+    self.mime_type = mime_type\n+\n+\n+class BlobStorageIOError(IOError, retry.PermanentException):\n+  \"\"\"Blob Strorage IO error that should not be retried.\"\"\"\n+  pass\n+\n+\n+class BlobStorageError(Exception):\n+  \"\"\"Blob Storage client error.\"\"\"\n+  def __init__(self, message=None, code=None):\n+    self.message = message\n+    self.code = code\n+\n+\n+class BlobStorageIO(object):\n+  \"\"\"Azure Blob Storage I/O client.\"\"\"\n+  def __init__(self, client=None):\n+    connect_str = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n+    if client is None:\n+      self.client = BlobServiceClient.from_connection_string(connect_str)\n+    else:\n+      self.client = client\n+    if not AZURE_DEPS_INSTALLED:\n+      raise RuntimeError('Azure dependencies are not installed. Unable to run.')\n+\n+  def open(\n+      self,\n+      filename,\n+      mode='r',\n+      read_buffer_size=DEFAULT_READ_BUFFER_SIZE,\n+      mime_type='application/octet-stream'):\n+    \"\"\"Open an Azure Blob Storage file path for reading or writing.\n+\n+    Args:\n+      filename (str): Azure Blob Storage file path in the form\n+                      ``azfs://<storage-account>/<container>/<path>``.\n+      mode (str): ``'r'`` for reading or ``'w'`` for writing.\n+      read_buffer_size (int): Buffer size to use during read operations.\n+      mime_type (str): Mime type to set for write operations.\n+\n+    Returns:\n+      Azure Blob Storage file object.\n+    Raises:\n+      ValueError: Invalid open file mode.\n+    \"\"\"\n+    if mode == 'r' or mode == 'rb':\n+      downloader = BlobStorageDownloader(\n+          self.client, filename, buffer_size=read_buffer_size)\n+      return io.BufferedReader(\n+          DownloaderStream(\n+              downloader, read_buffer_size=read_buffer_size, mode=mode),\n+          buffer_size=read_buffer_size)\n+    elif mode == 'w' or mode == 'wb':\n+      uploader = BlobStorageUploader(self.client, filename, mime_type)\n+      return io.BufferedWriter(\n+          UploaderStream(uploader, mode=mode), buffer_size=128 * 1024)\n+    else:\n+      raise ValueError('Invalid file open mode: %s.' % mode)\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def copy(self, src, dest):\n+    \"\"\"Copies a single Azure Blob Storage blob from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+\n+    Raises:\n+      TimeoutError: on timeout.\n+    \"\"\"\n+    src_storage_account, src_container, src_blob = parse_azfs_path(\n+        src, get_account=True)\n+    dest_container, dest_blob = parse_azfs_path(dest)\n+\n+    source_blob = get_azfs_url(src_storage_account, src_container, src_blob)\n+    copied_blob = self.client.get_blob_client(dest_container, dest_blob)\n+\n+    try:\n+      copied_blob.start_copy_from_url(source_blob)\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy operation is already an idempotent operation protected\n+  # by retry decorators.\n+  def copy_tree(self, src, dest):\n+    \"\"\"Renames the given Azure Blob storage directory and its contents\n+    recursively from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) where exception is None if the\n+      operation succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    assert src.endswith('/')\n+    assert dest.endswith('/')\n+\n+    results = []\n+    for entry in self.list_prefix(src):\n+      rel_path = entry[len(src):]\n+      try:\n+        self.copy(entry, dest + rel_path)\n+        results.append((entry, dest + rel_path, None))\n+      except BlobStorageError as e:\n+        results.append((entry, dest + rel_path, e))\n+\n+    return results\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy operation is already an idempotent operation protected\n+  # by retry decorators.\n+  def copy_paths(self, src_dest_pairs):\n+    \"\"\"Copies the given Azure Blob Storage blobs from src to dest. This can\n+    handle directory or file paths.\n+\n+    Args:\n+      src_dest_pairs: List of (src, dest) tuples of\n+                      azfs://<storage-account>/<container>/[name] file paths\n+                      to copy from src to dest.\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) in the same order as the\n+      src_dest_pairs argument, where exception is None if the operation\n+      succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    if not src_dest_pairs:\n+      return []\n+\n+    results = []\n+\n+    for src_path, dest_path in src_dest_pairs:\n+      # Case 1. They are directories.\n+      if src_path.endswith('/') and dest_path.endswith('/'):\n+        try:\n+          results += self.copy_tree(src_path, dest_path)\n+        except BlobStorageError as e:\n+          results.append((src_path, dest_path, e))\n+\n+      # Case 2. They are individual blobs.\n+      elif not src_path.endswith('/') and not dest_path.endswith('/'):\n+        try:\n+          self.copy(src_path, dest_path)\n+          results.append((src_path, dest_path, None))\n+        except BlobStorageError as e:\n+          results.append((src_path, dest_path, e))\n+\n+      # Mismatched paths (one directory, one non-directory) get an error.\n+      else:\n+        e = BlobStorageError(\n+            \"Unable to copy mismatched paths\" +\n+            \"(directory, non-directory): %s, %s\" % (src_path, dest_path),\n+            400)\n+        results.append((src_path, dest_path, e))\n+\n+    return results\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def rename(self, src, dest):\n+    \"\"\"Renames the given Azure Blob Storage blob from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    self.copy(src, dest)\n+    self.delete(src)\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def rename_files(self, src_dest_pairs):\n+    \"\"\"Renames the given Azure Blob Storage blobs from src to dest.\n+\n+    Args:\n+      src_dest_pairs: List of (src, dest) tuples of\n+                      azfs://<storage-account>/<container>/[name]\n+                      file paths to rename from src to dest.\n+    Returns: List of tuples of (src, dest, exception) in the same order as the\n+             src_dest_pairs argument, where exception is None if the operation\n+             succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    if not src_dest_pairs:\n+      return []\n+\n+    for src, dest in src_dest_pairs:\n+      if src.endswith('/') or dest.endswith('/'):\n+        raise ValueError('Unable to rename a directory.')\n+\n+    # Results from copy operation.\n+    copy_results = self.copy_paths(src_dest_pairs)\n+    paths_to_delete = \\\n+        [src for (src, _, error) in copy_results if error is None]\n+    # Results from delete operation.\n+    delete_results = self.delete_files(paths_to_delete)\n+\n+    # Get rename file results (list of tuples).\n+    results = []\n+\n+    # Using a dictionary will make the operation faster.\n+    delete_results_dict = {src: error for (src, error) in delete_results}\n+\n+    for src, dest, error in copy_results:\n+      # If there was an error in the copy operation.\n+      if error is not None:\n+        results.append((src, dest, error))\n+      # If there was an error in the delete operation.\n+      elif delete_results_dict[src] is not None:\n+        results.append((src, dest, delete_results_dict[src]))\n+      # If there was no error in the operations.\n+      else:\n+        results.append((src, dest, None))\n+\n+    return results\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def exists(self, path):\n+    \"\"\"Returns whether the given Azure Blob Storage blob exists.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      blob_to_check.get_blob_properties()\n+      return True\n+    except ResourceNotFoundError as e:\n+      if e.status_code == 404:\n+        # HTTP 404 indicates that the file did not exist.\n+        return False\n+      else:\n+        # We re-raise all other exceptions.\n+        raise\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def size(self, path):\n+    \"\"\"Returns the size of a single Blob Storage blob.\n+\n+    This method does not perform glob expansion. Hence the\n+    given path must be for a single Blob Storage blob.\n+\n+    Returns: size of the Blob Storage blob in bytes.\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    return properties.size\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def last_updated(self, path):\n+    \"\"\"Returns the last updated epoch time of a single\n+    Azure Blob Storage blob.\n+\n+    This method does not perform glob expansion. Hence the\n+    given path must be for a single Azure Blob Storage blob.\n+\n+    Returns: last updated time of the Azure Blob Storage blob\n+    in seconds.\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    datatime = properties.last_modified\n+    return (\n+        time.mktime(datatime.timetuple()) - time.timezone +\n+        datatime.microsecond / 1000000.0)\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def checksum(self, path):\n+    \"\"\"Looks up the checksum of an Azure Blob Storage blob.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    return properties.etag\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def delete(self, path):\n+    \"\"\"Deletes a single blob at the given Azure Blob Storage path.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_delete = self.client.get_blob_client(container, blob)\n+    try:\n+      blob_to_delete.delete_blob()\n+    except ResourceNotFoundError as e:\n+      if e.status_code == 404:\n+        # Return success when the file doesn't exist anymore for idempotency.\n+        return\n+      else:\n+        logging.error('HTTP error while deleting file %s', path)\n+        raise e\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def delete_paths(self, paths):", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njc3NDQzOA=="}, "originalCommit": {"oid": "4c5ab4ca7b4f137dce5f65a6349e0f8f1aefecb5"}, "originalPosition": 436}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk4MDc0OTY0OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/io/azure/blobstorageio.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQyMTo1Nzo0M1rOHGsKIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNVQyMTo1Nzo0M1rOHGsKIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3Njc3Njk5NA==", "bodyText": "I think you should handle both BlobStorageError and PartialBatchErrorException on all blob storage operations (PartialBatchErrorException is raised in, for example, Azure/azure-sdk-for-python#13183) -- otherwise, what ends up happening is that only the status code from PartialBatchErrorException is retrieved, but the message is silenced and not logged at all.", "url": "https://github.com/apache/beam/pull/12492#discussion_r476776994", "createdAt": "2020-08-25T21:57:43Z", "author": {"login": "epicfaace"}, "path": "sdks/python/apache_beam/io/azure/blobstorageio.py", "diffHunk": "@@ -0,0 +1,664 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"Azure Blob Storage client.\n+\"\"\"\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import errno\n+import io\n+import logging\n+import os\n+import re\n+import tempfile\n+import time\n+from builtins import object\n+\n+from apache_beam.io.filesystemio import Downloader\n+from apache_beam.io.filesystemio import DownloaderStream\n+from apache_beam.io.filesystemio import Uploader\n+from apache_beam.io.filesystemio import UploaderStream\n+from apache_beam.utils import retry\n+\n+_LOGGER = logging.getLogger(__name__)\n+\n+try:\n+  # pylint: disable=wrong-import-order, wrong-import-position\n+  # pylint: disable=ungrouped-imports\n+  from azure.core.exceptions import ResourceNotFoundError\n+  from azure.storage.blob import (\n+      BlobServiceClient,\n+      ContentSettings,\n+  )\n+  AZURE_DEPS_INSTALLED = True\n+except ImportError:\n+  AZURE_DEPS_INSTALLED = False\n+\n+DEFAULT_READ_BUFFER_SIZE = 16 * 1024 * 1024\n+\n+MAX_BATCH_OPERATION_SIZE = 100\n+\n+\n+def parse_azfs_path(azfs_path, blob_optional=False, get_account=False):\n+  \"\"\"Return the storage account, the container and\n+  blob names of the given azfs:// path.\n+  \"\"\"\n+  match = re.match(\n+      '^azfs://([a-z0-9]{3,24})/([a-z0-9](?![a-z0-9-]*--[a-z0-9-]*)'\n+      '[a-z0-9-]{1,61}[a-z0-9])/(.*)$',\n+      azfs_path)\n+  if match is None or (match.group(3) == '' and not blob_optional):\n+    raise ValueError(\n+        'Azure Blob Storage path must be in the form '\n+        'azfs://<storage-account>/<container>/<path>.')\n+  result = None\n+  if get_account:\n+    result = match.group(1), match.group(2), match.group(3)\n+  else:\n+    result = match.group(2), match.group(3)\n+  return result\n+\n+\n+def get_azfs_url(storage_account, container, blob=''):\n+  \"\"\"Returns the url in the form of\n+   https://account.blob.core.windows.net/container/blob-name\n+  \"\"\"\n+  return 'https://' + storage_account + '.blob.core.windows.net/' + \\\n+          container + '/' + blob\n+\n+\n+class Blob():\n+  \"\"\"A Blob in Azure Blob Storage.\"\"\"\n+  def __init__(self, etag, name, last_updated, size, mime_type):\n+    self.etag = etag\n+    self.name = name\n+    self.last_updated = last_updated\n+    self.size = size\n+    self.mime_type = mime_type\n+\n+\n+class BlobStorageIOError(IOError, retry.PermanentException):\n+  \"\"\"Blob Strorage IO error that should not be retried.\"\"\"\n+  pass\n+\n+\n+class BlobStorageError(Exception):\n+  \"\"\"Blob Storage client error.\"\"\"\n+  def __init__(self, message=None, code=None):\n+    self.message = message\n+    self.code = code\n+\n+\n+class BlobStorageIO(object):\n+  \"\"\"Azure Blob Storage I/O client.\"\"\"\n+  def __init__(self, client=None):\n+    connect_str = os.getenv('AZURE_STORAGE_CONNECTION_STRING')\n+    if client is None:\n+      self.client = BlobServiceClient.from_connection_string(connect_str)\n+    else:\n+      self.client = client\n+    if not AZURE_DEPS_INSTALLED:\n+      raise RuntimeError('Azure dependencies are not installed. Unable to run.')\n+\n+  def open(\n+      self,\n+      filename,\n+      mode='r',\n+      read_buffer_size=DEFAULT_READ_BUFFER_SIZE,\n+      mime_type='application/octet-stream'):\n+    \"\"\"Open an Azure Blob Storage file path for reading or writing.\n+\n+    Args:\n+      filename (str): Azure Blob Storage file path in the form\n+                      ``azfs://<storage-account>/<container>/<path>``.\n+      mode (str): ``'r'`` for reading or ``'w'`` for writing.\n+      read_buffer_size (int): Buffer size to use during read operations.\n+      mime_type (str): Mime type to set for write operations.\n+\n+    Returns:\n+      Azure Blob Storage file object.\n+    Raises:\n+      ValueError: Invalid open file mode.\n+    \"\"\"\n+    if mode == 'r' or mode == 'rb':\n+      downloader = BlobStorageDownloader(\n+          self.client, filename, buffer_size=read_buffer_size)\n+      return io.BufferedReader(\n+          DownloaderStream(\n+              downloader, read_buffer_size=read_buffer_size, mode=mode),\n+          buffer_size=read_buffer_size)\n+    elif mode == 'w' or mode == 'wb':\n+      uploader = BlobStorageUploader(self.client, filename, mime_type)\n+      return io.BufferedWriter(\n+          UploaderStream(uploader, mode=mode), buffer_size=128 * 1024)\n+    else:\n+      raise ValueError('Invalid file open mode: %s.' % mode)\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def copy(self, src, dest):\n+    \"\"\"Copies a single Azure Blob Storage blob from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+\n+    Raises:\n+      TimeoutError: on timeout.\n+    \"\"\"\n+    src_storage_account, src_container, src_blob = parse_azfs_path(\n+        src, get_account=True)\n+    dest_container, dest_blob = parse_azfs_path(dest)\n+\n+    source_blob = get_azfs_url(src_storage_account, src_container, src_blob)\n+    copied_blob = self.client.get_blob_client(dest_container, dest_blob)\n+\n+    try:\n+      copied_blob.start_copy_from_url(source_blob)\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy operation is already an idempotent operation protected\n+  # by retry decorators.\n+  def copy_tree(self, src, dest):\n+    \"\"\"Renames the given Azure Blob storage directory and its contents\n+    recursively from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) where exception is None if the\n+      operation succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    assert src.endswith('/')\n+    assert dest.endswith('/')\n+\n+    results = []\n+    for entry in self.list_prefix(src):\n+      rel_path = entry[len(src):]\n+      try:\n+        self.copy(entry, dest + rel_path)\n+        results.append((entry, dest + rel_path, None))\n+      except BlobStorageError as e:\n+        results.append((entry, dest + rel_path, e))\n+\n+    return results\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy operation is already an idempotent operation protected\n+  # by retry decorators.\n+  def copy_paths(self, src_dest_pairs):\n+    \"\"\"Copies the given Azure Blob Storage blobs from src to dest. This can\n+    handle directory or file paths.\n+\n+    Args:\n+      src_dest_pairs: List of (src, dest) tuples of\n+                      azfs://<storage-account>/<container>/[name] file paths\n+                      to copy from src to dest.\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) in the same order as the\n+      src_dest_pairs argument, where exception is None if the operation\n+      succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    if not src_dest_pairs:\n+      return []\n+\n+    results = []\n+\n+    for src_path, dest_path in src_dest_pairs:\n+      # Case 1. They are directories.\n+      if src_path.endswith('/') and dest_path.endswith('/'):\n+        try:\n+          results += self.copy_tree(src_path, dest_path)\n+        except BlobStorageError as e:\n+          results.append((src_path, dest_path, e))\n+\n+      # Case 2. They are individual blobs.\n+      elif not src_path.endswith('/') and not dest_path.endswith('/'):\n+        try:\n+          self.copy(src_path, dest_path)\n+          results.append((src_path, dest_path, None))\n+        except BlobStorageError as e:\n+          results.append((src_path, dest_path, e))\n+\n+      # Mismatched paths (one directory, one non-directory) get an error.\n+      else:\n+        e = BlobStorageError(\n+            \"Unable to copy mismatched paths\" +\n+            \"(directory, non-directory): %s, %s\" % (src_path, dest_path),\n+            400)\n+        results.append((src_path, dest_path, e))\n+\n+    return results\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def rename(self, src, dest):\n+    \"\"\"Renames the given Azure Blob Storage blob from src to dest.\n+\n+    Args:\n+      src: Blob Storage file path pattern in the form\n+           azfs://<storage-account>/<container>/[name].\n+      dest: Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    self.copy(src, dest)\n+    self.delete(src)\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def rename_files(self, src_dest_pairs):\n+    \"\"\"Renames the given Azure Blob Storage blobs from src to dest.\n+\n+    Args:\n+      src_dest_pairs: List of (src, dest) tuples of\n+                      azfs://<storage-account>/<container>/[name]\n+                      file paths to rename from src to dest.\n+    Returns: List of tuples of (src, dest, exception) in the same order as the\n+             src_dest_pairs argument, where exception is None if the operation\n+             succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    if not src_dest_pairs:\n+      return []\n+\n+    for src, dest in src_dest_pairs:\n+      if src.endswith('/') or dest.endswith('/'):\n+        raise ValueError('Unable to rename a directory.')\n+\n+    # Results from copy operation.\n+    copy_results = self.copy_paths(src_dest_pairs)\n+    paths_to_delete = \\\n+        [src for (src, _, error) in copy_results if error is None]\n+    # Results from delete operation.\n+    delete_results = self.delete_files(paths_to_delete)\n+\n+    # Get rename file results (list of tuples).\n+    results = []\n+\n+    # Using a dictionary will make the operation faster.\n+    delete_results_dict = {src: error for (src, error) in delete_results}\n+\n+    for src, dest, error in copy_results:\n+      # If there was an error in the copy operation.\n+      if error is not None:\n+        results.append((src, dest, error))\n+      # If there was an error in the delete operation.\n+      elif delete_results_dict[src] is not None:\n+        results.append((src, dest, delete_results_dict[src]))\n+      # If there was no error in the operations.\n+      else:\n+        results.append((src, dest, None))\n+\n+    return results\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def exists(self, path):\n+    \"\"\"Returns whether the given Azure Blob Storage blob exists.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      blob_to_check.get_blob_properties()\n+      return True\n+    except ResourceNotFoundError as e:\n+      if e.status_code == 404:\n+        # HTTP 404 indicates that the file did not exist.\n+        return False\n+      else:\n+        # We re-raise all other exceptions.\n+        raise\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def size(self, path):\n+    \"\"\"Returns the size of a single Blob Storage blob.\n+\n+    This method does not perform glob expansion. Hence the\n+    given path must be for a single Blob Storage blob.\n+\n+    Returns: size of the Blob Storage blob in bytes.\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    return properties.size\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def last_updated(self, path):\n+    \"\"\"Returns the last updated epoch time of a single\n+    Azure Blob Storage blob.\n+\n+    This method does not perform glob expansion. Hence the\n+    given path must be for a single Azure Blob Storage blob.\n+\n+    Returns: last updated time of the Azure Blob Storage blob\n+    in seconds.\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    datatime = properties.last_modified\n+    return (\n+        time.mktime(datatime.timetuple()) - time.timezone +\n+        datatime.microsecond / 1000000.0)\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def checksum(self, path):\n+    \"\"\"Looks up the checksum of an Azure Blob Storage blob.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_check = self.client.get_blob_client(container, blob)\n+    try:\n+      properties = blob_to_check.get_blob_properties()\n+    except ResourceNotFoundError as e:\n+      message = e.reason\n+      code = e.status_code\n+      raise BlobStorageError(message, code)\n+\n+    return properties.etag\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def delete(self, path):\n+    \"\"\"Deletes a single blob at the given Azure Blob Storage path.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name].\n+    \"\"\"\n+    container, blob = parse_azfs_path(path)\n+    blob_to_delete = self.client.get_blob_client(container, blob)\n+    try:\n+      blob_to_delete.delete_blob()\n+    except ResourceNotFoundError as e:\n+      if e.status_code == 404:\n+        # Return success when the file doesn't exist anymore for idempotency.\n+        return\n+      else:\n+        logging.error('HTTP error while deleting file %s', path)\n+        raise e\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def delete_paths(self, paths):\n+    \"\"\"Deletes the given Azure Blob Storage blobs from src to dest.\n+    This can handle directory or file paths.\n+\n+    Args:\n+      paths: list of Azure Blob Storage paths in the form\n+             azfs://<storage-account>/<container>/[name] that give the\n+             file blobs to be deleted.\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) in the same order as the\n+      src_dest_pairs argument, where exception is 202 if the operation\n+      succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    directories, blobs = [], []\n+\n+    # Retrieve directories and not directories.\n+    for path in paths:\n+      if path.endswith('/'):\n+        directories.append(path)\n+      else:\n+        blobs.append(path)\n+\n+    results = {}\n+\n+    for directory in directories:\n+      directory_result = dict(self.delete_tree(directory))\n+      results.update(directory_result)\n+\n+    blobs_results = dict(self.delete_files(blobs))\n+    results.update(blobs_results)\n+\n+    return results\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def delete_tree(self, root):\n+    \"\"\"Deletes all blobs under the given Azure BlobStorage virtual\n+    directory.\n+\n+    Args:\n+      path: Azure Blob Storage file path pattern in the form\n+            azfs://<storage-account>/<container>/[name]\n+            (ending with a \"/\").\n+\n+    Returns:\n+      List of tuples of (path, exception), where each path is a blob\n+      under the given root. exception is 202 if the operation succeeded\n+      or the relevant exception if the operation failed.\n+    \"\"\"\n+    assert root.endswith('/')\n+\n+    # Get the blob under the root directory.\n+    paths_to_delete = self.list_prefix(root)\n+\n+    return self.delete_files(paths_to_delete)\n+\n+  # We intentionally do not decorate this method with a retry, since the\n+  # underlying copy and delete operations are already idempotent operations\n+  # protected by retry decorators.\n+  def delete_files(self, paths):\n+    \"\"\"Deletes the given Azure Blob Storage blobs from src to dest.\n+\n+    Args:\n+      paths: list of Azure Blob Storage paths in the form\n+             azfs://<storage-account>/<container>/[name] that give the\n+             file blobs to be deleted.\n+\n+    Returns:\n+      List of tuples of (src, dest, exception) in the same order as the\n+      src_dest_pairs argument, where exception is 202 if the operation\n+      succeeded or the relevant exception if the operation failed.\n+    \"\"\"\n+    if not paths:\n+      return []\n+\n+    # Group blobs into containers.\n+    containers, blobs = zip(*[parse_azfs_path(path, get_account=False) \\\n+        for path in paths])\n+\n+    grouped_blobs = {container: [] for container in containers}\n+\n+    # Fill dictionary.\n+    for container, blob in zip(containers, blobs):\n+      grouped_blobs[container].append(blob)\n+\n+    results = {}\n+\n+    # Delete minibatches of blobs for each container.\n+    for container, blobs in grouped_blobs.items():\n+      for i in range(0, len(blobs), MAX_BATCH_OPERATION_SIZE):\n+        blobs_to_delete = blobs[i:i + MAX_BATCH_OPERATION_SIZE]\n+        results.update(self._delete_batch(container, blobs_to_delete))\n+\n+    final_results = \\\n+        [(path, results[parse_azfs_path(path, get_account=False)]) \\\n+        for path in paths]\n+\n+    return final_results\n+\n+  @retry.with_exponential_backoff(\n+      retry_filter=retry.retry_on_beam_io_error_filter)\n+  def _delete_batch(self, container, blobs):\n+    \"\"\"A helper method. Azure Blob Storage Python Client allows batch\n+    deletions for blobs within the same container.\n+\n+    Args:\n+      container: container name.\n+      blobs: list of blobs to be deleted.\n+\n+    Returns:\n+      Dictionary of the form {(container, blob): error}, where error is\n+      202 if the operation succeeded.\n+    \"\"\"\n+    container_client = self.client.get_container_client(container)\n+    results = {}\n+\n+    try:\n+      response = container_client.delete_blobs(\n+          *blobs, raise_on_any_failure=False)\n+\n+      for blob, error in zip(blobs, response):\n+        results[(container, blob)] = error.status_code\n+\n+    except BlobStorageError as e:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4c5ab4ca7b4f137dce5f65a6349e0f8f1aefecb5"}, "originalPosition": 561}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 859, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}