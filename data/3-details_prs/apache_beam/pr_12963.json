{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk0OTM3ODg0", "number": 12963, "title": "[BEAM-10983] Add getting started from Spark page", "bodyText": "Add a new page to get started for users that already know PySpark.\nR: @rosetn\nR: @aaltay\nR: @bradmiro - can you please take a look to check if the Spark code makes sense?\nStaged: http://apache-beam-website-pull-requests.storage.googleapis.com/12963/get-started/from-spark/index.html\n\nThank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:\n\n Choose reviewer(s) and mention them in a comment (R: @username).\n Format the pull request title like [BEAM-XXX] Fixes bug in ApproximateQuantiles, where you replace BEAM-XXX with the appropriate JIRA issue, if applicable. This will automatically link the pull request to the issue.\n Update CHANGES.md with noteworthy changes.\n If this contribution is large, please file an Apache Individual Contributor License Agreement.\n\nSee the Contributor Guide for more tips on how to make review process smoother.\nPost-Commit Tests Status (on master branch)\n\n\n\nLang\nSDK\nDataflow\nFlink\nSamza\nSpark\nTwister2\n\n\n\n\nGo\n\n---\n\n---\n\n---\n\n\nJava\n\n\n\n\n\n\n\n\nPython\n\n\n\n---\n\n---\n\n\nXLang\n\n---\n\n---\n\n---\n\n\n\nPre-Commit Tests Status (on master branch)\n\n\n\n---\nJava\nPython\nGo\nWebsite\nWhitespace\nTypescript\n\n\n\n\nNon-portable\n\n \n\n\n\n\n\n\nPortable\n---\n\n---\n---\n---\n---\n\n\n\nSee .test-infra/jenkins/README for trigger phrase, status and link of all Jenkins jobs.\nGitHub Actions Tests Status (on master branch)\n\n\n\nSee CI.md for more information about GitHub Actions CI.", "createdAt": "2020-09-29T15:54:28Z", "url": "https://github.com/apache/beam/pull/12963", "merged": true, "mergeCommit": {"oid": "2ad28542ec051dac6ebb8f0c6ea0c1b86a70f2cf"}, "closed": true, "closedAt": "2021-01-07T19:19:38Z", "author": {"login": "davidcavazos"}, "timelineItems": {"totalCount": 18, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdNqRH5AH2gAyNDk0OTM3ODg0Ojg1NGVkNWJkMDZmODg4NjA0NDNkZmQwZTMxM2NiYzZmOTg1YzFhNmU=", "endCursor": "Y3Vyc29yOnYyOpPPAAABdmIUJMAH2gAyNDk0OTM3ODg0Ojk1NTc0NzA3ZjA2ODE3N2Q3YmY1OTEyNzlkNGZiMWYyZjgzMDkzNWY=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestCommit", "commit": {"oid": "854ed5bd06f88860443dfd0e313cbc6f985c1a6e", "author": {"user": {"login": "davidcavazos", "name": "David Cavazos"}}, "url": "https://github.com/apache/beam/commit/854ed5bd06f88860443dfd0e313cbc6f985c1a6e", "committedDate": "2020-09-29T15:52:58Z", "message": "[BEAM-10983] Add getting started from Spark page"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d", "author": {"user": {"login": "davidcavazos", "name": "David Cavazos"}}, "url": "https://github.com/apache/beam/commit/c484e68489db6551aff70f6dde765ec0ca89d58d", "committedDate": "2020-09-29T17:17:00Z", "message": "Add \"From Apache Spark\" entry to side menu"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDk4ODQ2NjEw", "url": "https://github.com/apache/beam/pull/12963#pullrequestreview-498846610", "createdAt": "2020-09-29T20:18:18Z", "commit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDoxODoxOFrOHZ_nGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDo1MTozOFrOHaBPsg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAxODY1MQ==", "bodyText": "The Beam and Spark APIs are mostly equivalent. The backend stuff, not so much.\nI might reword this as, \"The Beam and Spark APIs are similar.\"", "url": "https://github.com/apache/beam/pull/12963#discussion_r497018651", "createdAt": "2020-09-29T20:18:18Z", "author": {"login": "ibzib"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,245 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\"); \n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS, \n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+Beam and Spark are mostly equivalent, so you already know the basic concepts.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAyMTYwMw==", "bodyText": "It might be worth mentioning a couple other important details here:\n\nBeam pipelines are constructed lazily, in other words, no computation happens until pipeline.run() is called.\nwith beam.Pipeline() as pipeline implicitly calls pipeline.run().", "url": "https://github.com/apache/beam/pull/12963#discussion_r497021603", "createdAt": "2020-09-29T20:23:45Z", "author": {"login": "ibzib"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,245 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\"); \n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS, \n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+Beam and Spark are mostly equivalent, so you already know the basic concepts.\n+\n+A collection of elements in Spark is called a _Resilient Distributed Dataset_ (RDD),\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    result = (\n+        sc.parallelize([1, 2, 3, 4])\n+        .map(lambda x: x * 2)\n+        .reduce(lambda x, y: x + y)\n+    )\n+    print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAyMjgzNA==", "bodyText": "Nit: should we add a label here too for consistency?", "url": "https://github.com/apache/beam/pull/12963#discussion_r497022834", "createdAt": "2020-09-29T20:25:59Z", "author": {"login": "ibzib"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,245 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\"); \n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS, \n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+Beam and Spark are mostly equivalent, so you already know the basic concepts.\n+\n+A collection of elements in Spark is called a _Resilient Distributed Dataset_ (RDD),\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    result = (\n+        sc.parallelize([1, 2, 3, 4])\n+        .map(lambda x: x * 2)\n+        .reduce(lambda x, y: x + y)\n+    )\n+    print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+A label can optionally be added to a transform using the\n+_right shift operator_ `>>` like `data | 'My description' >> beam.Map(...)`.\n+This serves both as comments and makes your pipeline easier to debug.\n+\n+This is how the pipeline looks after adding labels.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | 'Create numbers' >> beam.Create([1, 2, 3, 4])\n+        | 'Multiply by two' >> beam.Map(lambda x: x * 2)\n+        | 'Sum everything' >> beam.CombineGlobally(sum)\n+        | beam.Map(print)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAyNjgzMA==", "bodyText": "It'd be really nice if each Beam transform was linked to its respective documentation/transforms page.", "url": "https://github.com/apache/beam/pull/12963#discussion_r497026830", "createdAt": "2020-09-29T20:33:16Z", "author": {"login": "ibzib"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,245 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\"); \n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS, \n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+Beam and Spark are mostly equivalent, so you already know the basic concepts.\n+\n+A collection of elements in Spark is called a _Resilient Distributed Dataset_ (RDD),\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    result = (\n+        sc.parallelize([1, 2, 3, 4])\n+        .map(lambda x: x * 2)\n+        .reduce(lambda x, y: x + y)\n+    )\n+    print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+A label can optionally be added to a transform using the\n+_right shift operator_ `>>` like `data | 'My description' >> beam.Map(...)`.\n+This serves both as comments and makes your pipeline easier to debug.\n+\n+This is how the pipeline looks after adding labels.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | 'Create numbers' >> beam.Create([1, 2, 3, 4])\n+        | 'Multiply by two' >> beam.Map(lambda x: x * 2)\n+        | 'Sum everything' >> beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+## Setup\n+\n+Here's a comparison on how to get started both in PySpark and Beam.\n+\n+{{< table >}}\n+<table>\n+<tr>\n+    <th></th>\n+    <th>PySpark</th>\n+    <th>Beam</th>\n+</tr>\n+<tr>\n+    <td><b>Install</b></td>\n+    <td><code>$ pip install pyspark</code></td>\n+    <td><code>$ pip install apache-beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Imports</b></td>\n+    <td><code>import pyspark</code></td>\n+    <td><code>import apache_beam as beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating a<br>local pipeline</b></td>\n+    <td>\n+        <code>with pyspark.SparkContext() as sc:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+    <td>\n+        <code>with beam.Pipeline() as pipeline:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Creating values</b></td>\n+    <td><code>values = sc.parallelize([1, 2, 3, 4])</code></td>\n+    <td><code>values = pipeline | beam.Create([1, 2, 3, 4])</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating<br>key-value pairs</b></td>\n+    <td>\n+        <code>pairs = sc.parallelize([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+    <td>\n+        <code>pairs = pipeline | beam.Create([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Running a<br>local pipeline</b></td>\n+    <td><code>$ spark-submit spark_pipeline.py</code></td>\n+    <td><code>$ python beam_pipeline.py</code></td>\n+</tr>\n+</table>\n+{{< /table >}}\n+\n+## Transforms\n+\n+Here are the equivalents of some common transforms in both PySpark and Beam.\n+\n+{{< table >}}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d"}, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAzMzE2Mw==", "bodyText": "Probably better to use lists instead of tuples in Beam. I'm not sure tuples are adequately tested/supported: https://the-asf.slack.com/archives/CBDNLQZM1/p1598454585014000", "url": "https://github.com/apache/beam/pull/12963#discussion_r497033163", "createdAt": "2020-09-29T20:39:38Z", "author": {"login": "ibzib"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,245 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\"); \n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS, \n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+Beam and Spark are mostly equivalent, so you already know the basic concepts.\n+\n+A collection of elements in Spark is called a _Resilient Distributed Dataset_ (RDD),\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    result = (\n+        sc.parallelize([1, 2, 3, 4])\n+        .map(lambda x: x * 2)\n+        .reduce(lambda x, y: x + y)\n+    )\n+    print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+A label can optionally be added to a transform using the\n+_right shift operator_ `>>` like `data | 'My description' >> beam.Map(...)`.\n+This serves both as comments and makes your pipeline easier to debug.\n+\n+This is how the pipeline looks after adding labels.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | 'Create numbers' >> beam.Create([1, 2, 3, 4])\n+        | 'Multiply by two' >> beam.Map(lambda x: x * 2)\n+        | 'Sum everything' >> beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+## Setup\n+\n+Here's a comparison on how to get started both in PySpark and Beam.\n+\n+{{< table >}}\n+<table>\n+<tr>\n+    <th></th>\n+    <th>PySpark</th>\n+    <th>Beam</th>\n+</tr>\n+<tr>\n+    <td><b>Install</b></td>\n+    <td><code>$ pip install pyspark</code></td>\n+    <td><code>$ pip install apache-beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Imports</b></td>\n+    <td><code>import pyspark</code></td>\n+    <td><code>import apache_beam as beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating a<br>local pipeline</b></td>\n+    <td>\n+        <code>with pyspark.SparkContext() as sc:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+    <td>\n+        <code>with beam.Pipeline() as pipeline:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Creating values</b></td>\n+    <td><code>values = sc.parallelize([1, 2, 3, 4])</code></td>\n+    <td><code>values = pipeline | beam.Create([1, 2, 3, 4])</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating<br>key-value pairs</b></td>\n+    <td>\n+        <code>pairs = sc.parallelize([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+    <td>\n+        <code>pairs = pipeline | beam.Create([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d"}, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAzODM4OQ==", "bodyText": "Nit: clarify what you mean here by \"Python value\" (in this case it's an int value).", "url": "https://github.com/apache/beam/pull/12963#discussion_r497038389", "createdAt": "2020-09-29T20:44:44Z", "author": {"login": "ibzib"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,245 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\"); \n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS, \n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+Beam and Spark are mostly equivalent, so you already know the basic concepts.\n+\n+A collection of elements in Spark is called a _Resilient Distributed Dataset_ (RDD),\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    result = (\n+        sc.parallelize([1, 2, 3, 4])\n+        .map(lambda x: x * 2)\n+        .reduce(lambda x, y: x + y)\n+    )\n+    print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+A label can optionally be added to a transform using the\n+_right shift operator_ `>>` like `data | 'My description' >> beam.Map(...)`.\n+This serves both as comments and makes your pipeline easier to debug.\n+\n+This is how the pipeline looks after adding labels.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | 'Create numbers' >> beam.Create([1, 2, 3, 4])\n+        | 'Multiply by two' >> beam.Map(lambda x: x * 2)\n+        | 'Sum everything' >> beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+## Setup\n+\n+Here's a comparison on how to get started both in PySpark and Beam.\n+\n+{{< table >}}\n+<table>\n+<tr>\n+    <th></th>\n+    <th>PySpark</th>\n+    <th>Beam</th>\n+</tr>\n+<tr>\n+    <td><b>Install</b></td>\n+    <td><code>$ pip install pyspark</code></td>\n+    <td><code>$ pip install apache-beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Imports</b></td>\n+    <td><code>import pyspark</code></td>\n+    <td><code>import apache_beam as beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating a<br>local pipeline</b></td>\n+    <td>\n+        <code>with pyspark.SparkContext() as sc:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+    <td>\n+        <code>with beam.Pipeline() as pipeline:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Creating values</b></td>\n+    <td><code>values = sc.parallelize([1, 2, 3, 4])</code></td>\n+    <td><code>values = pipeline | beam.Create([1, 2, 3, 4])</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating<br>key-value pairs</b></td>\n+    <td>\n+        <code>pairs = sc.parallelize([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+    <td>\n+        <code>pairs = pipeline | beam.Create([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Running a<br>local pipeline</b></td>\n+    <td><code>$ spark-submit spark_pipeline.py</code></td>\n+    <td><code>$ python beam_pipeline.py</code></td>\n+</tr>\n+</table>\n+{{< /table >}}\n+\n+## Transforms\n+\n+Here are the equivalents of some common transforms in both PySpark and Beam.\n+\n+{{< table >}}\n+|                   | PySpark                               | Beam                                                    |\n+|-------------------|---------------------------------------|---------------------------------------------------------|\n+| **Map**           | `values.map(lambda x: x * 2)`         | `values | beam.Map(lambda x: x * 2)`                    |\n+| **Filter**        | `values.filter(lambda x: x % 2 == 0)` | `values | beam.Filter(lambda x: x % 2 == 0)`            |\n+| **FlatMap**       | `values.flatMap(lambda x: range(x))`  | `values | beam.FlatMap(lambda x: range(x))`             |\n+| **Group by key**  | `pairs.groupByKey()`                  | `pairs | beam.GroupByKey()`                             |\n+| **Reduce**        | `values.reduce(lambda x, y: x+y)`     | `values | beam.CombineGlobally(sum)`                    |\n+| **Reduce by key** | `pairs.reduceByKey(lambda x, y: x+y)` | `pairs | beam.CombinePerKey(sum)`                       |\n+| **Distinct**      | `values.distinct()`                   | `values | beam.Distinct()`                              |\n+| **Count**         | `values.count()`                      | `values | beam.combiners.Count.Globally()`              |\n+| **Count by key**  | `pairs.countByKey()`                  | `pairs | beam.combiners.Count.PerKey()`                 |\n+| **Take smallest** | `values.takeOrdered(3)`               | `values | beam.combiners.Top.Smallest(3)`               |\n+| **Take largest**  | `values.takeOrdered(3, lambda x: -x)` | `values | beam.combiners.Top.Largest(3)`                |\n+| **Random sample** | `values.takeSample(False, 3)`         | `values | beam.combiners.Sample.FixedSizeGlobally(3)`   |\n+| **Union**         | `values.union(otherValues)`           | `(values, otherValues) | beam.Flatten()`                |\n+| **Co-group**      | `pairs.cogroup(otherPairs)`           | `{'Xs': pairs, 'Ys': otherPairs} | beam.CoGroupByKey()` |\n+{{< /table >}}\n+\n+> \u2139\ufe0f To learn more about the transforms available in Beam, check the\n+> [Python transform gallery](/documentation/transforms/python/overview).\n+\n+## Using calculated values\n+\n+Since we are working in potentially distributed environments,\n+we can't guarantee that the results we've calculated are available at any given machine.\n+\n+In PySpark, we can get a result from a collection of elements (RDD) by using\n+`data.collect()`, or other aggregations such as `reduce()`, `count()` and more.\n+\n+Here's an example to scale numbers into a range between zero and one.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    values = sc.parallelize([1, 2, 3, 4])\n+    total = values.reduce(lambda x, y: x + y)\n+\n+    # We can simply use `total` since it's already a Python value from `reduce`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d"}, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA0NTQyNg==", "bodyText": "Link to /documentation/programming-guide/#side-inputs.", "url": "https://github.com/apache/beam/pull/12963#discussion_r497045426", "createdAt": "2020-09-29T20:51:38Z", "author": {"login": "ibzib"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,245 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\"); \n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS, \n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+Beam and Spark are mostly equivalent, so you already know the basic concepts.\n+\n+A collection of elements in Spark is called a _Resilient Distributed Dataset_ (RDD),\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    result = (\n+        sc.parallelize([1, 2, 3, 4])\n+        .map(lambda x: x * 2)\n+        .reduce(lambda x, y: x + y)\n+    )\n+    print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+A label can optionally be added to a transform using the\n+_right shift operator_ `>>` like `data | 'My description' >> beam.Map(...)`.\n+This serves both as comments and makes your pipeline easier to debug.\n+\n+This is how the pipeline looks after adding labels.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | 'Create numbers' >> beam.Create([1, 2, 3, 4])\n+        | 'Multiply by two' >> beam.Map(lambda x: x * 2)\n+        | 'Sum everything' >> beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+## Setup\n+\n+Here's a comparison on how to get started both in PySpark and Beam.\n+\n+{{< table >}}\n+<table>\n+<tr>\n+    <th></th>\n+    <th>PySpark</th>\n+    <th>Beam</th>\n+</tr>\n+<tr>\n+    <td><b>Install</b></td>\n+    <td><code>$ pip install pyspark</code></td>\n+    <td><code>$ pip install apache-beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Imports</b></td>\n+    <td><code>import pyspark</code></td>\n+    <td><code>import apache_beam as beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating a<br>local pipeline</b></td>\n+    <td>\n+        <code>with pyspark.SparkContext() as sc:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+    <td>\n+        <code>with beam.Pipeline() as pipeline:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Creating values</b></td>\n+    <td><code>values = sc.parallelize([1, 2, 3, 4])</code></td>\n+    <td><code>values = pipeline | beam.Create([1, 2, 3, 4])</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating<br>key-value pairs</b></td>\n+    <td>\n+        <code>pairs = sc.parallelize([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+    <td>\n+        <code>pairs = pipeline | beam.Create([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Running a<br>local pipeline</b></td>\n+    <td><code>$ spark-submit spark_pipeline.py</code></td>\n+    <td><code>$ python beam_pipeline.py</code></td>\n+</tr>\n+</table>\n+{{< /table >}}\n+\n+## Transforms\n+\n+Here are the equivalents of some common transforms in both PySpark and Beam.\n+\n+{{< table >}}\n+|                   | PySpark                               | Beam                                                    |\n+|-------------------|---------------------------------------|---------------------------------------------------------|\n+| **Map**           | `values.map(lambda x: x * 2)`         | `values | beam.Map(lambda x: x * 2)`                    |\n+| **Filter**        | `values.filter(lambda x: x % 2 == 0)` | `values | beam.Filter(lambda x: x % 2 == 0)`            |\n+| **FlatMap**       | `values.flatMap(lambda x: range(x))`  | `values | beam.FlatMap(lambda x: range(x))`             |\n+| **Group by key**  | `pairs.groupByKey()`                  | `pairs | beam.GroupByKey()`                             |\n+| **Reduce**        | `values.reduce(lambda x, y: x+y)`     | `values | beam.CombineGlobally(sum)`                    |\n+| **Reduce by key** | `pairs.reduceByKey(lambda x, y: x+y)` | `pairs | beam.CombinePerKey(sum)`                       |\n+| **Distinct**      | `values.distinct()`                   | `values | beam.Distinct()`                              |\n+| **Count**         | `values.count()`                      | `values | beam.combiners.Count.Globally()`              |\n+| **Count by key**  | `pairs.countByKey()`                  | `pairs | beam.combiners.Count.PerKey()`                 |\n+| **Take smallest** | `values.takeOrdered(3)`               | `values | beam.combiners.Top.Smallest(3)`               |\n+| **Take largest**  | `values.takeOrdered(3, lambda x: -x)` | `values | beam.combiners.Top.Largest(3)`                |\n+| **Random sample** | `values.takeSample(False, 3)`         | `values | beam.combiners.Sample.FixedSizeGlobally(3)`   |\n+| **Union**         | `values.union(otherValues)`           | `(values, otherValues) | beam.Flatten()`                |\n+| **Co-group**      | `pairs.cogroup(otherPairs)`           | `{'Xs': pairs, 'Ys': otherPairs} | beam.CoGroupByKey()` |\n+{{< /table >}}\n+\n+> \u2139\ufe0f To learn more about the transforms available in Beam, check the\n+> [Python transform gallery](/documentation/transforms/python/overview).\n+\n+## Using calculated values\n+\n+Since we are working in potentially distributed environments,\n+we can't guarantee that the results we've calculated are available at any given machine.\n+\n+In PySpark, we can get a result from a collection of elements (RDD) by using\n+`data.collect()`, or other aggregations such as `reduce()`, `count()` and more.\n+\n+Here's an example to scale numbers into a range between zero and one.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    values = sc.parallelize([1, 2, 3, 4])\n+    total = values.reduce(lambda x, y: x + y)\n+\n+    # We can simply use `total` since it's already a Python value from `reduce`.\n+    scaled_values = values.map(lambda x: x / total)\n+\n+    # But to access `scaled_values`, we need to call `collect`.\n+    print(scaled_values.collect())\n+{{< /highlight >}}\n+\n+In Beam the results from _all_ transforms result in a PCollection.\n+We use _side inputs_ to feed a PCollection into a transform and access its values.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d"}, "originalPosition": 206}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "17aed03f86421d228d40c6e55201a341643edb40", "author": {"user": {"login": "davidcavazos", "name": "David Cavazos"}}, "url": "https://github.com/apache/beam/commit/17aed03f86421d228d40c6e55201a341643edb40", "committedDate": "2020-10-19T22:03:23Z", "message": "Add links to transform catalog"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eaff2e92d05dd091b135cfaa8f2efe411ec7fba0", "author": {"user": {"login": "davidcavazos", "name": "David Cavazos"}}, "url": "https://github.com/apache/beam/commit/eaff2e92d05dd091b135cfaa8f2efe411ec7fba0", "committedDate": "2020-10-19T22:03:45Z", "message": "Merge branch 'master' of github.com:apache/beam into coming-from-spark"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "26a2a04d04cc2c4030357291a09f8d262d9c67f3", "author": {"user": {"login": "davidcavazos", "name": "David Cavazos"}}, "url": "https://github.com/apache/beam/commit/26a2a04d04cc2c4030357291a09f8d262d9c67f3", "committedDate": "2020-10-19T22:07:42Z", "message": "Add more links"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEyMTkzOTEw", "url": "https://github.com/apache/beam/pull/12963#pullrequestreview-512193910", "createdAt": "2020-10-19T22:06:49Z", "commit": {"oid": "eaff2e92d05dd091b135cfaa8f2efe411ec7fba0"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQyMjowNjo0OVrOHkjYmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQyMjoxNzo1MlrOHkjqTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA5MDUyMg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            This means that when you _pipe_ `|` data you're only _decalring_ the\n          \n          \n            \n            This means that when you _pipe_ `|` data you're only _declaring_ the", "url": "https://github.com/apache/beam/pull/12963#discussion_r508090522", "createdAt": "2020-10-19T22:06:49Z", "author": {"login": "ibzib"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -71,6 +71,17 @@ with beam.Pipeline() as pipeline:\n > That's because we can only access the elements of a PCollection\n > from within a PTransform.\n \n+Another thing to note is that Beam pipelines are constructed _lazily_.\n+This means that when you _pipe_ `|` data you're only _decalring_ the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eaff2e92d05dd091b135cfaa8f2efe411ec7fba0"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA5NTA1Mw==", "bodyText": "I don't have an @apache.org email so I can't open the discussion.\n\nAnyone should be able to join the ASF slack. There is an invite link on the Beam website: https://beam.apache.org/community/contact-us/\nThe case I linked might be an isolated bug. If tuples have worked for you, I'm fine with keeping them in the documentation.", "url": "https://github.com/apache/beam/pull/12963#discussion_r508095053", "createdAt": "2020-10-19T22:17:52Z", "author": {"login": "ibzib"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,245 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\"); \n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS, \n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+Beam and Spark are mostly equivalent, so you already know the basic concepts.\n+\n+A collection of elements in Spark is called a _Resilient Distributed Dataset_ (RDD),\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    result = (\n+        sc.parallelize([1, 2, 3, 4])\n+        .map(lambda x: x * 2)\n+        .reduce(lambda x, y: x + y)\n+    )\n+    print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+A label can optionally be added to a transform using the\n+_right shift operator_ `>>` like `data | 'My description' >> beam.Map(...)`.\n+This serves both as comments and makes your pipeline easier to debug.\n+\n+This is how the pipeline looks after adding labels.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | 'Create numbers' >> beam.Create([1, 2, 3, 4])\n+        | 'Multiply by two' >> beam.Map(lambda x: x * 2)\n+        | 'Sum everything' >> beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+## Setup\n+\n+Here's a comparison on how to get started both in PySpark and Beam.\n+\n+{{< table >}}\n+<table>\n+<tr>\n+    <th></th>\n+    <th>PySpark</th>\n+    <th>Beam</th>\n+</tr>\n+<tr>\n+    <td><b>Install</b></td>\n+    <td><code>$ pip install pyspark</code></td>\n+    <td><code>$ pip install apache-beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Imports</b></td>\n+    <td><code>import pyspark</code></td>\n+    <td><code>import apache_beam as beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating a<br>local pipeline</b></td>\n+    <td>\n+        <code>with pyspark.SparkContext() as sc:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+    <td>\n+        <code>with beam.Pipeline() as pipeline:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Creating values</b></td>\n+    <td><code>values = sc.parallelize([1, 2, 3, 4])</code></td>\n+    <td><code>values = pipeline | beam.Create([1, 2, 3, 4])</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating<br>key-value pairs</b></td>\n+    <td>\n+        <code>pairs = sc.parallelize([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+    <td>\n+        <code>pairs = pipeline | beam.Create([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAzMzE2Mw=="}, "originalCommit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d"}, "originalPosition": 141}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2f397998454a12bb04ae028519e47da950db9690", "author": {"user": {"login": "davidcavazos", "name": "David Cavazos"}}, "url": "https://github.com/apache/beam/commit/2f397998454a12bb04ae028519e47da950db9690", "committedDate": "2020-10-20T16:49:43Z", "message": "Add RDD/DataFrame clarification"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEzNjc0MjUz", "url": "https://github.com/apache/beam/pull/12963#pullrequestreview-513674253", "createdAt": "2020-10-21T13:24:12Z", "commit": {"oid": "e5d9edd69621203ade20c539e60d27fc961c9ba4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxMzoyNDoxM1rOHlrxjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxMzoyNDoxM1rOHlrxjA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTI3NjU1Ng==", "bodyText": "I am not sure if worth but since we mention lazy computation we probably may mention what triggers 'producing results', in Spark is done by Spark's actions e.g. collect(), etc and in Beam by outputting the data (you can relate this to the print mention). Notice that I saw the warning of the with section but we can double mention this here and/or mention p.run()).\nNote this is just an extra suggestion but it is not mandatory at all, it is already clear, this is 'extra details'.", "url": "https://github.com/apache/beam/pull/12963#discussion_r509276556", "createdAt": "2020-10-21T13:24:13Z", "author": {"login": "iemejia"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,261 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+The Beam and Spark APIs are similar, so you already know the basic concepts.\n+\n+Spark stores data _Spark DataFrames_ for structured data,\n+and in _Resilient Distributed Datasets_ (RDD) for unstructured data.\n+We are using RDDs for this guide.\n+\n+A _Spark RDD_ represents a collection of elements,\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+sc = pyspark.SparkContext()\n+result = (\n+    sc.parallelize([1, 2, 3, 4])\n+    .map(lambda x: x * 2)\n+    .reduce(lambda x, y: x + y)\n+)\n+print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+Another thing to note is that Beam pipelines are constructed _lazily_.\n+This means that when you _pipe_ `|` data you're only _declaring_ the\n+transformations and the order you want them to happen,\n+but the actual computation doesn't happen.\n+The pipeline is run _after_ the `with beam.Pipeline() as pipeline` context has\n+closed.\n+The pipeline is then sent to your runner of choice and it processes the data.\n+\n+> \u2139\ufe0f When the `with beam.Pipeline() as pipeline` context closes,\n+> it implicitly calls `pipeline.run()` which triggers the computation to happen.\n+\n+A label can optionally be added to a transform using the\n+_right shift operator_ `>>` like `data | 'My description' >> beam.Map(...)`.\n+This serves both as comments and makes your pipeline easier to debug.\n+\n+This is how the pipeline looks after adding labels.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | 'Create numbers' >> beam.Create([1, 2, 3, 4])\n+        | 'Multiply by two' >> beam.Map(lambda x: x * 2)\n+        | 'Sum everything' >> beam.CombineGlobally(sum)\n+        | 'Print results' >> beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+## Setup\n+\n+Here's a comparison on how to get started both in PySpark and Beam.\n+\n+{{< table >}}\n+<table>\n+<tr>\n+    <th></th>\n+    <th>PySpark</th>\n+    <th>Beam</th>\n+</tr>\n+<tr>\n+    <td><b>Install</b></td>\n+    <td><code>$ pip install pyspark</code></td>\n+    <td><code>$ pip install apache-beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Imports</b></td>\n+    <td><code>import pyspark</code></td>\n+    <td><code>import apache_beam as beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating a<br>local pipeline</b></td>\n+    <td>\n+        <code>sc = pyspark.SparkContext() as sc:</code><br>\n+        <code># Your pipeline code here.</code>\n+    </td>\n+    <td>\n+        <code>with beam.Pipeline() as pipeline:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Creating values</b></td>\n+    <td><code>values = sc.parallelize([1, 2, 3, 4])</code></td>\n+    <td><code>values = pipeline | beam.Create([1, 2, 3, 4])</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating<br>key-value pairs</b></td>\n+    <td>\n+        <code>pairs = sc.parallelize([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+    <td>\n+        <code>pairs = pipeline | beam.Create([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+</tr>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5d9edd69621203ade20c539e60d27fc961c9ba4"}, "originalPosition": 161}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIyNjU1NTg2", "url": "https://github.com/apache/beam/pull/12963#pullrequestreview-522655586", "createdAt": "2020-11-03T16:12:20Z", "commit": {"oid": "e5d9edd69621203ade20c539e60d27fc961c9ba4"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNjoxMjoyMFrOHs2FRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNjo0NTo0N1rOHs3hlA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc4NTQ3OQ==", "bodyText": "The actual Overview page has a Getting Started section you can include this new doc in.", "url": "https://github.com/apache/beam/pull/12963#discussion_r516785479", "createdAt": "2020-11-03T16:12:20Z", "author": {"login": "rosetn"}, "path": "website/www/site/layouts/partials/section-menu/en/get-started.html", "diffHunk": "@@ -22,12 +22,13 @@\n     <li><a href=\"/get-started/quickstart-go/\">Quickstart - Go</a></li>\n   </ul>\n </li>\n+<li><a href=\"/get-started/from-spark/\">From Apache Spark</a></li>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5d9edd69621203ade20c539e60d27fc961c9ba4"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc4OTI0Ng==", "bodyText": "WDYT about replacing \"is easy\" with \"is familiar\" or removing this sentence, since you explain the connection in the next sentence?", "url": "https://github.com/apache/beam/pull/12963#discussion_r516789246", "createdAt": "2020-11-03T16:17:42Z", "author": {"login": "rosetn"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,261 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5d9edd69621203ade20c539e60d27fc961c9ba4"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgwODA3Mg==", "bodyText": "I'd consider only italicizing the new Spark terms. The font emphasis can create accessibility issues if it's too frequent.", "url": "https://github.com/apache/beam/pull/12963#discussion_r516808072", "createdAt": "2020-11-03T16:44:22Z", "author": {"login": "rosetn"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,261 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5d9edd69621203ade20c539e60d27fc961c9ba4"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgwOTEwOA==", "bodyText": "Replace \"how\" with \"what\"", "url": "https://github.com/apache/beam/pull/12963#discussion_r516809108", "createdAt": "2020-11-03T16:45:47Z", "author": {"login": "rosetn"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,261 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+The Beam and Spark APIs are similar, so you already know the basic concepts.\n+\n+Spark stores data _Spark DataFrames_ for structured data,\n+and in _Resilient Distributed Datasets_ (RDD) for unstructured data.\n+We are using RDDs for this guide.\n+\n+A _Spark RDD_ represents a collection of elements,\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+sc = pyspark.SparkContext()\n+result = (\n+    sc.parallelize([1, 2, 3, 4])\n+    .map(lambda x: x * 2)\n+    .reduce(lambda x, y: x + y)\n+)\n+print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5d9edd69621203ade20c539e60d27fc961c9ba4"}, "originalPosition": 59}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIzODI1MzMw", "url": "https://github.com/apache/beam/pull/12963#pullrequestreview-523825330", "createdAt": "2020-11-05T00:08:18Z", "commit": {"oid": "e5d9edd69621203ade20c539e60d27fc961c9ba4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwMDowODoxOFrOHtuLaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwMDowODoxOFrOHtuLaw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzcwNDU1NQ==", "bodyText": "I realized we should probably clarify somewhere that \"Beam Python on the Spark runner\" and PySpark are completed unrelated, since that seems to be a common misconception.", "url": "https://github.com/apache/beam/pull/12963#discussion_r517704555", "createdAt": "2020-11-05T00:08:18Z", "author": {"login": "ibzib"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,261 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+The Beam and Spark APIs are similar, so you already know the basic concepts.\n+\n+Spark stores data _Spark DataFrames_ for structured data,\n+and in _Resilient Distributed Datasets_ (RDD) for unstructured data.\n+We are using RDDs for this guide.\n+\n+A _Spark RDD_ represents a collection of elements,\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+sc = pyspark.SparkContext()\n+result = (\n+    sc.parallelize([1, 2, 3, 4])\n+    .map(lambda x: x * 2)\n+    .reduce(lambda x, y: x + y)\n+)\n+print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+Another thing to note is that Beam pipelines are constructed _lazily_.\n+This means that when you _pipe_ `|` data you're only _declaring_ the\n+transformations and the order you want them to happen,\n+but the actual computation doesn't happen.\n+The pipeline is run _after_ the `with beam.Pipeline() as pipeline` context has\n+closed.\n+The pipeline is then sent to your runner of choice and it processes the data.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5d9edd69621203ade20c539e60d27fc961c9ba4"}, "originalPosition": 84}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a751df81bcb17186edc8ef22f14a8ed79a30a630", "author": {"user": {"login": "davidcavazos", "name": "David Cavazos"}}, "url": "https://github.com/apache/beam/commit/a751df81bcb17186edc8ef22f14a8ed79a30a630", "committedDate": "2020-11-24T19:32:45Z", "message": "Merge branch 'master' of github.com:apache/beam into coming-from-spark"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6c338cef9b01c52fcc530904551893cbfc529e17", "author": {"user": {"login": "davidcavazos", "name": "David Cavazos"}}, "url": "https://github.com/apache/beam/commit/6c338cef9b01c52fcc530904551893cbfc529e17", "committedDate": "2020-12-11T14:38:46Z", "message": "Add link to from-spark page"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a3406b3558a77a1030dfa3d5d41477d86ea82217", "author": {"user": {"login": "davidcavazos", "name": "David Cavazos"}}, "url": "https://github.com/apache/beam/commit/a3406b3558a77a1030dfa3d5d41477d86ea82217", "committedDate": "2020-12-11T14:39:28Z", "message": "Added extra explanations"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "67af84a51c471e7c53fc23a43acf597f60233273", "author": {"user": {"login": "davidcavazos", "name": "David Cavazos"}}, "url": "https://github.com/apache/beam/commit/67af84a51c471e7c53fc23a43acf597f60233273", "committedDate": "2020-12-11T14:39:43Z", "message": "Merge branch 'master' of github.com:apache/beam into coming-from-spark"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e5d9edd69621203ade20c539e60d27fc961c9ba4", "author": {"user": {"login": "davidcavazos", "name": "David Cavazos"}}, "url": "https://github.com/apache/beam/commit/e5d9edd69621203ade20c539e60d27fc961c9ba4", "committedDate": "2020-10-20T18:33:23Z", "message": "Fix typo\n\nCo-authored-by: Kyle Weaver <kcweaver@google.com>"}, "afterCommit": {"oid": "67af84a51c471e7c53fc23a43acf597f60233273", "author": {"user": {"login": "davidcavazos", "name": "David Cavazos"}}, "url": "https://github.com/apache/beam/commit/67af84a51c471e7c53fc23a43acf597f60233273", "committedDate": "2020-12-11T14:39:43Z", "message": "Merge branch 'master' of github.com:apache/beam into coming-from-spark"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTUwNjQ3ODY3", "url": "https://github.com/apache/beam/pull/12963#pullrequestreview-550647867", "createdAt": "2020-12-11T23:42:59Z", "commit": {"oid": "67af84a51c471e7c53fc23a43acf597f60233273"}, "state": "APPROVED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQyMzo0Mjo1OVrOIEVt3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQyMzo0NDoxMVrOIEVxpw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTQyMTAyMg==", "bodyText": "typo--declaring", "url": "https://github.com/apache/beam/pull/12963#discussion_r541421022", "createdAt": "2020-12-11T23:42:59Z", "author": {"login": "rosetn"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,268 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is familiar.\n+The Beam and Spark APIs are similar, so you already know the basic concepts.\n+\n+Spark stores data _Spark DataFrames_ for structured data,\n+and in _Resilient Distributed Datasets_ (RDD) for unstructured data.\n+We are using RDDs for this guide.\n+\n+A Spark RDD represents a collection of elements,\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+sc = pyspark.SparkContext()\n+result = (\n+    sc.parallelize([1, 2, 3, 4])\n+    .map(lambda x: x * 2)\n+    .reduce(lambda x, y: x + y)\n+)\n+print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's what an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+Another thing to note is that Beam pipelines are constructed _lazily_.\n+This means that when you pipe `|` data you're only _decalring_ the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67af84a51c471e7c53fc23a43acf597f60233273"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTQyMTk5MQ==", "bodyText": "comm after count()", "url": "https://github.com/apache/beam/pull/12963#discussion_r541421991", "createdAt": "2020-12-11T23:44:11Z", "author": {"login": "rosetn"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,268 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is familiar.\n+The Beam and Spark APIs are similar, so you already know the basic concepts.\n+\n+Spark stores data _Spark DataFrames_ for structured data,\n+and in _Resilient Distributed Datasets_ (RDD) for unstructured data.\n+We are using RDDs for this guide.\n+\n+A Spark RDD represents a collection of elements,\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+sc = pyspark.SparkContext()\n+result = (\n+    sc.parallelize([1, 2, 3, 4])\n+    .map(lambda x: x * 2)\n+    .reduce(lambda x, y: x + y)\n+)\n+print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's what an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+Another thing to note is that Beam pipelines are constructed _lazily_.\n+This means that when you pipe `|` data you're only _decalring_ the\n+transformations and the order you want them to happen,\n+but the actual computation doesn't happen.\n+The pipeline is run _after_ the `with beam.Pipeline() as pipeline` context has\n+closed.\n+\n+> \u2139\ufe0f When the `with beam.Pipeline() as pipeline` context closes,\n+> it implicitly calls `pipeline.run()` which triggers the computation to happen.\n+\n+The pipeline is then sent to your\n+[runner of choice](https://beam.apache.org/documentation/runners/capability-matrix/)\n+and it processes the data.\n+\n+> \u2139\ufe0f The pipeline can run locally with the _DirectRunner_,\n+> or in a distributed runner such as Flink, Spark, or Dataflow.\n+> The Spark runner is not related to PySpark.\n+\n+A label can optionally be added to a transform using the\n+_right shift operator_ `>>` like `data | 'My description' >> beam.Map(...)`.\n+This serves both as comments and makes your pipeline easier to debug.\n+\n+This is how the pipeline looks after adding labels.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | 'Create numbers' >> beam.Create([1, 2, 3, 4])\n+        | 'Multiply by two' >> beam.Map(lambda x: x * 2)\n+        | 'Sum everything' >> beam.CombineGlobally(sum)\n+        | 'Print results' >> beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+## Setup\n+\n+Here's a comparison on how to get started both in PySpark and Beam.\n+\n+{{< table >}}\n+<table>\n+<tr>\n+    <th></th>\n+    <th>PySpark</th>\n+    <th>Beam</th>\n+</tr>\n+<tr>\n+    <td><b>Install</b></td>\n+    <td><code>$ pip install pyspark</code></td>\n+    <td><code>$ pip install apache-beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Imports</b></td>\n+    <td><code>import pyspark</code></td>\n+    <td><code>import apache_beam as beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating a<br>local pipeline</b></td>\n+    <td>\n+        <code>sc = pyspark.SparkContext() as sc:</code><br>\n+        <code># Your pipeline code here.</code>\n+    </td>\n+    <td>\n+        <code>with beam.Pipeline() as pipeline:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Creating values</b></td>\n+    <td><code>values = sc.parallelize([1, 2, 3, 4])</code></td>\n+    <td><code>values = pipeline | beam.Create([1, 2, 3, 4])</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating<br>key-value pairs</b></td>\n+    <td>\n+        <code>pairs = sc.parallelize([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+    <td>\n+        <code>pairs = pipeline | beam.Create([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Running a<br>local pipeline</b></td>\n+    <td><code>$ spark-submit spark_pipeline.py</code></td>\n+    <td><code>$ python beam_pipeline.py</code></td>\n+</tr>\n+</table>\n+{{< /table >}}\n+\n+## Transforms\n+\n+Here are the equivalents of some common transforms in both PySpark and Beam.\n+\n+{{< table >}}\n+|                                                                                  | PySpark                               | Beam                                                    |\n+|----------------------------------------------------------------------------------|---------------------------------------|---------------------------------------------------------|\n+| [**Map**](/documentation/transforms/python/elementwise/map/)                     | `values.map(lambda x: x * 2)`         | `values | beam.Map(lambda x: x * 2)`                    |\n+| [**Filter**](/documentation/transforms/python/elementwise/filter/)               | `values.filter(lambda x: x % 2 == 0)` | `values | beam.Filter(lambda x: x % 2 == 0)`            |\n+| [**FlatMap**](/documentation/transforms/python/elementwise/flatmap/)             | `values.flatMap(lambda x: range(x))`  | `values | beam.FlatMap(lambda x: range(x))`             |\n+| [**Group by key**](/documentation/transforms/python/aggregation/groupbykey/)     | `pairs.groupByKey()`                  | `pairs | beam.GroupByKey()`                             |\n+| [**Reduce**](/documentation/transforms/python/aggregation/combineglobally/)      | `values.reduce(lambda x, y: x+y)`     | `values | beam.CombineGlobally(sum)`                    |\n+| [**Reduce by key**](/documentation/transforms/python/aggregation/combineperkey/) | `pairs.reduceByKey(lambda x, y: x+y)` | `pairs | beam.CombinePerKey(sum)`                       |\n+| [**Distinct**](/documentation/transforms/python/aggregation/distinct/)           | `values.distinct()`                   | `values | beam.Distinct()`                              |\n+| [**Count**](/documentation/transforms/python/aggregation/count/)                 | `values.count()`                      | `values | beam.combiners.Count.Globally()`              |\n+| [**Count by key**](/documentation/transforms/python/aggregation/count/)          | `pairs.countByKey()`                  | `pairs | beam.combiners.Count.PerKey()`                 |\n+| [**Take smallest**](/documentation/transforms/python/aggregation/top/)           | `values.takeOrdered(3)`               | `values | beam.combiners.Top.Smallest(3)`               |\n+| [**Take largest**](/documentation/transforms/python/aggregation/top/)            | `values.takeOrdered(3, lambda x: -x)` | `values | beam.combiners.Top.Largest(3)`                |\n+| [**Random sample**](/documentation/transforms/python/aggregation/sample/)        | `values.takeSample(False, 3)`         | `values | beam.combiners.Sample.FixedSizeGlobally(3)`   |\n+| [**Union**](/documentation/transforms/python/other/flatten/)                     | `values.union(otherValues)`           | `(values, otherValues) | beam.Flatten()`                |\n+| [**Co-group**](/documentation/transforms/python/aggregation/cogroupbykey/)       | `pairs.cogroup(otherPairs)`           | `{'Xs': pairs, 'Ys': otherPairs} | beam.CoGroupByKey()` |\n+{{< /table >}}\n+\n+> \u2139\ufe0f To learn more about the transforms available in Beam, check the\n+> [Python transform gallery](/documentation/transforms/python/overview).\n+\n+## Using calculated values\n+\n+Since we are working in potentially distributed environments,\n+we can't guarantee that the results we've calculated are available at any given machine.\n+\n+In PySpark, we can get a result from a collection of elements (RDD) by using\n+`data.collect()`, or other aggregations such as `reduce()`, `count()` and more.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67af84a51c471e7c53fc23a43acf597f60233273"}, "originalPosition": 209}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "95574707f068177d7bf591279d4fb1f2f830935f", "author": {"user": {"login": "davidcavazos", "name": "David Cavazos"}}, "url": "https://github.com/apache/beam/commit/95574707f068177d7bf591279d4fb1f2f830935f", "committedDate": "2020-12-14T16:27:36Z", "message": "Fixed typos + removed italics"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2613, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}