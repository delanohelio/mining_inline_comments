{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQyNjczNDgw", "number": 12149, "reviewThreads": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QwNDoxNTo0MlrOENrUeQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDoyMzo0OFrOET426Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNzc2Njk3OnYy", "diffSide": "RIGHT", "path": "sdks/java/container/boot.go", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QwNDoxNTo0MlrOGwa6Jg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QwODoxMTo1MFrOGwdx1g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyNTcwMg==", "bodyText": "You should not need this. You just have to make sure that dependencies correctly get determined (and staged) during cross language expansion.", "url": "https://github.com/apache/beam/pull/12149#discussion_r453425702", "createdAt": "2020-07-13T04:15:42Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/container/boot.go", "diffHunk": "@@ -122,6 +122,7 @@ func main() {\n \t\tfilepath.Join(jarsDir, \"beam-sdks-java-harness.jar\"),\n \t\tfilepath.Join(jarsDir, \"beam-sdks-java-io-kafka.jar\"),\n \t\tfilepath.Join(jarsDir, \"kafka-clients.jar\"),\n+\t\tfilepath.Join(jarsDir, \"beam-sdks-java-io-snowflake.jar\"),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQ3MjcyNg==", "bodyText": "I removed it, thanks.", "url": "https://github.com/apache/beam/pull/12149#discussion_r453472726", "createdAt": "2020-07-13T08:11:50Z", "author": {"login": "purbanow"}, "path": "sdks/java/container/boot.go", "diffHunk": "@@ -122,6 +122,7 @@ func main() {\n \t\tfilepath.Join(jarsDir, \"beam-sdks-java-harness.jar\"),\n \t\tfilepath.Join(jarsDir, \"beam-sdks-java-io-kafka.jar\"),\n \t\tfilepath.Join(jarsDir, \"kafka-clients.jar\"),\n+\t\tfilepath.Join(jarsDir, \"beam-sdks-java-io-snowflake.jar\"),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyNTcwMg=="}, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNzc2NzgxOnYy", "diffSide": "RIGHT", "path": "sdks/java/container/build.gradle", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QwNDoxNjowOFrOGwa6jQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QwODoxMjoyNFrOGwdyXg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyNTgwNQ==", "bodyText": "Ditto.", "url": "https://github.com/apache/beam/pull/12149#discussion_r453425805", "createdAt": "2020-07-13T04:16:08Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/container/build.gradle", "diffHunk": "@@ -49,6 +49,7 @@ dependencies {\n   dockerDependency project(\":sdks:java:io:kafka\")\n   // This dependency is set to 'provided' scope in :sdks:java:io:kafka\n   dockerDependency library.java.kafka_clients\n+  dockerDependency project(path: \":sdks:java:io:snowflake\", configuration: \"shadow\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQ3Mjg2Mg==", "bodyText": "I removed it, thanks.", "url": "https://github.com/apache/beam/pull/12149#discussion_r453472862", "createdAt": "2020-07-13T08:12:24Z", "author": {"login": "purbanow"}, "path": "sdks/java/container/build.gradle", "diffHunk": "@@ -49,6 +49,7 @@ dependencies {\n   dockerDependency project(\":sdks:java:io:kafka\")\n   // This dependency is set to 'provided' scope in :sdks:java:io:kafka\n   dockerDependency library.java.kafka_clients\n+  dockerDependency project(path: \":sdks:java:io:snowflake\", configuration: \"shadow\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyNTgwNQ=="}, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNzc2ODI2OnYy", "diffSide": "RIGHT", "path": "sdks/java/container/build.gradle", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QwNDoxNjoyMFrOGwa6yQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QwODoxMjo0MlrOGwdyow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyNTg2NQ==", "bodyText": "Ditto.", "url": "https://github.com/apache/beam/pull/12149#discussion_r453425865", "createdAt": "2020-07-13T04:16:20Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/container/build.gradle", "diffHunk": "@@ -60,6 +61,8 @@ task copyDockerfileDependencies(type: Copy) {\n   rename 'beam-sdks-java-harness-.*.jar', 'beam-sdks-java-harness.jar'\n   rename 'beam-sdks-java-io-kafka.*.jar', 'beam-sdks-java-io-kafka.jar'\n   rename 'kafka-clients.*.jar', 'kafka-clients.jar'\n+  rename 'beam-sdks-java-io-snowflake.*SNAPSHOT.jar', 'beam-sdks-java-io-snowflake.jar'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQ3MjkzMQ==", "bodyText": "I removed it, thanks.", "url": "https://github.com/apache/beam/pull/12149#discussion_r453472931", "createdAt": "2020-07-13T08:12:42Z", "author": {"login": "purbanow"}, "path": "sdks/java/container/build.gradle", "diffHunk": "@@ -60,6 +61,8 @@ task copyDockerfileDependencies(type: Copy) {\n   rename 'beam-sdks-java-harness-.*.jar', 'beam-sdks-java-harness.jar'\n   rename 'beam-sdks-java-io-kafka.*.jar', 'beam-sdks-java-io-kafka.jar'\n   rename 'kafka-clients.*.jar', 'kafka-clients.jar'\n+  rename 'beam-sdks-java-io-snowflake.*SNAPSHOT.jar', 'beam-sdks-java-io-snowflake.jar'", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyNTg2NQ=="}, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 12}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNzc2OTA3OnYy", "diffSide": "RIGHT", "path": "sdks/java/expansion-service/build.gradle", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QwNDoxNjo0NVrOGwa7MA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QwODoxMzozMlrOGwdzuw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyNTk2OA==", "bodyText": "Why are we adding a dependency here ?", "url": "https://github.com/apache/beam/pull/12149#discussion_r453425968", "createdAt": "2020-07-13T04:16:45Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/expansion-service/build.gradle", "diffHunk": "@@ -42,6 +42,8 @@ dependencies {\n   compile library.java.slf4j_api\n   runtimeOnly library.java.slf4j_jdk14\n   testCompile library.java.junit\n+\n+  runtime project(\":sdks:java:io:snowflake\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQ3MzIxMQ==", "bodyText": "I removed it and checked if everything is working, thanks for spotting this.", "url": "https://github.com/apache/beam/pull/12149#discussion_r453473211", "createdAt": "2020-07-13T08:13:32Z", "author": {"login": "purbanow"}, "path": "sdks/java/expansion-service/build.gradle", "diffHunk": "@@ -42,6 +42,8 @@ dependencies {\n   compile library.java.slf4j_api\n   runtimeOnly library.java.slf4j_jdk14\n   testCompile library.java.junit\n+\n+  runtime project(\":sdks:java:io:snowflake\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyNTk2OA=="}, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNzc3MjQ5OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QwNDoxOToxN1rOGwa9JQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QwOTowNzozOVrOGwfuVw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyNjQ2OQ==", "bodyText": "Why are we implementing separate methods here ? Also probably this should be a separate PR.", "url": "https://github.com/apache/beam/pull/12149#discussion_r453426469", "createdAt": "2020-07-13T04:19:17Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -960,26 +953,44 @@ public static DataSourceConfiguration create(DataSource dataSource) {\n      * @param credentials - an instance of {@link SnowflakeCredentials}.\n      */\n     public static DataSourceConfiguration create(SnowflakeCredentials credentials) {\n-      if (credentials instanceof UsernamePasswordSnowflakeCredentials) {\n-        return new AutoValue_SnowflakeIO_DataSourceConfiguration.Builder()\n-            .setValidate(true)\n-            .setUsername(((UsernamePasswordSnowflakeCredentials) credentials).getUsername())\n-            .setPassword(((UsernamePasswordSnowflakeCredentials) credentials).getPassword())\n-            .build();\n-      } else if (credentials instanceof OAuthTokenSnowflakeCredentials) {\n-        return new AutoValue_SnowflakeIO_DataSourceConfiguration.Builder()\n-            .setValidate(true)\n-            .setOauthToken(((OAuthTokenSnowflakeCredentials) credentials).getToken())\n-            .build();\n-      } else if (credentials instanceof KeyPairSnowflakeCredentials) {\n-        return new AutoValue_SnowflakeIO_DataSourceConfiguration.Builder()\n-            .setValidate(true)\n-            .setUsername(((KeyPairSnowflakeCredentials) credentials).getUsername())\n-            .setPrivateKey(((KeyPairSnowflakeCredentials) credentials).getPrivateKey())\n-            .build();\n-      }\n-      throw new IllegalArgumentException(\n-          \"Can't create DataSourceConfiguration from given credentials\");\n+      return credentials.createSnowflakeDataSourceConfiguration();\n+    }\n+\n+    /**\n+     * Creates {@link DataSourceConfiguration} from instance of {@link", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 63}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzUwNDU5OQ==", "bodyText": "I removed this improvement from this PR.", "url": "https://github.com/apache/beam/pull/12149#discussion_r453504599", "createdAt": "2020-07-13T09:07:39Z", "author": {"login": "purbanow"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/SnowflakeIO.java", "diffHunk": "@@ -960,26 +953,44 @@ public static DataSourceConfiguration create(DataSource dataSource) {\n      * @param credentials - an instance of {@link SnowflakeCredentials}.\n      */\n     public static DataSourceConfiguration create(SnowflakeCredentials credentials) {\n-      if (credentials instanceof UsernamePasswordSnowflakeCredentials) {\n-        return new AutoValue_SnowflakeIO_DataSourceConfiguration.Builder()\n-            .setValidate(true)\n-            .setUsername(((UsernamePasswordSnowflakeCredentials) credentials).getUsername())\n-            .setPassword(((UsernamePasswordSnowflakeCredentials) credentials).getPassword())\n-            .build();\n-      } else if (credentials instanceof OAuthTokenSnowflakeCredentials) {\n-        return new AutoValue_SnowflakeIO_DataSourceConfiguration.Builder()\n-            .setValidate(true)\n-            .setOauthToken(((OAuthTokenSnowflakeCredentials) credentials).getToken())\n-            .build();\n-      } else if (credentials instanceof KeyPairSnowflakeCredentials) {\n-        return new AutoValue_SnowflakeIO_DataSourceConfiguration.Builder()\n-            .setValidate(true)\n-            .setUsername(((KeyPairSnowflakeCredentials) credentials).getUsername())\n-            .setPrivateKey(((KeyPairSnowflakeCredentials) credentials).getPrivateKey())\n-            .build();\n-      }\n-      throw new IllegalArgumentException(\n-          \"Can't create DataSourceConfiguration from given credentials\");\n+      return credentials.createSnowflakeDataSourceConfiguration();\n+    }\n+\n+    /**\n+     * Creates {@link DataSourceConfiguration} from instance of {@link", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyNjQ2OQ=="}, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNzc3NTQ5OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/credentials/OAuthTokenSnowflakeCredentials.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QwNDoyMToxMlrOGwa-6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QwOTowODoxNlrOGwfvzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyNjkyMA==", "bodyText": "I suggest moving changes to credentials to a separate PR since this seems to be unrelated to x-lang changes.", "url": "https://github.com/apache/beam/pull/12149#discussion_r453426920", "createdAt": "2020-07-13T04:21:12Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/credentials/OAuthTokenSnowflakeCredentials.java", "diffHunk": "@@ -28,4 +30,9 @@ public OAuthTokenSnowflakeCredentials(String token) {\n   public String getToken() {\n     return token;\n   }\n+\n+  @Override", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzUwNDk3Mg==", "bodyText": "I removed this improvement from this PR and I will prepare separate PR.", "url": "https://github.com/apache/beam/pull/12149#discussion_r453504972", "createdAt": "2020-07-13T09:08:16Z", "author": {"login": "purbanow"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/credentials/OAuthTokenSnowflakeCredentials.java", "diffHunk": "@@ -28,4 +30,9 @@ public OAuthTokenSnowflakeCredentials(String token) {\n   public String getToken() {\n     return token;\n   }\n+\n+  @Override", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyNjkyMA=="}, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNzc3NzAxOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/xlang/Configuration.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QwNDoyMjozNVrOGwa_2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QwODoyMjo0N1rOGweByA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyNzE2MA==", "bodyText": "Probably just leave this in the package \"org.apache.beam.sdk.io.snowflake\" ? Is there a need to add a new package ? (and if so probably use long form \"crosslanguage\").", "url": "https://github.com/apache/beam/pull/12149#discussion_r453427160", "createdAt": "2020-07-13T04:22:35Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/xlang/Configuration.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake.xlang;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQ3NjgwOA==", "bodyText": "It makes sense for me to keep it in separate package. If you insist on keeping it in org.apache.beam.sdk.io.snowflakepackage, i will remove crosslanguage package", "url": "https://github.com/apache/beam/pull/12149#discussion_r453476808", "createdAt": "2020-07-13T08:22:47Z", "author": {"login": "purbanow"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/xlang/Configuration.java", "diffHunk": "@@ -0,0 +1,130 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake.xlang;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyNzE2MA=="}, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNzc3ODYxOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/xlang/ExternalRead.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QwNDoyMzo1N1rOGwbAxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QwODowMjo1OFrOGwdvpQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyNzM5Ng==", "bodyText": "Probably call this SnowflakeReadRegistrar.", "url": "https://github.com/apache/beam/pull/12149#discussion_r453427396", "createdAt": "2020-07-13T04:23:57Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/xlang/ExternalRead.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake.xlang;\n+\n+import com.google.auto.service.AutoService;\n+import java.io.Serializable;\n+import java.nio.charset.Charset;\n+import java.util.Map;\n+import javax.sql.DataSource;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.expansion.ExternalTransformRegistrar;\n+import org.apache.beam.sdk.io.snowflake.SnowflakeIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory;\n+import org.apache.beam.sdk.transforms.ExternalTransformBuilder;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+\n+/** Exposes {@link SnowflakeIO.Read} as an external transform for cross-language usage. */\n+@Experimental\n+@AutoService(ExternalTransformRegistrar.class)\n+public final class ExternalRead implements ExternalTransformRegistrar {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 41}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQ3MjE2NQ==", "bodyText": "I changed name to SnowflakeReadRegistrar.", "url": "https://github.com/apache/beam/pull/12149#discussion_r453472165", "createdAt": "2020-07-13T08:02:58Z", "author": {"login": "purbanow"}, "path": "sdks/java/io/snowflake/src/main/java/org/apache/beam/sdk/io/snowflake/xlang/ExternalRead.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.snowflake.xlang;\n+\n+import com.google.auto.service.AutoService;\n+import java.io.Serializable;\n+import java.nio.charset.Charset;\n+import java.util.Map;\n+import javax.sql.DataSource;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.expansion.ExternalTransformRegistrar;\n+import org.apache.beam.sdk.io.snowflake.SnowflakeIO;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentials;\n+import org.apache.beam.sdk.io.snowflake.credentials.SnowflakeCredentialsFactory;\n+import org.apache.beam.sdk.transforms.ExternalTransformBuilder;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.values.PBegin;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+\n+/** Exposes {@link SnowflakeIO.Read} as an external transform for cross-language usage. */\n+@Experimental\n+@AutoService(ExternalTransformRegistrar.class)\n+public final class ExternalRead implements ExternalTransformRegistrar {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyNzM5Ng=="}, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNzc4NDM0OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/io/external/snowflake.py", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QwNDoyNzo0OFrOGwbD5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yOFQxNDowMzozM1rOG4OJJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyODE5Ng==", "bodyText": "You have to either add snowflake jars to \"io:expansion-service\" or expose a new Snowflake shadow jar that this wrapper can use (for expansion and for staging for runtime). What will be the size diff of \"sdks:java:io:expansion-service:shadowJar\" after including Snowflake ? If that is large we should go for the second option since this jar is shared by a bunch of cross-language wrappers.", "url": "https://github.com/apache/beam/pull/12149#discussion_r453428196", "createdAt": "2020-07-13T04:27:48Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/io/external/snowflake.py", "diffHunk": "@@ -0,0 +1,144 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import typing\n+\n+from past.builtins import unicode\n+\n+import apache_beam as beam\n+from apache_beam.transforms.external import BeamJarExpansionService\n+from apache_beam.transforms.external import ExternalTransform\n+from apache_beam.transforms.external import NamedTupleBasedPayloadBuilder\n+\n+__all__ = ['ReadFromSnowflake']\n+\n+\n+def default_io_expansion_service():\n+  return BeamJarExpansionService('sdks:java:io:expansion-service:shadowJar')", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzUzMjU4Mg==", "bodyText": "After adding snowflake to \"io:expansion-service\", jar size increased from  52,4 MB to 117,4 MB, so I think we should go with the second option, can you give me some hints how to proceed with it?", "url": "https://github.com/apache/beam/pull/12149#discussion_r453532582", "createdAt": "2020-07-13T09:53:11Z", "author": {"login": "purbanow"}, "path": "sdks/python/apache_beam/io/external/snowflake.py", "diffHunk": "@@ -0,0 +1,144 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import typing\n+\n+from past.builtins import unicode\n+\n+import apache_beam as beam\n+from apache_beam.transforms.external import BeamJarExpansionService\n+from apache_beam.transforms.external import ExternalTransform\n+from apache_beam.transforms.external import NamedTupleBasedPayloadBuilder\n+\n+__all__ = ['ReadFromSnowflake']\n+\n+\n+def default_io_expansion_service():\n+  return BeamJarExpansionService('sdks:java:io:expansion-service:shadowJar')", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyODE5Ng=="}, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDIyOTAyNQ==", "bodyText": "One solution might be to add a new expansion service jar just for Snowflake but I'm not sure if we want to keep adding expansion service jars for various IOs. On the other hand I don't think basically doubling the size of existing IO expansion service jar is acceptable since that will affect the staging/pipeline construction time for users of all Java external sources.\n@robertwb for thoughts here.", "url": "https://github.com/apache/beam/pull/12149#discussion_r460229025", "createdAt": "2020-07-24T18:48:23Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/io/external/snowflake.py", "diffHunk": "@@ -0,0 +1,144 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import typing\n+\n+from past.builtins import unicode\n+\n+import apache_beam as beam\n+from apache_beam.transforms.external import BeamJarExpansionService\n+from apache_beam.transforms.external import ExternalTransform\n+from apache_beam.transforms.external import NamedTupleBasedPayloadBuilder\n+\n+__all__ = ['ReadFromSnowflake']\n+\n+\n+def default_io_expansion_service():\n+  return BeamJarExpansionService('sdks:java:io:expansion-service:shadowJar')", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyODE5Ng=="}, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTEzMjAxOQ==", "bodyText": "Yes, I think going with a separate jar makes sense given its size.\nTo do this, create a rule for a fat jar in sdks/java/io/snowflake/build.gradle that depends on both sdks:java:expansion-service and sdks:java:io:snowflake. (I'm not an expert on how the java nature creates fat jars; perhaps the simplest would be to put this in a new package sdks/java/io/snowflake/expansion-service whose sole contents would be exactly the same as sdks/java/io/expansion-service/build.gradle except depend on snowflake rather than kafka and jdbc.", "url": "https://github.com/apache/beam/pull/12149#discussion_r461132019", "createdAt": "2020-07-27T19:54:15Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/io/external/snowflake.py", "diffHunk": "@@ -0,0 +1,144 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import typing\n+\n+from past.builtins import unicode\n+\n+import apache_beam as beam\n+from apache_beam.transforms.external import BeamJarExpansionService\n+from apache_beam.transforms.external import ExternalTransform\n+from apache_beam.transforms.external import NamedTupleBasedPayloadBuilder\n+\n+__all__ = ['ReadFromSnowflake']\n+\n+\n+def default_io_expansion_service():\n+  return BeamJarExpansionService('sdks:java:io:expansion-service:shadowJar')", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyODE5Ng=="}, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTYwNTE1OQ==", "bodyText": "Thanks @robertwb. I added it", "url": "https://github.com/apache/beam/pull/12149#discussion_r461605159", "createdAt": "2020-07-28T14:03:33Z", "author": {"login": "purbanow"}, "path": "sdks/python/apache_beam/io/external/snowflake.py", "diffHunk": "@@ -0,0 +1,144 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import typing\n+\n+from past.builtins import unicode\n+\n+import apache_beam as beam\n+from apache_beam.transforms.external import BeamJarExpansionService\n+from apache_beam.transforms.external import ExternalTransform\n+from apache_beam.transforms.external import NamedTupleBasedPayloadBuilder\n+\n+__all__ = ['ReadFromSnowflake']\n+\n+\n+def default_io_expansion_service():\n+  return BeamJarExpansionService('sdks:java:io:expansion-service:shadowJar')", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyODE5Ng=="}, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNzc4NjM0OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/io/external/snowflake.py", "isResolved": true, "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QwNDoyOTozMVrOGwbFHQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQxNjoyOToxM1rOG6On1Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyODUwOQ==", "bodyText": "Have you tested this against portable Flink/Spark and or Dataflow. Please mention here the runners this have been tested for and supported. Also mention details about prerequisites for the user. See following for an example.\nhttps://github.com/apache/beam/blob/master/sdks/python/apache_beam/io/kafka.py#L18", "url": "https://github.com/apache/beam/pull/12149#discussion_r453428509", "createdAt": "2020-07-13T04:29:31Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/io/external/snowflake.py", "diffHunk": "@@ -0,0 +1,144 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import typing\n+\n+from past.builtins import unicode\n+\n+import apache_beam as beam\n+from apache_beam.transforms.external import BeamJarExpansionService\n+from apache_beam.transforms.external import ExternalTransform\n+from apache_beam.transforms.external import NamedTupleBasedPayloadBuilder\n+\n+__all__ = ['ReadFromSnowflake']\n+\n+\n+def default_io_expansion_service():\n+  return BeamJarExpansionService('sdks:java:io:expansion-service:shadowJar')\n+\n+\n+ReadFromSnowflakeSchema = typing.NamedTuple(\n+    'WriteToSnowflakeSchema',\n+    [\n+        ('server_name', unicode),\n+        ('schema', unicode),\n+        ('database', unicode),\n+        ('staging_bucket_name', unicode),\n+        ('storage_integration_name', unicode),\n+        ('username', typing.Optional[unicode]),\n+        ('password', typing.Optional[unicode]),\n+        ('private_key_path', typing.Optional[unicode]),\n+        ('private_key_passphrase', typing.Optional[unicode]),\n+        ('o_auth_token', typing.Optional[unicode]),\n+        ('table', typing.Optional[unicode]),\n+        ('query', typing.Optional[unicode]),\n+    ])\n+\n+\n+class ReadFromSnowflake(beam.PTransform):\n+  \"\"\"An external PTransform which reads from Snowflake.\"\"\"\n+\n+  URN = 'beam:external:java:snowflake:read:v1'\n+\n+  def __init__(\n+      self,\n+      server_name,\n+      schema,\n+      database,\n+      staging_bucket_name,\n+      storage_integration_name,\n+      csv_mapper,\n+      username=None,\n+      password=None,\n+      private_key_path=None,\n+      private_key_passphrase=None,\n+      o_auth_token=None,\n+      table=None,\n+      query=None,\n+      expansion_service=None):\n+    \"\"\"\n+    Initializes a read operation from Snowflake.\n+\n+    Required parameters:\n+    :param server_name: full Snowflake server name with the following format\n+        account.region.gcp.snowflakecomputing.com.\n+    :param schema: name of the Snowflake schema in the database to use.\n+    :param database: name of the Snowflake database to use.\n+    :param staging_bucket_name: name of the Google Cloud Storage bucket.\n+        Bucket will be used as a temporary location for storing CSV files.\n+        Those temporary directories will be named\n+        `sf_copy_csv_DATE_TIME_RANDOMSUFFIX`\n+        and they will be removed automatically once Read operation finishes.\n+    :param storage_integration_name: is the name of storage integration\n+        object created according to Snowflake documentation.\n+    :param csv_mapper: specifies a function which must translate\n+        user-defined object to array of strings.\n+        SnowflakeIO uses a COPY INTO <location> statement to\n+        move data from a Snowflake table to Google Cloud Storage as CSV files.\n+        These files are then downloaded via FileIO and processed line by line.\n+        Each line is split into an array of Strings using the OpenCSV\n+        The csv_mapper function job is to give the user the possibility to\n+        convert the array of Strings to a user-defined type,\n+        ie. GenericRecord for Avro or Parquet files, or custom objects.\n+            Example:\n+                ```\n+                    def csv_mapper(strings_array):\n+ \t\t                return User(strings_array[0], int(strings_array[1])))\n+                ```\n+    :param table or query: specifies a Snowflake table name or custom SQL query\n+    :param expansion_service: specifies URL of expansion service.\n+\n+    Authentication parameters:\n+    It's required to pass one of the following combinations of valid parameters:\n+    :param username and password: specifies username and password\n+        for username/password authentication method.\n+    :param private_key_path and private_key_passphrase:\n+        specifies a private key file and password\n+        for key/ pair authentication method.\n+    :param o_auth_token: specifies access token for OAuth authentication method.\n+    \"\"\"\n+\n+    self.params = ReadFromSnowflakeSchema(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzUzMjgyMA==", "bodyText": "I tested it against Flink.\nI added missing information.", "url": "https://github.com/apache/beam/pull/12149#discussion_r453532820", "createdAt": "2020-07-13T09:53:37Z", "author": {"login": "purbanow"}, "path": "sdks/python/apache_beam/io/external/snowflake.py", "diffHunk": "@@ -0,0 +1,144 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import typing\n+\n+from past.builtins import unicode\n+\n+import apache_beam as beam\n+from apache_beam.transforms.external import BeamJarExpansionService\n+from apache_beam.transforms.external import ExternalTransform\n+from apache_beam.transforms.external import NamedTupleBasedPayloadBuilder\n+\n+__all__ = ['ReadFromSnowflake']\n+\n+\n+def default_io_expansion_service():\n+  return BeamJarExpansionService('sdks:java:io:expansion-service:shadowJar')\n+\n+\n+ReadFromSnowflakeSchema = typing.NamedTuple(\n+    'WriteToSnowflakeSchema',\n+    [\n+        ('server_name', unicode),\n+        ('schema', unicode),\n+        ('database', unicode),\n+        ('staging_bucket_name', unicode),\n+        ('storage_integration_name', unicode),\n+        ('username', typing.Optional[unicode]),\n+        ('password', typing.Optional[unicode]),\n+        ('private_key_path', typing.Optional[unicode]),\n+        ('private_key_passphrase', typing.Optional[unicode]),\n+        ('o_auth_token', typing.Optional[unicode]),\n+        ('table', typing.Optional[unicode]),\n+        ('query', typing.Optional[unicode]),\n+    ])\n+\n+\n+class ReadFromSnowflake(beam.PTransform):\n+  \"\"\"An external PTransform which reads from Snowflake.\"\"\"\n+\n+  URN = 'beam:external:java:snowflake:read:v1'\n+\n+  def __init__(\n+      self,\n+      server_name,\n+      schema,\n+      database,\n+      staging_bucket_name,\n+      storage_integration_name,\n+      csv_mapper,\n+      username=None,\n+      password=None,\n+      private_key_path=None,\n+      private_key_passphrase=None,\n+      o_auth_token=None,\n+      table=None,\n+      query=None,\n+      expansion_service=None):\n+    \"\"\"\n+    Initializes a read operation from Snowflake.\n+\n+    Required parameters:\n+    :param server_name: full Snowflake server name with the following format\n+        account.region.gcp.snowflakecomputing.com.\n+    :param schema: name of the Snowflake schema in the database to use.\n+    :param database: name of the Snowflake database to use.\n+    :param staging_bucket_name: name of the Google Cloud Storage bucket.\n+        Bucket will be used as a temporary location for storing CSV files.\n+        Those temporary directories will be named\n+        `sf_copy_csv_DATE_TIME_RANDOMSUFFIX`\n+        and they will be removed automatically once Read operation finishes.\n+    :param storage_integration_name: is the name of storage integration\n+        object created according to Snowflake documentation.\n+    :param csv_mapper: specifies a function which must translate\n+        user-defined object to array of strings.\n+        SnowflakeIO uses a COPY INTO <location> statement to\n+        move data from a Snowflake table to Google Cloud Storage as CSV files.\n+        These files are then downloaded via FileIO and processed line by line.\n+        Each line is split into an array of Strings using the OpenCSV\n+        The csv_mapper function job is to give the user the possibility to\n+        convert the array of Strings to a user-defined type,\n+        ie. GenericRecord for Avro or Parquet files, or custom objects.\n+            Example:\n+                ```\n+                    def csv_mapper(strings_array):\n+ \t\t                return User(strings_array[0], int(strings_array[1])))\n+                ```\n+    :param table or query: specifies a Snowflake table name or custom SQL query\n+    :param expansion_service: specifies URL of expansion service.\n+\n+    Authentication parameters:\n+    It's required to pass one of the following combinations of valid parameters:\n+    :param username and password: specifies username and password\n+        for username/password authentication method.\n+    :param private_key_path and private_key_passphrase:\n+        specifies a private key file and password\n+        for key/ pair authentication method.\n+    :param o_auth_token: specifies access token for OAuth authentication method.\n+    \"\"\"\n+\n+    self.params = ReadFromSnowflakeSchema(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyODUwOQ=="}, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDI0MDgyOQ==", "bodyText": "Can you try against Dataflow as well ? Instructions are here: https://github.com/apache/beam/tree/master/sdks/python/apache_beam/examples/kafkataxi\n(it's for Kafka but you should be able to adapt)", "url": "https://github.com/apache/beam/pull/12149#discussion_r460240829", "createdAt": "2020-07-24T19:14:35Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/io/external/snowflake.py", "diffHunk": "@@ -0,0 +1,144 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import typing\n+\n+from past.builtins import unicode\n+\n+import apache_beam as beam\n+from apache_beam.transforms.external import BeamJarExpansionService\n+from apache_beam.transforms.external import ExternalTransform\n+from apache_beam.transforms.external import NamedTupleBasedPayloadBuilder\n+\n+__all__ = ['ReadFromSnowflake']\n+\n+\n+def default_io_expansion_service():\n+  return BeamJarExpansionService('sdks:java:io:expansion-service:shadowJar')\n+\n+\n+ReadFromSnowflakeSchema = typing.NamedTuple(\n+    'WriteToSnowflakeSchema',\n+    [\n+        ('server_name', unicode),\n+        ('schema', unicode),\n+        ('database', unicode),\n+        ('staging_bucket_name', unicode),\n+        ('storage_integration_name', unicode),\n+        ('username', typing.Optional[unicode]),\n+        ('password', typing.Optional[unicode]),\n+        ('private_key_path', typing.Optional[unicode]),\n+        ('private_key_passphrase', typing.Optional[unicode]),\n+        ('o_auth_token', typing.Optional[unicode]),\n+        ('table', typing.Optional[unicode]),\n+        ('query', typing.Optional[unicode]),\n+    ])\n+\n+\n+class ReadFromSnowflake(beam.PTransform):\n+  \"\"\"An external PTransform which reads from Snowflake.\"\"\"\n+\n+  URN = 'beam:external:java:snowflake:read:v1'\n+\n+  def __init__(\n+      self,\n+      server_name,\n+      schema,\n+      database,\n+      staging_bucket_name,\n+      storage_integration_name,\n+      csv_mapper,\n+      username=None,\n+      password=None,\n+      private_key_path=None,\n+      private_key_passphrase=None,\n+      o_auth_token=None,\n+      table=None,\n+      query=None,\n+      expansion_service=None):\n+    \"\"\"\n+    Initializes a read operation from Snowflake.\n+\n+    Required parameters:\n+    :param server_name: full Snowflake server name with the following format\n+        account.region.gcp.snowflakecomputing.com.\n+    :param schema: name of the Snowflake schema in the database to use.\n+    :param database: name of the Snowflake database to use.\n+    :param staging_bucket_name: name of the Google Cloud Storage bucket.\n+        Bucket will be used as a temporary location for storing CSV files.\n+        Those temporary directories will be named\n+        `sf_copy_csv_DATE_TIME_RANDOMSUFFIX`\n+        and they will be removed automatically once Read operation finishes.\n+    :param storage_integration_name: is the name of storage integration\n+        object created according to Snowflake documentation.\n+    :param csv_mapper: specifies a function which must translate\n+        user-defined object to array of strings.\n+        SnowflakeIO uses a COPY INTO <location> statement to\n+        move data from a Snowflake table to Google Cloud Storage as CSV files.\n+        These files are then downloaded via FileIO and processed line by line.\n+        Each line is split into an array of Strings using the OpenCSV\n+        The csv_mapper function job is to give the user the possibility to\n+        convert the array of Strings to a user-defined type,\n+        ie. GenericRecord for Avro or Parquet files, or custom objects.\n+            Example:\n+                ```\n+                    def csv_mapper(strings_array):\n+ \t\t                return User(strings_array[0], int(strings_array[1])))\n+                ```\n+    :param table or query: specifies a Snowflake table name or custom SQL query\n+    :param expansion_service: specifies URL of expansion service.\n+\n+    Authentication parameters:\n+    It's required to pass one of the following combinations of valid parameters:\n+    :param username and password: specifies username and password\n+        for username/password authentication method.\n+    :param private_key_path and private_key_passphrase:\n+        specifies a private key file and password\n+        for key/ pair authentication method.\n+    :param o_auth_token: specifies access token for OAuth authentication method.\n+    \"\"\"\n+\n+    self.params = ReadFromSnowflakeSchema(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyODUwOQ=="}, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MTUyNTgyMw==", "bodyText": "@chamikaramj, I tried to reproduce it for Snowflake but I'm not able to overcome the following error:\nNotImplementedError: Execution of [<ExternalTransform(PTransform) label=[ExternalTransform(beam:external:java:snowflake:read:v1)]>] not implemented in runner <apache_beam.runners.dataflow.dataflow_runner.DataflowRunner \nDo you have some idea how to solve this?", "url": "https://github.com/apache/beam/pull/12149#discussion_r461525823", "createdAt": "2020-07-28T12:01:35Z", "author": {"login": "purbanow"}, "path": "sdks/python/apache_beam/io/external/snowflake.py", "diffHunk": "@@ -0,0 +1,144 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import typing\n+\n+from past.builtins import unicode\n+\n+import apache_beam as beam\n+from apache_beam.transforms.external import BeamJarExpansionService\n+from apache_beam.transforms.external import ExternalTransform\n+from apache_beam.transforms.external import NamedTupleBasedPayloadBuilder\n+\n+__all__ = ['ReadFromSnowflake']\n+\n+\n+def default_io_expansion_service():\n+  return BeamJarExpansionService('sdks:java:io:expansion-service:shadowJar')\n+\n+\n+ReadFromSnowflakeSchema = typing.NamedTuple(\n+    'WriteToSnowflakeSchema',\n+    [\n+        ('server_name', unicode),\n+        ('schema', unicode),\n+        ('database', unicode),\n+        ('staging_bucket_name', unicode),\n+        ('storage_integration_name', unicode),\n+        ('username', typing.Optional[unicode]),\n+        ('password', typing.Optional[unicode]),\n+        ('private_key_path', typing.Optional[unicode]),\n+        ('private_key_passphrase', typing.Optional[unicode]),\n+        ('o_auth_token', typing.Optional[unicode]),\n+        ('table', typing.Optional[unicode]),\n+        ('query', typing.Optional[unicode]),\n+    ])\n+\n+\n+class ReadFromSnowflake(beam.PTransform):\n+  \"\"\"An external PTransform which reads from Snowflake.\"\"\"\n+\n+  URN = 'beam:external:java:snowflake:read:v1'\n+\n+  def __init__(\n+      self,\n+      server_name,\n+      schema,\n+      database,\n+      staging_bucket_name,\n+      storage_integration_name,\n+      csv_mapper,\n+      username=None,\n+      password=None,\n+      private_key_path=None,\n+      private_key_passphrase=None,\n+      o_auth_token=None,\n+      table=None,\n+      query=None,\n+      expansion_service=None):\n+    \"\"\"\n+    Initializes a read operation from Snowflake.\n+\n+    Required parameters:\n+    :param server_name: full Snowflake server name with the following format\n+        account.region.gcp.snowflakecomputing.com.\n+    :param schema: name of the Snowflake schema in the database to use.\n+    :param database: name of the Snowflake database to use.\n+    :param staging_bucket_name: name of the Google Cloud Storage bucket.\n+        Bucket will be used as a temporary location for storing CSV files.\n+        Those temporary directories will be named\n+        `sf_copy_csv_DATE_TIME_RANDOMSUFFIX`\n+        and they will be removed automatically once Read operation finishes.\n+    :param storage_integration_name: is the name of storage integration\n+        object created according to Snowflake documentation.\n+    :param csv_mapper: specifies a function which must translate\n+        user-defined object to array of strings.\n+        SnowflakeIO uses a COPY INTO <location> statement to\n+        move data from a Snowflake table to Google Cloud Storage as CSV files.\n+        These files are then downloaded via FileIO and processed line by line.\n+        Each line is split into an array of Strings using the OpenCSV\n+        The csv_mapper function job is to give the user the possibility to\n+        convert the array of Strings to a user-defined type,\n+        ie. GenericRecord for Avro or Parquet files, or custom objects.\n+            Example:\n+                ```\n+                    def csv_mapper(strings_array):\n+ \t\t                return User(strings_array[0], int(strings_array[1])))\n+                ```\n+    :param table or query: specifies a Snowflake table name or custom SQL query\n+    :param expansion_service: specifies URL of expansion service.\n+\n+    Authentication parameters:\n+    It's required to pass one of the following combinations of valid parameters:\n+    :param username and password: specifies username and password\n+        for username/password authentication method.\n+    :param private_key_path and private_key_passphrase:\n+        specifies a private key file and password\n+        for key/ pair authentication method.\n+    :param o_auth_token: specifies access token for OAuth authentication method.\n+    \"\"\"\n+\n+    self.params = ReadFromSnowflakeSchema(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyODUwOQ=="}, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0MjIxMw==", "bodyText": "I added instructions for running Kafka against Dataflow for HEAD to here: https://github.com/apache/beam/blob/master/sdks/python/apache_beam/examples/kafkataxi/README.md\nFor example, you need to setup \"--experiments=use_runner_v2\" and \"--sdk_harness_container_image_overrides\". Feel free to try this out and update the pydoc above if successful.", "url": "https://github.com/apache/beam/pull/12149#discussion_r463342213", "createdAt": "2020-07-31T00:26:26Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/io/external/snowflake.py", "diffHunk": "@@ -0,0 +1,144 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import typing\n+\n+from past.builtins import unicode\n+\n+import apache_beam as beam\n+from apache_beam.transforms.external import BeamJarExpansionService\n+from apache_beam.transforms.external import ExternalTransform\n+from apache_beam.transforms.external import NamedTupleBasedPayloadBuilder\n+\n+__all__ = ['ReadFromSnowflake']\n+\n+\n+def default_io_expansion_service():\n+  return BeamJarExpansionService('sdks:java:io:expansion-service:shadowJar')\n+\n+\n+ReadFromSnowflakeSchema = typing.NamedTuple(\n+    'WriteToSnowflakeSchema',\n+    [\n+        ('server_name', unicode),\n+        ('schema', unicode),\n+        ('database', unicode),\n+        ('staging_bucket_name', unicode),\n+        ('storage_integration_name', unicode),\n+        ('username', typing.Optional[unicode]),\n+        ('password', typing.Optional[unicode]),\n+        ('private_key_path', typing.Optional[unicode]),\n+        ('private_key_passphrase', typing.Optional[unicode]),\n+        ('o_auth_token', typing.Optional[unicode]),\n+        ('table', typing.Optional[unicode]),\n+        ('query', typing.Optional[unicode]),\n+    ])\n+\n+\n+class ReadFromSnowflake(beam.PTransform):\n+  \"\"\"An external PTransform which reads from Snowflake.\"\"\"\n+\n+  URN = 'beam:external:java:snowflake:read:v1'\n+\n+  def __init__(\n+      self,\n+      server_name,\n+      schema,\n+      database,\n+      staging_bucket_name,\n+      storage_integration_name,\n+      csv_mapper,\n+      username=None,\n+      password=None,\n+      private_key_path=None,\n+      private_key_passphrase=None,\n+      o_auth_token=None,\n+      table=None,\n+      query=None,\n+      expansion_service=None):\n+    \"\"\"\n+    Initializes a read operation from Snowflake.\n+\n+    Required parameters:\n+    :param server_name: full Snowflake server name with the following format\n+        account.region.gcp.snowflakecomputing.com.\n+    :param schema: name of the Snowflake schema in the database to use.\n+    :param database: name of the Snowflake database to use.\n+    :param staging_bucket_name: name of the Google Cloud Storage bucket.\n+        Bucket will be used as a temporary location for storing CSV files.\n+        Those temporary directories will be named\n+        `sf_copy_csv_DATE_TIME_RANDOMSUFFIX`\n+        and they will be removed automatically once Read operation finishes.\n+    :param storage_integration_name: is the name of storage integration\n+        object created according to Snowflake documentation.\n+    :param csv_mapper: specifies a function which must translate\n+        user-defined object to array of strings.\n+        SnowflakeIO uses a COPY INTO <location> statement to\n+        move data from a Snowflake table to Google Cloud Storage as CSV files.\n+        These files are then downloaded via FileIO and processed line by line.\n+        Each line is split into an array of Strings using the OpenCSV\n+        The csv_mapper function job is to give the user the possibility to\n+        convert the array of Strings to a user-defined type,\n+        ie. GenericRecord for Avro or Parquet files, or custom objects.\n+            Example:\n+                ```\n+                    def csv_mapper(strings_array):\n+ \t\t                return User(strings_array[0], int(strings_array[1])))\n+                ```\n+    :param table or query: specifies a Snowflake table name or custom SQL query\n+    :param expansion_service: specifies URL of expansion service.\n+\n+    Authentication parameters:\n+    It's required to pass one of the following combinations of valid parameters:\n+    :param username and password: specifies username and password\n+        for username/password authentication method.\n+    :param private_key_path and private_key_passphrase:\n+        specifies a private key file and password\n+        for key/ pair authentication method.\n+    :param o_auth_token: specifies access token for OAuth authentication method.\n+    \"\"\"\n+\n+    self.params = ReadFromSnowflakeSchema(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyODUwOQ=="}, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQ1MzQ0Ng==", "bodyText": "@chamikaramj . I'm getting following error:\n  File \"/Users/urban/Desktop/beam/sdks/python/apache_beam/utils/urns.py\", line 186, in from_runner_api\n    parameter_type, constructor = cls._known_urns[fn_proto.urn]\nKeyError: 'beam:window_fn:serialized_java:v1'", "url": "https://github.com/apache/beam/pull/12149#discussion_r463453446", "createdAt": "2020-07-31T07:37:38Z", "author": {"login": "purbanow"}, "path": "sdks/python/apache_beam/io/external/snowflake.py", "diffHunk": "@@ -0,0 +1,144 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import typing\n+\n+from past.builtins import unicode\n+\n+import apache_beam as beam\n+from apache_beam.transforms.external import BeamJarExpansionService\n+from apache_beam.transforms.external import ExternalTransform\n+from apache_beam.transforms.external import NamedTupleBasedPayloadBuilder\n+\n+__all__ = ['ReadFromSnowflake']\n+\n+\n+def default_io_expansion_service():\n+  return BeamJarExpansionService('sdks:java:io:expansion-service:shadowJar')\n+\n+\n+ReadFromSnowflakeSchema = typing.NamedTuple(\n+    'WriteToSnowflakeSchema',\n+    [\n+        ('server_name', unicode),\n+        ('schema', unicode),\n+        ('database', unicode),\n+        ('staging_bucket_name', unicode),\n+        ('storage_integration_name', unicode),\n+        ('username', typing.Optional[unicode]),\n+        ('password', typing.Optional[unicode]),\n+        ('private_key_path', typing.Optional[unicode]),\n+        ('private_key_passphrase', typing.Optional[unicode]),\n+        ('o_auth_token', typing.Optional[unicode]),\n+        ('table', typing.Optional[unicode]),\n+        ('query', typing.Optional[unicode]),\n+    ])\n+\n+\n+class ReadFromSnowflake(beam.PTransform):\n+  \"\"\"An external PTransform which reads from Snowflake.\"\"\"\n+\n+  URN = 'beam:external:java:snowflake:read:v1'\n+\n+  def __init__(\n+      self,\n+      server_name,\n+      schema,\n+      database,\n+      staging_bucket_name,\n+      storage_integration_name,\n+      csv_mapper,\n+      username=None,\n+      password=None,\n+      private_key_path=None,\n+      private_key_passphrase=None,\n+      o_auth_token=None,\n+      table=None,\n+      query=None,\n+      expansion_service=None):\n+    \"\"\"\n+    Initializes a read operation from Snowflake.\n+\n+    Required parameters:\n+    :param server_name: full Snowflake server name with the following format\n+        account.region.gcp.snowflakecomputing.com.\n+    :param schema: name of the Snowflake schema in the database to use.\n+    :param database: name of the Snowflake database to use.\n+    :param staging_bucket_name: name of the Google Cloud Storage bucket.\n+        Bucket will be used as a temporary location for storing CSV files.\n+        Those temporary directories will be named\n+        `sf_copy_csv_DATE_TIME_RANDOMSUFFIX`\n+        and they will be removed automatically once Read operation finishes.\n+    :param storage_integration_name: is the name of storage integration\n+        object created according to Snowflake documentation.\n+    :param csv_mapper: specifies a function which must translate\n+        user-defined object to array of strings.\n+        SnowflakeIO uses a COPY INTO <location> statement to\n+        move data from a Snowflake table to Google Cloud Storage as CSV files.\n+        These files are then downloaded via FileIO and processed line by line.\n+        Each line is split into an array of Strings using the OpenCSV\n+        The csv_mapper function job is to give the user the possibility to\n+        convert the array of Strings to a user-defined type,\n+        ie. GenericRecord for Avro or Parquet files, or custom objects.\n+            Example:\n+                ```\n+                    def csv_mapper(strings_array):\n+ \t\t                return User(strings_array[0], int(strings_array[1])))\n+                ```\n+    :param table or query: specifies a Snowflake table name or custom SQL query\n+    :param expansion_service: specifies URL of expansion service.\n+\n+    Authentication parameters:\n+    It's required to pass one of the following combinations of valid parameters:\n+    :param username and password: specifies username and password\n+        for username/password authentication method.\n+    :param private_key_path and private_key_passphrase:\n+        specifies a private key file and password\n+        for key/ pair authentication method.\n+    :param o_auth_token: specifies access token for OAuth authentication method.\n+    \"\"\"\n+\n+    self.params = ReadFromSnowflakeSchema(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyODUwOQ=="}, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 119}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzcxMDE2NQ==", "bodyText": "Thanks. This is potentially due to https://issues.apache.org/jira/browse/BEAM-10507.", "url": "https://github.com/apache/beam/pull/12149#discussion_r463710165", "createdAt": "2020-07-31T16:29:13Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/io/external/snowflake.py", "diffHunk": "@@ -0,0 +1,144 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import typing\n+\n+from past.builtins import unicode\n+\n+import apache_beam as beam\n+from apache_beam.transforms.external import BeamJarExpansionService\n+from apache_beam.transforms.external import ExternalTransform\n+from apache_beam.transforms.external import NamedTupleBasedPayloadBuilder\n+\n+__all__ = ['ReadFromSnowflake']\n+\n+\n+def default_io_expansion_service():\n+  return BeamJarExpansionService('sdks:java:io:expansion-service:shadowJar')\n+\n+\n+ReadFromSnowflakeSchema = typing.NamedTuple(\n+    'WriteToSnowflakeSchema',\n+    [\n+        ('server_name', unicode),\n+        ('schema', unicode),\n+        ('database', unicode),\n+        ('staging_bucket_name', unicode),\n+        ('storage_integration_name', unicode),\n+        ('username', typing.Optional[unicode]),\n+        ('password', typing.Optional[unicode]),\n+        ('private_key_path', typing.Optional[unicode]),\n+        ('private_key_passphrase', typing.Optional[unicode]),\n+        ('o_auth_token', typing.Optional[unicode]),\n+        ('table', typing.Optional[unicode]),\n+        ('query', typing.Optional[unicode]),\n+    ])\n+\n+\n+class ReadFromSnowflake(beam.PTransform):\n+  \"\"\"An external PTransform which reads from Snowflake.\"\"\"\n+\n+  URN = 'beam:external:java:snowflake:read:v1'\n+\n+  def __init__(\n+      self,\n+      server_name,\n+      schema,\n+      database,\n+      staging_bucket_name,\n+      storage_integration_name,\n+      csv_mapper,\n+      username=None,\n+      password=None,\n+      private_key_path=None,\n+      private_key_passphrase=None,\n+      o_auth_token=None,\n+      table=None,\n+      query=None,\n+      expansion_service=None):\n+    \"\"\"\n+    Initializes a read operation from Snowflake.\n+\n+    Required parameters:\n+    :param server_name: full Snowflake server name with the following format\n+        account.region.gcp.snowflakecomputing.com.\n+    :param schema: name of the Snowflake schema in the database to use.\n+    :param database: name of the Snowflake database to use.\n+    :param staging_bucket_name: name of the Google Cloud Storage bucket.\n+        Bucket will be used as a temporary location for storing CSV files.\n+        Those temporary directories will be named\n+        `sf_copy_csv_DATE_TIME_RANDOMSUFFIX`\n+        and they will be removed automatically once Read operation finishes.\n+    :param storage_integration_name: is the name of storage integration\n+        object created according to Snowflake documentation.\n+    :param csv_mapper: specifies a function which must translate\n+        user-defined object to array of strings.\n+        SnowflakeIO uses a COPY INTO <location> statement to\n+        move data from a Snowflake table to Google Cloud Storage as CSV files.\n+        These files are then downloaded via FileIO and processed line by line.\n+        Each line is split into an array of Strings using the OpenCSV\n+        The csv_mapper function job is to give the user the possibility to\n+        convert the array of Strings to a user-defined type,\n+        ie. GenericRecord for Avro or Parquet files, or custom objects.\n+            Example:\n+                ```\n+                    def csv_mapper(strings_array):\n+ \t\t                return User(strings_array[0], int(strings_array[1])))\n+                ```\n+    :param table or query: specifies a Snowflake table name or custom SQL query\n+    :param expansion_service: specifies URL of expansion service.\n+\n+    Authentication parameters:\n+    It's required to pass one of the following combinations of valid parameters:\n+    :param username and password: specifies username and password\n+        for username/password authentication method.\n+    :param private_key_path and private_key_passphrase:\n+        specifies a private key file and password\n+        for key/ pair authentication method.\n+    :param o_auth_token: specifies access token for OAuth authentication method.\n+    \"\"\"\n+\n+    self.params = ReadFromSnowflakeSchema(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzQyODUwOQ=="}, "originalCommit": {"oid": "be8ce595829b2b86e0e66f7ce26efe3eeba23f72"}, "originalPosition": 119}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5Mjg5OTYxOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/io/external/snowflake.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwMDoyMzo0OFrOG54Hvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0zMVQwNjoyNToyOFrOG59XxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0MTUwMw==", "bodyText": "Pls drop \"This option is only available for Beam 2.22.0 and later\". This was just for Kafka.", "url": "https://github.com/apache/beam/pull/12149#discussion_r463341503", "createdAt": "2020-07-31T00:23:48Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/io/external/snowflake.py", "diffHunk": "@@ -0,0 +1,185 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import typing\n+\n+from past.builtins import unicode\n+\n+import apache_beam as beam\n+from apache_beam.transforms.external import BeamJarExpansionService\n+from apache_beam.transforms.external import ExternalTransform\n+from apache_beam.transforms.external import NamedTupleBasedPayloadBuilder\n+\n+\"\"\"Snowflake transforms tested against Flink portable runner.\n+  **Setup**\n+  Transforms provided in this module are cross-language transforms\n+  implemented in the Beam Java SDK. During the pipeline construction, Python SDK\n+  will connect to a Java expansion service to expand these transforms.\n+  To facilitate this, a small amount of setup is needed before using these\n+  transforms in a Beam Python pipeline.\n+  There are several ways to setup cross-language Snowflake transforms.\n+  * Option 1: use the default expansion service\n+  * Option 2: specify a custom expansion service\n+  See below for details regarding each of these options.\n+  *Option 1: Use the default expansion service*\n+  This is the recommended and easiest setup option for using Python Kafka\n+  transforms. This option is only available for Beam 2.22.0 and later.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9f42a997c017828f731db07ec40b33d7bceef139"}, "originalPosition": 44}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzQyNzUyNA==", "bodyText": "Thanks, I fixed it.", "url": "https://github.com/apache/beam/pull/12149#discussion_r463427524", "createdAt": "2020-07-31T06:25:28Z", "author": {"login": "purbanow"}, "path": "sdks/python/apache_beam/io/external/snowflake.py", "diffHunk": "@@ -0,0 +1,185 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import typing\n+\n+from past.builtins import unicode\n+\n+import apache_beam as beam\n+from apache_beam.transforms.external import BeamJarExpansionService\n+from apache_beam.transforms.external import ExternalTransform\n+from apache_beam.transforms.external import NamedTupleBasedPayloadBuilder\n+\n+\"\"\"Snowflake transforms tested against Flink portable runner.\n+  **Setup**\n+  Transforms provided in this module are cross-language transforms\n+  implemented in the Beam Java SDK. During the pipeline construction, Python SDK\n+  will connect to a Java expansion service to expand these transforms.\n+  To facilitate this, a small amount of setup is needed before using these\n+  transforms in a Beam Python pipeline.\n+  There are several ways to setup cross-language Snowflake transforms.\n+  * Option 1: use the default expansion service\n+  * Option 2: specify a custom expansion service\n+  See below for details regarding each of these options.\n+  *Option 1: Use the default expansion service*\n+  This is the recommended and easiest setup option for using Python Kafka\n+  transforms. This option is only available for Beam 2.22.0 and later.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MzM0MTUwMw=="}, "originalCommit": {"oid": "9f42a997c017828f731db07ec40b33d7bceef139"}, "originalPosition": 44}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3428, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}