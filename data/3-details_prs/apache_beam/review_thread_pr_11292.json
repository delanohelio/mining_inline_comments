{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0Mzk3Njk4NDU4", "number": 11292, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwMDozMzoyNlrODvWrbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQyMDoyNDoyOVrODyOwMQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUwOTgxMjI5OnYy", "diffSide": "LEFT", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryServices.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QwMDozMzoyNlrOGBuzeA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0wN1QxNzo1NDoxMFrOGCPYiA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ2ODYwMA==", "bodyText": "Is this removing a public API?", "url": "https://github.com/apache/beam/pull/11292#discussion_r404468600", "createdAt": "2020-04-07T00:33:26Z", "author": {"login": "aaltay"}, "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryServices.java", "diffHunk": "@@ -101,10 +101,6 @@ JobStatistics dryRunQuery(String projectId, JobConfigurationQuery queryConfig, S\n     @Nullable\n     Table getTable(TableReference tableRef) throws InterruptedException, IOException;\n \n-    @Nullable", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4f15d7e52211cf1dc08bd36b7fcf6bd1003ff3ef"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDk5NjA4OA==", "bodyText": "BigQueryServices is a public interface, although it's meant as a private implementation for BigQuery sources and sinks and is not surfaced at the BigQueryIO level. I can restore this functionality and simply stop calling it from BigQueryStorageStreamSource.", "url": "https://github.com/apache/beam/pull/11292#discussion_r404996088", "createdAt": "2020-04-07T17:44:07Z", "author": {"login": "kmjung"}, "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryServices.java", "diffHunk": "@@ -101,10 +101,6 @@ JobStatistics dryRunQuery(String projectId, JobConfigurationQuery queryConfig, S\n     @Nullable\n     Table getTable(TableReference tableRef) throws InterruptedException, IOException;\n \n-    @Nullable", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ2ODYwMA=="}, "originalCommit": {"oid": "4f15d7e52211cf1dc08bd36b7fcf6bd1003ff3ef"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNTAwMjM3Ng==", "bodyText": "Done.", "url": "https://github.com/apache/beam/pull/11292#discussion_r405002376", "createdAt": "2020-04-07T17:54:10Z", "author": {"login": "kmjung"}, "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryServices.java", "diffHunk": "@@ -101,10 +101,6 @@ JobStatistics dryRunQuery(String projectId, JobConfigurationQuery queryConfig, S\n     @Nullable\n     Table getTable(TableReference tableRef) throws InterruptedException, IOException;\n \n-    @Nullable", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwNDQ2ODYwMA=="}, "originalCommit": {"oid": "4f15d7e52211cf1dc08bd36b7fcf6bd1003ff3ef"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzOTk0MTMzOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryStorageSourceBase.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQyMDoxNDo1M1rOGGJ6Ug==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQyMDozNTo1MVrOGGKkiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTEwNzAyNg==", "bodyText": "Not sure how this trims the schema ? Does readSession.getAvroSchema() somehow has a smaller number of fields than targetTable.getSchema() ?", "url": "https://github.com/apache/beam/pull/11292#discussion_r409107026", "createdAt": "2020-04-15T20:14:53Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryStorageSourceBase.java", "diffHunk": "@@ -149,11 +151,14 @@\n       return ImmutableList.of();\n     }\n \n+    Schema sessionSchema = new Schema.Parser().parse(readSession.getAvroSchema().getSchema());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20aee714aa9a51319ec39c10f4404c5d0994648b"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTExNzgzMw==", "bodyText": "With this change, we're no longer specifying the list of selected fields to the tables.get call from which the BigQuery schema is taken; as a result, we get the entire table schema back, so we have to trim it on the client side in the case where the client has specified selected fields. The Avro schema is returned as part of the read session and contains only the selected fields, so we use it as the basis for trimming the BigQuery schema.", "url": "https://github.com/apache/beam/pull/11292#discussion_r409117833", "createdAt": "2020-04-15T20:35:51Z", "author": {"login": "kmjung"}, "path": "sdks/java/io/google-cloud-platform/src/main/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryStorageSourceBase.java", "diffHunk": "@@ -149,11 +151,14 @@\n       return ImmutableList.of();\n     }\n \n+    Schema sessionSchema = new Schema.Parser().parse(readSession.getAvroSchema().getSchema());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTEwNzAyNg=="}, "originalCommit": {"oid": "20aee714aa9a51319ec39c10f4404c5d0994648b"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjUzOTk3MTA1OnYy", "diffSide": "LEFT", "path": "sdks/java/io/google-cloud-platform/src/test/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIOStorageReadTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQyMDoyNDoyOVrOGGKNTg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNC0xNVQyMDozODowN1rOGGKpBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTExMTg4Ng==", "bodyText": "Is this a feature regression for the storage API based read path ?", "url": "https://github.com/apache/beam/pull/11292#discussion_r409111886", "createdAt": "2020-04-15T20:24:29Z", "author": {"login": "chamikaramj"}, "path": "sdks/java/io/google-cloud-platform/src/test/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIOStorageReadTest.java", "diffHunk": "@@ -1368,19 +1368,14 @@ public void testStreamSourceSplitAtFractionFailsWhenParentIsPastSplitPoint() thr\n   public void testReadFromBigQueryIO() throws Exception {\n     fakeDatasetService.createDataset(\"foo.com:project\", \"dataset\", \"\", \"\", null);\n     TableReference tableRef = BigQueryHelpers.parseTableSpec(\"foo.com:project:dataset.table\");\n-\n-    Table table =\n-        new Table().setTableReference(tableRef).setNumBytes(10L).setSchema(new TableSchema());\n-\n+    Table table = new Table().setTableReference(tableRef).setNumBytes(10L).setSchema(TABLE_SCHEMA);\n     fakeDatasetService.createTable(table);\n \n     CreateReadSessionRequest expectedCreateReadSessionRequest =\n         CreateReadSessionRequest.newBuilder()\n             .setParent(\"projects/project-id\")\n             .setTableReference(BigQueryHelpers.toTableRefProto(tableRef))\n             .setRequestedStreams(10)\n-            .setReadOptions(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "20aee714aa9a51319ec39c10f4404c5d0994648b"}, "originalPosition": 209}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTExODk4MQ==", "bodyText": "I'm not sure that I understand your question here. We used to have e2e tests for BigQueryIO with the read API only for the case where the caller specified all fields as selected fields; with this change, now we have two tests -- one which covers the case where no selected fields are covered (which is effectively the same as the case where all fields are specified), and another which covers the case where only a subset of fields are specified.", "url": "https://github.com/apache/beam/pull/11292#discussion_r409118981", "createdAt": "2020-04-15T20:38:07Z", "author": {"login": "kmjung"}, "path": "sdks/java/io/google-cloud-platform/src/test/java/org/apache/beam/sdk/io/gcp/bigquery/BigQueryIOStorageReadTest.java", "diffHunk": "@@ -1368,19 +1368,14 @@ public void testStreamSourceSplitAtFractionFailsWhenParentIsPastSplitPoint() thr\n   public void testReadFromBigQueryIO() throws Exception {\n     fakeDatasetService.createDataset(\"foo.com:project\", \"dataset\", \"\", \"\", null);\n     TableReference tableRef = BigQueryHelpers.parseTableSpec(\"foo.com:project:dataset.table\");\n-\n-    Table table =\n-        new Table().setTableReference(tableRef).setNumBytes(10L).setSchema(new TableSchema());\n-\n+    Table table = new Table().setTableReference(tableRef).setNumBytes(10L).setSchema(TABLE_SCHEMA);\n     fakeDatasetService.createTable(table);\n \n     CreateReadSessionRequest expectedCreateReadSessionRequest =\n         CreateReadSessionRequest.newBuilder()\n             .setParent(\"projects/project-id\")\n             .setTableReference(BigQueryHelpers.toTableRefProto(tableRef))\n             .setRequestedStreams(10)\n-            .setReadOptions(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQwOTExMTg4Ng=="}, "originalCommit": {"oid": "20aee714aa9a51319ec39c10f4404c5d0994648b"}, "originalPosition": 209}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1612, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}