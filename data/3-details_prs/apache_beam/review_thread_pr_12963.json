{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDk0OTM3ODg0", "number": 12963, "reviewThreads": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDoxODoxOFrOEou8uQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQyMzo0NDoxMVrOFERpvw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMTQ3NzA1OnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/get-started/from-spark.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDoxODoxOFrOHZ_nGw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQyMToyMTozNlrOHkiIRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAxODY1MQ==", "bodyText": "The Beam and Spark APIs are mostly equivalent. The backend stuff, not so much.\nI might reword this as, \"The Beam and Spark APIs are similar.\"", "url": "https://github.com/apache/beam/pull/12963#discussion_r497018651", "createdAt": "2020-09-29T20:18:18Z", "author": {"login": "ibzib"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,245 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\"); \n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS, \n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+Beam and Spark are mostly equivalent, so you already know the basic concepts.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA2OTk1OA==", "bodyText": "Thanks, done", "url": "https://github.com/apache/beam/pull/12963#discussion_r508069958", "createdAt": "2020-10-19T21:21:36Z", "author": {"login": "davidcavazos"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,245 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\"); \n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS, \n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+Beam and Spark are mostly equivalent, so you already know the basic concepts.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAxODY1MQ=="}, "originalCommit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d"}, "originalPosition": 24}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMTQ5NjA3OnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/get-started/from-spark.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDoyMzo0NVrOHZ_yow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQyMTo0MjoyOFrOHkiueQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAyMTYwMw==", "bodyText": "It might be worth mentioning a couple other important details here:\n\nBeam pipelines are constructed lazily, in other words, no computation happens until pipeline.run() is called.\nwith beam.Pipeline() as pipeline implicitly calls pipeline.run().", "url": "https://github.com/apache/beam/pull/12963#discussion_r497021603", "createdAt": "2020-09-29T20:23:45Z", "author": {"login": "ibzib"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,245 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\"); \n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS, \n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+Beam and Spark are mostly equivalent, so you already know the basic concepts.\n+\n+A collection of elements in Spark is called a _Resilient Distributed Dataset_ (RDD),\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    result = (\n+        sc.parallelize([1, 2, 3, 4])\n+        .map(lambda x: x * 2)\n+        .reduce(lambda x, y: x + y)\n+    )\n+    print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA3OTczNw==", "bodyText": "I added a note just below the example.", "url": "https://github.com/apache/beam/pull/12963#discussion_r508079737", "createdAt": "2020-10-19T21:42:28Z", "author": {"login": "davidcavazos"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,245 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\"); \n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS, \n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+Beam and Spark are mostly equivalent, so you already know the basic concepts.\n+\n+A collection of elements in Spark is called a _Resilient Distributed Dataset_ (RDD),\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    result = (\n+        sc.parallelize([1, 2, 3, 4])\n+        .map(lambda x: x * 2)\n+        .reduce(lambda x, y: x + y)\n+    )\n+    print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAyMTYwMw=="}, "originalCommit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMTUwNDIxOnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/get-started/from-spark.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDoyNTo1OVrOHZ_3cg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQyMToyMjozMlrOHkiJ4g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAyMjgzNA==", "bodyText": "Nit: should we add a label here too for consistency?", "url": "https://github.com/apache/beam/pull/12963#discussion_r497022834", "createdAt": "2020-09-29T20:25:59Z", "author": {"login": "ibzib"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,245 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\"); \n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS, \n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+Beam and Spark are mostly equivalent, so you already know the basic concepts.\n+\n+A collection of elements in Spark is called a _Resilient Distributed Dataset_ (RDD),\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    result = (\n+        sc.parallelize([1, 2, 3, 4])\n+        .map(lambda x: x * 2)\n+        .reduce(lambda x, y: x + y)\n+    )\n+    print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+A label can optionally be added to a transform using the\n+_right shift operator_ `>>` like `data | 'My description' >> beam.Map(...)`.\n+This serves both as comments and makes your pipeline easier to debug.\n+\n+This is how the pipeline looks after adding labels.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | 'Create numbers' >> beam.Create([1, 2, 3, 4])\n+        | 'Multiply by two' >> beam.Map(lambda x: x * 2)\n+        | 'Sum everything' >> beam.CombineGlobally(sum)\n+        | beam.Map(print)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d"}, "originalPosition": 89}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA3MDM3MA==", "bodyText": "Added", "url": "https://github.com/apache/beam/pull/12963#discussion_r508070370", "createdAt": "2020-10-19T21:22:32Z", "author": {"login": "davidcavazos"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,245 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\"); \n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS, \n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+Beam and Spark are mostly equivalent, so you already know the basic concepts.\n+\n+A collection of elements in Spark is called a _Resilient Distributed Dataset_ (RDD),\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    result = (\n+        sc.parallelize([1, 2, 3, 4])\n+        .map(lambda x: x * 2)\n+        .reduce(lambda x, y: x + y)\n+    )\n+    print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+A label can optionally be added to a transform using the\n+_right shift operator_ `>>` like `data | 'My description' >> beam.Map(...)`.\n+This serves both as comments and makes your pipeline easier to debug.\n+\n+This is how the pipeline looks after adding labels.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | 'Create numbers' >> beam.Create([1, 2, 3, 4])\n+        | 'Multiply by two' >> beam.Map(lambda x: x * 2)\n+        | 'Sum everything' >> beam.CombineGlobally(sum)\n+        | beam.Map(print)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAyMjgzNA=="}, "originalCommit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d"}, "originalPosition": 89}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMTUyOTYxOnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/get-started/from-spark.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDozMzoxNlrOHaAHDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQyMTo1MTozMlrOHki-xg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAyNjgzMA==", "bodyText": "It'd be really nice if each Beam transform was linked to its respective documentation/transforms page.", "url": "https://github.com/apache/beam/pull/12963#discussion_r497026830", "createdAt": "2020-09-29T20:33:16Z", "author": {"login": "ibzib"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,245 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\"); \n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS, \n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+Beam and Spark are mostly equivalent, so you already know the basic concepts.\n+\n+A collection of elements in Spark is called a _Resilient Distributed Dataset_ (RDD),\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    result = (\n+        sc.parallelize([1, 2, 3, 4])\n+        .map(lambda x: x * 2)\n+        .reduce(lambda x, y: x + y)\n+    )\n+    print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+A label can optionally be added to a transform using the\n+_right shift operator_ `>>` like `data | 'My description' >> beam.Map(...)`.\n+This serves both as comments and makes your pipeline easier to debug.\n+\n+This is how the pipeline looks after adding labels.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | 'Create numbers' >> beam.Create([1, 2, 3, 4])\n+        | 'Multiply by two' >> beam.Map(lambda x: x * 2)\n+        | 'Sum everything' >> beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+## Setup\n+\n+Here's a comparison on how to get started both in PySpark and Beam.\n+\n+{{< table >}}\n+<table>\n+<tr>\n+    <th></th>\n+    <th>PySpark</th>\n+    <th>Beam</th>\n+</tr>\n+<tr>\n+    <td><b>Install</b></td>\n+    <td><code>$ pip install pyspark</code></td>\n+    <td><code>$ pip install apache-beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Imports</b></td>\n+    <td><code>import pyspark</code></td>\n+    <td><code>import apache_beam as beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating a<br>local pipeline</b></td>\n+    <td>\n+        <code>with pyspark.SparkContext() as sc:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+    <td>\n+        <code>with beam.Pipeline() as pipeline:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Creating values</b></td>\n+    <td><code>values = sc.parallelize([1, 2, 3, 4])</code></td>\n+    <td><code>values = pipeline | beam.Create([1, 2, 3, 4])</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating<br>key-value pairs</b></td>\n+    <td>\n+        <code>pairs = sc.parallelize([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+    <td>\n+        <code>pairs = pipeline | beam.Create([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Running a<br>local pipeline</b></td>\n+    <td><code>$ spark-submit spark_pipeline.py</code></td>\n+    <td><code>$ python beam_pipeline.py</code></td>\n+</tr>\n+</table>\n+{{< /table >}}\n+\n+## Transforms\n+\n+Here are the equivalents of some common transforms in both PySpark and Beam.\n+\n+{{< table >}}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d"}, "originalPosition": 159}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA4MzkxMA==", "bodyText": "Sure, the table gets a little uglier, and definitely over 80 characters long, but I can do that.\nPS: I tried using reference links, but the website doesn't like them to be inside a table (even HTML instead of Markdown), so they have to be inlined if we want them here.", "url": "https://github.com/apache/beam/pull/12963#discussion_r508083910", "createdAt": "2020-10-19T21:51:32Z", "author": {"login": "davidcavazos"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,245 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\"); \n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS, \n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+Beam and Spark are mostly equivalent, so you already know the basic concepts.\n+\n+A collection of elements in Spark is called a _Resilient Distributed Dataset_ (RDD),\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    result = (\n+        sc.parallelize([1, 2, 3, 4])\n+        .map(lambda x: x * 2)\n+        .reduce(lambda x, y: x + y)\n+    )\n+    print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+A label can optionally be added to a transform using the\n+_right shift operator_ `>>` like `data | 'My description' >> beam.Map(...)`.\n+This serves both as comments and makes your pipeline easier to debug.\n+\n+This is how the pipeline looks after adding labels.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | 'Create numbers' >> beam.Create([1, 2, 3, 4])\n+        | 'Multiply by two' >> beam.Map(lambda x: x * 2)\n+        | 'Sum everything' >> beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+## Setup\n+\n+Here's a comparison on how to get started both in PySpark and Beam.\n+\n+{{< table >}}\n+<table>\n+<tr>\n+    <th></th>\n+    <th>PySpark</th>\n+    <th>Beam</th>\n+</tr>\n+<tr>\n+    <td><b>Install</b></td>\n+    <td><code>$ pip install pyspark</code></td>\n+    <td><code>$ pip install apache-beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Imports</b></td>\n+    <td><code>import pyspark</code></td>\n+    <td><code>import apache_beam as beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating a<br>local pipeline</b></td>\n+    <td>\n+        <code>with pyspark.SparkContext() as sc:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+    <td>\n+        <code>with beam.Pipeline() as pipeline:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Creating values</b></td>\n+    <td><code>values = sc.parallelize([1, 2, 3, 4])</code></td>\n+    <td><code>values = pipeline | beam.Create([1, 2, 3, 4])</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating<br>key-value pairs</b></td>\n+    <td>\n+        <code>pairs = sc.parallelize([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+    <td>\n+        <code>pairs = pipeline | beam.Create([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Running a<br>local pipeline</b></td>\n+    <td><code>$ spark-submit spark_pipeline.py</code></td>\n+    <td><code>$ python beam_pipeline.py</code></td>\n+</tr>\n+</table>\n+{{< /table >}}\n+\n+## Transforms\n+\n+Here are the equivalents of some common transforms in both PySpark and Beam.\n+\n+{{< table >}}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAyNjgzMA=="}, "originalCommit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d"}, "originalPosition": 159}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMTU2NzA3OnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/get-started/from-spark.md", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDozOTozOFrOHaAfyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQyMjoxNzo1MlrOHkjqTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAzMzE2Mw==", "bodyText": "Probably better to use lists instead of tuples in Beam. I'm not sure tuples are adequately tested/supported: https://the-asf.slack.com/archives/CBDNLQZM1/p1598454585014000", "url": "https://github.com/apache/beam/pull/12963#discussion_r497033163", "createdAt": "2020-09-29T20:39:38Z", "author": {"login": "ibzib"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,245 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\"); \n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS, \n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+Beam and Spark are mostly equivalent, so you already know the basic concepts.\n+\n+A collection of elements in Spark is called a _Resilient Distributed Dataset_ (RDD),\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    result = (\n+        sc.parallelize([1, 2, 3, 4])\n+        .map(lambda x: x * 2)\n+        .reduce(lambda x, y: x + y)\n+    )\n+    print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+A label can optionally be added to a transform using the\n+_right shift operator_ `>>` like `data | 'My description' >> beam.Map(...)`.\n+This serves both as comments and makes your pipeline easier to debug.\n+\n+This is how the pipeline looks after adding labels.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | 'Create numbers' >> beam.Create([1, 2, 3, 4])\n+        | 'Multiply by two' >> beam.Map(lambda x: x * 2)\n+        | 'Sum everything' >> beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+## Setup\n+\n+Here's a comparison on how to get started both in PySpark and Beam.\n+\n+{{< table >}}\n+<table>\n+<tr>\n+    <th></th>\n+    <th>PySpark</th>\n+    <th>Beam</th>\n+</tr>\n+<tr>\n+    <td><b>Install</b></td>\n+    <td><code>$ pip install pyspark</code></td>\n+    <td><code>$ pip install apache-beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Imports</b></td>\n+    <td><code>import pyspark</code></td>\n+    <td><code>import apache_beam as beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating a<br>local pipeline</b></td>\n+    <td>\n+        <code>with pyspark.SparkContext() as sc:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+    <td>\n+        <code>with beam.Pipeline() as pipeline:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Creating values</b></td>\n+    <td><code>values = sc.parallelize([1, 2, 3, 4])</code></td>\n+    <td><code>values = pipeline | beam.Create([1, 2, 3, 4])</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating<br>key-value pairs</b></td>\n+    <td>\n+        <code>pairs = sc.parallelize([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+    <td>\n+        <code>pairs = pipeline | beam.Create([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d"}, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA4MjQ2Mw==", "bodyText": "I don't have an @apache.org email so I can't open the discussion.\nI thought tuples were supported better than lists. I've been using tuples for all the samples and I've never had an issue. I also think type-wise, tuples are \"allowed\" to have elements of any/different types, while lists should only be of one type. On a type checker like mypy or any other typed language, it would be a compile error to have a list of two different types, unless you use Any which should be discouraged.\nI think I would prefer to keep these as tuples, @aaltay what do you think?", "url": "https://github.com/apache/beam/pull/12963#discussion_r508082463", "createdAt": "2020-10-19T21:48:32Z", "author": {"login": "davidcavazos"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,245 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\"); \n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS, \n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+Beam and Spark are mostly equivalent, so you already know the basic concepts.\n+\n+A collection of elements in Spark is called a _Resilient Distributed Dataset_ (RDD),\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    result = (\n+        sc.parallelize([1, 2, 3, 4])\n+        .map(lambda x: x * 2)\n+        .reduce(lambda x, y: x + y)\n+    )\n+    print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+A label can optionally be added to a transform using the\n+_right shift operator_ `>>` like `data | 'My description' >> beam.Map(...)`.\n+This serves both as comments and makes your pipeline easier to debug.\n+\n+This is how the pipeline looks after adding labels.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | 'Create numbers' >> beam.Create([1, 2, 3, 4])\n+        | 'Multiply by two' >> beam.Map(lambda x: x * 2)\n+        | 'Sum everything' >> beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+## Setup\n+\n+Here's a comparison on how to get started both in PySpark and Beam.\n+\n+{{< table >}}\n+<table>\n+<tr>\n+    <th></th>\n+    <th>PySpark</th>\n+    <th>Beam</th>\n+</tr>\n+<tr>\n+    <td><b>Install</b></td>\n+    <td><code>$ pip install pyspark</code></td>\n+    <td><code>$ pip install apache-beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Imports</b></td>\n+    <td><code>import pyspark</code></td>\n+    <td><code>import apache_beam as beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating a<br>local pipeline</b></td>\n+    <td>\n+        <code>with pyspark.SparkContext() as sc:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+    <td>\n+        <code>with beam.Pipeline() as pipeline:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Creating values</b></td>\n+    <td><code>values = sc.parallelize([1, 2, 3, 4])</code></td>\n+    <td><code>values = pipeline | beam.Create([1, 2, 3, 4])</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating<br>key-value pairs</b></td>\n+    <td>\n+        <code>pairs = sc.parallelize([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+    <td>\n+        <code>pairs = pipeline | beam.Create([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAzMzE2Mw=="}, "originalCommit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d"}, "originalPosition": 141}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA5NTA1Mw==", "bodyText": "I don't have an @apache.org email so I can't open the discussion.\n\nAnyone should be able to join the ASF slack. There is an invite link on the Beam website: https://beam.apache.org/community/contact-us/\nThe case I linked might be an isolated bug. If tuples have worked for you, I'm fine with keeping them in the documentation.", "url": "https://github.com/apache/beam/pull/12963#discussion_r508095053", "createdAt": "2020-10-19T22:17:52Z", "author": {"login": "ibzib"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,245 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\"); \n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS, \n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+Beam and Spark are mostly equivalent, so you already know the basic concepts.\n+\n+A collection of elements in Spark is called a _Resilient Distributed Dataset_ (RDD),\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    result = (\n+        sc.parallelize([1, 2, 3, 4])\n+        .map(lambda x: x * 2)\n+        .reduce(lambda x, y: x + y)\n+    )\n+    print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+A label can optionally be added to a transform using the\n+_right shift operator_ `>>` like `data | 'My description' >> beam.Map(...)`.\n+This serves both as comments and makes your pipeline easier to debug.\n+\n+This is how the pipeline looks after adding labels.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | 'Create numbers' >> beam.Create([1, 2, 3, 4])\n+        | 'Multiply by two' >> beam.Map(lambda x: x * 2)\n+        | 'Sum everything' >> beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+## Setup\n+\n+Here's a comparison on how to get started both in PySpark and Beam.\n+\n+{{< table >}}\n+<table>\n+<tr>\n+    <th></th>\n+    <th>PySpark</th>\n+    <th>Beam</th>\n+</tr>\n+<tr>\n+    <td><b>Install</b></td>\n+    <td><code>$ pip install pyspark</code></td>\n+    <td><code>$ pip install apache-beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Imports</b></td>\n+    <td><code>import pyspark</code></td>\n+    <td><code>import apache_beam as beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating a<br>local pipeline</b></td>\n+    <td>\n+        <code>with pyspark.SparkContext() as sc:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+    <td>\n+        <code>with beam.Pipeline() as pipeline:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Creating values</b></td>\n+    <td><code>values = sc.parallelize([1, 2, 3, 4])</code></td>\n+    <td><code>values = pipeline | beam.Create([1, 2, 3, 4])</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating<br>key-value pairs</b></td>\n+    <td>\n+        <code>pairs = sc.parallelize([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+    <td>\n+        <code>pairs = pipeline | beam.Create([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAzMzE2Mw=="}, "originalCommit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d"}, "originalPosition": 141}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMTU5NzcwOnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/get-started/from-spark.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDo0NDo0NFrOHaA0NQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQyMjowNjoxNVrOHkjXnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAzODM4OQ==", "bodyText": "Nit: clarify what you mean here by \"Python value\" (in this case it's an int value).", "url": "https://github.com/apache/beam/pull/12963#discussion_r497038389", "createdAt": "2020-09-29T20:44:44Z", "author": {"login": "ibzib"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,245 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\"); \n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS, \n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+Beam and Spark are mostly equivalent, so you already know the basic concepts.\n+\n+A collection of elements in Spark is called a _Resilient Distributed Dataset_ (RDD),\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    result = (\n+        sc.parallelize([1, 2, 3, 4])\n+        .map(lambda x: x * 2)\n+        .reduce(lambda x, y: x + y)\n+    )\n+    print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+A label can optionally be added to a transform using the\n+_right shift operator_ `>>` like `data | 'My description' >> beam.Map(...)`.\n+This serves both as comments and makes your pipeline easier to debug.\n+\n+This is how the pipeline looks after adding labels.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | 'Create numbers' >> beam.Create([1, 2, 3, 4])\n+        | 'Multiply by two' >> beam.Map(lambda x: x * 2)\n+        | 'Sum everything' >> beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+## Setup\n+\n+Here's a comparison on how to get started both in PySpark and Beam.\n+\n+{{< table >}}\n+<table>\n+<tr>\n+    <th></th>\n+    <th>PySpark</th>\n+    <th>Beam</th>\n+</tr>\n+<tr>\n+    <td><b>Install</b></td>\n+    <td><code>$ pip install pyspark</code></td>\n+    <td><code>$ pip install apache-beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Imports</b></td>\n+    <td><code>import pyspark</code></td>\n+    <td><code>import apache_beam as beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating a<br>local pipeline</b></td>\n+    <td>\n+        <code>with pyspark.SparkContext() as sc:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+    <td>\n+        <code>with beam.Pipeline() as pipeline:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Creating values</b></td>\n+    <td><code>values = sc.parallelize([1, 2, 3, 4])</code></td>\n+    <td><code>values = pipeline | beam.Create([1, 2, 3, 4])</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating<br>key-value pairs</b></td>\n+    <td>\n+        <code>pairs = sc.parallelize([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+    <td>\n+        <code>pairs = pipeline | beam.Create([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Running a<br>local pipeline</b></td>\n+    <td><code>$ spark-submit spark_pipeline.py</code></td>\n+    <td><code>$ python beam_pipeline.py</code></td>\n+</tr>\n+</table>\n+{{< /table >}}\n+\n+## Transforms\n+\n+Here are the equivalents of some common transforms in both PySpark and Beam.\n+\n+{{< table >}}\n+|                   | PySpark                               | Beam                                                    |\n+|-------------------|---------------------------------------|---------------------------------------------------------|\n+| **Map**           | `values.map(lambda x: x * 2)`         | `values | beam.Map(lambda x: x * 2)`                    |\n+| **Filter**        | `values.filter(lambda x: x % 2 == 0)` | `values | beam.Filter(lambda x: x % 2 == 0)`            |\n+| **FlatMap**       | `values.flatMap(lambda x: range(x))`  | `values | beam.FlatMap(lambda x: range(x))`             |\n+| **Group by key**  | `pairs.groupByKey()`                  | `pairs | beam.GroupByKey()`                             |\n+| **Reduce**        | `values.reduce(lambda x, y: x+y)`     | `values | beam.CombineGlobally(sum)`                    |\n+| **Reduce by key** | `pairs.reduceByKey(lambda x, y: x+y)` | `pairs | beam.CombinePerKey(sum)`                       |\n+| **Distinct**      | `values.distinct()`                   | `values | beam.Distinct()`                              |\n+| **Count**         | `values.count()`                      | `values | beam.combiners.Count.Globally()`              |\n+| **Count by key**  | `pairs.countByKey()`                  | `pairs | beam.combiners.Count.PerKey()`                 |\n+| **Take smallest** | `values.takeOrdered(3)`               | `values | beam.combiners.Top.Smallest(3)`               |\n+| **Take largest**  | `values.takeOrdered(3, lambda x: -x)` | `values | beam.combiners.Top.Largest(3)`                |\n+| **Random sample** | `values.takeSample(False, 3)`         | `values | beam.combiners.Sample.FixedSizeGlobally(3)`   |\n+| **Union**         | `values.union(otherValues)`           | `(values, otherValues) | beam.Flatten()`                |\n+| **Co-group**      | `pairs.cogroup(otherPairs)`           | `{'Xs': pairs, 'Ys': otherPairs} | beam.CoGroupByKey()` |\n+{{< /table >}}\n+\n+> \u2139\ufe0f To learn more about the transforms available in Beam, check the\n+> [Python transform gallery](/documentation/transforms/python/overview).\n+\n+## Using calculated values\n+\n+Since we are working in potentially distributed environments,\n+we can't guarantee that the results we've calculated are available at any given machine.\n+\n+In PySpark, we can get a result from a collection of elements (RDD) by using\n+`data.collect()`, or other aggregations such as `reduce()`, `count()` and more.\n+\n+Here's an example to scale numbers into a range between zero and one.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    values = sc.parallelize([1, 2, 3, 4])\n+    total = values.reduce(lambda x, y: x + y)\n+\n+    # We can simply use `total` since it's already a Python value from `reduce`.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d"}, "originalPosition": 198}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA5MDI3MQ==", "bodyText": "Done", "url": "https://github.com/apache/beam/pull/12963#discussion_r508090271", "createdAt": "2020-10-19T22:06:15Z", "author": {"login": "davidcavazos"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,245 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\"); \n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS, \n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+Beam and Spark are mostly equivalent, so you already know the basic concepts.\n+\n+A collection of elements in Spark is called a _Resilient Distributed Dataset_ (RDD),\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    result = (\n+        sc.parallelize([1, 2, 3, 4])\n+        .map(lambda x: x * 2)\n+        .reduce(lambda x, y: x + y)\n+    )\n+    print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+A label can optionally be added to a transform using the\n+_right shift operator_ `>>` like `data | 'My description' >> beam.Map(...)`.\n+This serves both as comments and makes your pipeline easier to debug.\n+\n+This is how the pipeline looks after adding labels.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | 'Create numbers' >> beam.Create([1, 2, 3, 4])\n+        | 'Multiply by two' >> beam.Map(lambda x: x * 2)\n+        | 'Sum everything' >> beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+## Setup\n+\n+Here's a comparison on how to get started both in PySpark and Beam.\n+\n+{{< table >}}\n+<table>\n+<tr>\n+    <th></th>\n+    <th>PySpark</th>\n+    <th>Beam</th>\n+</tr>\n+<tr>\n+    <td><b>Install</b></td>\n+    <td><code>$ pip install pyspark</code></td>\n+    <td><code>$ pip install apache-beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Imports</b></td>\n+    <td><code>import pyspark</code></td>\n+    <td><code>import apache_beam as beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating a<br>local pipeline</b></td>\n+    <td>\n+        <code>with pyspark.SparkContext() as sc:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+    <td>\n+        <code>with beam.Pipeline() as pipeline:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Creating values</b></td>\n+    <td><code>values = sc.parallelize([1, 2, 3, 4])</code></td>\n+    <td><code>values = pipeline | beam.Create([1, 2, 3, 4])</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating<br>key-value pairs</b></td>\n+    <td>\n+        <code>pairs = sc.parallelize([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+    <td>\n+        <code>pairs = pipeline | beam.Create([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Running a<br>local pipeline</b></td>\n+    <td><code>$ spark-submit spark_pipeline.py</code></td>\n+    <td><code>$ python beam_pipeline.py</code></td>\n+</tr>\n+</table>\n+{{< /table >}}\n+\n+## Transforms\n+\n+Here are the equivalents of some common transforms in both PySpark and Beam.\n+\n+{{< table >}}\n+|                   | PySpark                               | Beam                                                    |\n+|-------------------|---------------------------------------|---------------------------------------------------------|\n+| **Map**           | `values.map(lambda x: x * 2)`         | `values | beam.Map(lambda x: x * 2)`                    |\n+| **Filter**        | `values.filter(lambda x: x % 2 == 0)` | `values | beam.Filter(lambda x: x % 2 == 0)`            |\n+| **FlatMap**       | `values.flatMap(lambda x: range(x))`  | `values | beam.FlatMap(lambda x: range(x))`             |\n+| **Group by key**  | `pairs.groupByKey()`                  | `pairs | beam.GroupByKey()`                             |\n+| **Reduce**        | `values.reduce(lambda x, y: x+y)`     | `values | beam.CombineGlobally(sum)`                    |\n+| **Reduce by key** | `pairs.reduceByKey(lambda x, y: x+y)` | `pairs | beam.CombinePerKey(sum)`                       |\n+| **Distinct**      | `values.distinct()`                   | `values | beam.Distinct()`                              |\n+| **Count**         | `values.count()`                      | `values | beam.combiners.Count.Globally()`              |\n+| **Count by key**  | `pairs.countByKey()`                  | `pairs | beam.combiners.Count.PerKey()`                 |\n+| **Take smallest** | `values.takeOrdered(3)`               | `values | beam.combiners.Top.Smallest(3)`               |\n+| **Take largest**  | `values.takeOrdered(3, lambda x: -x)` | `values | beam.combiners.Top.Largest(3)`                |\n+| **Random sample** | `values.takeSample(False, 3)`         | `values | beam.combiners.Sample.FixedSizeGlobally(3)`   |\n+| **Union**         | `values.union(otherValues)`           | `(values, otherValues) | beam.Flatten()`                |\n+| **Co-group**      | `pairs.cogroup(otherPairs)`           | `{'Xs': pairs, 'Ys': otherPairs} | beam.CoGroupByKey()` |\n+{{< /table >}}\n+\n+> \u2139\ufe0f To learn more about the transforms available in Beam, check the\n+> [Python transform gallery](/documentation/transforms/python/overview).\n+\n+## Using calculated values\n+\n+Since we are working in potentially distributed environments,\n+we can't guarantee that the results we've calculated are available at any given machine.\n+\n+In PySpark, we can get a result from a collection of elements (RDD) by using\n+`data.collect()`, or other aggregations such as `reduce()`, `count()` and more.\n+\n+Here's an example to scale numbers into a range between zero and one.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    values = sc.parallelize([1, 2, 3, 4])\n+    total = values.reduce(lambda x, y: x + y)\n+\n+    # We can simply use `total` since it's already a Python value from `reduce`.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzAzODM4OQ=="}, "originalCommit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d"}, "originalPosition": 198}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzExMTYzOTIyOnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/get-started/from-spark.md", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOVQyMDo1MTozOFrOHaBPsg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQyMjowNjoyMVrOHkjXzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA0NTQyNg==", "bodyText": "Link to /documentation/programming-guide/#side-inputs.", "url": "https://github.com/apache/beam/pull/12963#discussion_r497045426", "createdAt": "2020-09-29T20:51:38Z", "author": {"login": "ibzib"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,245 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\"); \n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS, \n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+Beam and Spark are mostly equivalent, so you already know the basic concepts.\n+\n+A collection of elements in Spark is called a _Resilient Distributed Dataset_ (RDD),\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    result = (\n+        sc.parallelize([1, 2, 3, 4])\n+        .map(lambda x: x * 2)\n+        .reduce(lambda x, y: x + y)\n+    )\n+    print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+A label can optionally be added to a transform using the\n+_right shift operator_ `>>` like `data | 'My description' >> beam.Map(...)`.\n+This serves both as comments and makes your pipeline easier to debug.\n+\n+This is how the pipeline looks after adding labels.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | 'Create numbers' >> beam.Create([1, 2, 3, 4])\n+        | 'Multiply by two' >> beam.Map(lambda x: x * 2)\n+        | 'Sum everything' >> beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+## Setup\n+\n+Here's a comparison on how to get started both in PySpark and Beam.\n+\n+{{< table >}}\n+<table>\n+<tr>\n+    <th></th>\n+    <th>PySpark</th>\n+    <th>Beam</th>\n+</tr>\n+<tr>\n+    <td><b>Install</b></td>\n+    <td><code>$ pip install pyspark</code></td>\n+    <td><code>$ pip install apache-beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Imports</b></td>\n+    <td><code>import pyspark</code></td>\n+    <td><code>import apache_beam as beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating a<br>local pipeline</b></td>\n+    <td>\n+        <code>with pyspark.SparkContext() as sc:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+    <td>\n+        <code>with beam.Pipeline() as pipeline:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Creating values</b></td>\n+    <td><code>values = sc.parallelize([1, 2, 3, 4])</code></td>\n+    <td><code>values = pipeline | beam.Create([1, 2, 3, 4])</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating<br>key-value pairs</b></td>\n+    <td>\n+        <code>pairs = sc.parallelize([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+    <td>\n+        <code>pairs = pipeline | beam.Create([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Running a<br>local pipeline</b></td>\n+    <td><code>$ spark-submit spark_pipeline.py</code></td>\n+    <td><code>$ python beam_pipeline.py</code></td>\n+</tr>\n+</table>\n+{{< /table >}}\n+\n+## Transforms\n+\n+Here are the equivalents of some common transforms in both PySpark and Beam.\n+\n+{{< table >}}\n+|                   | PySpark                               | Beam                                                    |\n+|-------------------|---------------------------------------|---------------------------------------------------------|\n+| **Map**           | `values.map(lambda x: x * 2)`         | `values | beam.Map(lambda x: x * 2)`                    |\n+| **Filter**        | `values.filter(lambda x: x % 2 == 0)` | `values | beam.Filter(lambda x: x % 2 == 0)`            |\n+| **FlatMap**       | `values.flatMap(lambda x: range(x))`  | `values | beam.FlatMap(lambda x: range(x))`             |\n+| **Group by key**  | `pairs.groupByKey()`                  | `pairs | beam.GroupByKey()`                             |\n+| **Reduce**        | `values.reduce(lambda x, y: x+y)`     | `values | beam.CombineGlobally(sum)`                    |\n+| **Reduce by key** | `pairs.reduceByKey(lambda x, y: x+y)` | `pairs | beam.CombinePerKey(sum)`                       |\n+| **Distinct**      | `values.distinct()`                   | `values | beam.Distinct()`                              |\n+| **Count**         | `values.count()`                      | `values | beam.combiners.Count.Globally()`              |\n+| **Count by key**  | `pairs.countByKey()`                  | `pairs | beam.combiners.Count.PerKey()`                 |\n+| **Take smallest** | `values.takeOrdered(3)`               | `values | beam.combiners.Top.Smallest(3)`               |\n+| **Take largest**  | `values.takeOrdered(3, lambda x: -x)` | `values | beam.combiners.Top.Largest(3)`                |\n+| **Random sample** | `values.takeSample(False, 3)`         | `values | beam.combiners.Sample.FixedSizeGlobally(3)`   |\n+| **Union**         | `values.union(otherValues)`           | `(values, otherValues) | beam.Flatten()`                |\n+| **Co-group**      | `pairs.cogroup(otherPairs)`           | `{'Xs': pairs, 'Ys': otherPairs} | beam.CoGroupByKey()` |\n+{{< /table >}}\n+\n+> \u2139\ufe0f To learn more about the transforms available in Beam, check the\n+> [Python transform gallery](/documentation/transforms/python/overview).\n+\n+## Using calculated values\n+\n+Since we are working in potentially distributed environments,\n+we can't guarantee that the results we've calculated are available at any given machine.\n+\n+In PySpark, we can get a result from a collection of elements (RDD) by using\n+`data.collect()`, or other aggregations such as `reduce()`, `count()` and more.\n+\n+Here's an example to scale numbers into a range between zero and one.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    values = sc.parallelize([1, 2, 3, 4])\n+    total = values.reduce(lambda x, y: x + y)\n+\n+    # We can simply use `total` since it's already a Python value from `reduce`.\n+    scaled_values = values.map(lambda x: x / total)\n+\n+    # But to access `scaled_values`, we need to call `collect`.\n+    print(scaled_values.collect())\n+{{< /highlight >}}\n+\n+In Beam the results from _all_ transforms result in a PCollection.\n+We use _side inputs_ to feed a PCollection into a transform and access its values.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d"}, "originalPosition": 206}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA5MDAwMA==", "bodyText": "Done", "url": "https://github.com/apache/beam/pull/12963#discussion_r508090000", "createdAt": "2020-10-19T22:05:34Z", "author": {"login": "davidcavazos"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,245 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\"); \n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS, \n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+Beam and Spark are mostly equivalent, so you already know the basic concepts.\n+\n+A collection of elements in Spark is called a _Resilient Distributed Dataset_ (RDD),\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    result = (\n+        sc.parallelize([1, 2, 3, 4])\n+        .map(lambda x: x * 2)\n+        .reduce(lambda x, y: x + y)\n+    )\n+    print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+A label can optionally be added to a transform using the\n+_right shift operator_ `>>` like `data | 'My description' >> beam.Map(...)`.\n+This serves both as comments and makes your pipeline easier to debug.\n+\n+This is how the pipeline looks after adding labels.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | 'Create numbers' >> beam.Create([1, 2, 3, 4])\n+        | 'Multiply by two' >> beam.Map(lambda x: x * 2)\n+        | 'Sum everything' >> beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+## Setup\n+\n+Here's a comparison on how to get started both in PySpark and Beam.\n+\n+{{< table >}}\n+<table>\n+<tr>\n+    <th></th>\n+    <th>PySpark</th>\n+    <th>Beam</th>\n+</tr>\n+<tr>\n+    <td><b>Install</b></td>\n+    <td><code>$ pip install pyspark</code></td>\n+    <td><code>$ pip install apache-beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Imports</b></td>\n+    <td><code>import pyspark</code></td>\n+    <td><code>import apache_beam as beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating a<br>local pipeline</b></td>\n+    <td>\n+        <code>with pyspark.SparkContext() as sc:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+    <td>\n+        <code>with beam.Pipeline() as pipeline:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Creating values</b></td>\n+    <td><code>values = sc.parallelize([1, 2, 3, 4])</code></td>\n+    <td><code>values = pipeline | beam.Create([1, 2, 3, 4])</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating<br>key-value pairs</b></td>\n+    <td>\n+        <code>pairs = sc.parallelize([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+    <td>\n+        <code>pairs = pipeline | beam.Create([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Running a<br>local pipeline</b></td>\n+    <td><code>$ spark-submit spark_pipeline.py</code></td>\n+    <td><code>$ python beam_pipeline.py</code></td>\n+</tr>\n+</table>\n+{{< /table >}}\n+\n+## Transforms\n+\n+Here are the equivalents of some common transforms in both PySpark and Beam.\n+\n+{{< table >}}\n+|                   | PySpark                               | Beam                                                    |\n+|-------------------|---------------------------------------|---------------------------------------------------------|\n+| **Map**           | `values.map(lambda x: x * 2)`         | `values | beam.Map(lambda x: x * 2)`                    |\n+| **Filter**        | `values.filter(lambda x: x % 2 == 0)` | `values | beam.Filter(lambda x: x % 2 == 0)`            |\n+| **FlatMap**       | `values.flatMap(lambda x: range(x))`  | `values | beam.FlatMap(lambda x: range(x))`             |\n+| **Group by key**  | `pairs.groupByKey()`                  | `pairs | beam.GroupByKey()`                             |\n+| **Reduce**        | `values.reduce(lambda x, y: x+y)`     | `values | beam.CombineGlobally(sum)`                    |\n+| **Reduce by key** | `pairs.reduceByKey(lambda x, y: x+y)` | `pairs | beam.CombinePerKey(sum)`                       |\n+| **Distinct**      | `values.distinct()`                   | `values | beam.Distinct()`                              |\n+| **Count**         | `values.count()`                      | `values | beam.combiners.Count.Globally()`              |\n+| **Count by key**  | `pairs.countByKey()`                  | `pairs | beam.combiners.Count.PerKey()`                 |\n+| **Take smallest** | `values.takeOrdered(3)`               | `values | beam.combiners.Top.Smallest(3)`               |\n+| **Take largest**  | `values.takeOrdered(3, lambda x: -x)` | `values | beam.combiners.Top.Largest(3)`                |\n+| **Random sample** | `values.takeSample(False, 3)`         | `values | beam.combiners.Sample.FixedSizeGlobally(3)`   |\n+| **Union**         | `values.union(otherValues)`           | `(values, otherValues) | beam.Flatten()`                |\n+| **Co-group**      | `pairs.cogroup(otherPairs)`           | `{'Xs': pairs, 'Ys': otherPairs} | beam.CoGroupByKey()` |\n+{{< /table >}}\n+\n+> \u2139\ufe0f To learn more about the transforms available in Beam, check the\n+> [Python transform gallery](/documentation/transforms/python/overview).\n+\n+## Using calculated values\n+\n+Since we are working in potentially distributed environments,\n+we can't guarantee that the results we've calculated are available at any given machine.\n+\n+In PySpark, we can get a result from a collection of elements (RDD) by using\n+`data.collect()`, or other aggregations such as `reduce()`, `count()` and more.\n+\n+Here's an example to scale numbers into a range between zero and one.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    values = sc.parallelize([1, 2, 3, 4])\n+    total = values.reduce(lambda x, y: x + y)\n+\n+    # We can simply use `total` since it's already a Python value from `reduce`.\n+    scaled_values = values.map(lambda x: x / total)\n+\n+    # But to access `scaled_values`, we need to call `collect`.\n+    print(scaled_values.collect())\n+{{< /highlight >}}\n+\n+In Beam the results from _all_ transforms result in a PCollection.\n+We use _side inputs_ to feed a PCollection into a transform and access its values.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA0NTQyNg=="}, "originalCommit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d"}, "originalPosition": 206}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA5MDMxNg==", "bodyText": "Done", "url": "https://github.com/apache/beam/pull/12963#discussion_r508090316", "createdAt": "2020-10-19T22:06:21Z", "author": {"login": "davidcavazos"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,245 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\"); \n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS, \n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+Beam and Spark are mostly equivalent, so you already know the basic concepts.\n+\n+A collection of elements in Spark is called a _Resilient Distributed Dataset_ (RDD),\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    result = (\n+        sc.parallelize([1, 2, 3, 4])\n+        .map(lambda x: x * 2)\n+        .reduce(lambda x, y: x + y)\n+    )\n+    print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+A label can optionally be added to a transform using the\n+_right shift operator_ `>>` like `data | 'My description' >> beam.Map(...)`.\n+This serves both as comments and makes your pipeline easier to debug.\n+\n+This is how the pipeline looks after adding labels.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | 'Create numbers' >> beam.Create([1, 2, 3, 4])\n+        | 'Multiply by two' >> beam.Map(lambda x: x * 2)\n+        | 'Sum everything' >> beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+## Setup\n+\n+Here's a comparison on how to get started both in PySpark and Beam.\n+\n+{{< table >}}\n+<table>\n+<tr>\n+    <th></th>\n+    <th>PySpark</th>\n+    <th>Beam</th>\n+</tr>\n+<tr>\n+    <td><b>Install</b></td>\n+    <td><code>$ pip install pyspark</code></td>\n+    <td><code>$ pip install apache-beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Imports</b></td>\n+    <td><code>import pyspark</code></td>\n+    <td><code>import apache_beam as beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating a<br>local pipeline</b></td>\n+    <td>\n+        <code>with pyspark.SparkContext() as sc:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+    <td>\n+        <code>with beam.Pipeline() as pipeline:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Creating values</b></td>\n+    <td><code>values = sc.parallelize([1, 2, 3, 4])</code></td>\n+    <td><code>values = pipeline | beam.Create([1, 2, 3, 4])</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating<br>key-value pairs</b></td>\n+    <td>\n+        <code>pairs = sc.parallelize([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+    <td>\n+        <code>pairs = pipeline | beam.Create([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Running a<br>local pipeline</b></td>\n+    <td><code>$ spark-submit spark_pipeline.py</code></td>\n+    <td><code>$ python beam_pipeline.py</code></td>\n+</tr>\n+</table>\n+{{< /table >}}\n+\n+## Transforms\n+\n+Here are the equivalents of some common transforms in both PySpark and Beam.\n+\n+{{< table >}}\n+|                   | PySpark                               | Beam                                                    |\n+|-------------------|---------------------------------------|---------------------------------------------------------|\n+| **Map**           | `values.map(lambda x: x * 2)`         | `values | beam.Map(lambda x: x * 2)`                    |\n+| **Filter**        | `values.filter(lambda x: x % 2 == 0)` | `values | beam.Filter(lambda x: x % 2 == 0)`            |\n+| **FlatMap**       | `values.flatMap(lambda x: range(x))`  | `values | beam.FlatMap(lambda x: range(x))`             |\n+| **Group by key**  | `pairs.groupByKey()`                  | `pairs | beam.GroupByKey()`                             |\n+| **Reduce**        | `values.reduce(lambda x, y: x+y)`     | `values | beam.CombineGlobally(sum)`                    |\n+| **Reduce by key** | `pairs.reduceByKey(lambda x, y: x+y)` | `pairs | beam.CombinePerKey(sum)`                       |\n+| **Distinct**      | `values.distinct()`                   | `values | beam.Distinct()`                              |\n+| **Count**         | `values.count()`                      | `values | beam.combiners.Count.Globally()`              |\n+| **Count by key**  | `pairs.countByKey()`                  | `pairs | beam.combiners.Count.PerKey()`                 |\n+| **Take smallest** | `values.takeOrdered(3)`               | `values | beam.combiners.Top.Smallest(3)`               |\n+| **Take largest**  | `values.takeOrdered(3, lambda x: -x)` | `values | beam.combiners.Top.Largest(3)`                |\n+| **Random sample** | `values.takeSample(False, 3)`         | `values | beam.combiners.Sample.FixedSizeGlobally(3)`   |\n+| **Union**         | `values.union(otherValues)`           | `(values, otherValues) | beam.Flatten()`                |\n+| **Co-group**      | `pairs.cogroup(otherPairs)`           | `{'Xs': pairs, 'Ys': otherPairs} | beam.CoGroupByKey()` |\n+{{< /table >}}\n+\n+> \u2139\ufe0f To learn more about the transforms available in Beam, check the\n+> [Python transform gallery](/documentation/transforms/python/overview).\n+\n+## Using calculated values\n+\n+Since we are working in potentially distributed environments,\n+we can't guarantee that the results we've calculated are available at any given machine.\n+\n+In PySpark, we can get a result from a collection of elements (RDD) by using\n+`data.collect()`, or other aggregations such as `reduce()`, `count()` and more.\n+\n+Here's an example to scale numbers into a range between zero and one.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+with pyspark.SparkContext() as sc:\n+    values = sc.parallelize([1, 2, 3, 4])\n+    total = values.reduce(lambda x, y: x + y)\n+\n+    # We can simply use `total` since it's already a Python value from `reduce`.\n+    scaled_values = values.map(lambda x: x / total)\n+\n+    # But to access `scaled_values`, we need to call `collect`.\n+    print(scaled_values.collect())\n+{{< /highlight >}}\n+\n+In Beam the results from _all_ transforms result in a PCollection.\n+We use _side inputs_ to feed a PCollection into a transform and access its values.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NzA0NTQyNg=="}, "originalCommit": {"oid": "c484e68489db6551aff70f6dde765ec0ca89d58d"}, "originalPosition": 206}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE4MjY1OTQwOnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/get-started/from-spark.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQyMjowNjo0OVrOHkjYmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xOVQyMjowNjo0OVrOHkjYmg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwODA5MDUyMg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            This means that when you _pipe_ `|` data you're only _decalring_ the\n          \n          \n            \n            This means that when you _pipe_ `|` data you're only _declaring_ the", "url": "https://github.com/apache/beam/pull/12963#discussion_r508090522", "createdAt": "2020-10-19T22:06:49Z", "author": {"login": "ibzib"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -71,6 +71,17 @@ with beam.Pipeline() as pipeline:\n > That's because we can only access the elements of a PCollection\n > from within a PTransform.\n \n+Another thing to note is that Beam pipelines are constructed _lazily_.\n+This means that when you _pipe_ `|` data you're only _decalring_ the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "eaff2e92d05dd091b135cfaa8f2efe411ec7fba0"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzE5MDI1Mjg5OnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/get-started/from-spark.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0yMVQxMzoyNDoxM1rOHlrxjA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxNDozNDoxMVrOID7bYw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTI3NjU1Ng==", "bodyText": "I am not sure if worth but since we mention lazy computation we probably may mention what triggers 'producing results', in Spark is done by Spark's actions e.g. collect(), etc and in Beam by outputting the data (you can relate this to the print mention). Notice that I saw the warning of the with section but we can double mention this here and/or mention p.run()).\nNote this is just an extra suggestion but it is not mandatory at all, it is already clear, this is 'extra details'.", "url": "https://github.com/apache/beam/pull/12963#discussion_r509276556", "createdAt": "2020-10-21T13:24:13Z", "author": {"login": "iemejia"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,261 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+The Beam and Spark APIs are similar, so you already know the basic concepts.\n+\n+Spark stores data _Spark DataFrames_ for structured data,\n+and in _Resilient Distributed Datasets_ (RDD) for unstructured data.\n+We are using RDDs for this guide.\n+\n+A _Spark RDD_ represents a collection of elements,\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+sc = pyspark.SparkContext()\n+result = (\n+    sc.parallelize([1, 2, 3, 4])\n+    .map(lambda x: x * 2)\n+    .reduce(lambda x, y: x + y)\n+)\n+print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+Another thing to note is that Beam pipelines are constructed _lazily_.\n+This means that when you _pipe_ `|` data you're only _declaring_ the\n+transformations and the order you want them to happen,\n+but the actual computation doesn't happen.\n+The pipeline is run _after_ the `with beam.Pipeline() as pipeline` context has\n+closed.\n+The pipeline is then sent to your runner of choice and it processes the data.\n+\n+> \u2139\ufe0f When the `with beam.Pipeline() as pipeline` context closes,\n+> it implicitly calls `pipeline.run()` which triggers the computation to happen.\n+\n+A label can optionally be added to a transform using the\n+_right shift operator_ `>>` like `data | 'My description' >> beam.Map(...)`.\n+This serves both as comments and makes your pipeline easier to debug.\n+\n+This is how the pipeline looks after adding labels.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | 'Create numbers' >> beam.Create([1, 2, 3, 4])\n+        | 'Multiply by two' >> beam.Map(lambda x: x * 2)\n+        | 'Sum everything' >> beam.CombineGlobally(sum)\n+        | 'Print results' >> beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+## Setup\n+\n+Here's a comparison on how to get started both in PySpark and Beam.\n+\n+{{< table >}}\n+<table>\n+<tr>\n+    <th></th>\n+    <th>PySpark</th>\n+    <th>Beam</th>\n+</tr>\n+<tr>\n+    <td><b>Install</b></td>\n+    <td><code>$ pip install pyspark</code></td>\n+    <td><code>$ pip install apache-beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Imports</b></td>\n+    <td><code>import pyspark</code></td>\n+    <td><code>import apache_beam as beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating a<br>local pipeline</b></td>\n+    <td>\n+        <code>sc = pyspark.SparkContext() as sc:</code><br>\n+        <code># Your pipeline code here.</code>\n+    </td>\n+    <td>\n+        <code>with beam.Pipeline() as pipeline:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Creating values</b></td>\n+    <td><code>values = sc.parallelize([1, 2, 3, 4])</code></td>\n+    <td><code>values = pipeline | beam.Create([1, 2, 3, 4])</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating<br>key-value pairs</b></td>\n+    <td>\n+        <code>pairs = sc.parallelize([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+    <td>\n+        <code>pairs = pipeline | beam.Create([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+</tr>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5d9edd69621203ade20c539e60d27fc961c9ba4"}, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDk5MDMwNw==", "bodyText": "If it's already clear, I think I'll leave it out in favor of simplicity. Sometimes less is more :)", "url": "https://github.com/apache/beam/pull/12963#discussion_r540990307", "createdAt": "2020-12-11T14:34:11Z", "author": {"login": "davidcavazos"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,261 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+The Beam and Spark APIs are similar, so you already know the basic concepts.\n+\n+Spark stores data _Spark DataFrames_ for structured data,\n+and in _Resilient Distributed Datasets_ (RDD) for unstructured data.\n+We are using RDDs for this guide.\n+\n+A _Spark RDD_ represents a collection of elements,\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+sc = pyspark.SparkContext()\n+result = (\n+    sc.parallelize([1, 2, 3, 4])\n+    .map(lambda x: x * 2)\n+    .reduce(lambda x, y: x + y)\n+)\n+print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+Another thing to note is that Beam pipelines are constructed _lazily_.\n+This means that when you _pipe_ `|` data you're only _declaring_ the\n+transformations and the order you want them to happen,\n+but the actual computation doesn't happen.\n+The pipeline is run _after_ the `with beam.Pipeline() as pipeline` context has\n+closed.\n+The pipeline is then sent to your runner of choice and it processes the data.\n+\n+> \u2139\ufe0f When the `with beam.Pipeline() as pipeline` context closes,\n+> it implicitly calls `pipeline.run()` which triggers the computation to happen.\n+\n+A label can optionally be added to a transform using the\n+_right shift operator_ `>>` like `data | 'My description' >> beam.Map(...)`.\n+This serves both as comments and makes your pipeline easier to debug.\n+\n+This is how the pipeline looks after adding labels.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | 'Create numbers' >> beam.Create([1, 2, 3, 4])\n+        | 'Multiply by two' >> beam.Map(lambda x: x * 2)\n+        | 'Sum everything' >> beam.CombineGlobally(sum)\n+        | 'Print results' >> beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+## Setup\n+\n+Here's a comparison on how to get started both in PySpark and Beam.\n+\n+{{< table >}}\n+<table>\n+<tr>\n+    <th></th>\n+    <th>PySpark</th>\n+    <th>Beam</th>\n+</tr>\n+<tr>\n+    <td><b>Install</b></td>\n+    <td><code>$ pip install pyspark</code></td>\n+    <td><code>$ pip install apache-beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Imports</b></td>\n+    <td><code>import pyspark</code></td>\n+    <td><code>import apache_beam as beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating a<br>local pipeline</b></td>\n+    <td>\n+        <code>sc = pyspark.SparkContext() as sc:</code><br>\n+        <code># Your pipeline code here.</code>\n+    </td>\n+    <td>\n+        <code>with beam.Pipeline() as pipeline:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Creating values</b></td>\n+    <td><code>values = sc.parallelize([1, 2, 3, 4])</code></td>\n+    <td><code>values = pipeline | beam.Create([1, 2, 3, 4])</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating<br>key-value pairs</b></td>\n+    <td>\n+        <code>pairs = sc.parallelize([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+    <td>\n+        <code>pairs = pipeline | beam.Create([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+</tr>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwOTI3NjU1Ng=="}, "originalCommit": {"oid": "e5d9edd69621203ade20c539e60d27fc961c9ba4"}, "originalPosition": 161}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzODgyMDkyOnYy", "diffSide": "RIGHT", "path": "website/www/site/layouts/partials/section-menu/en/get-started.html", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNjoxMjoyMFrOHs2FRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxNDozNzo1NVrOID7lGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc4NTQ3OQ==", "bodyText": "The actual Overview page has a Getting Started section you can include this new doc in.", "url": "https://github.com/apache/beam/pull/12963#discussion_r516785479", "createdAt": "2020-11-03T16:12:20Z", "author": {"login": "rosetn"}, "path": "website/www/site/layouts/partials/section-menu/en/get-started.html", "diffHunk": "@@ -22,12 +22,13 @@\n     <li><a href=\"/get-started/quickstart-go/\">Quickstart - Go</a></li>\n   </ul>\n </li>\n+<li><a href=\"/get-started/from-spark/\">From Apache Spark</a></li>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5d9edd69621203ade20c539e60d27fc961c9ba4"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDk5Mjc5Mw==", "bodyText": "Thanks, I just added a link there too", "url": "https://github.com/apache/beam/pull/12963#discussion_r540992793", "createdAt": "2020-12-11T14:37:55Z", "author": {"login": "davidcavazos"}, "path": "website/www/site/layouts/partials/section-menu/en/get-started.html", "diffHunk": "@@ -22,12 +22,13 @@\n     <li><a href=\"/get-started/quickstart-go/\">Quickstart - Go</a></li>\n   </ul>\n </li>\n+<li><a href=\"/get-started/from-spark/\">From Apache Spark</a></li>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc4NTQ3OQ=="}, "originalCommit": {"oid": "e5d9edd69621203ade20c539e60d27fc961c9ba4"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzODg0MzY2OnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/get-started/from-spark.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNjoxNzo0MlrOHs2T_g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQxOTozMzo1OFrOH5SMaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc4OTI0Ng==", "bodyText": "WDYT about replacing \"is easy\" with \"is familiar\" or removing this sentence, since you explain the connection in the next sentence?", "url": "https://github.com/apache/beam/pull/12963#discussion_r516789246", "createdAt": "2020-11-03T16:17:42Z", "author": {"login": "rosetn"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,261 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5d9edd69621203ade20c539e60d27fc961c9ba4"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTgyODk2OA==", "bodyText": "Sounds good, done", "url": "https://github.com/apache/beam/pull/12963#discussion_r529828968", "createdAt": "2020-11-24T19:33:58Z", "author": {"login": "davidcavazos"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,261 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjc4OTI0Ng=="}, "originalCommit": {"oid": "e5d9edd69621203ade20c539e60d27fc961c9ba4"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzODk1OTY4OnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/get-started/from-spark.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNjo0NDoyMlrOHs3diA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQxOTozMzo0MVrOH5SL2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgwODA3Mg==", "bodyText": "I'd consider only italicizing the new Spark terms. The font emphasis can create accessibility issues if it's too frequent.", "url": "https://github.com/apache/beam/pull/12963#discussion_r516808072", "createdAt": "2020-11-03T16:44:22Z", "author": {"login": "rosetn"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,261 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5d9edd69621203ade20c539e60d27fc961c9ba4"}, "originalPosition": 18}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTgyODgyNw==", "bodyText": "Got it, thanks", "url": "https://github.com/apache/beam/pull/12963#discussion_r529828827", "createdAt": "2020-11-24T19:33:41Z", "author": {"login": "davidcavazos"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,261 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgwODA3Mg=="}, "originalCommit": {"oid": "e5d9edd69621203ade20c539e60d27fc961c9ba4"}, "originalPosition": 18}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzIzODk2NjA1OnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/get-started/from-spark.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wM1QxNjo0NTo0N1rOHs3hlA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQxOTozNzo0MVrOH5SUUg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgwOTEwOA==", "bodyText": "Replace \"how\" with \"what\"", "url": "https://github.com/apache/beam/pull/12963#discussion_r516809108", "createdAt": "2020-11-03T16:45:47Z", "author": {"login": "rosetn"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,261 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+The Beam and Spark APIs are similar, so you already know the basic concepts.\n+\n+Spark stores data _Spark DataFrames_ for structured data,\n+and in _Resilient Distributed Datasets_ (RDD) for unstructured data.\n+We are using RDDs for this guide.\n+\n+A _Spark RDD_ represents a collection of elements,\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+sc = pyspark.SparkContext()\n+result = (\n+    sc.parallelize([1, 2, 3, 4])\n+    .map(lambda x: x * 2)\n+    .reduce(lambda x, y: x + y)\n+)\n+print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5d9edd69621203ade20c539e60d27fc961c9ba4"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTgzMDk5NA==", "bodyText": "Done", "url": "https://github.com/apache/beam/pull/12963#discussion_r529830994", "createdAt": "2020-11-24T19:37:41Z", "author": {"login": "davidcavazos"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,261 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+The Beam and Spark APIs are similar, so you already know the basic concepts.\n+\n+Spark stores data _Spark DataFrames_ for structured data,\n+and in _Resilient Distributed Datasets_ (RDD) for unstructured data.\n+We are using RDDs for this guide.\n+\n+A _Spark RDD_ represents a collection of elements,\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+sc = pyspark.SparkContext()\n+result = (\n+    sc.parallelize([1, 2, 3, 4])\n+    .map(lambda x: x * 2)\n+    .reduce(lambda x, y: x + y)\n+)\n+print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNjgwOTEwOA=="}, "originalCommit": {"oid": "e5d9edd69621203ade20c539e60d27fc961c9ba4"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI0NDc0NDQzOnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/get-started/from-spark.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0wNVQwMDowODoxOFrOHtuLaw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQxNDozMzowM1rOID7Yeg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzcwNDU1NQ==", "bodyText": "I realized we should probably clarify somewhere that \"Beam Python on the Spark runner\" and PySpark are completed unrelated, since that seems to be a common misconception.", "url": "https://github.com/apache/beam/pull/12963#discussion_r517704555", "createdAt": "2020-11-05T00:08:18Z", "author": {"login": "ibzib"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,261 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+The Beam and Spark APIs are similar, so you already know the basic concepts.\n+\n+Spark stores data _Spark DataFrames_ for structured data,\n+and in _Resilient Distributed Datasets_ (RDD) for unstructured data.\n+We are using RDDs for this guide.\n+\n+A _Spark RDD_ represents a collection of elements,\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+sc = pyspark.SparkContext()\n+result = (\n+    sc.parallelize([1, 2, 3, 4])\n+    .map(lambda x: x * 2)\n+    .reduce(lambda x, y: x + y)\n+)\n+print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+Another thing to note is that Beam pipelines are constructed _lazily_.\n+This means that when you _pipe_ `|` data you're only _declaring_ the\n+transformations and the order you want them to happen,\n+but the actual computation doesn't happen.\n+The pipeline is run _after_ the `with beam.Pipeline() as pipeline` context has\n+closed.\n+The pipeline is then sent to your runner of choice and it processes the data.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e5d9edd69621203ade20c539e60d27fc961c9ba4"}, "originalPosition": 84}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MDk4OTU2Mg==", "bodyText": "Done", "url": "https://github.com/apache/beam/pull/12963#discussion_r540989562", "createdAt": "2020-12-11T14:33:03Z", "author": {"login": "davidcavazos"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,261 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is easy.\n+The Beam and Spark APIs are similar, so you already know the basic concepts.\n+\n+Spark stores data _Spark DataFrames_ for structured data,\n+and in _Resilient Distributed Datasets_ (RDD) for unstructured data.\n+We are using RDDs for this guide.\n+\n+A _Spark RDD_ represents a collection of elements,\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+sc = pyspark.SparkContext()\n+result = (\n+    sc.parallelize([1, 2, 3, 4])\n+    .map(lambda x: x * 2)\n+    .reduce(lambda x, y: x + y)\n+)\n+print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's how an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+Another thing to note is that Beam pipelines are constructed _lazily_.\n+This means that when you _pipe_ `|` data you're only _declaring_ the\n+transformations and the order you want them to happen,\n+but the actual computation doesn't happen.\n+The pipeline is run _after_ the `with beam.Pipeline() as pipeline` context has\n+closed.\n+The pipeline is then sent to your runner of choice and it processes the data.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUxNzcwNDU1NQ=="}, "originalCommit": {"oid": "e5d9edd69621203ade20c539e60d27fc961c9ba4"}, "originalPosition": 84}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQwMDI3MjIyOnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/get-started/from-spark.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQyMzo0Mjo1OVrOIEVt3g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNFQxNjoyNjo0M1rOIFY0iA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTQyMTAyMg==", "bodyText": "typo--declaring", "url": "https://github.com/apache/beam/pull/12963#discussion_r541421022", "createdAt": "2020-12-11T23:42:59Z", "author": {"login": "rosetn"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,268 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is familiar.\n+The Beam and Spark APIs are similar, so you already know the basic concepts.\n+\n+Spark stores data _Spark DataFrames_ for structured data,\n+and in _Resilient Distributed Datasets_ (RDD) for unstructured data.\n+We are using RDDs for this guide.\n+\n+A Spark RDD represents a collection of elements,\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+sc = pyspark.SparkContext()\n+result = (\n+    sc.parallelize([1, 2, 3, 4])\n+    .map(lambda x: x * 2)\n+    .reduce(lambda x, y: x + y)\n+)\n+print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's what an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+Another thing to note is that Beam pipelines are constructed _lazily_.\n+This means that when you pipe `|` data you're only _decalring_ the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67af84a51c471e7c53fc23a43acf597f60233273"}, "originalPosition": 79}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjUyMDQ1Ng==", "bodyText": "Thanks, fixed", "url": "https://github.com/apache/beam/pull/12963#discussion_r542520456", "createdAt": "2020-12-14T16:26:43Z", "author": {"login": "davidcavazos"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,268 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is familiar.\n+The Beam and Spark APIs are similar, so you already know the basic concepts.\n+\n+Spark stores data _Spark DataFrames_ for structured data,\n+and in _Resilient Distributed Datasets_ (RDD) for unstructured data.\n+We are using RDDs for this guide.\n+\n+A Spark RDD represents a collection of elements,\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+sc = pyspark.SparkContext()\n+result = (\n+    sc.parallelize([1, 2, 3, 4])\n+    .map(lambda x: x * 2)\n+    .reduce(lambda x, y: x + y)\n+)\n+print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's what an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+Another thing to note is that Beam pipelines are constructed _lazily_.\n+This means that when you pipe `|` data you're only _decalring_ the", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTQyMTAyMg=="}, "originalCommit": {"oid": "67af84a51c471e7c53fc23a43acf597f60233273"}, "originalPosition": 79}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQwMDI3ODM5OnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/get-started/from-spark.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xMVQyMzo0NDoxMVrOIEVxpw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0xNFQxNjoyNjo1OVrOIFY1Ug==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTQyMTk5MQ==", "bodyText": "comm after count()", "url": "https://github.com/apache/beam/pull/12963#discussion_r541421991", "createdAt": "2020-12-11T23:44:11Z", "author": {"login": "rosetn"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,268 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is familiar.\n+The Beam and Spark APIs are similar, so you already know the basic concepts.\n+\n+Spark stores data _Spark DataFrames_ for structured data,\n+and in _Resilient Distributed Datasets_ (RDD) for unstructured data.\n+We are using RDDs for this guide.\n+\n+A Spark RDD represents a collection of elements,\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+sc = pyspark.SparkContext()\n+result = (\n+    sc.parallelize([1, 2, 3, 4])\n+    .map(lambda x: x * 2)\n+    .reduce(lambda x, y: x + y)\n+)\n+print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's what an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+Another thing to note is that Beam pipelines are constructed _lazily_.\n+This means that when you pipe `|` data you're only _decalring_ the\n+transformations and the order you want them to happen,\n+but the actual computation doesn't happen.\n+The pipeline is run _after_ the `with beam.Pipeline() as pipeline` context has\n+closed.\n+\n+> \u2139\ufe0f When the `with beam.Pipeline() as pipeline` context closes,\n+> it implicitly calls `pipeline.run()` which triggers the computation to happen.\n+\n+The pipeline is then sent to your\n+[runner of choice](https://beam.apache.org/documentation/runners/capability-matrix/)\n+and it processes the data.\n+\n+> \u2139\ufe0f The pipeline can run locally with the _DirectRunner_,\n+> or in a distributed runner such as Flink, Spark, or Dataflow.\n+> The Spark runner is not related to PySpark.\n+\n+A label can optionally be added to a transform using the\n+_right shift operator_ `>>` like `data | 'My description' >> beam.Map(...)`.\n+This serves both as comments and makes your pipeline easier to debug.\n+\n+This is how the pipeline looks after adding labels.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | 'Create numbers' >> beam.Create([1, 2, 3, 4])\n+        | 'Multiply by two' >> beam.Map(lambda x: x * 2)\n+        | 'Sum everything' >> beam.CombineGlobally(sum)\n+        | 'Print results' >> beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+## Setup\n+\n+Here's a comparison on how to get started both in PySpark and Beam.\n+\n+{{< table >}}\n+<table>\n+<tr>\n+    <th></th>\n+    <th>PySpark</th>\n+    <th>Beam</th>\n+</tr>\n+<tr>\n+    <td><b>Install</b></td>\n+    <td><code>$ pip install pyspark</code></td>\n+    <td><code>$ pip install apache-beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Imports</b></td>\n+    <td><code>import pyspark</code></td>\n+    <td><code>import apache_beam as beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating a<br>local pipeline</b></td>\n+    <td>\n+        <code>sc = pyspark.SparkContext() as sc:</code><br>\n+        <code># Your pipeline code here.</code>\n+    </td>\n+    <td>\n+        <code>with beam.Pipeline() as pipeline:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Creating values</b></td>\n+    <td><code>values = sc.parallelize([1, 2, 3, 4])</code></td>\n+    <td><code>values = pipeline | beam.Create([1, 2, 3, 4])</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating<br>key-value pairs</b></td>\n+    <td>\n+        <code>pairs = sc.parallelize([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+    <td>\n+        <code>pairs = pipeline | beam.Create([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Running a<br>local pipeline</b></td>\n+    <td><code>$ spark-submit spark_pipeline.py</code></td>\n+    <td><code>$ python beam_pipeline.py</code></td>\n+</tr>\n+</table>\n+{{< /table >}}\n+\n+## Transforms\n+\n+Here are the equivalents of some common transforms in both PySpark and Beam.\n+\n+{{< table >}}\n+|                                                                                  | PySpark                               | Beam                                                    |\n+|----------------------------------------------------------------------------------|---------------------------------------|---------------------------------------------------------|\n+| [**Map**](/documentation/transforms/python/elementwise/map/)                     | `values.map(lambda x: x * 2)`         | `values | beam.Map(lambda x: x * 2)`                    |\n+| [**Filter**](/documentation/transforms/python/elementwise/filter/)               | `values.filter(lambda x: x % 2 == 0)` | `values | beam.Filter(lambda x: x % 2 == 0)`            |\n+| [**FlatMap**](/documentation/transforms/python/elementwise/flatmap/)             | `values.flatMap(lambda x: range(x))`  | `values | beam.FlatMap(lambda x: range(x))`             |\n+| [**Group by key**](/documentation/transforms/python/aggregation/groupbykey/)     | `pairs.groupByKey()`                  | `pairs | beam.GroupByKey()`                             |\n+| [**Reduce**](/documentation/transforms/python/aggregation/combineglobally/)      | `values.reduce(lambda x, y: x+y)`     | `values | beam.CombineGlobally(sum)`                    |\n+| [**Reduce by key**](/documentation/transforms/python/aggregation/combineperkey/) | `pairs.reduceByKey(lambda x, y: x+y)` | `pairs | beam.CombinePerKey(sum)`                       |\n+| [**Distinct**](/documentation/transforms/python/aggregation/distinct/)           | `values.distinct()`                   | `values | beam.Distinct()`                              |\n+| [**Count**](/documentation/transforms/python/aggregation/count/)                 | `values.count()`                      | `values | beam.combiners.Count.Globally()`              |\n+| [**Count by key**](/documentation/transforms/python/aggregation/count/)          | `pairs.countByKey()`                  | `pairs | beam.combiners.Count.PerKey()`                 |\n+| [**Take smallest**](/documentation/transforms/python/aggregation/top/)           | `values.takeOrdered(3)`               | `values | beam.combiners.Top.Smallest(3)`               |\n+| [**Take largest**](/documentation/transforms/python/aggregation/top/)            | `values.takeOrdered(3, lambda x: -x)` | `values | beam.combiners.Top.Largest(3)`                |\n+| [**Random sample**](/documentation/transforms/python/aggregation/sample/)        | `values.takeSample(False, 3)`         | `values | beam.combiners.Sample.FixedSizeGlobally(3)`   |\n+| [**Union**](/documentation/transforms/python/other/flatten/)                     | `values.union(otherValues)`           | `(values, otherValues) | beam.Flatten()`                |\n+| [**Co-group**](/documentation/transforms/python/aggregation/cogroupbykey/)       | `pairs.cogroup(otherPairs)`           | `{'Xs': pairs, 'Ys': otherPairs} | beam.CoGroupByKey()` |\n+{{< /table >}}\n+\n+> \u2139\ufe0f To learn more about the transforms available in Beam, check the\n+> [Python transform gallery](/documentation/transforms/python/overview).\n+\n+## Using calculated values\n+\n+Since we are working in potentially distributed environments,\n+we can't guarantee that the results we've calculated are available at any given machine.\n+\n+In PySpark, we can get a result from a collection of elements (RDD) by using\n+`data.collect()`, or other aggregations such as `reduce()`, `count()` and more.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67af84a51c471e7c53fc23a43acf597f60233273"}, "originalPosition": 209}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MjUyMDY1OA==", "bodyText": "Done", "url": "https://github.com/apache/beam/pull/12963#discussion_r542520658", "createdAt": "2020-12-14T16:26:59Z", "author": {"login": "davidcavazos"}, "path": "website/www/site/content/en/get-started/from-spark.md", "diffHunk": "@@ -0,0 +1,268 @@\n+---\n+title: \"Getting started from Apache Spark\"\n+---\n+<!--\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+-->\n+\n+# Getting started from Apache Spark\n+\n+{{< localstorage language language-py >}}\n+\n+If you already know [_Apache Spark_](http://spark.apache.org/),\n+learning _Apache Beam_ is familiar.\n+The Beam and Spark APIs are similar, so you already know the basic concepts.\n+\n+Spark stores data _Spark DataFrames_ for structured data,\n+and in _Resilient Distributed Datasets_ (RDD) for unstructured data.\n+We are using RDDs for this guide.\n+\n+A Spark RDD represents a collection of elements,\n+while in Beam it's called a _Parallel Collection_ (PCollection).\n+A PCollection in Beam does _not_ have any ordering guarantees.\n+\n+Likewise, a transform in Beam is called a _Parallel Transform_ (PTransform).\n+\n+Here are some examples of common operations and their equivalent between PySpark and Beam.\n+\n+## Overview\n+\n+Here's a simple example of a PySpark pipeline that takes the numbers from one to four,\n+multiplies them by two, adds all the values together, and prints the result.\n+\n+{{< highlight py >}}\n+import pyspark\n+\n+sc = pyspark.SparkContext()\n+result = (\n+    sc.parallelize([1, 2, 3, 4])\n+    .map(lambda x: x * 2)\n+    .reduce(lambda x, y: x + y)\n+)\n+print(result)\n+{{< /highlight >}}\n+\n+In Beam you _pipe_ your data through the pipeline using the\n+_pipe operator_ `|` like `data | beam.Map(...)` instead of chaining\n+methods like `data.map(...)`, but they're doing the same thing.\n+\n+Here's what an equivalent pipeline looks like in Beam.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | beam.Create([1, 2, 3, 4])\n+        | beam.Map(lambda x: x * 2)\n+        | beam.CombineGlobally(sum)\n+        | beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+> \u2139\ufe0f Note that we called `print` inside a `Map` transform.\n+> That's because we can only access the elements of a PCollection\n+> from within a PTransform.\n+\n+Another thing to note is that Beam pipelines are constructed _lazily_.\n+This means that when you pipe `|` data you're only _decalring_ the\n+transformations and the order you want them to happen,\n+but the actual computation doesn't happen.\n+The pipeline is run _after_ the `with beam.Pipeline() as pipeline` context has\n+closed.\n+\n+> \u2139\ufe0f When the `with beam.Pipeline() as pipeline` context closes,\n+> it implicitly calls `pipeline.run()` which triggers the computation to happen.\n+\n+The pipeline is then sent to your\n+[runner of choice](https://beam.apache.org/documentation/runners/capability-matrix/)\n+and it processes the data.\n+\n+> \u2139\ufe0f The pipeline can run locally with the _DirectRunner_,\n+> or in a distributed runner such as Flink, Spark, or Dataflow.\n+> The Spark runner is not related to PySpark.\n+\n+A label can optionally be added to a transform using the\n+_right shift operator_ `>>` like `data | 'My description' >> beam.Map(...)`.\n+This serves both as comments and makes your pipeline easier to debug.\n+\n+This is how the pipeline looks after adding labels.\n+\n+{{< highlight py >}}\n+import apache_beam as beam\n+\n+with beam.Pipeline() as pipeline:\n+    result = (\n+        pipeline\n+        | 'Create numbers' >> beam.Create([1, 2, 3, 4])\n+        | 'Multiply by two' >> beam.Map(lambda x: x * 2)\n+        | 'Sum everything' >> beam.CombineGlobally(sum)\n+        | 'Print results' >> beam.Map(print)\n+    )\n+{{< /highlight >}}\n+\n+## Setup\n+\n+Here's a comparison on how to get started both in PySpark and Beam.\n+\n+{{< table >}}\n+<table>\n+<tr>\n+    <th></th>\n+    <th>PySpark</th>\n+    <th>Beam</th>\n+</tr>\n+<tr>\n+    <td><b>Install</b></td>\n+    <td><code>$ pip install pyspark</code></td>\n+    <td><code>$ pip install apache-beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Imports</b></td>\n+    <td><code>import pyspark</code></td>\n+    <td><code>import apache_beam as beam</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating a<br>local pipeline</b></td>\n+    <td>\n+        <code>sc = pyspark.SparkContext() as sc:</code><br>\n+        <code># Your pipeline code here.</code>\n+    </td>\n+    <td>\n+        <code>with beam.Pipeline() as pipeline:</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;# Your pipeline code here.</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Creating values</b></td>\n+    <td><code>values = sc.parallelize([1, 2, 3, 4])</code></td>\n+    <td><code>values = pipeline | beam.Create([1, 2, 3, 4])</code></td>\n+</tr>\n+<tr>\n+    <td><b>Creating<br>key-value pairs</b></td>\n+    <td>\n+        <code>pairs = sc.parallelize([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+    <td>\n+        <code>pairs = pipeline | beam.Create([</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key1', 'value1'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key2', 'value2'),</code><br>\n+        <code>&nbsp;&nbsp;&nbsp;&nbsp;('key3', 'value3'),</code><br>\n+        <code>])</code>\n+    </td>\n+</tr>\n+<tr>\n+    <td><b>Running a<br>local pipeline</b></td>\n+    <td><code>$ spark-submit spark_pipeline.py</code></td>\n+    <td><code>$ python beam_pipeline.py</code></td>\n+</tr>\n+</table>\n+{{< /table >}}\n+\n+## Transforms\n+\n+Here are the equivalents of some common transforms in both PySpark and Beam.\n+\n+{{< table >}}\n+|                                                                                  | PySpark                               | Beam                                                    |\n+|----------------------------------------------------------------------------------|---------------------------------------|---------------------------------------------------------|\n+| [**Map**](/documentation/transforms/python/elementwise/map/)                     | `values.map(lambda x: x * 2)`         | `values | beam.Map(lambda x: x * 2)`                    |\n+| [**Filter**](/documentation/transforms/python/elementwise/filter/)               | `values.filter(lambda x: x % 2 == 0)` | `values | beam.Filter(lambda x: x % 2 == 0)`            |\n+| [**FlatMap**](/documentation/transforms/python/elementwise/flatmap/)             | `values.flatMap(lambda x: range(x))`  | `values | beam.FlatMap(lambda x: range(x))`             |\n+| [**Group by key**](/documentation/transforms/python/aggregation/groupbykey/)     | `pairs.groupByKey()`                  | `pairs | beam.GroupByKey()`                             |\n+| [**Reduce**](/documentation/transforms/python/aggregation/combineglobally/)      | `values.reduce(lambda x, y: x+y)`     | `values | beam.CombineGlobally(sum)`                    |\n+| [**Reduce by key**](/documentation/transforms/python/aggregation/combineperkey/) | `pairs.reduceByKey(lambda x, y: x+y)` | `pairs | beam.CombinePerKey(sum)`                       |\n+| [**Distinct**](/documentation/transforms/python/aggregation/distinct/)           | `values.distinct()`                   | `values | beam.Distinct()`                              |\n+| [**Count**](/documentation/transforms/python/aggregation/count/)                 | `values.count()`                      | `values | beam.combiners.Count.Globally()`              |\n+| [**Count by key**](/documentation/transforms/python/aggregation/count/)          | `pairs.countByKey()`                  | `pairs | beam.combiners.Count.PerKey()`                 |\n+| [**Take smallest**](/documentation/transforms/python/aggregation/top/)           | `values.takeOrdered(3)`               | `values | beam.combiners.Top.Smallest(3)`               |\n+| [**Take largest**](/documentation/transforms/python/aggregation/top/)            | `values.takeOrdered(3, lambda x: -x)` | `values | beam.combiners.Top.Largest(3)`                |\n+| [**Random sample**](/documentation/transforms/python/aggregation/sample/)        | `values.takeSample(False, 3)`         | `values | beam.combiners.Sample.FixedSizeGlobally(3)`   |\n+| [**Union**](/documentation/transforms/python/other/flatten/)                     | `values.union(otherValues)`           | `(values, otherValues) | beam.Flatten()`                |\n+| [**Co-group**](/documentation/transforms/python/aggregation/cogroupbykey/)       | `pairs.cogroup(otherPairs)`           | `{'Xs': pairs, 'Ys': otherPairs} | beam.CoGroupByKey()` |\n+{{< /table >}}\n+\n+> \u2139\ufe0f To learn more about the transforms available in Beam, check the\n+> [Python transform gallery](/documentation/transforms/python/overview).\n+\n+## Using calculated values\n+\n+Since we are working in potentially distributed environments,\n+we can't guarantee that the results we've calculated are available at any given machine.\n+\n+In PySpark, we can get a result from a collection of elements (RDD) by using\n+`data.collect()`, or other aggregations such as `reduce()`, `count()` and more.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0MTQyMTk5MQ=="}, "originalCommit": {"oid": "67af84a51c471e7c53fc23a43acf597f60233273"}, "originalPosition": 209}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3265, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}