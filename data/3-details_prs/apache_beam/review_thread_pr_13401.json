{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI1MDI1MzUz", "number": 13401, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMDo0MToyMlrOE7wZdw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMTozNDoyNVrOE7w0Bw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMDk0MzkxOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/dataframe/expressions.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMDo0MToyMlrOH3lASg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMDo0MToyMlrOH3lASg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0MDAxMA==", "bodyText": "Note this only stores the most recently computed result. We may want to also verify that each input partitioning yields equivalent results (modulo ordering).", "url": "https://github.com/apache/beam/pull/13401#discussion_r528040010", "createdAt": "2020-11-21T00:41:22Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/dataframe/expressions.py", "diffHunk": "@@ -67,28 +72,57 @@ def is_scalar(expr):\n         result = super(PartitioningSession, self).evaluate(expr)\n       else:\n         scaler_args = [arg for arg in expr.args() if is_scalar(arg)]\n-        parts = collections.defaultdict(\n-            lambda: Session({arg: self.evaluate(arg)\n-                             for arg in scaler_args}))\n-        for arg in expr.args():\n-          if not is_scalar(arg):\n-            input = self.evaluate(arg)\n-            for key, part in expr.requires_partition_by().test_partition_fn(\n-                input):\n-              parts[key]._bindings[arg] = part\n-        if not parts:\n-          parts[None]  # Create at least one entry.\n-\n-        results = []\n-        for session in parts.values():\n-          if any(len(session.lookup(arg)) for arg in expr.args()\n-                 if not is_scalar(arg)):\n-            results.append(session.evaluate(expr))\n-        if results:\n-          result = pd.concat(results)\n-        else:\n-          # Choose any single session.\n-          result = next(iter(parts.values())).evaluate(expr)\n+\n+        def evaluate_with(input_partitioning):\n+          parts = collections.defaultdict(\n+              lambda: Session({arg: self.evaluate(arg)\n+                               for arg in scaler_args}))\n+          for arg in expr.args():\n+            if not is_scalar(arg):\n+              input = self.evaluate(arg)\n+              for key, part in input_partitioning.test_partition_fn(input):\n+                parts[key]._bindings[arg] = part\n+          if not parts:\n+            parts[None]  # Create at least one entry.\n+\n+          results = []\n+          for session in parts.values():\n+            if any(len(session.lookup(arg)) for arg in expr.args()\n+                   if not is_scalar(arg)):\n+              results.append(session.evaluate(expr))\n+\n+          expected_output_partitioning = expr.preserves_partition_by(\n+          ) if input_partitioning.is_subpartitioning_of(\n+              expr.preserves_partition_by()) else input_partitioning\n+\n+          if not expected_output_partitioning.check(results):\n+            raise AssertionError(\n+                f\"\"\"Expression does not preserve partitioning!\n+                Expression: {expr}\n+                Requires: {expr.requires_partition_by()}\n+                Preserves: {expr.preserves_partition_by()}\n+                Input partitioning: {input_partitioning}\n+                Expected output partitioning: {expected_output_partitioning}\n+                \"\"\")\n+\n+          if results:\n+            return pd.concat(results)\n+          else:\n+            # Choose any single session.\n+            return next(iter(parts.values())).evaluate(expr)\n+\n+        input_partitioning = expr.requires_partition_by()\n+\n+        while input_partitioning is not None:\n+          result = evaluate_with(input_partitioning)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ac1e3a3705ecd00bfdafea8bda207c419bc42633"}, "originalPosition": 88}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMTAwMjEyOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/dataframe/expressions.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMToyNToxMVrOH3lfXQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMToyNToxMVrOH3lfXQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0Nzk2NQ==", "bodyText": "Well, it could be something different...", "url": "https://github.com/apache/beam/pull/13401#discussion_r528047965", "createdAt": "2020-11-21T01:25:11Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/expressions.py", "diffHunk": "@@ -67,28 +72,57 @@ def is_scalar(expr):\n         result = super(PartitioningSession, self).evaluate(expr)\n       else:\n         scaler_args = [arg for arg in expr.args() if is_scalar(arg)]\n-        parts = collections.defaultdict(\n-            lambda: Session({arg: self.evaluate(arg)\n-                             for arg in scaler_args}))\n-        for arg in expr.args():\n-          if not is_scalar(arg):\n-            input = self.evaluate(arg)\n-            for key, part in expr.requires_partition_by().test_partition_fn(\n-                input):\n-              parts[key]._bindings[arg] = part\n-        if not parts:\n-          parts[None]  # Create at least one entry.\n-\n-        results = []\n-        for session in parts.values():\n-          if any(len(session.lookup(arg)) for arg in expr.args()\n-                 if not is_scalar(arg)):\n-            results.append(session.evaluate(expr))\n-        if results:\n-          result = pd.concat(results)\n-        else:\n-          # Choose any single session.\n-          result = next(iter(parts.values())).evaluate(expr)\n+\n+        def evaluate_with(input_partitioning):\n+          parts = collections.defaultdict(\n+              lambda: Session({arg: self.evaluate(arg)\n+                               for arg in scaler_args}))\n+          for arg in expr.args():\n+            if not is_scalar(arg):\n+              input = self.evaluate(arg)\n+              for key, part in input_partitioning.test_partition_fn(input):\n+                parts[key]._bindings[arg] = part\n+          if not parts:\n+            parts[None]  # Create at least one entry.\n+\n+          results = []\n+          for session in parts.values():\n+            if any(len(session.lookup(arg)) for arg in expr.args()\n+                   if not is_scalar(arg)):\n+              results.append(session.evaluate(expr))\n+\n+          expected_output_partitioning = expr.preserves_partition_by(\n+          ) if input_partitioning.is_subpartitioning_of(\n+              expr.preserves_partition_by()) else input_partitioning\n+\n+          if not expected_output_partitioning.check(results):\n+            raise AssertionError(\n+                f\"\"\"Expression does not preserve partitioning!\n+                Expression: {expr}\n+                Requires: {expr.requires_partition_by()}\n+                Preserves: {expr.preserves_partition_by()}\n+                Input partitioning: {input_partitioning}\n+                Expected output partitioning: {expected_output_partitioning}\n+                \"\"\")\n+\n+          if results:\n+            return pd.concat(results)\n+          else:\n+            # Choose any single session.\n+            return next(iter(parts.values())).evaluate(expr)\n+\n+        input_partitioning = expr.requires_partition_by()\n+\n+        while input_partitioning is not None:\n+          result = evaluate_with(input_partitioning)\n+\n+          if input_partitioning == partitionings.Nothing():\n+            input_partitioning = partitionings.Index()\n+          elif isinstance(input_partitioning, partitionings.Index):\n+            input_partitioning = partitionings.Singleton()\n+          else:  # partitionings.Singleton()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79a08776618b8e1474862479fe6fd83f381758dd"}, "originalPosition": 94}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMTAwMzI3OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/dataframe/expressions.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMToyNjoxMlrOH3lf7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMToyNjoxMlrOH3lf7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0ODExMQ==", "bodyText": "This loop isn't obvious to follow. Perhaps iterate over set([input_partitioning, Nothing(), Index(), Singleton()]) and continue in the cases where it's not a subpartition of index_partitioning.", "url": "https://github.com/apache/beam/pull/13401#discussion_r528048111", "createdAt": "2020-11-21T01:26:12Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/expressions.py", "diffHunk": "@@ -67,28 +72,57 @@ def is_scalar(expr):\n         result = super(PartitioningSession, self).evaluate(expr)\n       else:\n         scaler_args = [arg for arg in expr.args() if is_scalar(arg)]\n-        parts = collections.defaultdict(\n-            lambda: Session({arg: self.evaluate(arg)\n-                             for arg in scaler_args}))\n-        for arg in expr.args():\n-          if not is_scalar(arg):\n-            input = self.evaluate(arg)\n-            for key, part in expr.requires_partition_by().test_partition_fn(\n-                input):\n-              parts[key]._bindings[arg] = part\n-        if not parts:\n-          parts[None]  # Create at least one entry.\n-\n-        results = []\n-        for session in parts.values():\n-          if any(len(session.lookup(arg)) for arg in expr.args()\n-                 if not is_scalar(arg)):\n-            results.append(session.evaluate(expr))\n-        if results:\n-          result = pd.concat(results)\n-        else:\n-          # Choose any single session.\n-          result = next(iter(parts.values())).evaluate(expr)\n+\n+        def evaluate_with(input_partitioning):\n+          parts = collections.defaultdict(\n+              lambda: Session({arg: self.evaluate(arg)\n+                               for arg in scaler_args}))\n+          for arg in expr.args():\n+            if not is_scalar(arg):\n+              input = self.evaluate(arg)\n+              for key, part in input_partitioning.test_partition_fn(input):\n+                parts[key]._bindings[arg] = part\n+          if not parts:\n+            parts[None]  # Create at least one entry.\n+\n+          results = []\n+          for session in parts.values():\n+            if any(len(session.lookup(arg)) for arg in expr.args()\n+                   if not is_scalar(arg)):\n+              results.append(session.evaluate(expr))\n+\n+          expected_output_partitioning = expr.preserves_partition_by(\n+          ) if input_partitioning.is_subpartitioning_of(\n+              expr.preserves_partition_by()) else input_partitioning\n+\n+          if not expected_output_partitioning.check(results):\n+            raise AssertionError(\n+                f\"\"\"Expression does not preserve partitioning!\n+                Expression: {expr}\n+                Requires: {expr.requires_partition_by()}\n+                Preserves: {expr.preserves_partition_by()}\n+                Input partitioning: {input_partitioning}\n+                Expected output partitioning: {expected_output_partitioning}\n+                \"\"\")\n+\n+          if results:\n+            return pd.concat(results)\n+          else:\n+            # Choose any single session.\n+            return next(iter(parts.values())).evaluate(expr)\n+\n+        input_partitioning = expr.requires_partition_by()\n+\n+        while input_partitioning is not None:\n+          result = evaluate_with(input_partitioning)\n+\n+          if input_partitioning == partitionings.Nothing():", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79a08776618b8e1474862479fe6fd83f381758dd"}, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMTAwNDM4OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/dataframe/frames.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMToyNzoxM1rOH3lgkA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQwMToyNjozOVrOH4mG9w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0ODI3Mg==", "bodyText": "Did yapf suggest this?", "url": "https://github.com/apache/beam/pull/13401#discussion_r528048272", "createdAt": "2020-11-21T01:27:13Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/frames.py", "diffHunk": "@@ -1401,17 +1403,20 @@ def replace(self, limit, **kwargs):\n   def reset_index(self, level=None, **kwargs):\n     if level is not None and not isinstance(level, (tuple, list)):\n       level = [level]\n+\n     if level is None or len(level) == self._expr.proxy().index.nlevels:\n       # TODO: Could do distributed re-index with offsets.\n       requires_partition_by = partitionings.Singleton()\n     else:\n       requires_partition_by = partitionings.Nothing()\n+\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79a08776618b8e1474862479fe6fd83f381758dd"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTEwNjY3OQ==", "bodyText": "I think this was me, removed it", "url": "https://github.com/apache/beam/pull/13401#discussion_r529106679", "createdAt": "2020-11-24T01:26:39Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/dataframe/frames.py", "diffHunk": "@@ -1401,17 +1403,20 @@ def replace(self, limit, **kwargs):\n   def reset_index(self, level=None, **kwargs):\n     if level is not None and not isinstance(level, (tuple, list)):\n       level = [level]\n+\n     if level is None or len(level) == self._expr.proxy().index.nlevels:\n       # TODO: Could do distributed re-index with offsets.\n       requires_partition_by = partitionings.Singleton()\n     else:\n       requires_partition_by = partitionings.Nothing()\n+\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0ODI3Mg=="}, "originalCommit": {"oid": "79a08776618b8e1474862479fe6fd83f381758dd"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMTAwNTYyOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/dataframe/expressions.py", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMToyODozNVrOH3lhPw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQwMToyNjowNVrOH4mEnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0ODQ0Nw==", "bodyText": "I wonder how expensive this will get. Hopefully not too bad. It could, however, mess up anything that depends on the random seed.", "url": "https://github.com/apache/beam/pull/13401#discussion_r528048447", "createdAt": "2020-11-21T01:28:35Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/expressions.py", "diffHunk": "@@ -48,10 +48,15 @@ def lookup(self, expr):  #  type: (Expression) -> Any\n class PartitioningSession(Session):\n   \"\"\"An extension of Session that enforces actual partitioning of inputs.\n \n-  When evaluating an expression, inputs are partitioned according to its\n-  `requires_partition_by` specifications, the expression is evaluated on each\n-  partition separately, and the final result concatinated, as if this were\n-  actually executed in a parallel manner.\n+  Each expression is evaluated multiple times for various supported", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79a08776618b8e1474862479fe6fd83f381758dd"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkyMzc1Nw==", "bodyText": "Yeah I think the performance impact is worth it. We could probably reduce it by being a little more discerning in what gets re-executed. For example I don't think there's any value in doing this for expressions that have preserves=Nothing().\nReally good point about the random seed. I could bracket the runs with calls to random.getstate() and random.setstate()", "url": "https://github.com/apache/beam/pull/13401#discussion_r528923757", "createdAt": "2020-11-23T18:50:55Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/dataframe/expressions.py", "diffHunk": "@@ -48,10 +48,15 @@ def lookup(self, expr):  #  type: (Expression) -> Any\n class PartitioningSession(Session):\n   \"\"\"An extension of Session that enforces actual partitioning of inputs.\n \n-  When evaluating an expression, inputs are partitioned according to its\n-  `requires_partition_by` specifications, the expression is evaluated on each\n-  partition separately, and the final result concatinated, as if this were\n-  actually executed in a parallel manner.\n+  Each expression is evaluated multiple times for various supported", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0ODQ0Nw=="}, "originalCommit": {"oid": "79a08776618b8e1474862479fe6fd83f381758dd"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTA4MjY4OQ==", "bodyText": "Let's do that.", "url": "https://github.com/apache/beam/pull/13401#discussion_r529082689", "createdAt": "2020-11-24T00:25:27Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/expressions.py", "diffHunk": "@@ -48,10 +48,15 @@ def lookup(self, expr):  #  type: (Expression) -> Any\n class PartitioningSession(Session):\n   \"\"\"An extension of Session that enforces actual partitioning of inputs.\n \n-  When evaluating an expression, inputs are partitioned according to its\n-  `requires_partition_by` specifications, the expression is evaluated on each\n-  partition separately, and the final result concatinated, as if this were\n-  actually executed in a parallel manner.\n+  Each expression is evaluated multiple times for various supported", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0ODQ0Nw=="}, "originalCommit": {"oid": "79a08776618b8e1474862479fe6fd83f381758dd"}, "originalPosition": 8}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTEwNjA3Nw==", "bodyText": "Done", "url": "https://github.com/apache/beam/pull/13401#discussion_r529106077", "createdAt": "2020-11-24T01:26:05Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/dataframe/expressions.py", "diffHunk": "@@ -48,10 +48,15 @@ def lookup(self, expr):  #  type: (Expression) -> Any\n class PartitioningSession(Session):\n   \"\"\"An extension of Session that enforces actual partitioning of inputs.\n \n-  When evaluating an expression, inputs are partitioned according to its\n-  `requires_partition_by` specifications, the expression is evaluated on each\n-  partition separately, and the final result concatinated, as if this were\n-  actually executed in a parallel manner.\n+  Each expression is evaluated multiple times for various supported", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0ODQ0Nw=="}, "originalCommit": {"oid": "79a08776618b8e1474862479fe6fd83f381758dd"}, "originalPosition": 8}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxMTAxMTkxOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/dataframe/partitionings.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMVQwMTozNDoyNVrOH3lkng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNFQwMToyNTozOFrOH4mC9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0OTMxMA==", "bodyText": "This isn't a very strong check if the dataframes are fairly sparse (as they are in most examples). We could try concat + repartition and verify the results are the same.", "url": "https://github.com/apache/beam/pull/13401#discussion_r528049310", "createdAt": "2020-11-21T01:34:25Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/partitionings.py", "diffHunk": "@@ -115,6 +115,23 @@ def partition_fn(self, df, num_partitions):\n     for key in range(num_partitions):\n       yield key, df[hashes % num_partitions == key]\n \n+  def check(self, dfs):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "79a08776618b8e1474862479fe6fd83f381758dd"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyOTEwNTY1NA==", "bodyText": "Added a TODO to do this in a later PR", "url": "https://github.com/apache/beam/pull/13401#discussion_r529105654", "createdAt": "2020-11-24T01:25:38Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/dataframe/partitionings.py", "diffHunk": "@@ -115,6 +115,23 @@ def partition_fn(self, df, num_partitions):\n     for key in range(num_partitions):\n       yield key, df[hashes % num_partitions == key]\n \n+  def check(self, dfs):", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODA0OTMxMA=="}, "originalCommit": {"oid": "79a08776618b8e1474862479fe6fd83f381758dd"}, "originalPosition": 4}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2629, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}