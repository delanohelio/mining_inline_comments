{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQ3ODc2MjYw", "number": 12232, "reviewThreads": {"totalCount": 16, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMlQwOTo1MjoxN1rOENlKlA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wMlQxOTo0NzozNFrOEUaaww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgyNjc1ODYwOnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xMlQwOTo1MjoxN1rOGwS6WA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwOToxOTo1MlrOGx1qnQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzI5NDY4MA==", "bodyText": "In my last PR (#12073), Rui suggested there is a row comparator available for use. I found it a private class in BeamSortRel. I just wonder if I could copy the code from there (maybe also and a reference?). What is the correct way of doing it?", "url": "https://github.com/apache/beam/pull/12232#discussion_r453294680", "createdAt": "2020-07-12T09:52:17Z", "author": {"login": "Mark-Zeng"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.vendor.calcite.v1_20_0.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPPattern;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.OrderKey;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.BeamCostModel;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.NodeStats;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptCluster;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** {@link BeamRelNode} to replace a {@link Match} node. */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n+\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+\n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));\n+      }\n+      Schema mySchema = schemaBuilder.build();\n+\n+      // partition according to the partition keys\n+      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n+\n+      // group by keys\n+      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n+          keyedUpstream\n+              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n+              .apply(GroupByKey.create());\n+\n+      // sort within each keyed partition\n+      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n+          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n+\n+      // apply the pattern match in each partition\n+      ArrayList<CEPPattern> cepPattern =\n+          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n+      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n+      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n+          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n+\n+      // apply the ParDo for the measures clause\n+      // for now, output the all rows of each pattern matched (for testing purpose)\n+      PCollection<Row> outStream =\n+          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+\n+      return outStream;\n+    }\n+\n+    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n+\n+      @ProcessElement\n+      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n+        for (Row i : keyRows.getValue()) {\n+          out.output(i);\n+        }\n+      }\n+    }\n+\n+    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n+    // support only one row per match for now.\n+    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final ArrayList<CEPPattern> pattern;\n+      private final String regexPattern;\n+\n+      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n+        this.pattern = pattern;\n+        this.regexPattern = regexPattern;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<>();\n+        StringBuilder patternString = new StringBuilder();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+          // check pattern of row i\n+          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n+          for (int j = 0; j < pattern.size(); ++j) {\n+            CEPPattern tryPattern = pattern.get(j);\n+            if (tryPattern.evalRow(i)) {\n+              patternOfRow = tryPattern.toString();\n+            }\n+          }\n+          patternString.append(patternOfRow);\n+        }\n+\n+        Pattern p = Pattern.compile(regexPattern);\n+        Matcher m = p.matcher(patternString.toString());\n+        // if the pattern is (A B+ C),\n+        // it should return a List three rows matching A B C respectively\n+        if (m.matches()) {\n+          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n+        }\n+      }\n+    }\n+\n+    private static class SortPerKey extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final Schema cSchema;\n+      private final ArrayList<OrderKey> orderKeys;\n+\n+      public SortPerKey(Schema cSchema, RelCollation orderKeys) {\n+        this.cSchema = cSchema;\n+\n+        List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n+        Collections.reverse(revOrderKeys);\n+        ArrayList<OrderKey> revOrderKeysList = new ArrayList<>();\n+        for (RelFieldCollation i : revOrderKeys) {\n+          int fIndex = i.getFieldIndex();\n+          RelFieldCollation.Direction dir = i.getDirection();\n+          if (dir == RelFieldCollation.Direction.ASCENDING) {\n+            revOrderKeysList.add(new OrderKey(fIndex, false));\n+          } else {\n+            revOrderKeysList.add(new OrderKey(fIndex, true));\n+          }\n+        }\n+\n+        this.orderKeys = revOrderKeysList;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<Row>();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+        }\n+        for (OrderKey i : orderKeys) {\n+          int fIndex = i.getIndex();\n+          boolean dir = i.getDir();\n+          rows.sort(new SortComparator(fIndex, dir));\n+        }\n+        // TODO: Change the comparator to the row comparator:\n+        // https://github.com/apache/beam/blob/master/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamSortRel.java#L373\n+\n+        out.output(KV.of(keyRows.getKey(), rows));\n+      }\n+\n+      private class SortComparator implements Comparator<Row> {\n+\n+        private final int fIndex;\n+        private final int inv;\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 286}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk3OTgwMA==", "bodyText": "Hi you can update that class to public to use it.", "url": "https://github.com/apache/beam/pull/12232#discussion_r453979800", "createdAt": "2020-07-13T22:27:14Z", "author": {"login": "amaliujia"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.vendor.calcite.v1_20_0.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPPattern;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.OrderKey;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.BeamCostModel;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.NodeStats;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptCluster;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** {@link BeamRelNode} to replace a {@link Match} node. */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n+\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+\n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));\n+      }\n+      Schema mySchema = schemaBuilder.build();\n+\n+      // partition according to the partition keys\n+      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n+\n+      // group by keys\n+      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n+          keyedUpstream\n+              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n+              .apply(GroupByKey.create());\n+\n+      // sort within each keyed partition\n+      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n+          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n+\n+      // apply the pattern match in each partition\n+      ArrayList<CEPPattern> cepPattern =\n+          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n+      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n+      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n+          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n+\n+      // apply the ParDo for the measures clause\n+      // for now, output the all rows of each pattern matched (for testing purpose)\n+      PCollection<Row> outStream =\n+          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+\n+      return outStream;\n+    }\n+\n+    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n+\n+      @ProcessElement\n+      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n+        for (Row i : keyRows.getValue()) {\n+          out.output(i);\n+        }\n+      }\n+    }\n+\n+    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n+    // support only one row per match for now.\n+    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final ArrayList<CEPPattern> pattern;\n+      private final String regexPattern;\n+\n+      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n+        this.pattern = pattern;\n+        this.regexPattern = regexPattern;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<>();\n+        StringBuilder patternString = new StringBuilder();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+          // check pattern of row i\n+          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n+          for (int j = 0; j < pattern.size(); ++j) {\n+            CEPPattern tryPattern = pattern.get(j);\n+            if (tryPattern.evalRow(i)) {\n+              patternOfRow = tryPattern.toString();\n+            }\n+          }\n+          patternString.append(patternOfRow);\n+        }\n+\n+        Pattern p = Pattern.compile(regexPattern);\n+        Matcher m = p.matcher(patternString.toString());\n+        // if the pattern is (A B+ C),\n+        // it should return a List three rows matching A B C respectively\n+        if (m.matches()) {\n+          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n+        }\n+      }\n+    }\n+\n+    private static class SortPerKey extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final Schema cSchema;\n+      private final ArrayList<OrderKey> orderKeys;\n+\n+      public SortPerKey(Schema cSchema, RelCollation orderKeys) {\n+        this.cSchema = cSchema;\n+\n+        List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n+        Collections.reverse(revOrderKeys);\n+        ArrayList<OrderKey> revOrderKeysList = new ArrayList<>();\n+        for (RelFieldCollation i : revOrderKeys) {\n+          int fIndex = i.getFieldIndex();\n+          RelFieldCollation.Direction dir = i.getDirection();\n+          if (dir == RelFieldCollation.Direction.ASCENDING) {\n+            revOrderKeysList.add(new OrderKey(fIndex, false));\n+          } else {\n+            revOrderKeysList.add(new OrderKey(fIndex, true));\n+          }\n+        }\n+\n+        this.orderKeys = revOrderKeysList;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<Row>();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+        }\n+        for (OrderKey i : orderKeys) {\n+          int fIndex = i.getIndex();\n+          boolean dir = i.getDir();\n+          rows.sort(new SortComparator(fIndex, dir));\n+        }\n+        // TODO: Change the comparator to the row comparator:\n+        // https://github.com/apache/beam/blob/master/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamSortRel.java#L373\n+\n+        out.output(KV.of(keyRows.getKey(), rows));\n+      }\n+\n+      private class SortComparator implements Comparator<Row> {\n+\n+        private final int fIndex;\n+        private final int inv;\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzI5NDY4MA=="}, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 286}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk4OTgzNA==", "bodyText": "In fact what you are doing is ok. This is minor.", "url": "https://github.com/apache/beam/pull/12232#discussion_r453989834", "createdAt": "2020-07-13T22:45:54Z", "author": {"login": "amaliujia"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.vendor.calcite.v1_20_0.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPPattern;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.OrderKey;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.BeamCostModel;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.NodeStats;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptCluster;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** {@link BeamRelNode} to replace a {@link Match} node. */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n+\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+\n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));\n+      }\n+      Schema mySchema = schemaBuilder.build();\n+\n+      // partition according to the partition keys\n+      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n+\n+      // group by keys\n+      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n+          keyedUpstream\n+              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n+              .apply(GroupByKey.create());\n+\n+      // sort within each keyed partition\n+      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n+          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n+\n+      // apply the pattern match in each partition\n+      ArrayList<CEPPattern> cepPattern =\n+          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n+      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n+      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n+          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n+\n+      // apply the ParDo for the measures clause\n+      // for now, output the all rows of each pattern matched (for testing purpose)\n+      PCollection<Row> outStream =\n+          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+\n+      return outStream;\n+    }\n+\n+    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n+\n+      @ProcessElement\n+      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n+        for (Row i : keyRows.getValue()) {\n+          out.output(i);\n+        }\n+      }\n+    }\n+\n+    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n+    // support only one row per match for now.\n+    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final ArrayList<CEPPattern> pattern;\n+      private final String regexPattern;\n+\n+      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n+        this.pattern = pattern;\n+        this.regexPattern = regexPattern;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<>();\n+        StringBuilder patternString = new StringBuilder();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+          // check pattern of row i\n+          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n+          for (int j = 0; j < pattern.size(); ++j) {\n+            CEPPattern tryPattern = pattern.get(j);\n+            if (tryPattern.evalRow(i)) {\n+              patternOfRow = tryPattern.toString();\n+            }\n+          }\n+          patternString.append(patternOfRow);\n+        }\n+\n+        Pattern p = Pattern.compile(regexPattern);\n+        Matcher m = p.matcher(patternString.toString());\n+        // if the pattern is (A B+ C),\n+        // it should return a List three rows matching A B C respectively\n+        if (m.matches()) {\n+          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n+        }\n+      }\n+    }\n+\n+    private static class SortPerKey extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final Schema cSchema;\n+      private final ArrayList<OrderKey> orderKeys;\n+\n+      public SortPerKey(Schema cSchema, RelCollation orderKeys) {\n+        this.cSchema = cSchema;\n+\n+        List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n+        Collections.reverse(revOrderKeys);\n+        ArrayList<OrderKey> revOrderKeysList = new ArrayList<>();\n+        for (RelFieldCollation i : revOrderKeys) {\n+          int fIndex = i.getFieldIndex();\n+          RelFieldCollation.Direction dir = i.getDirection();\n+          if (dir == RelFieldCollation.Direction.ASCENDING) {\n+            revOrderKeysList.add(new OrderKey(fIndex, false));\n+          } else {\n+            revOrderKeysList.add(new OrderKey(fIndex, true));\n+          }\n+        }\n+\n+        this.orderKeys = revOrderKeysList;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<Row>();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+        }\n+        for (OrderKey i : orderKeys) {\n+          int fIndex = i.getIndex();\n+          boolean dir = i.getDir();\n+          rows.sort(new SortComparator(fIndex, dir));\n+        }\n+        // TODO: Change the comparator to the row comparator:\n+        // https://github.com/apache/beam/blob/master/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamSortRel.java#L373\n+\n+        out.output(KV.of(keyRows.getKey(), rows));\n+      }\n+\n+      private class SortComparator implements Comparator<Row> {\n+\n+        private final int fIndex;\n+        private final int inv;\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzI5NDY4MA=="}, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 286}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkxMjY2OQ==", "bodyText": "I just updated the implementation using the comparator in BeamSortRel. You will see it in my next commit!", "url": "https://github.com/apache/beam/pull/12232#discussion_r454912669", "createdAt": "2020-07-15T09:19:52Z", "author": {"login": "Mark-Zeng"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.vendor.calcite.v1_20_0.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPPattern;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.OrderKey;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.BeamCostModel;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.NodeStats;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptCluster;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** {@link BeamRelNode} to replace a {@link Match} node. */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n+\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+\n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));\n+      }\n+      Schema mySchema = schemaBuilder.build();\n+\n+      // partition according to the partition keys\n+      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n+\n+      // group by keys\n+      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n+          keyedUpstream\n+              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n+              .apply(GroupByKey.create());\n+\n+      // sort within each keyed partition\n+      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n+          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n+\n+      // apply the pattern match in each partition\n+      ArrayList<CEPPattern> cepPattern =\n+          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n+      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n+      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n+          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n+\n+      // apply the ParDo for the measures clause\n+      // for now, output the all rows of each pattern matched (for testing purpose)\n+      PCollection<Row> outStream =\n+          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+\n+      return outStream;\n+    }\n+\n+    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n+\n+      @ProcessElement\n+      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n+        for (Row i : keyRows.getValue()) {\n+          out.output(i);\n+        }\n+      }\n+    }\n+\n+    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n+    // support only one row per match for now.\n+    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final ArrayList<CEPPattern> pattern;\n+      private final String regexPattern;\n+\n+      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n+        this.pattern = pattern;\n+        this.regexPattern = regexPattern;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<>();\n+        StringBuilder patternString = new StringBuilder();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+          // check pattern of row i\n+          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n+          for (int j = 0; j < pattern.size(); ++j) {\n+            CEPPattern tryPattern = pattern.get(j);\n+            if (tryPattern.evalRow(i)) {\n+              patternOfRow = tryPattern.toString();\n+            }\n+          }\n+          patternString.append(patternOfRow);\n+        }\n+\n+        Pattern p = Pattern.compile(regexPattern);\n+        Matcher m = p.matcher(patternString.toString());\n+        // if the pattern is (A B+ C),\n+        // it should return a List three rows matching A B C respectively\n+        if (m.matches()) {\n+          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n+        }\n+      }\n+    }\n+\n+    private static class SortPerKey extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final Schema cSchema;\n+      private final ArrayList<OrderKey> orderKeys;\n+\n+      public SortPerKey(Schema cSchema, RelCollation orderKeys) {\n+        this.cSchema = cSchema;\n+\n+        List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n+        Collections.reverse(revOrderKeys);\n+        ArrayList<OrderKey> revOrderKeysList = new ArrayList<>();\n+        for (RelFieldCollation i : revOrderKeys) {\n+          int fIndex = i.getFieldIndex();\n+          RelFieldCollation.Direction dir = i.getDirection();\n+          if (dir == RelFieldCollation.Direction.ASCENDING) {\n+            revOrderKeysList.add(new OrderKey(fIndex, false));\n+          } else {\n+            revOrderKeysList.add(new OrderKey(fIndex, true));\n+          }\n+        }\n+\n+        this.orderKeys = revOrderKeysList;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<Row>();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+        }\n+        for (OrderKey i : orderKeys) {\n+          int fIndex = i.getIndex();\n+          boolean dir = i.getDir();\n+          rows.sort(new SortComparator(fIndex, dir));\n+        }\n+        // TODO: Change the comparator to the row comparator:\n+        // https://github.com/apache/beam/blob/master/sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamSortRel.java#L373\n+\n+        out.output(KV.of(keyRows.getKey(), rows));\n+      }\n+\n+      private class SortComparator implements Comparator<Row> {\n+\n+        private final int fIndex;\n+        private final int inv;\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MzI5NDY4MA=="}, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 286}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzMTQ1MzE1OnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPOperand.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QyMjo0MDoxNVrOGw9NzQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwOToxODoxMFrOGx1nGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk4Nzc4OQ==", "bodyText": "This seems a unused class?", "url": "https://github.com/apache/beam/pull/12232#discussion_r453987789", "createdAt": "2020-07-13T22:40:15Z", "author": {"login": "amaliujia"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPOperand.java", "diffHunk": "@@ -0,0 +1,20 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.cep;\n+\n+public class CEPOperand {}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkxMTc2OA==", "bodyText": "This is indeed redundant.", "url": "https://github.com/apache/beam/pull/12232#discussion_r454911768", "createdAt": "2020-07-15T09:18:10Z", "author": {"login": "Mark-Zeng"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPOperand.java", "diffHunk": "@@ -0,0 +1,20 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.cep;\n+\n+public class CEPOperand {}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk4Nzc4OQ=="}, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 20}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzMTQ2MjU2OnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QyMjo0NDoxNFrOGw9TUg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQyMjowNToyM1rOGyS7lA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk4OTIwMg==", "bodyText": "To make sure I understand this example.\nDoes PATTERN(A B C) means it should produce rows, in which each three rows are a set, and in each set, names should be a, b, c and also in this order?", "url": "https://github.com/apache/beam/pull/12232#discussion_r453989202", "createdAt": "2020-07-13T22:44:14Z", "author": {"login": "amaliujia"}, "path": "sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.compilePipeline;\n+import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.registerTable;\n+\n+import org.apache.beam.sdk.extensions.sql.TestUtils;\n+import org.apache.beam.sdk.extensions.sql.meta.provider.test.TestBoundedTable;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.junit.Rule;\n+import org.junit.Test;\n+\n+public class BeamMatchRelTest {\n+\n+  @Rule public final TestPipeline pipeline = TestPipeline.create();\n+\n+  @Test\n+  public void matchLogicalPlanTest() {\n+    Schema schemaType =\n+        Schema.builder()\n+            .addInt32Field(\"id\")\n+            .addStringField(\"name\")\n+            .addInt32Field(\"proctime\")\n+            .build();\n+\n+    registerTable(\n+        \"TestTable\", TestBoundedTable.of(schemaType).addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3));\n+\n+    String sql =\n+        \"SELECT * \"\n+            + \"FROM TestTable \"\n+            + \"MATCH_RECOGNIZE (\"\n+            + \"PARTITION BY id \"\n+            + \"ORDER BY proctime \"\n+            + \"PATTERN (A B C) \"", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkyNTQwMQ==", "bodyText": "Yes, if by ''set'' you mean partition. There are 2 output modes: ALL ROWS PER MATCH and ONE ROW PER MATCH (default) which I have not implemented. I just output all rows from a match because I want to check if the pattern-match part works.", "url": "https://github.com/apache/beam/pull/12232#discussion_r454925401", "createdAt": "2020-07-15T09:41:31Z", "author": {"login": "Mark-Zeng"}, "path": "sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.compilePipeline;\n+import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.registerTable;\n+\n+import org.apache.beam.sdk.extensions.sql.TestUtils;\n+import org.apache.beam.sdk.extensions.sql.meta.provider.test.TestBoundedTable;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.junit.Rule;\n+import org.junit.Test;\n+\n+public class BeamMatchRelTest {\n+\n+  @Rule public final TestPipeline pipeline = TestPipeline.create();\n+\n+  @Test\n+  public void matchLogicalPlanTest() {\n+    Schema schemaType =\n+        Schema.builder()\n+            .addInt32Field(\"id\")\n+            .addStringField(\"name\")\n+            .addInt32Field(\"proctime\")\n+            .build();\n+\n+    registerTable(\n+        \"TestTable\", TestBoundedTable.of(schemaType).addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3));\n+\n+    String sql =\n+        \"SELECT * \"\n+            + \"FROM TestTable \"\n+            + \"MATCH_RECOGNIZE (\"\n+            + \"PARTITION BY id \"\n+            + \"ORDER BY proctime \"\n+            + \"PATTERN (A B C) \"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk4OTIwMg=="}, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkyODAwOA==", "bodyText": "In Flink CEP, it only supports ONE ROW PER MATCH and the output columns are determined by the PARTITION BY and the MEASURES (not implemented yet) clauses.", "url": "https://github.com/apache/beam/pull/12232#discussion_r454928008", "createdAt": "2020-07-15T09:45:54Z", "author": {"login": "Mark-Zeng"}, "path": "sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.compilePipeline;\n+import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.registerTable;\n+\n+import org.apache.beam.sdk.extensions.sql.TestUtils;\n+import org.apache.beam.sdk.extensions.sql.meta.provider.test.TestBoundedTable;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.junit.Rule;\n+import org.junit.Test;\n+\n+public class BeamMatchRelTest {\n+\n+  @Rule public final TestPipeline pipeline = TestPipeline.create();\n+\n+  @Test\n+  public void matchLogicalPlanTest() {\n+    Schema schemaType =\n+        Schema.builder()\n+            .addInt32Field(\"id\")\n+            .addStringField(\"name\")\n+            .addInt32Field(\"proctime\")\n+            .build();\n+\n+    registerTable(\n+        \"TestTable\", TestBoundedTable.of(schemaType).addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3));\n+\n+    String sql =\n+        \"SELECT * \"\n+            + \"FROM TestTable \"\n+            + \"MATCH_RECOGNIZE (\"\n+            + \"PARTITION BY id \"\n+            + \"ORDER BY proctime \"\n+            + \"PATTERN (A B C) \"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk4OTIwMg=="}, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 55}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTM5MjE0OA==", "bodyText": "I see. Thanks for clarification.", "url": "https://github.com/apache/beam/pull/12232#discussion_r455392148", "createdAt": "2020-07-15T22:05:23Z", "author": {"login": "amaliujia"}, "path": "sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java", "diffHunk": "@@ -0,0 +1,75 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.compilePipeline;\n+import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.registerTable;\n+\n+import org.apache.beam.sdk.extensions.sql.TestUtils;\n+import org.apache.beam.sdk.extensions.sql.meta.provider.test.TestBoundedTable;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.junit.Rule;\n+import org.junit.Test;\n+\n+public class BeamMatchRelTest {\n+\n+  @Rule public final TestPipeline pipeline = TestPipeline.create();\n+\n+  @Test\n+  public void matchLogicalPlanTest() {\n+    Schema schemaType =\n+        Schema.builder()\n+            .addInt32Field(\"id\")\n+            .addStringField(\"name\")\n+            .addInt32Field(\"proctime\")\n+            .build();\n+\n+    registerTable(\n+        \"TestTable\", TestBoundedTable.of(schemaType).addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3));\n+\n+    String sql =\n+        \"SELECT * \"\n+            + \"FROM TestTable \"\n+            + \"MATCH_RECOGNIZE (\"\n+            + \"PARTITION BY id \"\n+            + \"ORDER BY proctime \"\n+            + \"PATTERN (A B C) \"", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk4OTIwMg=="}, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 55}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzMTQ2Mzg0OnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QyMjo0NDo0NFrOGw9UDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QyMjo0NDo0NFrOGw9UDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk4OTM4OQ==", "bodyText": "For all new classes, please add javadoc to explain these classes (i.e. /** */). Adding comments are usual good idea to improve your code's readability.", "url": "https://github.com/apache/beam/pull/12232#discussion_r453989389", "createdAt": "2020-07-13T22:44:44Z", "author": {"login": "amaliujia"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rule/BeamMatchRule.java", "diffHunk": "@@ -0,0 +1,56 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rule;\n+\n+import org.apache.beam.sdk.extensions.sql.impl.rel.BeamLogicalConvention;\n+import org.apache.beam.sdk.extensions.sql.impl.rel.BeamMatchRel;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.Convention;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.convert.ConverterRule;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.logical.LogicalMatch;\n+\n+public class BeamMatchRule extends ConverterRule {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 28}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzMTQ3NTI1OnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QyMjo0OTo0MlrOGw9a2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQwOTozMjowMFrOGx2GtA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5MTEzMQ==", "bodyText": "Nit: upstreamSchema might be a better variable name.", "url": "https://github.com/apache/beam/pull/12232#discussion_r453991131", "createdAt": "2020-07-13T22:49:42Z", "author": {"login": "amaliujia"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.vendor.calcite.v1_20_0.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPPattern;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.OrderKey;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.BeamCostModel;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.NodeStats;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptCluster;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** {@link BeamRelNode} to replace a {@link Match} node. */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n+\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+\n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NDkxOTg2MA==", "bodyText": "Done.", "url": "https://github.com/apache/beam/pull/12232#discussion_r454919860", "createdAt": "2020-07-15T09:32:00Z", "author": {"login": "Mark-Zeng"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.vendor.calcite.v1_20_0.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPPattern;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.OrderKey;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.BeamCostModel;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.NodeStats;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptCluster;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** {@link BeamRelNode} to replace a {@link Match} node. */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n+\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+\n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5MTEzMQ=="}, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 152}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzMTQ4Mjc1OnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QyMjo1MzowNVrOGw9fQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMlQxNDowMDozM1rOG1j0KQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5MjI1Nw==", "bodyText": "Ah so is collectionSchema's field name the same as varNode's name (including that $)?\nSee Schema.getName API: https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/schemas/Schema.java#L1270", "url": "https://github.com/apache/beam/pull/12232#discussion_r453992257", "createdAt": "2020-07-13T22:53:05Z", "author": {"login": "amaliujia"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.vendor.calcite.v1_20_0.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPPattern;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.OrderKey;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.BeamCostModel;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.NodeStats;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptCluster;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** {@link BeamRelNode} to replace a {@link Match} node. */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n+\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+\n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE5NzgzNg==", "bodyText": "The varNode is an instance of RexVariable. I wanted to get the index or column name from it; the getName method returns a string like '$9' which is the field index preceded by a dollar sign. This is a very awkward way of extracting the information, but I could not think of a better one.", "url": "https://github.com/apache/beam/pull/12232#discussion_r455197836", "createdAt": "2020-07-15T17:01:15Z", "author": {"login": "Mark-Zeng"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.vendor.calcite.v1_20_0.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPPattern;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.OrderKey;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.BeamCostModel;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.NodeStats;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptCluster;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** {@link BeamRelNode} to replace a {@link Match} node. */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n+\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+\n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5MjI1Nw=="}, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 158}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1ODgxNDUwNQ==", "bodyText": "I just spotted that I used the wrong class. The subclass RexInputRef of RexVariable has the information of both the column index and field (column) name. This part will change in my later commits.", "url": "https://github.com/apache/beam/pull/12232#discussion_r458814505", "createdAt": "2020-07-22T14:00:33Z", "author": {"login": "Mark-Zeng"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.vendor.calcite.v1_20_0.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPPattern;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.OrderKey;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.BeamCostModel;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.NodeStats;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptCluster;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** {@link BeamRelNode} to replace a {@link Match} node. */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n+\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+\n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5MjI1Nw=="}, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 158}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzMTQ4MzkwOnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QyMjo1MzozNlrOGw9f6A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxNzowMzowM1rOGyHI1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5MjQyNA==", "bodyText": "Nit: name it PartitionKeySchema might be more readable.", "url": "https://github.com/apache/beam/pull/12232#discussion_r453992424", "createdAt": "2020-07-13T22:53:36Z", "author": {"login": "amaliujia"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.vendor.calcite.v1_20_0.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPPattern;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.OrderKey;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.BeamCostModel;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.NodeStats;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptCluster;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** {@link BeamRelNode} to replace a {@link Match} node. */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n+\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+\n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));\n+      }\n+      Schema mySchema = schemaBuilder.build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTE5ODkzMg==", "bodyText": "Agree. Still need practice on naming variables : )", "url": "https://github.com/apache/beam/pull/12232#discussion_r455198932", "createdAt": "2020-07-15T17:03:03Z", "author": {"login": "Mark-Zeng"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.vendor.calcite.v1_20_0.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPPattern;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.OrderKey;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.BeamCostModel;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.NodeStats;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptCluster;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** {@link BeamRelNode} to replace a {@link Match} node. */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n+\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+\n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));\n+      }\n+      Schema mySchema = schemaBuilder.build();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5MjQyNA=="}, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 160}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzMTQ5MjU3OnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QyMjo1NzozNlrOGw9lRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxNzowNzozNlrOGyHTGA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5Mzc5OQ==", "bodyText": "In fact, there is also a NullDirection to consider (Null first/Null last): https://github.com/apache/calcite/blob/master/core/src/main/java/org/apache/calcite/rel/RelFieldCollation.java#L185\nIt is ok to not handle it for now, but please leave a TODO comment (i.e. // TODO: handle NullDirection)", "url": "https://github.com/apache/beam/pull/12232#discussion_r453993799", "createdAt": "2020-07-13T22:57:36Z", "author": {"login": "amaliujia"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.vendor.calcite.v1_20_0.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPPattern;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.OrderKey;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.BeamCostModel;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.NodeStats;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptCluster;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** {@link BeamRelNode} to replace a {@link Match} node. */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n+\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+\n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));\n+      }\n+      Schema mySchema = schemaBuilder.build();\n+\n+      // partition according to the partition keys\n+      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n+\n+      // group by keys\n+      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n+          keyedUpstream\n+              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n+              .apply(GroupByKey.create());\n+\n+      // sort within each keyed partition\n+      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n+          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n+\n+      // apply the pattern match in each partition\n+      ArrayList<CEPPattern> cepPattern =\n+          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n+      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n+      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n+          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n+\n+      // apply the ParDo for the measures clause\n+      // for now, output the all rows of each pattern matched (for testing purpose)\n+      PCollection<Row> outStream =\n+          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+\n+      return outStream;\n+    }\n+\n+    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n+\n+      @ProcessElement\n+      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n+        for (Row i : keyRows.getValue()) {\n+          out.output(i);\n+        }\n+      }\n+    }\n+\n+    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n+    // support only one row per match for now.\n+    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final ArrayList<CEPPattern> pattern;\n+      private final String regexPattern;\n+\n+      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n+        this.pattern = pattern;\n+        this.regexPattern = regexPattern;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<>();\n+        StringBuilder patternString = new StringBuilder();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+          // check pattern of row i\n+          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n+          for (int j = 0; j < pattern.size(); ++j) {\n+            CEPPattern tryPattern = pattern.get(j);\n+            if (tryPattern.evalRow(i)) {\n+              patternOfRow = tryPattern.toString();\n+            }\n+          }\n+          patternString.append(patternOfRow);\n+        }\n+\n+        Pattern p = Pattern.compile(regexPattern);\n+        Matcher m = p.matcher(patternString.toString());\n+        // if the pattern is (A B+ C),\n+        // it should return a List three rows matching A B C respectively\n+        if (m.matches()) {\n+          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n+        }\n+      }\n+    }\n+\n+    private static class SortPerKey extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final Schema cSchema;\n+      private final ArrayList<OrderKey> orderKeys;\n+\n+      public SortPerKey(Schema cSchema, RelCollation orderKeys) {\n+        this.cSchema = cSchema;\n+\n+        List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n+        Collections.reverse(revOrderKeys);\n+        ArrayList<OrderKey> revOrderKeysList = new ArrayList<>();\n+        for (RelFieldCollation i : revOrderKeys) {\n+          int fIndex = i.getFieldIndex();\n+          RelFieldCollation.Direction dir = i.getDirection();\n+          if (dir == RelFieldCollation.Direction.ASCENDING) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 254}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTIwMTU2MA==", "bodyText": "I have just added the implementation for it.", "url": "https://github.com/apache/beam/pull/12232#discussion_r455201560", "createdAt": "2020-07-15T17:07:36Z", "author": {"login": "Mark-Zeng"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.vendor.calcite.v1_20_0.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPPattern;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.OrderKey;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.BeamCostModel;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.NodeStats;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptCluster;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** {@link BeamRelNode} to replace a {@link Match} node. */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n+\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+\n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));\n+      }\n+      Schema mySchema = schemaBuilder.build();\n+\n+      // partition according to the partition keys\n+      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n+\n+      // group by keys\n+      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n+          keyedUpstream\n+              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n+              .apply(GroupByKey.create());\n+\n+      // sort within each keyed partition\n+      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n+          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n+\n+      // apply the pattern match in each partition\n+      ArrayList<CEPPattern> cepPattern =\n+          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n+      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n+      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n+          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n+\n+      // apply the ParDo for the measures clause\n+      // for now, output the all rows of each pattern matched (for testing purpose)\n+      PCollection<Row> outStream =\n+          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+\n+      return outStream;\n+    }\n+\n+    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n+\n+      @ProcessElement\n+      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n+        for (Row i : keyRows.getValue()) {\n+          out.output(i);\n+        }\n+      }\n+    }\n+\n+    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n+    // support only one row per match for now.\n+    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final ArrayList<CEPPattern> pattern;\n+      private final String regexPattern;\n+\n+      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n+        this.pattern = pattern;\n+        this.regexPattern = regexPattern;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<>();\n+        StringBuilder patternString = new StringBuilder();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+          // check pattern of row i\n+          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n+          for (int j = 0; j < pattern.size(); ++j) {\n+            CEPPattern tryPattern = pattern.get(j);\n+            if (tryPattern.evalRow(i)) {\n+              patternOfRow = tryPattern.toString();\n+            }\n+          }\n+          patternString.append(patternOfRow);\n+        }\n+\n+        Pattern p = Pattern.compile(regexPattern);\n+        Matcher m = p.matcher(patternString.toString());\n+        // if the pattern is (A B+ C),\n+        // it should return a List three rows matching A B C respectively\n+        if (m.matches()) {\n+          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n+        }\n+      }\n+    }\n+\n+    private static class SortPerKey extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final Schema cSchema;\n+      private final ArrayList<OrderKey> orderKeys;\n+\n+      public SortPerKey(Schema cSchema, RelCollation orderKeys) {\n+        this.cSchema = cSchema;\n+\n+        List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n+        Collections.reverse(revOrderKeys);\n+        ArrayList<OrderKey> revOrderKeysList = new ArrayList<>();\n+        for (RelFieldCollation i : revOrderKeys) {\n+          int fIndex = i.getFieldIndex();\n+          RelFieldCollation.Direction dir = i.getDirection();\n+          if (dir == RelFieldCollation.Direction.ASCENDING) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5Mzc5OQ=="}, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 254}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzMTQ5ODYzOnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QyMzowMDoxNFrOGw9o2g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxNzoxMjo0NVrOGyHftw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5NDcxNA==", "bodyText": "I think you got to make Pattern p as a variable to compile once?", "url": "https://github.com/apache/beam/pull/12232#discussion_r453994714", "createdAt": "2020-07-13T23:00:14Z", "author": {"login": "amaliujia"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.vendor.calcite.v1_20_0.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPPattern;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.OrderKey;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.BeamCostModel;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.NodeStats;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptCluster;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** {@link BeamRelNode} to replace a {@link Match} node. */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n+\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+\n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));\n+      }\n+      Schema mySchema = schemaBuilder.build();\n+\n+      // partition according to the partition keys\n+      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n+\n+      // group by keys\n+      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n+          keyedUpstream\n+              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n+              .apply(GroupByKey.create());\n+\n+      // sort within each keyed partition\n+      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n+          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n+\n+      // apply the pattern match in each partition\n+      ArrayList<CEPPattern> cepPattern =\n+          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n+      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n+      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n+          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n+\n+      // apply the ParDo for the measures clause\n+      // for now, output the all rows of each pattern matched (for testing purpose)\n+      PCollection<Row> outStream =\n+          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+\n+      return outStream;\n+    }\n+\n+    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n+\n+      @ProcessElement\n+      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n+        for (Row i : keyRows.getValue()) {\n+          out.output(i);\n+        }\n+      }\n+    }\n+\n+    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n+    // support only one row per match for now.\n+    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final ArrayList<CEPPattern> pattern;\n+      private final String regexPattern;\n+\n+      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n+        this.pattern = pattern;\n+        this.regexPattern = regexPattern;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<>();\n+        StringBuilder patternString = new StringBuilder();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+          // check pattern of row i\n+          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n+          for (int j = 0; j < pattern.size(); ++j) {\n+            CEPPattern tryPattern = pattern.get(j);\n+            if (tryPattern.evalRow(i)) {\n+              patternOfRow = tryPattern.toString();\n+            }\n+          }\n+          patternString.append(patternOfRow);\n+        }\n+\n+        Pattern p = Pattern.compile(regexPattern);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 230}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTIwNDc5MQ==", "bodyText": "I am very new to the regex library of Java. This my first time of using it. I just followed the Oracle doc on regex and the example in it.", "url": "https://github.com/apache/beam/pull/12232#discussion_r455204791", "createdAt": "2020-07-15T17:12:45Z", "author": {"login": "Mark-Zeng"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.vendor.calcite.v1_20_0.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPPattern;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.OrderKey;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.BeamCostModel;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.NodeStats;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptCluster;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** {@link BeamRelNode} to replace a {@link Match} node. */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n+\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+\n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));\n+      }\n+      Schema mySchema = schemaBuilder.build();\n+\n+      // partition according to the partition keys\n+      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n+\n+      // group by keys\n+      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n+          keyedUpstream\n+              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n+              .apply(GroupByKey.create());\n+\n+      // sort within each keyed partition\n+      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n+          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));\n+\n+      // apply the pattern match in each partition\n+      ArrayList<CEPPattern> cepPattern =\n+          CEPUtil.getCEPPatternFromPattern(collectionSchema, (RexCall) pattern, patternDefs);\n+      String regexPattern = CEPUtil.getRegexFromPattern((RexCall) pattern);\n+      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n+          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n+\n+      // apply the ParDo for the measures clause\n+      // for now, output the all rows of each pattern matched (for testing purpose)\n+      PCollection<Row> outStream =\n+          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(collectionSchema);\n+\n+      return outStream;\n+    }\n+\n+    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n+\n+      @ProcessElement\n+      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n+        for (Row i : keyRows.getValue()) {\n+          out.output(i);\n+        }\n+      }\n+    }\n+\n+    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n+    // support only one row per match for now.\n+    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final ArrayList<CEPPattern> pattern;\n+      private final String regexPattern;\n+\n+      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n+        this.pattern = pattern;\n+        this.regexPattern = regexPattern;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<>();\n+        StringBuilder patternString = new StringBuilder();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+          // check pattern of row i\n+          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n+          for (int j = 0; j < pattern.size(); ++j) {\n+            CEPPattern tryPattern = pattern.get(j);\n+            if (tryPattern.evalRow(i)) {\n+              patternOfRow = tryPattern.toString();\n+            }\n+          }\n+          patternString.append(patternOfRow);\n+        }\n+\n+        Pattern p = Pattern.compile(regexPattern);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5NDcxNA=="}, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 230}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzMTUwMzQzOnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPTypeName.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QyMzowMjoyOFrOGw9rtg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xNVQxNzozMToyOVrOGyIVWg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5NTQ0Ng==", "bodyText": "Is it possible to reuse https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/schemas/Schema.java#L413?", "url": "https://github.com/apache/beam/pull/12232#discussion_r453995446", "createdAt": "2020-07-13T23:02:28Z", "author": {"login": "amaliujia"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPTypeName.java", "diffHunk": "@@ -0,0 +1,33 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.cep;\n+\n+import java.io.Serializable;\n+\n+public enum CEPTypeName implements Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5NjI0OQ==", "bodyText": "It is ok though if you still want to use a separate enum for CEP types, since it is a standalone library.\nJust curious, is there a type that is not covered by https://github.com/apache/beam/blob/master/sdks/java/core/src/main/java/org/apache/beam/sdk/schemas/Schema.java#L413?", "url": "https://github.com/apache/beam/pull/12232#discussion_r453996249", "createdAt": "2020-07-13T23:04:51Z", "author": {"login": "amaliujia"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPTypeName.java", "diffHunk": "@@ -0,0 +1,33 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.cep;\n+\n+import java.io.Serializable;\n+\n+public enum CEPTypeName implements Serializable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5NTQ0Ng=="}, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTIxMzk0OQ==", "bodyText": "I think it is possible; reusing the schema types seems natural also. I will update it in my next commit.", "url": "https://github.com/apache/beam/pull/12232#discussion_r455213949", "createdAt": "2020-07-15T17:26:51Z", "author": {"login": "Mark-Zeng"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPTypeName.java", "diffHunk": "@@ -0,0 +1,33 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.cep;\n+\n+import java.io.Serializable;\n+\n+public enum CEPTypeName implements Serializable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5NTQ0Ng=="}, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1NTIxODUyMg==", "bodyText": "There are some differences. RexLiteral class seems to have a wider support for time interval. And I just found in the page that the value of the Double type in a literal node is BigDecimal? weird.", "url": "https://github.com/apache/beam/pull/12232#discussion_r455218522", "createdAt": "2020-07-15T17:31:29Z", "author": {"login": "Mark-Zeng"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPTypeName.java", "diffHunk": "@@ -0,0 +1,33 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.cep;\n+\n+import java.io.Serializable;\n+\n+public enum CEPTypeName implements Serializable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5NTQ0Ng=="}, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 22}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjgzMTUxMjY5OnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QyMzowNjozMFrOGw9xAw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0xM1QyMzowNjozMFrOGw9xAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Mzk5NjgwMw==", "bodyText": "This will rely on an assumption that Fusion will fuse operators here so the sorted result will be preserved for the next match transform. In most of the runners (if not all) this should be true.", "url": "https://github.com/apache/beam/pull/12232#discussion_r453996803", "createdAt": "2020-07-13T23:06:30Z", "author": {"login": "amaliujia"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "diffHunk": "@@ -0,0 +1,380 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.vendor.calcite.v1_20_0.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.Comparator;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPPattern;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.OrderKey;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.BeamCostModel;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.NodeStats;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptCluster;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** {@link BeamRelNode} to replace a {@link Match} node. */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n+\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+\n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema collectionSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(collectionSchema.getField(index));\n+      }\n+      Schema mySchema = schemaBuilder.build();\n+\n+      // partition according to the partition keys\n+      PCollection<KV<Row, Row>> keyedUpstream = upstream.apply(ParDo.of(new MapKeys(mySchema)));\n+\n+      // group by keys\n+      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n+          keyedUpstream\n+              .setCoder(KvCoder.of(RowCoder.of(mySchema), RowCoder.of(collectionSchema)))\n+              .apply(GroupByKey.create());\n+\n+      // sort within each keyed partition\n+      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n+          groupedUpstream.apply(ParDo.of(new SortPerKey(collectionSchema, orderKeys)));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "1727e170ef88ed8150a7fd30f6f9254ef1031548"}, "originalPosition": 173}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg1NzE5NDA3OnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNjozNDoxOVrOG0qB5g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yMVQwNjozNDoxOVrOG0qB5g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1Nzg2Nzc1MA==", "bodyText": "I realized this should be a while loop. And for regex implementation, the default after match strategy is \"skip past last row\". I will change it to a while loop in my next commit.", "url": "https://github.com/apache/beam/pull/12232#discussion_r457867750", "createdAt": "2020-07-21T06:34:19Z", "author": {"login": "Mark-Zeng"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRel.java", "diffHunk": "@@ -0,0 +1,338 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil.makeOrderKeysFromCollation;\n+import static org.apache.beam.vendor.calcite.v1_20_0.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.SortedSet;\n+import java.util.regex.Matcher;\n+import java.util.regex.Pattern;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.coders.RowCoder;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPPattern;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.CEPUtil;\n+import org.apache.beam.sdk.extensions.sql.impl.cep.OrderKey;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.BeamCostModel;\n+import org.apache.beam.sdk.extensions.sql.impl.planner.NodeStats;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.GroupByKey;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.PCollectionList;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptCluster;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelOptPlanner;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.plan.RelTraitSet;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.core.Match;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.metadata.RelMetadataQuery;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.type.RelDataType;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexVariable;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * {@code BeamRelNode} to replace a {@code Match} node.\n+ *\n+ * <p>The {@code BeamMatchRel} is the Beam implementation of {@code MATCH_RECOGNIZE} in SQL.\n+ *\n+ * <p>For now, the underline implementation is based on java.util.regex.\n+ */\n+public class BeamMatchRel extends Match implements BeamRelNode {\n+\n+  public static final Logger LOG = LoggerFactory.getLogger(BeamMatchRel.class);\n+\n+  public BeamMatchRel(\n+      RelOptCluster cluster,\n+      RelTraitSet traitSet,\n+      RelNode input,\n+      RelDataType rowType,\n+      RexNode pattern,\n+      boolean strictStart,\n+      boolean strictEnd,\n+      Map<String, RexNode> patternDefinitions,\n+      Map<String, RexNode> measures,\n+      RexNode after,\n+      Map<String, ? extends SortedSet<String>> subsets,\n+      boolean allRows,\n+      List<RexNode> partitionKeys,\n+      RelCollation orderKeys,\n+      RexNode interval) {\n+\n+    super(\n+        cluster,\n+        traitSet,\n+        input,\n+        rowType,\n+        pattern,\n+        strictStart,\n+        strictEnd,\n+        patternDefinitions,\n+        measures,\n+        after,\n+        subsets,\n+        allRows,\n+        partitionKeys,\n+        orderKeys,\n+        interval);\n+  }\n+\n+  @Override\n+  public BeamCostModel beamComputeSelfCost(RelOptPlanner planner, RelMetadataQuery mq) {\n+    return BeamCostModel.FACTORY.makeTinyCost(); // return constant costModel for now\n+  }\n+\n+  @Override\n+  public NodeStats estimateNodeStats(RelMetadataQuery mq) {\n+    // a simple way of getting some estimate data\n+    // to be examined further\n+    NodeStats inputEstimate = BeamSqlRelUtils.getNodeStats(input, mq);\n+    double numRows = inputEstimate.getRowCount();\n+    double winSize = inputEstimate.getWindow();\n+    double rate = inputEstimate.getRate();\n+\n+    return NodeStats.create(numRows, rate, winSize).multiply(0.5);\n+  }\n+\n+  @Override\n+  public PTransform<PCollectionList<Row>, PCollection<Row>> buildPTransform() {\n+\n+    return new MatchTransform(partitionKeys, orderKeys, pattern, patternDefinitions);\n+  }\n+\n+  private static class MatchTransform extends PTransform<PCollectionList<Row>, PCollection<Row>> {\n+\n+    private final List<RexNode> parKeys;\n+    private final RelCollation orderKeys;\n+    private final RexNode pattern;\n+    private final Map<String, RexNode> patternDefs;\n+\n+    public MatchTransform(\n+        List<RexNode> parKeys,\n+        RelCollation orderKeys,\n+        RexNode pattern,\n+        Map<String, RexNode> patternDefs) {\n+      this.parKeys = parKeys;\n+      this.orderKeys = orderKeys;\n+      this.pattern = pattern;\n+      this.patternDefs = patternDefs;\n+    }\n+\n+    @Override\n+    public PCollection<Row> expand(PCollectionList<Row> pinput) {\n+      checkArgument(\n+          pinput.size() == 1,\n+          \"Wrong number of inputs for %s: %s\",\n+          BeamMatchRel.class.getSimpleName(),\n+          pinput);\n+      PCollection<Row> upstream = pinput.get(0);\n+\n+      Schema upstreamSchema = upstream.getSchema();\n+\n+      Schema.Builder schemaBuilder = new Schema.Builder();\n+      for (RexNode i : parKeys) {\n+        RexVariable varNode = (RexVariable) i;\n+        int index = Integer.parseInt(varNode.getName().substring(1)); // get rid of `$`\n+        schemaBuilder.addField(upstreamSchema.getField(index));\n+      }\n+      Schema partitionKeySchema = schemaBuilder.build();\n+\n+      // partition according to the partition keys\n+      PCollection<KV<Row, Row>> keyedUpstream =\n+          upstream.apply(ParDo.of(new MapKeys(partitionKeySchema)));\n+\n+      // group by keys\n+      PCollection<KV<Row, Iterable<Row>>> groupedUpstream =\n+          keyedUpstream\n+              .setCoder(KvCoder.of(RowCoder.of(partitionKeySchema), RowCoder.of(upstreamSchema)))\n+              .apply(GroupByKey.create());\n+\n+      // sort within each keyed partition\n+      ArrayList<OrderKey> orderKeyList = makeOrderKeysFromCollation(orderKeys);\n+      // This will rely on an assumption that Fusion will fuse\n+      // operators here so the sorted result will be preserved\n+      // for the next match transform.\n+      // In most of the runners (if not all) this should be true.\n+      PCollection<KV<Row, Iterable<Row>>> orderedUpstream =\n+          groupedUpstream.apply(ParDo.of(new SortPerKey(upstreamSchema, orderKeyList)));\n+\n+      // apply the pattern match in each partition\n+      ArrayList<CEPPattern> cepPattern =\n+          CEPUtil.getCEPPatternFromPattern(upstreamSchema, pattern, patternDefs);\n+      String regexPattern = CEPUtil.getRegexFromPattern(pattern);\n+      PCollection<KV<Row, Iterable<Row>>> matchedUpstream =\n+          orderedUpstream.apply(ParDo.of(new MatchPattern(cepPattern, regexPattern)));\n+\n+      // apply the ParDo for the measures clause\n+      // for now, output all rows of each pattern matched (for testing purpose)\n+      // TODO: add ONE ROW PER MATCH and MEASURES implementation.\n+      PCollection<Row> outStream =\n+          matchedUpstream.apply(ParDo.of(new Measure())).setRowSchema(upstreamSchema);\n+\n+      return outStream;\n+    }\n+\n+    private static class Measure extends DoFn<KV<Row, Iterable<Row>>, Row> {\n+\n+      @ProcessElement\n+      public void processElement(@Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<Row> out) {\n+        for (Row i : keyRows.getValue()) {\n+          out.output(i);\n+        }\n+      }\n+    }\n+\n+    // TODO: support both ALL ROWS PER MATCH and ONE ROW PER MATCH.\n+    // support only one row per match for now.\n+    private static class MatchPattern extends DoFn<KV<Row, Iterable<Row>>, KV<Row, Iterable<Row>>> {\n+\n+      private final ArrayList<CEPPattern> pattern;\n+      private final String regexPattern;\n+\n+      MatchPattern(ArrayList<CEPPattern> pattern, String regexPattern) {\n+        this.pattern = pattern;\n+        this.regexPattern = regexPattern;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(\n+          @Element KV<Row, Iterable<Row>> keyRows, OutputReceiver<KV<Row, Iterable<Row>>> out) {\n+        ArrayList<Row> rows = new ArrayList<>();\n+        StringBuilder patternString = new StringBuilder();\n+        for (Row i : keyRows.getValue()) {\n+          rows.add(i);\n+          // check pattern of row i\n+          String patternOfRow = \" \"; // a row with no matched pattern is marked by a space\n+          for (int j = 0; j < pattern.size(); ++j) {\n+            CEPPattern tryPattern = pattern.get(j);\n+            if (tryPattern.evalRow(i)) {\n+              patternOfRow = tryPattern.getPatternVar();\n+            }\n+          }\n+          patternString.append(patternOfRow);\n+        }\n+\n+        Pattern p = Pattern.compile(regexPattern);\n+        Matcher m = p.matcher(patternString.toString());\n+        // if the pattern is (A B+ C),\n+        // it should return a List three rows matching A B C respectively\n+        if (m.matches()) {\n+          out.output(KV.of(keyRows.getKey(), rows.subList(m.start(), m.end())));\n+        }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e4652aedd6a3c1a9914d0ffe28b5f46bbb2bae38"}, "originalPosition": 245}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3MzEwNjYxOnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPCall.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwMDo0MjozNVrOG3BBzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMDo0Nzo1M1rOG605cA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM0MTcxMQ==", "bodyText": "Is it correct to not handle other classes?\nIf so can you add an exception in the last else?", "url": "https://github.com/apache/beam/pull/12232#discussion_r460341711", "createdAt": "2020-07-25T00:42:35Z", "author": {"login": "amaliujia"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPCall.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.cep;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexLiteral;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexPatternFieldRef;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.sql.SqlOperator;\n+\n+/**\n+ * A {@code CEPCall} instance represents an operation (node) that contains an operator and a list of\n+ * operands. It has the similar functionality as Calcite's {@code RexCall}.\n+ */\n+public class CEPCall extends CEPOperation {\n+\n+  private final CEPOperator operator;\n+  private final List<CEPOperation> operands;\n+\n+  private CEPCall(CEPOperator operator, List<CEPOperation> operands) {\n+    this.operator = operator;\n+    this.operands = operands;\n+  }\n+\n+  public CEPOperator getOperator() {\n+    return operator;\n+  }\n+\n+  public List<CEPOperation> getOperands() {\n+    return operands;\n+  }\n+\n+  public static CEPCall of(RexCall operation) {\n+    SqlOperator call = operation.getOperator();\n+    CEPOperator myOp = CEPOperator.of(call);\n+\n+    ArrayList<CEPOperation> operandsList = new ArrayList<>();\n+    for (RexNode i : operation.getOperands()) {\n+      if (i.getClass() == RexCall.class) {\n+        CEPCall callToAdd = CEPCall.of((RexCall) i);\n+        operandsList.add(callToAdd);\n+      } else if (i.getClass() == RexLiteral.class) {\n+        RexLiteral lit = (RexLiteral) i;\n+        CEPLiteral litToAdd = CEPLiteral.of(lit);\n+        operandsList.add(litToAdd);\n+      } else if (i.getClass() == RexPatternFieldRef.class) {\n+        RexPatternFieldRef fieldRef = (RexPatternFieldRef) i;\n+        CEPFieldRef fieldRefToAdd = CEPFieldRef.of(fieldRef);\n+        operandsList.add(fieldRefToAdd);\n+      }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e4652aedd6a3c1a9914d0ffe28b5f46bbb2bae38"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk2NjU1Mg==", "bodyText": "will do.", "url": "https://github.com/apache/beam/pull/12232#discussion_r463966552", "createdAt": "2020-08-01T14:21:14Z", "author": {"login": "Mark-Zeng"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPCall.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.cep;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexLiteral;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexPatternFieldRef;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.sql.SqlOperator;\n+\n+/**\n+ * A {@code CEPCall} instance represents an operation (node) that contains an operator and a list of\n+ * operands. It has the similar functionality as Calcite's {@code RexCall}.\n+ */\n+public class CEPCall extends CEPOperation {\n+\n+  private final CEPOperator operator;\n+  private final List<CEPOperation> operands;\n+\n+  private CEPCall(CEPOperator operator, List<CEPOperation> operands) {\n+    this.operator = operator;\n+    this.operands = operands;\n+  }\n+\n+  public CEPOperator getOperator() {\n+    return operator;\n+  }\n+\n+  public List<CEPOperation> getOperands() {\n+    return operands;\n+  }\n+\n+  public static CEPCall of(RexCall operation) {\n+    SqlOperator call = operation.getOperator();\n+    CEPOperator myOp = CEPOperator.of(call);\n+\n+    ArrayList<CEPOperation> operandsList = new ArrayList<>();\n+    for (RexNode i : operation.getOperands()) {\n+      if (i.getClass() == RexCall.class) {\n+        CEPCall callToAdd = CEPCall.of((RexCall) i);\n+        operandsList.add(callToAdd);\n+      } else if (i.getClass() == RexLiteral.class) {\n+        RexLiteral lit = (RexLiteral) i;\n+        CEPLiteral litToAdd = CEPLiteral.of(lit);\n+        operandsList.add(litToAdd);\n+      } else if (i.getClass() == RexPatternFieldRef.class) {\n+        RexPatternFieldRef fieldRef = (RexPatternFieldRef) i;\n+        CEPFieldRef fieldRefToAdd = CEPFieldRef.of(fieldRef);\n+        operandsList.add(fieldRefToAdd);\n+      }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM0MTcxMQ=="}, "originalCommit": {"oid": "e4652aedd6a3c1a9914d0ffe28b5f46bbb2bae38"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDExNjY3NA==", "bodyText": "Please print the RexNode so people can see which RexNode is not supported.\n \"the RexNode is not recognized: \" + i", "url": "https://github.com/apache/beam/pull/12232#discussion_r464116674", "createdAt": "2020-08-02T19:50:27Z", "author": {"login": "amaliujia"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPCall.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.cep;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexLiteral;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexPatternFieldRef;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.sql.SqlOperator;\n+\n+/**\n+ * A {@code CEPCall} instance represents an operation (node) that contains an operator and a list of\n+ * operands. It has the similar functionality as Calcite's {@code RexCall}.\n+ */\n+public class CEPCall extends CEPOperation {\n+\n+  private final CEPOperator operator;\n+  private final List<CEPOperation> operands;\n+\n+  private CEPCall(CEPOperator operator, List<CEPOperation> operands) {\n+    this.operator = operator;\n+    this.operands = operands;\n+  }\n+\n+  public CEPOperator getOperator() {\n+    return operator;\n+  }\n+\n+  public List<CEPOperation> getOperands() {\n+    return operands;\n+  }\n+\n+  public static CEPCall of(RexCall operation) {\n+    SqlOperator call = operation.getOperator();\n+    CEPOperator myOp = CEPOperator.of(call);\n+\n+    ArrayList<CEPOperation> operandsList = new ArrayList<>();\n+    for (RexNode i : operation.getOperands()) {\n+      if (i.getClass() == RexCall.class) {\n+        CEPCall callToAdd = CEPCall.of((RexCall) i);\n+        operandsList.add(callToAdd);\n+      } else if (i.getClass() == RexLiteral.class) {\n+        RexLiteral lit = (RexLiteral) i;\n+        CEPLiteral litToAdd = CEPLiteral.of(lit);\n+        operandsList.add(litToAdd);\n+      } else if (i.getClass() == RexPatternFieldRef.class) {\n+        RexPatternFieldRef fieldRef = (RexPatternFieldRef) i;\n+        CEPFieldRef fieldRefToAdd = CEPFieldRef.of(fieldRef);\n+        operandsList.add(fieldRefToAdd);\n+      }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM0MTcxMQ=="}, "originalCommit": {"oid": "e4652aedd6a3c1a9914d0ffe28b5f46bbb2bae38"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzNzI2NA==", "bodyText": "Ok", "url": "https://github.com/apache/beam/pull/12232#discussion_r464337264", "createdAt": "2020-08-03T10:47:53Z", "author": {"login": "Mark-Zeng"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPCall.java", "diffHunk": "@@ -0,0 +1,72 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.cep;\n+\n+import java.util.ArrayList;\n+import java.util.List;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexLiteral;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexPatternFieldRef;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.sql.SqlOperator;\n+\n+/**\n+ * A {@code CEPCall} instance represents an operation (node) that contains an operator and a list of\n+ * operands. It has the similar functionality as Calcite's {@code RexCall}.\n+ */\n+public class CEPCall extends CEPOperation {\n+\n+  private final CEPOperator operator;\n+  private final List<CEPOperation> operands;\n+\n+  private CEPCall(CEPOperator operator, List<CEPOperation> operands) {\n+    this.operator = operator;\n+    this.operands = operands;\n+  }\n+\n+  public CEPOperator getOperator() {\n+    return operator;\n+  }\n+\n+  public List<CEPOperation> getOperands() {\n+    return operands;\n+  }\n+\n+  public static CEPCall of(RexCall operation) {\n+    SqlOperator call = operation.getOperator();\n+    CEPOperator myOp = CEPOperator.of(call);\n+\n+    ArrayList<CEPOperation> operandsList = new ArrayList<>();\n+    for (RexNode i : operation.getOperands()) {\n+      if (i.getClass() == RexCall.class) {\n+        CEPCall callToAdd = CEPCall.of((RexCall) i);\n+        operandsList.add(callToAdd);\n+      } else if (i.getClass() == RexLiteral.class) {\n+        RexLiteral lit = (RexLiteral) i;\n+        CEPLiteral litToAdd = CEPLiteral.of(lit);\n+        operandsList.add(litToAdd);\n+      } else if (i.getClass() == RexPatternFieldRef.class) {\n+        RexPatternFieldRef fieldRef = (RexPatternFieldRef) i;\n+        CEPFieldRef fieldRefToAdd = CEPFieldRef.of(fieldRef);\n+        operandsList.add(fieldRefToAdd);\n+      }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM0MTcxMQ=="}, "originalCommit": {"oid": "e4652aedd6a3c1a9914d0ffe28b5f46bbb2bae38"}, "originalPosition": 67}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3MzEwNzc0OnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPLiteral.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwMDo0Mzo0NVrOG3BCbA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wMVQxNDoyMToxOVrOG6eRWw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM0MTg2OA==", "bodyText": "nit: no need add literal here.", "url": "https://github.com/apache/beam/pull/12232#discussion_r460341868", "createdAt": "2020-07-25T00:43:45Z", "author": {"login": "amaliujia"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPLiteral.java", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.cep;\n+\n+import java.math.BigDecimal;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexLiteral;\n+import org.joda.time.ReadableDateTime;\n+\n+/**\n+ * {@code CEPLiteral} represents a literal node. It corresponds to {@code RexLiteral} in Calcite.\n+ */\n+public class CEPLiteral extends CEPOperation {\n+\n+  private final Schema.TypeName typeName;\n+\n+  private CEPLiteral(Schema.TypeName typeName) {\n+    this.typeName = typeName;\n+  }\n+\n+  // TODO: deal with other types (byte, short...)\n+  public static CEPLiteral of(RexLiteral lit) {\n+    switch (lit.getTypeName()) {\n+      case INTEGER:\n+        return of(lit.getValueAs(Integer.class));\n+      case BIGINT:\n+        return of(lit.getValueAs(Long.class));\n+      case DECIMAL:\n+        return of(lit.getValueAs(BigDecimal.class));\n+      case FLOAT:\n+        return of(lit.getValueAs(Float.class));\n+      case DOUBLE:\n+        return of(lit.getValueAs(Double.class));\n+      case BOOLEAN:\n+        return of(lit.getValueAs(Boolean.class));\n+      case DATE:\n+        return of(lit.getValueAs(ReadableDateTime.class));\n+      case CHAR:\n+      case VARCHAR:\n+        return of(lit.getValueAs(String.class));\n+      default:\n+        throw new SqlConversionException(\n+            \"sql literal type not supported: \" + lit.getTypeName().toString());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e4652aedd6a3c1a9914d0ffe28b5f46bbb2bae38"}, "originalPosition": 59}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk2NjU1NQ==", "bodyText": "ok.", "url": "https://github.com/apache/beam/pull/12232#discussion_r463966555", "createdAt": "2020-08-01T14:21:19Z", "author": {"login": "Mark-Zeng"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPLiteral.java", "diffHunk": "@@ -0,0 +1,196 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.cep;\n+\n+import java.math.BigDecimal;\n+import org.apache.beam.sdk.extensions.sql.impl.SqlConversionException;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexLiteral;\n+import org.joda.time.ReadableDateTime;\n+\n+/**\n+ * {@code CEPLiteral} represents a literal node. It corresponds to {@code RexLiteral} in Calcite.\n+ */\n+public class CEPLiteral extends CEPOperation {\n+\n+  private final Schema.TypeName typeName;\n+\n+  private CEPLiteral(Schema.TypeName typeName) {\n+    this.typeName = typeName;\n+  }\n+\n+  // TODO: deal with other types (byte, short...)\n+  public static CEPLiteral of(RexLiteral lit) {\n+    switch (lit.getTypeName()) {\n+      case INTEGER:\n+        return of(lit.getValueAs(Integer.class));\n+      case BIGINT:\n+        return of(lit.getValueAs(Long.class));\n+      case DECIMAL:\n+        return of(lit.getValueAs(BigDecimal.class));\n+      case FLOAT:\n+        return of(lit.getValueAs(Float.class));\n+      case DOUBLE:\n+        return of(lit.getValueAs(Double.class));\n+      case BOOLEAN:\n+        return of(lit.getValueAs(Boolean.class));\n+      case DATE:\n+        return of(lit.getValueAs(ReadableDateTime.class));\n+      case CHAR:\n+      case VARCHAR:\n+        return of(lit.getValueAs(String.class));\n+      default:\n+        throw new SqlConversionException(\n+            \"sql literal type not supported: \" + lit.getTypeName().toString());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM0MTg2OA=="}, "originalCommit": {"oid": "e4652aedd6a3c1a9914d0ffe28b5f46bbb2bae38"}, "originalPosition": 59}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg3MzExODY1OnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0yNVQwMDo1NDoxMlrOG3BITg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMDo1MTowMVrOG60_DA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM0MzM3NA==", "bodyText": "This reverse seems not useful.", "url": "https://github.com/apache/beam/pull/12232#discussion_r460343374", "createdAt": "2020-07-25T00:54:12Z", "author": {"login": "amaliujia"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.cep;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexLiteral;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.sql.SqlKind;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.sql.SqlOperator;\n+\n+/**\n+ * Some utility methods for transforming Calcite's constructs into our own Beam constructs (for\n+ * serialization purpose).\n+ */\n+public class CEPUtil {\n+\n+  private static Quantifier getQuantifier(int start, int end, boolean isReluctant) {\n+    Quantifier quantToAdd;\n+    if (!isReluctant) {\n+      if (start == end) {\n+        quantToAdd = new Quantifier(\"{ \" + start + \" }\");\n+      } else {\n+        if (end == -1) {\n+          if (start == 0) {\n+            quantToAdd = Quantifier.ASTERISK;\n+          } else if (start == 1) {\n+            quantToAdd = Quantifier.PLUS;\n+          } else {\n+            quantToAdd = new Quantifier(\"{ \" + start + \" }\");\n+          }\n+        } else {\n+          if (start == 0 && end == 1) {\n+            quantToAdd = Quantifier.QMARK;\n+          } else if (start == -1) {\n+            quantToAdd = new Quantifier(\"{ , \" + end + \" }\");\n+          } else {\n+            quantToAdd = new Quantifier(\"{ \" + start + \" , }\");\n+          }\n+        }\n+      }\n+    } else {\n+      if (start == end) {\n+        quantToAdd = new Quantifier(\"{ \" + start + \" }?\");\n+      } else {\n+        if (end == -1) {\n+          if (start == 0) {\n+            quantToAdd = Quantifier.ASTERISK_RELUCTANT;\n+          } else if (start == 1) {\n+            quantToAdd = Quantifier.PLUS_RELUCTANT;\n+          } else {\n+            quantToAdd = new Quantifier(\"{ \" + start + \" }?\");\n+          }\n+        } else {\n+          if (start == 0 && end == 1) {\n+            quantToAdd = Quantifier.QMARK_RELUCTANT;\n+          } else if (start == -1) {\n+            quantToAdd = new Quantifier(\"{ , \" + end + \" }?\");\n+          } else {\n+            quantToAdd = new Quantifier(\"{ \" + start + \" , }?\");\n+          }\n+        }\n+      }\n+    }\n+\n+    return quantToAdd;\n+  }\n+\n+  /** Construct a list of {@code CEPPattern}s from a {@code RexNode}. */\n+  public static ArrayList<CEPPattern> getCEPPatternFromPattern(\n+      Schema upStreamSchema, RexNode call, Map<String, RexNode> patternDefs) {\n+    ArrayList<CEPPattern> patternList = new ArrayList<>();\n+    if (call.getClass() == RexLiteral.class) {\n+      String p = ((RexLiteral) call).getValueAs(String.class);\n+      RexNode pd = patternDefs.get(p);\n+      patternList.add(CEPPattern.of(upStreamSchema, p, (RexCall) pd, Quantifier.NONE));\n+    } else {\n+      RexCall patCall = (RexCall) call;\n+      SqlOperator operator = patCall.getOperator();\n+      List<RexNode> operands = patCall.getOperands();\n+\n+      // check if if the node has quantifier\n+      if (operator.getKind() == SqlKind.PATTERN_QUANTIFIER) {\n+        String p = ((RexLiteral) operands.get(0)).getValueAs(String.class);\n+        RexNode pd = patternDefs.get(p);\n+        int start = ((RexLiteral) operands.get(1)).getValueAs(Integer.class);\n+        int end = ((RexLiteral) operands.get(2)).getValueAs(Integer.class);\n+        boolean isReluctant = ((RexLiteral) operands.get(3)).getValueAs(Boolean.class);\n+\n+        patternList.add(\n+            CEPPattern.of(upStreamSchema, p, (RexCall) pd, getQuantifier(start, end, isReluctant)));\n+      } else {\n+        for (RexNode i : operands) {\n+          patternList.addAll(getCEPPatternFromPattern(upStreamSchema, i, patternDefs));\n+        }\n+      }\n+    }\n+    return patternList;\n+  }\n+\n+  /** Recursively construct a regular expression from a {@code RexNode}. */\n+  public static String getRegexFromPattern(RexNode call) {\n+    if (call.getClass() == RexLiteral.class) {\n+      return ((RexLiteral) call).getValueAs(String.class);\n+    } else {\n+      RexCall opr = (RexCall) call;\n+      SqlOperator operator = opr.getOperator();\n+      List<RexNode> operands = opr.getOperands();\n+      if (operator.getKind() == SqlKind.PATTERN_QUANTIFIER) {\n+        String p = ((RexLiteral) operands.get(0)).getValueAs(String.class);\n+        int start = ((RexLiteral) operands.get(1)).getValueAs(Integer.class);\n+        int end = ((RexLiteral) operands.get(2)).getValueAs(Integer.class);\n+        boolean isReluctant = ((RexLiteral) operands.get(3)).getValueAs(Boolean.class);\n+        Quantifier quantifier = getQuantifier(start, end, isReluctant);\n+        return p + quantifier.toString();\n+      }\n+      return getRegexFromPattern(opr.getOperands().get(0))\n+          + getRegexFromPattern(opr.getOperands().get(1));\n+    }\n+  }\n+\n+  /** Transform a list of keys in Calcite to {@code ORDER BY} to {@code OrderKey}s. */\n+  public static ArrayList<OrderKey> makeOrderKeysFromCollation(RelCollation orderKeys) {\n+    List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n+    Collections.reverse(revOrderKeys);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e4652aedd6a3c1a9914d0ffe28b5f46bbb2bae38"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Mzk2NjgxMQ==", "bodyText": "The thing is, for the order clause, the leftmost (the beginning) key is the most significant. I think the right way to sort should be starting from the least significant key to the most significant key. That is why I wanted to reverse the array.", "url": "https://github.com/apache/beam/pull/12232#discussion_r463966811", "createdAt": "2020-08-01T14:24:48Z", "author": {"login": "Mark-Zeng"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.cep;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexLiteral;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.sql.SqlKind;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.sql.SqlOperator;\n+\n+/**\n+ * Some utility methods for transforming Calcite's constructs into our own Beam constructs (for\n+ * serialization purpose).\n+ */\n+public class CEPUtil {\n+\n+  private static Quantifier getQuantifier(int start, int end, boolean isReluctant) {\n+    Quantifier quantToAdd;\n+    if (!isReluctant) {\n+      if (start == end) {\n+        quantToAdd = new Quantifier(\"{ \" + start + \" }\");\n+      } else {\n+        if (end == -1) {\n+          if (start == 0) {\n+            quantToAdd = Quantifier.ASTERISK;\n+          } else if (start == 1) {\n+            quantToAdd = Quantifier.PLUS;\n+          } else {\n+            quantToAdd = new Quantifier(\"{ \" + start + \" }\");\n+          }\n+        } else {\n+          if (start == 0 && end == 1) {\n+            quantToAdd = Quantifier.QMARK;\n+          } else if (start == -1) {\n+            quantToAdd = new Quantifier(\"{ , \" + end + \" }\");\n+          } else {\n+            quantToAdd = new Quantifier(\"{ \" + start + \" , }\");\n+          }\n+        }\n+      }\n+    } else {\n+      if (start == end) {\n+        quantToAdd = new Quantifier(\"{ \" + start + \" }?\");\n+      } else {\n+        if (end == -1) {\n+          if (start == 0) {\n+            quantToAdd = Quantifier.ASTERISK_RELUCTANT;\n+          } else if (start == 1) {\n+            quantToAdd = Quantifier.PLUS_RELUCTANT;\n+          } else {\n+            quantToAdd = new Quantifier(\"{ \" + start + \" }?\");\n+          }\n+        } else {\n+          if (start == 0 && end == 1) {\n+            quantToAdd = Quantifier.QMARK_RELUCTANT;\n+          } else if (start == -1) {\n+            quantToAdd = new Quantifier(\"{ , \" + end + \" }?\");\n+          } else {\n+            quantToAdd = new Quantifier(\"{ \" + start + \" , }?\");\n+          }\n+        }\n+      }\n+    }\n+\n+    return quantToAdd;\n+  }\n+\n+  /** Construct a list of {@code CEPPattern}s from a {@code RexNode}. */\n+  public static ArrayList<CEPPattern> getCEPPatternFromPattern(\n+      Schema upStreamSchema, RexNode call, Map<String, RexNode> patternDefs) {\n+    ArrayList<CEPPattern> patternList = new ArrayList<>();\n+    if (call.getClass() == RexLiteral.class) {\n+      String p = ((RexLiteral) call).getValueAs(String.class);\n+      RexNode pd = patternDefs.get(p);\n+      patternList.add(CEPPattern.of(upStreamSchema, p, (RexCall) pd, Quantifier.NONE));\n+    } else {\n+      RexCall patCall = (RexCall) call;\n+      SqlOperator operator = patCall.getOperator();\n+      List<RexNode> operands = patCall.getOperands();\n+\n+      // check if if the node has quantifier\n+      if (operator.getKind() == SqlKind.PATTERN_QUANTIFIER) {\n+        String p = ((RexLiteral) operands.get(0)).getValueAs(String.class);\n+        RexNode pd = patternDefs.get(p);\n+        int start = ((RexLiteral) operands.get(1)).getValueAs(Integer.class);\n+        int end = ((RexLiteral) operands.get(2)).getValueAs(Integer.class);\n+        boolean isReluctant = ((RexLiteral) operands.get(3)).getValueAs(Boolean.class);\n+\n+        patternList.add(\n+            CEPPattern.of(upStreamSchema, p, (RexCall) pd, getQuantifier(start, end, isReluctant)));\n+      } else {\n+        for (RexNode i : operands) {\n+          patternList.addAll(getCEPPatternFromPattern(upStreamSchema, i, patternDefs));\n+        }\n+      }\n+    }\n+    return patternList;\n+  }\n+\n+  /** Recursively construct a regular expression from a {@code RexNode}. */\n+  public static String getRegexFromPattern(RexNode call) {\n+    if (call.getClass() == RexLiteral.class) {\n+      return ((RexLiteral) call).getValueAs(String.class);\n+    } else {\n+      RexCall opr = (RexCall) call;\n+      SqlOperator operator = opr.getOperator();\n+      List<RexNode> operands = opr.getOperands();\n+      if (operator.getKind() == SqlKind.PATTERN_QUANTIFIER) {\n+        String p = ((RexLiteral) operands.get(0)).getValueAs(String.class);\n+        int start = ((RexLiteral) operands.get(1)).getValueAs(Integer.class);\n+        int end = ((RexLiteral) operands.get(2)).getValueAs(Integer.class);\n+        boolean isReluctant = ((RexLiteral) operands.get(3)).getValueAs(Boolean.class);\n+        Quantifier quantifier = getQuantifier(start, end, isReluctant);\n+        return p + quantifier.toString();\n+      }\n+      return getRegexFromPattern(opr.getOperands().get(0))\n+          + getRegexFromPattern(opr.getOperands().get(1));\n+    }\n+  }\n+\n+  /** Transform a list of keys in Calcite to {@code ORDER BY} to {@code OrderKey}s. */\n+  public static ArrayList<OrderKey> makeOrderKeysFromCollation(RelCollation orderKeys) {\n+    List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n+    Collections.reverse(revOrderKeys);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM0MzM3NA=="}, "originalCommit": {"oid": "e4652aedd6a3c1a9914d0ffe28b5f46bbb2bae38"}, "originalPosition": 146}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzODcwMA==", "bodyText": "I have changed this part as well. I think reversing the order could be confusing. I moved it into the SortKey transform.", "url": "https://github.com/apache/beam/pull/12232#discussion_r464338700", "createdAt": "2020-08-03T10:51:01Z", "author": {"login": "Mark-Zeng"}, "path": "sdks/java/extensions/sql/src/main/java/org/apache/beam/sdk/extensions/sql/impl/cep/CEPUtil.java", "diffHunk": "@@ -0,0 +1,155 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.cep;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rel.RelFieldCollation;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexCall;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexLiteral;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.rex.RexNode;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.sql.SqlKind;\n+import org.apache.beam.vendor.calcite.v1_20_0.org.apache.calcite.sql.SqlOperator;\n+\n+/**\n+ * Some utility methods for transforming Calcite's constructs into our own Beam constructs (for\n+ * serialization purpose).\n+ */\n+public class CEPUtil {\n+\n+  private static Quantifier getQuantifier(int start, int end, boolean isReluctant) {\n+    Quantifier quantToAdd;\n+    if (!isReluctant) {\n+      if (start == end) {\n+        quantToAdd = new Quantifier(\"{ \" + start + \" }\");\n+      } else {\n+        if (end == -1) {\n+          if (start == 0) {\n+            quantToAdd = Quantifier.ASTERISK;\n+          } else if (start == 1) {\n+            quantToAdd = Quantifier.PLUS;\n+          } else {\n+            quantToAdd = new Quantifier(\"{ \" + start + \" }\");\n+          }\n+        } else {\n+          if (start == 0 && end == 1) {\n+            quantToAdd = Quantifier.QMARK;\n+          } else if (start == -1) {\n+            quantToAdd = new Quantifier(\"{ , \" + end + \" }\");\n+          } else {\n+            quantToAdd = new Quantifier(\"{ \" + start + \" , }\");\n+          }\n+        }\n+      }\n+    } else {\n+      if (start == end) {\n+        quantToAdd = new Quantifier(\"{ \" + start + \" }?\");\n+      } else {\n+        if (end == -1) {\n+          if (start == 0) {\n+            quantToAdd = Quantifier.ASTERISK_RELUCTANT;\n+          } else if (start == 1) {\n+            quantToAdd = Quantifier.PLUS_RELUCTANT;\n+          } else {\n+            quantToAdd = new Quantifier(\"{ \" + start + \" }?\");\n+          }\n+        } else {\n+          if (start == 0 && end == 1) {\n+            quantToAdd = Quantifier.QMARK_RELUCTANT;\n+          } else if (start == -1) {\n+            quantToAdd = new Quantifier(\"{ , \" + end + \" }?\");\n+          } else {\n+            quantToAdd = new Quantifier(\"{ \" + start + \" , }?\");\n+          }\n+        }\n+      }\n+    }\n+\n+    return quantToAdd;\n+  }\n+\n+  /** Construct a list of {@code CEPPattern}s from a {@code RexNode}. */\n+  public static ArrayList<CEPPattern> getCEPPatternFromPattern(\n+      Schema upStreamSchema, RexNode call, Map<String, RexNode> patternDefs) {\n+    ArrayList<CEPPattern> patternList = new ArrayList<>();\n+    if (call.getClass() == RexLiteral.class) {\n+      String p = ((RexLiteral) call).getValueAs(String.class);\n+      RexNode pd = patternDefs.get(p);\n+      patternList.add(CEPPattern.of(upStreamSchema, p, (RexCall) pd, Quantifier.NONE));\n+    } else {\n+      RexCall patCall = (RexCall) call;\n+      SqlOperator operator = patCall.getOperator();\n+      List<RexNode> operands = patCall.getOperands();\n+\n+      // check if if the node has quantifier\n+      if (operator.getKind() == SqlKind.PATTERN_QUANTIFIER) {\n+        String p = ((RexLiteral) operands.get(0)).getValueAs(String.class);\n+        RexNode pd = patternDefs.get(p);\n+        int start = ((RexLiteral) operands.get(1)).getValueAs(Integer.class);\n+        int end = ((RexLiteral) operands.get(2)).getValueAs(Integer.class);\n+        boolean isReluctant = ((RexLiteral) operands.get(3)).getValueAs(Boolean.class);\n+\n+        patternList.add(\n+            CEPPattern.of(upStreamSchema, p, (RexCall) pd, getQuantifier(start, end, isReluctant)));\n+      } else {\n+        for (RexNode i : operands) {\n+          patternList.addAll(getCEPPatternFromPattern(upStreamSchema, i, patternDefs));\n+        }\n+      }\n+    }\n+    return patternList;\n+  }\n+\n+  /** Recursively construct a regular expression from a {@code RexNode}. */\n+  public static String getRegexFromPattern(RexNode call) {\n+    if (call.getClass() == RexLiteral.class) {\n+      return ((RexLiteral) call).getValueAs(String.class);\n+    } else {\n+      RexCall opr = (RexCall) call;\n+      SqlOperator operator = opr.getOperator();\n+      List<RexNode> operands = opr.getOperands();\n+      if (operator.getKind() == SqlKind.PATTERN_QUANTIFIER) {\n+        String p = ((RexLiteral) operands.get(0)).getValueAs(String.class);\n+        int start = ((RexLiteral) operands.get(1)).getValueAs(Integer.class);\n+        int end = ((RexLiteral) operands.get(2)).getValueAs(Integer.class);\n+        boolean isReluctant = ((RexLiteral) operands.get(3)).getValueAs(Boolean.class);\n+        Quantifier quantifier = getQuantifier(start, end, isReluctant);\n+        return p + quantifier.toString();\n+      }\n+      return getRegexFromPattern(opr.getOperands().get(0))\n+          + getRegexFromPattern(opr.getOperands().get(1));\n+    }\n+  }\n+\n+  /** Transform a list of keys in Calcite to {@code ORDER BY} to {@code OrderKey}s. */\n+  public static ArrayList<OrderKey> makeOrderKeysFromCollation(RelCollation orderKeys) {\n+    List<RelFieldCollation> revOrderKeys = orderKeys.getFieldCollations();\n+    Collections.reverse(revOrderKeys);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2MDM0MzM3NA=="}, "originalCommit": {"oid": "e4652aedd6a3c1a9914d0ffe28b5f46bbb2bae38"}, "originalPosition": 146}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjg5ODM5ODExOnYy", "diffSide": "RIGHT", "path": "sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wMlQxOTo0NzozNFrOG6naow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QxMDo1MTo1OFrOG61Atw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDExNjM4Nw==", "bodyText": "You can either remove this commented test, or leave it with @ignore(\"the reason to ignore this test\").", "url": "https://github.com/apache/beam/pull/12232#discussion_r464116387", "createdAt": "2020-08-02T19:47:34Z", "author": {"login": "amaliujia"}, "path": "sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.compilePipeline;\n+import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.registerTable;\n+\n+import org.apache.beam.sdk.extensions.sql.TestUtils;\n+import org.apache.beam.sdk.extensions.sql.meta.provider.test.TestBoundedTable;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.junit.Rule;\n+import org.junit.Test;\n+\n+/** Test for {@code BeamMatchRel}. */\n+public class BeamMatchRelTest {\n+\n+  @Rule public final TestPipeline pipeline = TestPipeline.create();\n+\n+  @Test\n+  public void matchLogicalPlanTest() {\n+    Schema schemaType =\n+        Schema.builder()\n+            .addInt32Field(\"id\")\n+            .addStringField(\"name\")\n+            .addInt32Field(\"proctime\")\n+            .build();\n+\n+    registerTable(\n+        \"TestTable\", TestBoundedTable.of(schemaType).addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3));\n+\n+    String sql =\n+        \"SELECT * \"\n+            + \"FROM TestTable \"\n+            + \"MATCH_RECOGNIZE (\"\n+            + \"PARTITION BY id \"\n+            + \"ORDER BY proctime \"\n+            + \"ALL ROWS PER MATCH \"\n+            + \"PATTERN (A B C) \"\n+            + \"DEFINE \"\n+            + \"A AS name = 'a', \"\n+            + \"B AS name = 'b', \"\n+            + \"C AS name = 'c' \"\n+            + \") AS T\";\n+\n+    PCollection<Row> result = compilePipeline(sql, pipeline);\n+\n+    PAssert.that(result)\n+        .containsInAnyOrder(\n+            TestUtils.RowsBuilder.of(\n+                    Schema.FieldType.INT32, \"id\",\n+                    Schema.FieldType.STRING, \"name\",\n+                    Schema.FieldType.INT32, \"proctime\")\n+                .addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3)\n+                .getRows());\n+\n+    pipeline.run().waitUntilFinish();\n+  }\n+\n+  @Test\n+  public void matchQuantifierTest() {\n+    Schema schemaType =\n+        Schema.builder()\n+            .addInt32Field(\"id\")\n+            .addStringField(\"name\")\n+            .addInt32Field(\"proctime\")\n+            .build();\n+\n+    registerTable(\n+        \"TestTable\",\n+        TestBoundedTable.of(schemaType).addRows(1, \"a\", 1, 1, \"a\", 2, 1, \"b\", 3, 1, \"c\", 4));\n+\n+    String sql =\n+        \"SELECT * \"\n+            + \"FROM TestTable \"\n+            + \"MATCH_RECOGNIZE (\"\n+            + \"PARTITION BY id \"\n+            + \"ORDER BY proctime \"\n+            + \"ALL ROWS PER MATCH \"\n+            + \"PATTERN (A+ B C) \"\n+            + \"DEFINE \"\n+            + \"A AS name = 'a', \"\n+            + \"B AS name = 'b', \"\n+            + \"C AS name = 'c' \"\n+            + \") AS T\";\n+\n+    PCollection<Row> result = compilePipeline(sql, pipeline);\n+\n+    PAssert.that(result)\n+        .containsInAnyOrder(\n+            TestUtils.RowsBuilder.of(\n+                    Schema.FieldType.INT32, \"id\",\n+                    Schema.FieldType.STRING, \"name\",\n+                    Schema.FieldType.INT32, \"proctime\")\n+                .addRows(1, \"a\", 1, 1, \"a\", 2, 1, \"b\", 3, 1, \"c\", 4)\n+                .getRows());\n+\n+    pipeline.run().waitUntilFinish();\n+  }\n+\n+  @Test\n+  public void matchMeasuresTest() {\n+    Schema schemaType =\n+        Schema.builder()\n+            .addInt32Field(\"id\")\n+            .addStringField(\"name\")\n+            .addInt32Field(\"proctime\")\n+            .build();\n+\n+    registerTable(\n+        \"TestTable\",\n+        TestBoundedTable.of(schemaType)\n+            .addRows(\n+                1, \"a\", 1, 1, \"a\", 2, 1, \"b\", 3, 1, \"c\", 4, 1, \"b\", 8, 1, \"a\", 7, 1, \"c\", 9, 2, \"a\",\n+                6, 2, \"b\", 10, 2, \"c\", 11, 5, \"a\", 0));\n+\n+    String sql =\n+        \"SELECT * \"\n+            + \"FROM TestTable \"\n+            + \"MATCH_RECOGNIZE (\"\n+            + \"PARTITION BY id \"\n+            + \"ORDER BY proctime \"\n+            + \"MEASURES \"\n+            + \"LAST (A.proctime) AS atime, \"\n+            + \"B.proctime AS btime, \"\n+            + \"C.proctime AS ctime \"\n+            + \"PATTERN (A+ B C) \"\n+            + \"DEFINE \"\n+            + \"A AS name = 'a', \"\n+            + \"B AS name = 'b', \"\n+            + \"C AS name = 'c' \"\n+            + \") AS T\";\n+\n+    PCollection<Row> result = compilePipeline(sql, pipeline);\n+\n+    PAssert.that(result)\n+        .containsInAnyOrder(\n+            TestUtils.RowsBuilder.of(\n+                    Schema.FieldType.INT32, \"id\",\n+                    Schema.FieldType.INT32, \"T.atime\",\n+                    Schema.FieldType.INT32, \"T.btime\",\n+                    Schema.FieldType.INT32, \"T.ctime\")\n+                .addRows(1, 2, 3, 4, 1, 7, 8, 9, 2, 6, 10, 11)\n+                .getRows());\n+\n+    pipeline.run().waitUntilFinish();\n+  }\n+\n+  /*", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f79922b3b4c1d3b2a88cea091ebb6257806b23aa"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDMzOTEyNw==", "bodyText": "I will ignore it for now. This test is wrote for testing the NFA.", "url": "https://github.com/apache/beam/pull/12232#discussion_r464339127", "createdAt": "2020-08-03T10:51:58Z", "author": {"login": "Mark-Zeng"}, "path": "sdks/java/extensions/sql/src/test/java/org/apache/beam/sdk/extensions/sql/impl/rel/BeamMatchRelTest.java", "diffHunk": "@@ -0,0 +1,234 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.extensions.sql.impl.rel;\n+\n+import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.compilePipeline;\n+import static org.apache.beam.sdk.extensions.sql.impl.rel.BaseRelTest.registerTable;\n+\n+import org.apache.beam.sdk.extensions.sql.TestUtils;\n+import org.apache.beam.sdk.extensions.sql.meta.provider.test.TestBoundedTable;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.testing.PAssert;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.junit.Rule;\n+import org.junit.Test;\n+\n+/** Test for {@code BeamMatchRel}. */\n+public class BeamMatchRelTest {\n+\n+  @Rule public final TestPipeline pipeline = TestPipeline.create();\n+\n+  @Test\n+  public void matchLogicalPlanTest() {\n+    Schema schemaType =\n+        Schema.builder()\n+            .addInt32Field(\"id\")\n+            .addStringField(\"name\")\n+            .addInt32Field(\"proctime\")\n+            .build();\n+\n+    registerTable(\n+        \"TestTable\", TestBoundedTable.of(schemaType).addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3));\n+\n+    String sql =\n+        \"SELECT * \"\n+            + \"FROM TestTable \"\n+            + \"MATCH_RECOGNIZE (\"\n+            + \"PARTITION BY id \"\n+            + \"ORDER BY proctime \"\n+            + \"ALL ROWS PER MATCH \"\n+            + \"PATTERN (A B C) \"\n+            + \"DEFINE \"\n+            + \"A AS name = 'a', \"\n+            + \"B AS name = 'b', \"\n+            + \"C AS name = 'c' \"\n+            + \") AS T\";\n+\n+    PCollection<Row> result = compilePipeline(sql, pipeline);\n+\n+    PAssert.that(result)\n+        .containsInAnyOrder(\n+            TestUtils.RowsBuilder.of(\n+                    Schema.FieldType.INT32, \"id\",\n+                    Schema.FieldType.STRING, \"name\",\n+                    Schema.FieldType.INT32, \"proctime\")\n+                .addRows(1, \"a\", 1, 1, \"b\", 2, 1, \"c\", 3)\n+                .getRows());\n+\n+    pipeline.run().waitUntilFinish();\n+  }\n+\n+  @Test\n+  public void matchQuantifierTest() {\n+    Schema schemaType =\n+        Schema.builder()\n+            .addInt32Field(\"id\")\n+            .addStringField(\"name\")\n+            .addInt32Field(\"proctime\")\n+            .build();\n+\n+    registerTable(\n+        \"TestTable\",\n+        TestBoundedTable.of(schemaType).addRows(1, \"a\", 1, 1, \"a\", 2, 1, \"b\", 3, 1, \"c\", 4));\n+\n+    String sql =\n+        \"SELECT * \"\n+            + \"FROM TestTable \"\n+            + \"MATCH_RECOGNIZE (\"\n+            + \"PARTITION BY id \"\n+            + \"ORDER BY proctime \"\n+            + \"ALL ROWS PER MATCH \"\n+            + \"PATTERN (A+ B C) \"\n+            + \"DEFINE \"\n+            + \"A AS name = 'a', \"\n+            + \"B AS name = 'b', \"\n+            + \"C AS name = 'c' \"\n+            + \") AS T\";\n+\n+    PCollection<Row> result = compilePipeline(sql, pipeline);\n+\n+    PAssert.that(result)\n+        .containsInAnyOrder(\n+            TestUtils.RowsBuilder.of(\n+                    Schema.FieldType.INT32, \"id\",\n+                    Schema.FieldType.STRING, \"name\",\n+                    Schema.FieldType.INT32, \"proctime\")\n+                .addRows(1, \"a\", 1, 1, \"a\", 2, 1, \"b\", 3, 1, \"c\", 4)\n+                .getRows());\n+\n+    pipeline.run().waitUntilFinish();\n+  }\n+\n+  @Test\n+  public void matchMeasuresTest() {\n+    Schema schemaType =\n+        Schema.builder()\n+            .addInt32Field(\"id\")\n+            .addStringField(\"name\")\n+            .addInt32Field(\"proctime\")\n+            .build();\n+\n+    registerTable(\n+        \"TestTable\",\n+        TestBoundedTable.of(schemaType)\n+            .addRows(\n+                1, \"a\", 1, 1, \"a\", 2, 1, \"b\", 3, 1, \"c\", 4, 1, \"b\", 8, 1, \"a\", 7, 1, \"c\", 9, 2, \"a\",\n+                6, 2, \"b\", 10, 2, \"c\", 11, 5, \"a\", 0));\n+\n+    String sql =\n+        \"SELECT * \"\n+            + \"FROM TestTable \"\n+            + \"MATCH_RECOGNIZE (\"\n+            + \"PARTITION BY id \"\n+            + \"ORDER BY proctime \"\n+            + \"MEASURES \"\n+            + \"LAST (A.proctime) AS atime, \"\n+            + \"B.proctime AS btime, \"\n+            + \"C.proctime AS ctime \"\n+            + \"PATTERN (A+ B C) \"\n+            + \"DEFINE \"\n+            + \"A AS name = 'a', \"\n+            + \"B AS name = 'b', \"\n+            + \"C AS name = 'c' \"\n+            + \") AS T\";\n+\n+    PCollection<Row> result = compilePipeline(sql, pipeline);\n+\n+    PAssert.that(result)\n+        .containsInAnyOrder(\n+            TestUtils.RowsBuilder.of(\n+                    Schema.FieldType.INT32, \"id\",\n+                    Schema.FieldType.INT32, \"T.atime\",\n+                    Schema.FieldType.INT32, \"T.btime\",\n+                    Schema.FieldType.INT32, \"T.ctime\")\n+                .addRows(1, 2, 3, 4, 1, 7, 8, 9, 2, 6, 10, 11)\n+                .getRows());\n+\n+    pipeline.run().waitUntilFinish();\n+  }\n+\n+  /*", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDExNjM4Nw=="}, "originalCommit": {"oid": "f79922b3b4c1d3b2a88cea091ebb6257806b23aa"}, "originalPosition": 167}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1029, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}