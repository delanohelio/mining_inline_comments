{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAwODU4MzY2", "number": 13066, "title": "[BEAM-11052] Memoize to_pcollection", "bodyText": "R: @robertwb\nPost-Commit Tests Status (on master branch)\n\n\n\nLang\nSDK\nDataflow\nFlink\nSamza\nSpark\nTwister2\n\n\n\n\nGo\n\n---\n\n---\n\n---\n\n\nJava\n\n\n\n\n\n\n\n\nPython\n\n\n\n---\n\n---\n\n\nXLang\n\n---\n\n---\n\n---\n\n\n\nPre-Commit Tests Status (on master branch)\n\n\n\n---\nJava\nPython\nGo\nWebsite\nWhitespace\nTypescript\n\n\n\n\nNon-portable\n\n \n\n\n\n\n\n\nPortable\n---\n\n---\n---\n---\n---\n\n\n\nSee .test-infra/jenkins/README for trigger phrase, status and link of all Jenkins jobs.\nGitHub Actions Tests Status (on master branch)\n\n\n\nSee CI.md for more information about GitHub Actions CI.", "createdAt": "2020-10-09T23:01:48Z", "url": "https://github.com/apache/beam/pull/13066", "merged": true, "mergeCommit": {"oid": "cbe87445d4259b6b485bc010231dda1895022d83"}, "closed": true, "closedAt": "2020-11-02T21:07:56Z", "author": {"login": "TheNeuralBit"}, "timelineItems": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABdQ-boTgFqTUwNjAzMDgwNg==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdYqiC4gH2gAyNTAwODU4MzY2OmIzNTRlMzJhM2I3NGI3M2YwZTBiODNmMjUwMTQ0MzliODVlZjlhZDM=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA2MDMwODA2", "url": "https://github.com/apache/beam/pull/13066#pullrequestreview-506030806", "createdAt": "2020-10-09T23:04:18Z", "commit": {"oid": "2a9906a7496941bb2d4aacc6b2e5c08027a4a73b"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQyMzowNDoxOFrOHfawMw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQyMzowNDoxOFrOHfawMw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcwNjIyNw==", "bodyText": "Note this uses a global cache which can be problematic in testing. I don't think we need to worry about pcollections being shared across pipelines though since the dataframe expressions should have a reference to the pipeline through the to_dataframe roots.", "url": "https://github.com/apache/beam/pull/13066#discussion_r502706227", "createdAt": "2020-10-09T23:04:18Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/python/apache_beam/dataframe/convert.py", "diffHunk": "@@ -67,6 +68,9 @@ def to_dataframe(\n       expressions.PlaceholderExpression(proxy, pcoll))\n \n \n+TO_PCOLLECTION_CACHE = {}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2a9906a7496941bb2d4aacc6b2e5c08027a4a73b"}, "originalPosition": 12}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTA2MDM3NzE1", "url": "https://github.com/apache/beam/pull/13066#pullrequestreview-506037715", "createdAt": "2020-10-09T23:36:32Z", "commit": {"oid": "2a9906a7496941bb2d4aacc6b2e5c08027a4a73b"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQyMzozNjozMlrOHfbKMA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0wOVQyMzo0MDo0M1rOHfbNRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcxMjg4MA==", "bodyText": "I'm actually more worried about global caches in production than testing--the expressions themselves should not collide between pipelines.\nMaybe we could use a weakref.WeakKeyDictionary.", "url": "https://github.com/apache/beam/pull/13066#discussion_r502712880", "createdAt": "2020-10-09T23:36:32Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/convert.py", "diffHunk": "@@ -67,6 +68,9 @@ def to_dataframe(\n       expressions.PlaceholderExpression(proxy, pcoll))\n \n \n+TO_PCOLLECTION_CACHE = {}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcwNjIyNw=="}, "originalCommit": {"oid": "2a9906a7496941bb2d4aacc6b2e5c08027a4a73b"}, "originalPosition": 12}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcxMzI0Mg==", "bodyText": "Put ()'s around ix, pc for better formatting.", "url": "https://github.com/apache/beam/pull/13066#discussion_r502713242", "createdAt": "2020-10-09T23:38:32Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/convert.py", "diffHunk": "@@ -138,21 +142,47 @@ def extract_input(placeholder):\n \n   placeholders = frozenset.union(\n       frozenset(), *[df._expr.placeholders() for df in dataframes])\n-  results = {p: extract_input(p)\n-             for p in placeholders\n-             } | label >> transforms._DataframeExpressionsTransform(\n-                 dict((ix, df._expr) for ix, df in enumerate(\n-                     dataframes)))  # type: Dict[Any, pvalue.PCollection]\n+\n+  # Exclude any dataframes that have already been converted to PCollections.\n+  # We only want to convert each DF expression once, then re-use.\n+  new_dataframes = [\n+      df for df in dataframes if df._expr not in TO_PCOLLECTION_CACHE\n+  ]\n+  new_results = {p: extract_input(p)\n+                 for p in placeholders\n+                 } | label >> transforms._DataframeExpressionsTransform(\n+                     dict(\n+                         (ix, df._expr) for ix, df in enumerate(new_dataframes))\n+                 )  # type: Dict[Any, pvalue.PCollection]\n+\n+  TO_PCOLLECTION_CACHE.update(\n+      {new_dataframes[ix]._expr: pc\n+       for ix, pc in new_results.items()})\n+\n+  raw_results = {\n+      ix: TO_PCOLLECTION_CACHE[df._expr]\n+      for ix, df in enumerate(dataframes)\n+  }\n \n   if yield_elements == \"schemas\":\n     results = {\n-        key: pc\n-        | \"Unbatch '%s'\" % dataframes[key]._expr._id >> schemas.UnbatchPandas(\n-            dataframes[key]._expr.proxy(), include_indexes=include_indexes)\n-        for (key, pc) in results.items()\n+        ix: _make_unbatched_pcoll(pc, dataframes[ix]._expr, include_indexes)\n+        for ix,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2a9906a7496941bb2d4aacc6b2e5c08027a4a73b"}, "originalPosition": 56}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwMjcxMzY3MA==", "bodyText": "Again, this grows without bound.", "url": "https://github.com/apache/beam/pull/13066#discussion_r502713670", "createdAt": "2020-10-09T23:40:43Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/convert.py", "diffHunk": "@@ -138,21 +142,47 @@ def extract_input(placeholder):\n \n   placeholders = frozenset.union(\n       frozenset(), *[df._expr.placeholders() for df in dataframes])\n-  results = {p: extract_input(p)\n-             for p in placeholders\n-             } | label >> transforms._DataframeExpressionsTransform(\n-                 dict((ix, df._expr) for ix, df in enumerate(\n-                     dataframes)))  # type: Dict[Any, pvalue.PCollection]\n+\n+  # Exclude any dataframes that have already been converted to PCollections.\n+  # We only want to convert each DF expression once, then re-use.\n+  new_dataframes = [\n+      df for df in dataframes if df._expr not in TO_PCOLLECTION_CACHE\n+  ]\n+  new_results = {p: extract_input(p)\n+                 for p in placeholders\n+                 } | label >> transforms._DataframeExpressionsTransform(\n+                     dict(\n+                         (ix, df._expr) for ix, df in enumerate(new_dataframes))\n+                 )  # type: Dict[Any, pvalue.PCollection]\n+\n+  TO_PCOLLECTION_CACHE.update(\n+      {new_dataframes[ix]._expr: pc\n+       for ix, pc in new_results.items()})\n+\n+  raw_results = {\n+      ix: TO_PCOLLECTION_CACHE[df._expr]\n+      for ix, df in enumerate(dataframes)\n+  }\n \n   if yield_elements == \"schemas\":\n     results = {\n-        key: pc\n-        | \"Unbatch '%s'\" % dataframes[key]._expr._id >> schemas.UnbatchPandas(\n-            dataframes[key]._expr.proxy(), include_indexes=include_indexes)\n-        for (key, pc) in results.items()\n+        ix: _make_unbatched_pcoll(pc, dataframes[ix]._expr, include_indexes)\n+        for ix,\n+        pc in raw_results.items()\n     }\n+  else:\n+    results = raw_results\n \n   if len(results) == 1 and not always_return_tuple:\n     return results[0]\n   else:\n     return tuple(value for key, value in sorted(results.items()))\n+\n+\n+memoize = functools.lru_cache(maxsize=None)\n+\n+\n+@memoize", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2a9906a7496941bb2d4aacc6b2e5c08027a4a73b"}, "originalPosition": 71}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTEwODI0NDY5", "url": "https://github.com/apache/beam/pull/13066#pullrequestreview-510824469", "createdAt": "2020-10-16T22:10:15Z", "commit": {"oid": "4f55313cd2e038f078d17ddea058b7b7540dd952"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQyMjoxMDoxNVrOHjRG1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMC0xNlQyMjoxODozMlrOHjRQOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc0MjQ4NQ==", "bodyText": "Dict comprehension (for consistency)?", "url": "https://github.com/apache/beam/pull/13066#discussion_r506742485", "createdAt": "2020-10-16T22:10:15Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/convert.py", "diffHunk": "@@ -138,19 +161,36 @@ def extract_input(placeholder):\n \n   placeholders = frozenset.union(\n       frozenset(), *[df._expr.placeholders() for df in dataframes])\n-  results = {p: extract_input(p)\n-             for p in placeholders\n-             } | label >> transforms._DataframeExpressionsTransform(\n-                 dict((ix, df._expr) for ix, df in enumerate(\n-                     dataframes)))  # type: Dict[Any, pvalue.PCollection]\n+\n+  # Exclude any dataframes that have already been converted to PCollections.\n+  # We only want to convert each DF expression once, then re-use.\n+  new_dataframes = [\n+      df for df in dataframes if df._expr._id not in TO_PCOLLECTION_CACHE\n+  ]\n+  new_results = {p: extract_input(p)\n+                 for p in placeholders\n+                 } | label >> transforms._DataframeExpressionsTransform(\n+                     dict(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4f55313cd2e038f078d17ddea058b7b7540dd952"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc0MzA3MA==", "bodyText": "Nice. Perhaps it's worth noting that the pipeline (indirectly) holds references to the transforms which keep both the collections and expressions alive. (Keeping the expressions alive is important to ensure their ids never get accidentally re-used.)", "url": "https://github.com/apache/beam/pull/13066#discussion_r506743070", "createdAt": "2020-10-16T22:12:09Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/convert.py", "diffHunk": "@@ -67,7 +69,28 @@ def to_dataframe(\n       expressions.PlaceholderExpression(proxy, pcoll))\n \n \n+# PCollections generated by to_pcollection are memoized.\n+# WeakValueDictionary is used so the caches are cleaned up with the parent\n+# pipelines", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4f55313cd2e038f078d17ddea058b7b7540dd952"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc0NDM3Mw==", "bodyText": "This test (and below) seem largely copies of the test above, but we don't even need to run the pipeline to test what this is testing. Perhaps it'd be better to limit it to what we're trying to test for clarity, i.e. the ids are the same.", "url": "https://github.com/apache/beam/pull/13066#discussion_r506744373", "createdAt": "2020-10-16T22:16:50Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/convert_test.py", "diffHunk": "@@ -85,6 +85,94 @@ def test_convert(self):\n       assert_that(pc_3a, equal_to(list(3 * a)), label='Check3a')\n       assert_that(pc_ab, equal_to(list(a * b)), label='Checkab')\n \n+  def test_convert_memoization(self):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4f55313cd2e038f078d17ddea058b7b7540dd952"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc0NDQxOQ==", "bodyText": "assertIs?", "url": "https://github.com/apache/beam/pull/13066#discussion_r506744419", "createdAt": "2020-10-16T22:16:56Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/convert_test.py", "diffHunk": "@@ -85,6 +85,94 @@ def test_convert(self):\n       assert_that(pc_3a, equal_to(list(3 * a)), label='Check3a')\n       assert_that(pc_ab, equal_to(list(a * b)), label='Checkab')\n \n+  def test_convert_memoization(self):\n+    with beam.Pipeline() as p:\n+      a = pd.Series([1, 2, 3])\n+      b = pd.Series([100, 200, 300])\n+\n+      pc_a = p | 'A' >> beam.Create([a])\n+      pc_b = p | 'B' >> beam.Create([b])\n+\n+      df_a = convert.to_dataframe(pc_a, proxy=a[:0])\n+      df_b = convert.to_dataframe(pc_b, proxy=b[:0])\n+\n+      df_2a = 2 * df_a\n+      df_3a = 3 * df_a\n+      df_ab = df_a * df_b\n+\n+      # Converting multiple results at a time can be more efficient.\n+      pc_2a_, pc_ab_ = convert.to_pcollection(df_2a, df_ab)\n+      # Converting the same expressions should yeild the same pcolls\n+      pc_3a, pc_2a, pc_ab = convert.to_pcollection(df_3a, df_2a, df_ab)\n+\n+      self.assertEqual(id(pc_2a), id(pc_2a_))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4f55313cd2e038f078d17ddea058b7b7540dd952"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUwNjc0NDg4OA==", "bodyText": "Move these tests into the former, so you can also assert that to_pcollection(x, yield_elements='schema') != to_pcollection(x, yield_elements='pandas') (i.e. no accidental cross-cache contamination).", "url": "https://github.com/apache/beam/pull/13066#discussion_r506744888", "createdAt": "2020-10-16T22:18:32Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/dataframe/convert_test.py", "diffHunk": "@@ -85,6 +85,94 @@ def test_convert(self):\n       assert_that(pc_3a, equal_to(list(3 * a)), label='Check3a')\n       assert_that(pc_ab, equal_to(list(a * b)), label='Checkab')\n \n+  def test_convert_memoization(self):\n+    with beam.Pipeline() as p:\n+      a = pd.Series([1, 2, 3])\n+      b = pd.Series([100, 200, 300])\n+\n+      pc_a = p | 'A' >> beam.Create([a])\n+      pc_b = p | 'B' >> beam.Create([b])\n+\n+      df_a = convert.to_dataframe(pc_a, proxy=a[:0])\n+      df_b = convert.to_dataframe(pc_b, proxy=b[:0])\n+\n+      df_2a = 2 * df_a\n+      df_3a = 3 * df_a\n+      df_ab = df_a * df_b\n+\n+      # Converting multiple results at a time can be more efficient.\n+      pc_2a_, pc_ab_ = convert.to_pcollection(df_2a, df_ab)\n+      # Converting the same expressions should yeild the same pcolls\n+      pc_3a, pc_2a, pc_ab = convert.to_pcollection(df_3a, df_2a, df_ab)\n+\n+      self.assertEqual(id(pc_2a), id(pc_2a_))\n+      self.assertEqual(id(pc_ab), id(pc_ab_))\n+\n+      assert_that(pc_2a, equal_to(list(2 * a)), label='Check2a')\n+      assert_that(pc_3a, equal_to(list(3 * a)), label='Check3a')\n+      assert_that(pc_ab, equal_to(list(a * b)), label='Checkab')\n+\n+  def test_convert_memoization_yield_pandas(self):\n+    with beam.Pipeline() as p:\n+      a = pd.Series([1, 2, 3])\n+      b = pd.Series([100, 200, 300])\n+\n+      pc_a = p | 'A' >> beam.Create([a])\n+      pc_b = p | 'B' >> beam.Create([b])\n+\n+      df_a = convert.to_dataframe(pc_a, proxy=a[:0])\n+      df_b = convert.to_dataframe(pc_b, proxy=b[:0])\n+\n+      df_2a = 2 * df_a\n+      df_3a = 3 * df_a\n+      df_ab = df_a * df_b\n+\n+      # Converting multiple results at a time can be more efficient.\n+      pc_2a_, pc_ab_ = convert.to_pcollection(df_2a, df_ab,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4f55313cd2e038f078d17ddea058b7b7540dd952"}, "originalPosition": 74}]}}, {"__typename": "PullRequestCommit", "commit": {"oid": "af356a62ecdb54af4d90b3706f12ad3246be8404", "author": {"user": {"login": "TheNeuralBit", "name": "Brian Hulette"}}, "url": "https://github.com/apache/beam/commit/af356a62ecdb54af4d90b3706f12ad3246be8404", "committedDate": "2020-10-28T22:37:01Z", "message": "Memoize to_pcollection"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "608faa0eed93214c4b75ba7cabd78e90be058135", "author": {"user": {"login": "TheNeuralBit", "name": "Brian Hulette"}}, "url": "https://github.com/apache/beam/commit/608faa0eed93214c4b75ba7cabd78e90be058135", "committedDate": "2020-10-28T22:39:34Z", "message": "Use WeakValueDictionary so cache doesn't grow without bound"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "10e66386f14d52d5d2b8e753e6bbbbbf5a6ad9bd", "author": {"user": {"login": "TheNeuralBit", "name": "Brian Hulette"}}, "url": "https://github.com/apache/beam/commit/10e66386f14d52d5d2b8e753e6bbbbbf5a6ad9bd", "committedDate": "2020-10-28T22:40:51Z", "message": "combine pandas memoization test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "037fb36a6f2511dacb7a0e0696e4e565b2a7d86f", "author": {"user": {"login": "TheNeuralBit", "name": "Brian Hulette"}}, "url": "https://github.com/apache/beam/commit/037fb36a6f2511dacb7a0e0696e4e565b2a7d86f", "committedDate": "2020-10-28T22:40:51Z", "message": "Don't add a DataframeTransform computing 0 dataframes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "6dc091646950e26dca4e126405492108a7003afb", "author": {"user": {"login": "TheNeuralBit", "name": "Brian Hulette"}}, "url": "https://github.com/apache/beam/commit/6dc091646950e26dca4e126405492108a7003afb", "committedDate": "2020-10-28T22:40:51Z", "message": "docs"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e7ebeec84f325c775fcfea38a7c057bf140667bd", "author": {"user": {"login": "TheNeuralBit", "name": "Brian Hulette"}}, "url": "https://github.com/apache/beam/commit/e7ebeec84f325c775fcfea38a7c057bf140667bd", "committedDate": "2020-10-20T16:20:58Z", "message": "docs"}, "afterCommit": {"oid": "6dc091646950e26dca4e126405492108a7003afb", "author": {"user": {"login": "TheNeuralBit", "name": "Brian Hulette"}}, "url": "https://github.com/apache/beam/commit/6dc091646950e26dca4e126405492108a7003afb", "committedDate": "2020-10-28T22:40:51Z", "message": "docs"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTIxOTAxMTI3", "url": "https://github.com/apache/beam/pull/13066#pullrequestreview-521901127", "createdAt": "2020-11-02T18:42:10Z", "commit": {"oid": "6dc091646950e26dca4e126405492108a7003afb"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "73b530cac073bf6b1516e29019744a2cd923e743", "author": {"user": {"login": "TheNeuralBit", "name": "Brian Hulette"}}, "url": "https://github.com/apache/beam/commit/73b530cac073bf6b1516e29019744a2cd923e743", "committedDate": "2020-11-02T19:43:36Z", "message": "lint"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "1db94a55c2e41b86979223cec64e5e0071b84435", "author": {"user": {"login": "TheNeuralBit", "name": "Brian Hulette"}}, "url": "https://github.com/apache/beam/commit/1db94a55c2e41b86979223cec64e5e0071b84435", "committedDate": "2020-11-02T20:11:38Z", "message": "mypy"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d458b2828d9bac1e0832d08323285584d6a3a71f", "author": {"user": {"login": "TheNeuralBit", "name": "Brian Hulette"}}, "url": "https://github.com/apache/beam/commit/d458b2828d9bac1e0832d08323285584d6a3a71f", "committedDate": "2020-11-02T20:17:56Z", "message": "mypy2"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b354e32a3b74b73f0e0b83f25014439b85ef9ad3", "author": {"user": {"login": "TheNeuralBit", "name": "Brian Hulette"}}, "url": "https://github.com/apache/beam/commit/b354e32a3b74b73f0e0b83f25014439b85ef9ad3", "committedDate": "2020-11-02T20:24:37Z", "message": "import order"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1909, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}