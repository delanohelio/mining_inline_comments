{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzgxNjIxNzU0", "number": 11005, "title": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager ", "bodyText": "Modifies the StreamingCache to subclass the CacheManager. This allows for the overriding of the source and sink transforms.\n\nThank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:\n\n Choose reviewer(s) and mention them in a comment (R: @username).\n Format the pull request title like [BEAM-XXX] Fixes bug in ApproximateQuantiles, where you replace BEAM-XXX with the appropriate JIRA issue, if applicable. This will automatically link the pull request to the issue.\n Update CHANGES.md with noteworthy changes.\n If this contribution is large, please file an Apache Individual Contributor License Agreement.\n\nSee the Contributor Guide for more tips on how to make review process smoother.\nPost-Commit Tests Status (on master branch)\n\n\n\nLang\nSDK\nApex\nDataflow\nFlink\nGearpump\nSamza\nSpark\n\n\n\n\nGo\n\n---\n---\n\n---\n---\n\n\n\nJava\n\n\n\n\n\n\n\n\n\nPython\n\n---\n\n\n---\n---\n\n\n\nXLang\n---\n---\n---\n\n---\n---\n\n\n\n\nPre-Commit Tests Status (on master branch)\n\n\n\n---\nJava\nPython\nGo\nWebsite\n\n\n\n\nNon-portable\n\n\n\n\n\n\nPortable\n---\n\n---\n---\n\n\n\nSee .test-infra/jenkins/README for trigger phrase, status and link of all Jenkins jobs.", "createdAt": "2020-02-28T22:19:45Z", "url": "https://github.com/apache/beam/pull/11005", "merged": true, "mergeCommit": {"oid": "bb9826ca35dcf5889749ec2f368d911b7e106c40"}, "closed": true, "closedAt": "2020-03-12T06:19:58Z", "author": {"login": "rohdesamuel"}, "timelineItems": {"totalCount": 21, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcI4TnRgBqjMwODM4MzM5NDk=", "endCursor": "Y3Vyc29yOnYyOpPPAAABcMveK-ABqjMxMjA4NjQ5NDI=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2c5ca4226da0e4b9c3e84f77810d5db1646f514d", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/2c5ca4226da0e4b9c3e84f77810d5db1646f514d", "committedDate": "2020-02-28T22:14:21Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}, "afterCommit": {"oid": "752cb6e9227553ffa0c82bf1f5d28dd3124d4c3c", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/752cb6e9227553ffa0c82bf1f5d28dd3124d4c3c", "committedDate": "2020-02-28T23:13:04Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "752cb6e9227553ffa0c82bf1f5d28dd3124d4c3c", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/752cb6e9227553ffa0c82bf1f5d28dd3124d4c3c", "committedDate": "2020-02-28T23:13:04Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}, "afterCommit": {"oid": "514c276f0370de22b13a46422c7af6312b9ef6aa", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/514c276f0370de22b13a46422c7af6312b9ef6aa", "committedDate": "2020-02-28T23:23:09Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "514c276f0370de22b13a46422c7af6312b9ef6aa", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/514c276f0370de22b13a46422c7af6312b9ef6aa", "committedDate": "2020-02-28T23:23:09Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}, "afterCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/62cb7a971fb8cda89f7c0c8987786ef3a170ef71", "committedDate": "2020-03-03T00:28:54Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY4Mjc5OTc4", "url": "https://github.com/apache/beam/pull/11005#pullrequestreview-368279978", "createdAt": "2020-03-03T19:56:50Z", "commit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "state": "COMMENTED", "comments": {"totalCount": 12, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QxOTo1Njo1MFrOFxUZCw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMDo0MDo1N1rOFxVvTw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI1ODYzNQ==", "bodyText": "Avoid backslashes for continuation. If needed, use ()'s (or just let yapf do it). (Creating the stub here seems a violation of encapsulation though, best to do it where it's used.)", "url": "https://github.com/apache/beam/pull/11005#discussion_r387258635", "createdAt": "2020-03-03T19:56:50Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/direct_runner.py", "diffHunk": "@@ -399,6 +395,15 @@ def visit_transform(self, applied_ptransform):\n     self.consumer_tracking_visitor = ConsumerTrackingPipelineVisitor()\n     pipeline.visit(self.consumer_tracking_visitor)\n \n+    test_stream_service_endpoint = \\", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI1OTE2Nw==", "bodyText": "Why was this removed?", "url": "https://github.com/apache/beam/pull/11005#discussion_r387259167", "createdAt": "2020-03-03T19:57:46Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/direct_runner.py", "diffHunk": "@@ -114,13 +117,6 @@ def visit_transform(self, applied_ptransform):\n     # FnApiRunner, and the pipeline was not meant to be run as streaming.\n     use_fnapi_runner = (_FnApiRunnerSupportVisitor().accept(pipeline))\n \n-    # Also ensure grpc is available.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 22}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI1OTYwNQ==", "bodyText": "This shouldn't be a pipeline option, it should be a parameter in the TestStream proto itself.", "url": "https://github.com/apache/beam/pull/11005#discussion_r387259605", "createdAt": "2020-03-03T19:58:35Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/options/pipeline_options.py", "diffHunk": "@@ -1127,6 +1127,12 @@ def _add_argparse_args(cls, parser):\n         help='The time to wait (in milliseconds) for test pipeline to finish. '\n         'If it is set to None, it will wait indefinitely until the job '\n         'is finished.')\n+    parser.add_argument(\n+        '--test_stream_service_endpoint',", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI2MDMwNw==", "bodyText": "We shouldn't be changing the API of TestClock. If the caller needs a float, convert it there.", "url": "https://github.com/apache/beam/pull/11005#discussion_r387260307", "createdAt": "2020-03-03T19:59:52Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/clock.py", "diffHunk": "@@ -44,11 +46,11 @@ def time(self):\n \n class TestClock(object):\n   \"\"\"Clock used for Testing\"\"\"\n-  def __init__(self, current_time=0):\n-    self._current_time = current_time\n+  def __init__(self, current_time=None):\n+    self._current_time = current_time if current_time else Timestamp()\n \n   def time(self):\n-    return self._current_time\n+    return float(self._current_time)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 20}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI2MjA0NA==", "bodyText": "Similarly, the test_stream_event_channel doesn't seem a property of the evaluation context, rather a property of the test stream (evaluator) itself.", "url": "https://github.com/apache/beam/pull/11005#discussion_r387262044", "createdAt": "2020-03-03T20:03:07Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/evaluation_context.py", "diffHunk": "@@ -273,7 +274,8 @@ def __init__(self,\n     ]  # type: List[Tuple[TransformExecutor, Timestamp]]\n     self._counter_factory = counters.CounterFactory()\n     self._metrics = DirectMetrics()\n-\n+    self._test_stream_event_stub = test_stream_event_stub\n+    self._test_stream_event_channel = None", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 16}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI2MjY0Nw==", "bodyText": "This logic feels odd, needs an explanation.", "url": "https://github.com/apache/beam/pull/11005#discussion_r387262647", "createdAt": "2020-03-03T20:04:20Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -212,6 +218,19 @@ class _TestStreamRootBundleProvider(RootBundleProvider):\n   \"\"\"\n   def get_root_bundles(self):\n     test_stream = self._applied_ptransform.transform\n+    if self._evaluation_context._test_stream_event_stub:\n+      stub = self._evaluation_context._test_stream_event_stub\n+\n+      if list(test_stream.output_tags) == [None]:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI2NzA1OQ==", "bodyText": "This is fragile--we should not be (attempting to) interpret values as dicts of windowed value parameters and creating globally windowed values if anything goes wrong. Instead, store the WindowedValue object in tv.value. (Alternatively, one could let events be full WindowedValues rather than TimestampedValues, but that might be a bigger change.)", "url": "https://github.com/apache/beam/pull/11005#discussion_r387267059", "createdAt": "2020-03-03T20:12:57Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -421,8 +440,12 @@ def process_element(self, element):\n       main_output = list(self._outputs)[0]\n       bundle = self._evaluation_context.create_bundle(main_output)\n       for tv in event.timestamped_values:\n-        bundle.output(\n-            GlobalWindows.windowed_value(tv.value, timestamp=tv.timestamp))\n+        # Unreify the value into the correct window.\n+        try:\n+          bundle.output(WindowedValue(**tv.value))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI2ODc0Mg==", "bodyText": "(For that matter, it seems odd to be mutating ElementEvents in an operator called _WatermarkControllerEvaluator.)", "url": "https://github.com/apache/beam/pull/11005#discussion_r387268742", "createdAt": "2020-03-03T20:16:12Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -421,8 +440,12 @@ def process_element(self, element):\n       main_output = list(self._outputs)[0]\n       bundle = self._evaluation_context.create_bundle(main_output)\n       for tv in event.timestamped_values:\n-        bundle.output(\n-            GlobalWindows.windowed_value(tv.value, timestamp=tv.timestamp))\n+        # Unreify the value into the correct window.\n+        try:\n+          bundle.output(WindowedValue(**tv.value))", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI2NzA1OQ=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 67}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI2OTM1Mw==", "bodyText": "Why is this being created here?", "url": "https://github.com/apache/beam/pull/11005#discussion_r387269353", "createdAt": "2020-03-03T20:17:25Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -212,6 +218,19 @@ class _TestStreamRootBundleProvider(RootBundleProvider):\n   \"\"\"\n   def get_root_bundles(self):\n     test_stream = self._applied_ptransform.transform\n+    if self._evaluation_context._test_stream_event_stub:\n+      stub = self._evaluation_context._test_stream_event_stub\n+\n+      if list(test_stream.output_tags) == [None]:\n+        event_request = beam_runner_api_pb2.EventsRequest()\n+      else:\n+        event_request = beam_runner_api_pb2.EventsRequest(\n+            keys=list(test_stream.output_tags))\n+\n+      test_stream_event_channel = stub.Events(event_request)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI3MDIxMQ==", "bodyText": "Why does the set of keys even need to be sent here?", "url": "https://github.com/apache/beam/pull/11005#discussion_r387270211", "createdAt": "2020-03-03T20:19:06Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -212,6 +218,19 @@ class _TestStreamRootBundleProvider(RootBundleProvider):\n   \"\"\"\n   def get_root_bundles(self):\n     test_stream = self._applied_ptransform.transform\n+    if self._evaluation_context._test_stream_event_stub:\n+      stub = self._evaluation_context._test_stream_event_stub\n+\n+      if list(test_stream.output_tags) == [None]:\n+        event_request = beam_runner_api_pb2.EventsRequest()\n+      else:\n+        event_request = beam_runner_api_pb2.EventsRequest(\n+            keys=list(test_stream.output_tags))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 50}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI3MTI1OQ==", "bodyText": "Don't mutate the input.", "url": "https://github.com/apache/beam/pull/11005#discussion_r387271259", "createdAt": "2020-03-03T20:21:11Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -432,6 +455,45 @@ def finish_bundle(self):\n         self, self.bundles, [], None, {None: self._watermark})\n \n \n+class PairWithTimingEvaluator(_TransformEvaluator):\n+  \"\"\"TransformEvaluator for the PairWithTiming transform.\n+\n+  This transform takes an element as an input and outputs\n+  KV(element, `TimingInfo`). Where the `TimingInfo` contains both the\n+  processing time timestamp and watermark.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      evaluation_context,\n+      applied_ptransform,\n+      input_committed_bundle,\n+      side_inputs):\n+    assert not side_inputs\n+    super(PairWithTimingEvaluator, self).__init__(\n+        evaluation_context,\n+        applied_ptransform,\n+        input_committed_bundle,\n+        side_inputs)\n+\n+  def start_bundle(self):\n+    main_output = list(self._outputs)[0]\n+    self.bundle = self._evaluation_context.create_bundle(main_output)\n+\n+  def process_element(self, element):\n+    watermark_manager = self._evaluation_context._watermark_manager\n+    watermarks = watermark_manager.get_watermarks(self._applied_ptransform)\n+\n+    output_watermark = watermarks.output_watermark\n+    now = Timestamp(seconds=watermark_manager._clock.time())\n+    timing_info = TimingInfo(now, output_watermark)\n+\n+    element.value = (element.value, timing_info)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 110}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzI4MDcxOQ==", "bodyText": "Don't we already have this translation logic elsewhere. Also, it seems we could unify events-from-memory vs. events-from-grpc more.", "url": "https://github.com/apache/beam/pull/11005#discussion_r387280719", "createdAt": "2020-03-03T20:40:57Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -471,7 +537,44 @@ def process_element(self, element):\n     # We can either have the _TestStream or the _WatermarkController to emit\n     # the elements. We chose to emit in the _WatermarkController so that the\n     # element is emitted at the correct watermark value.\n-    for event in self.test_stream.events(self.current_index):\n+    events = []\n+    if self.watermark == MIN_TIMESTAMP:\n+      for event in self.test_stream._set_up(self.test_stream.output_tags):\n+        events.append(event)\n+\n+    if self.test_stream_event_channel:\n+      try:\n+        event = next(self.test_stream_event_channel)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 150}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzY4MzgzNjQ1", "url": "https://github.com/apache/beam/pull/11005#pullrequestreview-368383645", "createdAt": "2020-03-03T22:48:17Z", "commit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wM1QyMjo0ODoxN1rOFxZdgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNFQwMDoyMDo1M1rOFxbYIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM0MTY5OQ==", "bodyText": "Thanks .iter([]) is slightly more idiomatic, but this is fine.", "url": "https://github.com/apache/beam/pull/11005#discussion_r387341699", "createdAt": "2020-03-03T22:48:17Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/interactive/cache_manager.py", "diffHunk": "@@ -167,20 +177,35 @@ def load_pcoder(self, *labels):\n         self._saved_pcoders[self._path(*labels)])\n \n   def read(self, *labels):\n+    # Return an iterator to an empty list if it doesn't exist.\n     if not self.exists(*labels):\n-      return [], -1\n+      return [].__iter__(), -1", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 49}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM0NTQ4MQ==", "bodyText": "This is not part of the public API for sinks. Instead, do\n      writer = sink.open_writer(init_result, path)\n      for v in values:\n        writer.write(v)\n      write_results.append(writer.close())\n\nhttps://github.com/apache/beam/blob/release-2.18.0/sdks/python/apache_beam/io/iobase.py#L668", "url": "https://github.com/apache/beam/pull/11005#discussion_r387345481", "createdAt": "2020-03-03T22:57:08Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/interactive/cache_manager.py", "diffHunk": "@@ -167,20 +177,35 @@ def load_pcoder(self, *labels):\n         self._saved_pcoders[self._path(*labels)])\n \n   def read(self, *labels):\n+    # Return an iterator to an empty list if it doesn't exist.\n     if not self.exists(*labels):\n-      return [], -1\n+      return [].__iter__(), -1\n \n-    source = self.source(*labels)\n+    # Otherwise, return a generator to the cached PCollection.\n+    source = self._source(*labels)\n     range_tracker = source.get_range_tracker(None, None)\n-    result = list(source.read(range_tracker))\n+    reader = source.read(range_tracker)\n     version = self._latest_version(*labels)\n-    return result, version\n+    return reader, version\n+\n+  def write(self, values, *labels):\n+    sink = self._sink(*labels)\n+    path = self._path(*labels)\n+    with open(path, 'wb') as f:\n+      for v in values:\n+        sink.write_record(f, v)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2Njg4Mw==", "bodyText": "Why is it best-effort?", "url": "https://github.com/apache/beam/pull/11005#discussion_r387366883", "createdAt": "2020-03-04T00:00:05Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "diffHunk": "@@ -19,15 +19,298 @@\n \n from __future__ import absolute_import\n \n+import itertools\n+import os\n+import shutil\n+import tempfile\n+import time\n+\n+import apache_beam as beam\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileHeader\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileRecord\n from apache_beam.portability.api.beam_runner_api_pb2 import TestStreamPayload\n+from apache_beam.runners.interactive.cache_manager import CacheManager\n+from apache_beam.runners.interactive.cache_manager import SafeFastPrimitivesCoder\n+from apache_beam.testing.test_stream import ReverseTestStream\n from apache_beam.utils import timestamp\n \n \n-class StreamingCache(object):\n+class StreamingCacheSink(beam.PTransform):\n+  \"\"\"A PTransform that writes TestStreamFile(Header|Records)s to file.\n+\n+  This transform takes in an arbitrary element stream and writes the best-effort", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 24}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2NzUxNw==", "bodyText": "Can you please file a JIRA to make this more general? (I don't see anything yet that would preclude writing something that works for all runners.)", "url": "https://github.com/apache/beam/pull/11005#discussion_r387367517", "createdAt": "2020-03-04T00:02:04Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "diffHunk": "@@ -19,15 +19,298 @@\n \n from __future__ import absolute_import\n \n+import itertools\n+import os\n+import shutil\n+import tempfile\n+import time\n+\n+import apache_beam as beam\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileHeader\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileRecord\n from apache_beam.portability.api.beam_runner_api_pb2 import TestStreamPayload\n+from apache_beam.runners.interactive.cache_manager import CacheManager\n+from apache_beam.runners.interactive.cache_manager import SafeFastPrimitivesCoder\n+from apache_beam.testing.test_stream import ReverseTestStream\n from apache_beam.utils import timestamp\n \n \n-class StreamingCache(object):\n+class StreamingCacheSink(beam.PTransform):\n+  \"\"\"A PTransform that writes TestStreamFile(Header|Records)s to file.\n+\n+  This transform takes in an arbitrary element stream and writes the best-effort\n+  list of TestStream events (as TestStreamFileRecords) to file.\n+\n+  Note that this PTransform is assumed to be only run on a single machine where\n+  the following assumptions are correct: elements come in ordered, no two\n+  transforms are writing to the same file. This PTransform is assumed to only\n+  run correctly with the DirectRunner.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 30}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM2ODQ3Mw==", "bodyText": "Just using os.path.exists would be cleaner.\nstart = time.time()\nwhile not os.path.exists(path):\n  time.sleep(1)\n  if time.time() - start > timeout_timestamp_secs:\n    raise RuntimeError(...)\nreturn open(path)", "url": "https://github.com/apache/beam/pull/11005#discussion_r387368473", "createdAt": "2020-03-04T00:05:22Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "diffHunk": "@@ -19,15 +19,298 @@\n \n from __future__ import absolute_import\n \n+import itertools\n+import os\n+import shutil\n+import tempfile\n+import time\n+\n+import apache_beam as beam\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileHeader\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileRecord\n from apache_beam.portability.api.beam_runner_api_pb2 import TestStreamPayload\n+from apache_beam.runners.interactive.cache_manager import CacheManager\n+from apache_beam.runners.interactive.cache_manager import SafeFastPrimitivesCoder\n+from apache_beam.testing.test_stream import ReverseTestStream\n from apache_beam.utils import timestamp\n \n \n-class StreamingCache(object):\n+class StreamingCacheSink(beam.PTransform):\n+  \"\"\"A PTransform that writes TestStreamFile(Header|Records)s to file.\n+\n+  This transform takes in an arbitrary element stream and writes the best-effort\n+  list of TestStream events (as TestStreamFileRecords) to file.\n+\n+  Note that this PTransform is assumed to be only run on a single machine where\n+  the following assumptions are correct: elements come in ordered, no two\n+  transforms are writing to the same file. This PTransform is assumed to only\n+  run correctly with the DirectRunner.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      filename,\n+      sample_resolution_sec,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._filename = filename\n+    self._sample_resolution_sec = sample_resolution_sec\n+    self._coder = coder\n+    self._path = os.path.join(self._cache_dir, self._filename)\n+\n+  @property\n+  def path(self):\n+    \"\"\"Returns the path the sink leads to.\"\"\"\n+    return self._path\n+\n+  def expand(self, pcoll):\n+    class StreamingWriteToText(beam.DoFn):\n+      \"\"\"DoFn that performs the writing.\n+\n+      Note that the other file writing methods cannot be used in streaming\n+      contexts.\n+      \"\"\"\n+      def __init__(self, full_path, coder=SafeFastPrimitivesCoder()):\n+        self._full_path = full_path\n+        self._coder = coder\n+\n+        # Try and make the given path.\n+        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n+\n+      def start_bundle(self):\n+        # Open the file for 'append-mode' and writing 'bytes'.\n+        self._fh = open(self._full_path, 'ab')\n+\n+      def finish_bundle(self):\n+        self._fh.close()\n+\n+      def process(self, e):\n+        \"\"\"Appends the given element to the file.\n+        \"\"\"\n+        self._fh.write(self._coder.encode(e))\n+        self._fh.write(b'\\n')\n+\n+    return (\n+        pcoll\n+        | ReverseTestStream(\n+            output_tag=self._filename,\n+            sample_resolution_sec=self._sample_resolution_sec,\n+            output_format=ReverseTestStream.Format.\n+            SERIALIZED_TEST_STREAM_FILE_RECORDS,\n+            coder=self._coder)\n+        | beam.ParDo(\n+            StreamingWriteToText(full_path=self._path, coder=self._coder)))\n+\n+\n+class StreamingCacheSource:\n+  \"\"\"A class that reads and parses TestStreamFile(Header|Reader)s.\n+\n+  This source operates in the following way:\n+\n+    1. Wait for up to `timeout_secs` for the file to be available.\n+    2. Read, parse, and emit the entire contents of the file\n+    3. Wait for more events to come or until `is_cache_complete` returns True\n+    4. If there are more events, then go to 2\n+    5. Otherwise, stop emitting.\n+\n+  This class is used to read from file and send its to the TestStream via the\n+  StreamingCacheManager.Reader.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      labels,\n+      is_cache_complete=None,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._coder = coder\n+    self._labels = labels\n+    self._is_cache_complete = (\n+        is_cache_complete if is_cache_complete else lambda: True)\n+\n+  def _wait_until_file_exists(self, timeout_secs=30):\n+    \"\"\"Blocks until the file exists for a maximum of timeout_secs.\n+    \"\"\"\n+    f = None\n+    now_secs = time.time()\n+    timeout_timestamp_secs = now_secs + timeout_secs\n+\n+    # Wait for up to `timeout_secs` for the file to be available.\n+    while f is None and now_secs < timeout_timestamp_secs:\n+      now_secs = time.time()\n+      try:\n+        path = os.path.join(self._cache_dir, *self._labels)\n+        f = open(path, mode='r')\n+      except EnvironmentError as e:\n+        # For Python2 and Python3 compatibility, this checks the\n+        # EnvironmentError to see if the file exists.\n+        # TODO: Change this to a FileNotFoundError when Python3 migration is\n+        # complete.\n+        import errno\n+        if e.errno != errno.ENOENT:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 133}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM3MTU2MQ==", "bodyText": "Is it possible for this to read part of a line? Perhaps the check should be whether it ends in a newline, otherwise you have to go back and try to read more.", "url": "https://github.com/apache/beam/pull/11005#discussion_r387371561", "createdAt": "2020-03-04T00:15:45Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "diffHunk": "@@ -19,15 +19,298 @@\n \n from __future__ import absolute_import\n \n+import itertools\n+import os\n+import shutil\n+import tempfile\n+import time\n+\n+import apache_beam as beam\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileHeader\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileRecord\n from apache_beam.portability.api.beam_runner_api_pb2 import TestStreamPayload\n+from apache_beam.runners.interactive.cache_manager import CacheManager\n+from apache_beam.runners.interactive.cache_manager import SafeFastPrimitivesCoder\n+from apache_beam.testing.test_stream import ReverseTestStream\n from apache_beam.utils import timestamp\n \n \n-class StreamingCache(object):\n+class StreamingCacheSink(beam.PTransform):\n+  \"\"\"A PTransform that writes TestStreamFile(Header|Records)s to file.\n+\n+  This transform takes in an arbitrary element stream and writes the best-effort\n+  list of TestStream events (as TestStreamFileRecords) to file.\n+\n+  Note that this PTransform is assumed to be only run on a single machine where\n+  the following assumptions are correct: elements come in ordered, no two\n+  transforms are writing to the same file. This PTransform is assumed to only\n+  run correctly with the DirectRunner.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      filename,\n+      sample_resolution_sec,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._filename = filename\n+    self._sample_resolution_sec = sample_resolution_sec\n+    self._coder = coder\n+    self._path = os.path.join(self._cache_dir, self._filename)\n+\n+  @property\n+  def path(self):\n+    \"\"\"Returns the path the sink leads to.\"\"\"\n+    return self._path\n+\n+  def expand(self, pcoll):\n+    class StreamingWriteToText(beam.DoFn):\n+      \"\"\"DoFn that performs the writing.\n+\n+      Note that the other file writing methods cannot be used in streaming\n+      contexts.\n+      \"\"\"\n+      def __init__(self, full_path, coder=SafeFastPrimitivesCoder()):\n+        self._full_path = full_path\n+        self._coder = coder\n+\n+        # Try and make the given path.\n+        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n+\n+      def start_bundle(self):\n+        # Open the file for 'append-mode' and writing 'bytes'.\n+        self._fh = open(self._full_path, 'ab')\n+\n+      def finish_bundle(self):\n+        self._fh.close()\n+\n+      def process(self, e):\n+        \"\"\"Appends the given element to the file.\n+        \"\"\"\n+        self._fh.write(self._coder.encode(e))\n+        self._fh.write(b'\\n')\n+\n+    return (\n+        pcoll\n+        | ReverseTestStream(\n+            output_tag=self._filename,\n+            sample_resolution_sec=self._sample_resolution_sec,\n+            output_format=ReverseTestStream.Format.\n+            SERIALIZED_TEST_STREAM_FILE_RECORDS,\n+            coder=self._coder)\n+        | beam.ParDo(\n+            StreamingWriteToText(full_path=self._path, coder=self._coder)))\n+\n+\n+class StreamingCacheSource:\n+  \"\"\"A class that reads and parses TestStreamFile(Header|Reader)s.\n+\n+  This source operates in the following way:\n+\n+    1. Wait for up to `timeout_secs` for the file to be available.\n+    2. Read, parse, and emit the entire contents of the file\n+    3. Wait for more events to come or until `is_cache_complete` returns True\n+    4. If there are more events, then go to 2\n+    5. Otherwise, stop emitting.\n+\n+  This class is used to read from file and send its to the TestStream via the\n+  StreamingCacheManager.Reader.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      labels,\n+      is_cache_complete=None,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._coder = coder\n+    self._labels = labels\n+    self._is_cache_complete = (\n+        is_cache_complete if is_cache_complete else lambda: True)\n+\n+  def _wait_until_file_exists(self, timeout_secs=30):\n+    \"\"\"Blocks until the file exists for a maximum of timeout_secs.\n+    \"\"\"\n+    f = None\n+    now_secs = time.time()\n+    timeout_timestamp_secs = now_secs + timeout_secs\n+\n+    # Wait for up to `timeout_secs` for the file to be available.\n+    while f is None and now_secs < timeout_timestamp_secs:\n+      now_secs = time.time()\n+      try:\n+        path = os.path.join(self._cache_dir, *self._labels)\n+        f = open(path, mode='r')\n+      except EnvironmentError as e:\n+        # For Python2 and Python3 compatibility, this checks the\n+        # EnvironmentError to see if the file exists.\n+        # TODO: Change this to a FileNotFoundError when Python3 migration is\n+        # complete.\n+        import errno\n+        if e.errno != errno.ENOENT:\n+          # Raise the exception if it is not a FileNotFoundError.\n+          raise\n+        time.sleep(1)\n+    if now_secs >= timeout_timestamp_secs:\n+      raise RuntimeError(\n+          \"Timed out waiting for file '{}' to be available\".format(path))\n+    return f\n+\n+  def _emit_from_file(self, fh, tail):\n+    \"\"\"Emits the TestStreamFile(Header|Record)s from file.\n+\n+    This returns a generator to be able to read all lines from the given file.\n+    If `tail` is True, then it will wait until the cache is complete to exit.\n+    Otherwise, it will read the file only once.\n+    \"\"\"\n+    # Always read at least once to read the whole file.\n+    while True:\n+      pos = fh.tell()\n+      line = fh.readline()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM3MzA5MQ==", "bodyText": "Why is this not done in the loop below?", "url": "https://github.com/apache/beam/pull/11005#discussion_r387373091", "createdAt": "2020-03-04T00:20:53Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "diffHunk": "@@ -19,15 +19,298 @@\n \n from __future__ import absolute_import\n \n+import itertools\n+import os\n+import shutil\n+import tempfile\n+import time\n+\n+import apache_beam as beam\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileHeader\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileRecord\n from apache_beam.portability.api.beam_runner_api_pb2 import TestStreamPayload\n+from apache_beam.runners.interactive.cache_manager import CacheManager\n+from apache_beam.runners.interactive.cache_manager import SafeFastPrimitivesCoder\n+from apache_beam.testing.test_stream import ReverseTestStream\n from apache_beam.utils import timestamp\n \n \n-class StreamingCache(object):\n+class StreamingCacheSink(beam.PTransform):\n+  \"\"\"A PTransform that writes TestStreamFile(Header|Records)s to file.\n+\n+  This transform takes in an arbitrary element stream and writes the best-effort\n+  list of TestStream events (as TestStreamFileRecords) to file.\n+\n+  Note that this PTransform is assumed to be only run on a single machine where\n+  the following assumptions are correct: elements come in ordered, no two\n+  transforms are writing to the same file. This PTransform is assumed to only\n+  run correctly with the DirectRunner.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      filename,\n+      sample_resolution_sec,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._filename = filename\n+    self._sample_resolution_sec = sample_resolution_sec\n+    self._coder = coder\n+    self._path = os.path.join(self._cache_dir, self._filename)\n+\n+  @property\n+  def path(self):\n+    \"\"\"Returns the path the sink leads to.\"\"\"\n+    return self._path\n+\n+  def expand(self, pcoll):\n+    class StreamingWriteToText(beam.DoFn):\n+      \"\"\"DoFn that performs the writing.\n+\n+      Note that the other file writing methods cannot be used in streaming\n+      contexts.\n+      \"\"\"\n+      def __init__(self, full_path, coder=SafeFastPrimitivesCoder()):\n+        self._full_path = full_path\n+        self._coder = coder\n+\n+        # Try and make the given path.\n+        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n+\n+      def start_bundle(self):\n+        # Open the file for 'append-mode' and writing 'bytes'.\n+        self._fh = open(self._full_path, 'ab')\n+\n+      def finish_bundle(self):\n+        self._fh.close()\n+\n+      def process(self, e):\n+        \"\"\"Appends the given element to the file.\n+        \"\"\"\n+        self._fh.write(self._coder.encode(e))\n+        self._fh.write(b'\\n')\n+\n+    return (\n+        pcoll\n+        | ReverseTestStream(\n+            output_tag=self._filename,\n+            sample_resolution_sec=self._sample_resolution_sec,\n+            output_format=ReverseTestStream.Format.\n+            SERIALIZED_TEST_STREAM_FILE_RECORDS,\n+            coder=self._coder)\n+        | beam.ParDo(\n+            StreamingWriteToText(full_path=self._path, coder=self._coder)))\n+\n+\n+class StreamingCacheSource:\n+  \"\"\"A class that reads and parses TestStreamFile(Header|Reader)s.\n+\n+  This source operates in the following way:\n+\n+    1. Wait for up to `timeout_secs` for the file to be available.\n+    2. Read, parse, and emit the entire contents of the file\n+    3. Wait for more events to come or until `is_cache_complete` returns True\n+    4. If there are more events, then go to 2\n+    5. Otherwise, stop emitting.\n+\n+  This class is used to read from file and send its to the TestStream via the\n+  StreamingCacheManager.Reader.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      labels,\n+      is_cache_complete=None,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._coder = coder\n+    self._labels = labels\n+    self._is_cache_complete = (\n+        is_cache_complete if is_cache_complete else lambda: True)\n+\n+  def _wait_until_file_exists(self, timeout_secs=30):\n+    \"\"\"Blocks until the file exists for a maximum of timeout_secs.\n+    \"\"\"\n+    f = None\n+    now_secs = time.time()\n+    timeout_timestamp_secs = now_secs + timeout_secs\n+\n+    # Wait for up to `timeout_secs` for the file to be available.\n+    while f is None and now_secs < timeout_timestamp_secs:\n+      now_secs = time.time()\n+      try:\n+        path = os.path.join(self._cache_dir, *self._labels)\n+        f = open(path, mode='r')\n+      except EnvironmentError as e:\n+        # For Python2 and Python3 compatibility, this checks the\n+        # EnvironmentError to see if the file exists.\n+        # TODO: Change this to a FileNotFoundError when Python3 migration is\n+        # complete.\n+        import errno\n+        if e.errno != errno.ENOENT:\n+          # Raise the exception if it is not a FileNotFoundError.\n+          raise\n+        time.sleep(1)\n+    if now_secs >= timeout_timestamp_secs:\n+      raise RuntimeError(\n+          \"Timed out waiting for file '{}' to be available\".format(path))\n+    return f\n+\n+  def _emit_from_file(self, fh, tail):\n+    \"\"\"Emits the TestStreamFile(Header|Record)s from file.\n+\n+    This returns a generator to be able to read all lines from the given file.\n+    If `tail` is True, then it will wait until the cache is complete to exit.\n+    Otherwise, it will read the file only once.\n+    \"\"\"\n+    # Always read at least once to read the whole file.\n+    while True:\n+      pos = fh.tell()\n+      line = fh.readline()\n+\n+      # Check if we are at EOF.\n+      if not line:\n+        # Complete reading only when the cache is complete.\n+        if self._is_cache_complete():\n+          break\n+\n+        if not tail:\n+          break\n+\n+        # Otherwise wait for new data in the file to be written.\n+        time.sleep(0.5)\n+        fh.seek(pos)\n+      else:\n+        # The first line at pos = 0 is always the header. Read the line without\n+        # the new line.\n+        if pos == 0:\n+          header = TestStreamFileHeader()\n+          header.ParseFromString(self._coder.decode(line[:-1]))\n+          yield header\n+        else:\n+          record = TestStreamFileRecord()\n+          record.ParseFromString(self._coder.decode(line[:-1]))\n+          yield record\n+\n+  def read(self, tail):\n+    \"\"\"Reads all TestStreamFile(Header|TestStreamFileRecord)s from file.\n+\n+    This returns a generator to be able to read all lines from the given file.\n+    If `tail` is True, then it will wait until the cache is complete to exit.\n+    Otherwise, it will read the file only once.\n+    \"\"\"\n+    with self._wait_until_file_exists() as f:\n+      for e in self._emit_from_file(f, tail):\n+        yield e\n+\n+\n+class StreamingCache(CacheManager):\n   \"\"\"Abstraction that holds the logic for reading and writing to cache.\n   \"\"\"\n-  def __init__(self, readers):\n-    self._readers = readers\n+  def __init__(\n+      self, cache_dir, is_cache_complete=None, sample_resolution_sec=0.1):\n+    self._sample_resolution_sec = sample_resolution_sec\n+    self._is_cache_complete = is_cache_complete\n+\n+    if cache_dir:\n+      self._cache_dir = cache_dir\n+    else:\n+      self._cache_dir = tempfile.mkdtemp(\n+          prefix='interactive-temp-', dir=os.environ.get('TEST_TMPDIR', None))\n+\n+    # List of saved pcoders keyed by PCollection path. It is OK to keep this\n+    # list in memory because once FileBasedCacheManager object is\n+    # destroyed/re-created it loses the access to previously written cache\n+    # objects anyways even if cache_dir already exists. In other words,\n+    # it is not possible to resume execution of Beam pipeline from the\n+    # saved cache if FileBasedCacheManager has been reset.\n+    #\n+    # However, if we are to implement better cache persistence, one needs\n+    # to take care of keeping consistency between the cached PCollection\n+    # and its PCoder type.\n+    self._saved_pcoders = {}\n+    self._default_pcoder = SafeFastPrimitivesCoder()\n+\n+  def exists(self, *labels):\n+    path = os.path.join(self._cache_dir, *labels)\n+    return os.path.exists(path)\n+\n+  # TODO(srohde): Modify this to return the correct version.\n+  def read(self, *labels):\n+    \"\"\"Returns a generator to read all records from file.\n+\n+    Does not tail.\n+    \"\"\"\n+    if not self.exists(*labels):\n+      return [].__iter__(), -1\n+\n+    reader = StreamingCacheSource(\n+        self._cache_dir, labels,\n+        is_cache_complete=self._is_cache_complete).read(tail=False)\n+    header = next(reader)\n+    return StreamingCache.Reader([header], [reader]).read(), 1\n+\n+  def read_multiple(self, labels):\n+    \"\"\"Returns a generator to read all records from file.\n+\n+    Does tail until the cache is complete. This is because it is used in the\n+    TestStreamServiceController to read from file which is only used during\n+    pipeline runtime which needs to block.\n+    \"\"\"\n+    readers = [\n+        StreamingCacheSource(\n+            self._cache_dir, l,\n+            is_cache_complete=self._is_cache_complete).read(tail=True)\n+        for l in labels\n+    ]\n+    headers = [next(r) for r in readers]\n+    return StreamingCache.Reader(headers, readers).read()\n+\n+  def write(self, values, *labels):\n+    \"\"\"Writes the given values to cache.\n+    \"\"\"\n+    to_write = [v.SerializeToString() for v in values]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 257}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/62cb7a971fb8cda89f7c0c8987786ef3a170ef71", "committedDate": "2020-03-03T00:28:54Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}, "afterCommit": {"oid": "3bc5d8228b37b778a74d2c7afeca9ad88e11ba0a", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/3bc5d8228b37b778a74d2c7afeca9ad88e11ba0a", "committedDate": "2020-03-05T00:11:03Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3bc5d8228b37b778a74d2c7afeca9ad88e11ba0a", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/3bc5d8228b37b778a74d2c7afeca9ad88e11ba0a", "committedDate": "2020-03-05T00:11:03Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}, "afterCommit": {"oid": "f41c34fab25b229505d3c20e91118d79808cdbea", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/f41c34fab25b229505d3c20e91118d79808cdbea", "committedDate": "2020-03-05T00:42:25Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "f41c34fab25b229505d3c20e91118d79808cdbea", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/f41c34fab25b229505d3c20e91118d79808cdbea", "committedDate": "2020-03-05T00:42:25Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}, "afterCommit": {"oid": "2d1d5aac7eda241f1b07708b406dfc02e8e8669f", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/2d1d5aac7eda241f1b07708b406dfc02e8e8669f", "committedDate": "2020-03-05T18:40:05Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "2d1d5aac7eda241f1b07708b406dfc02e8e8669f", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/2d1d5aac7eda241f1b07708b406dfc02e8e8669f", "committedDate": "2020-03-05T18:40:05Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}, "afterCommit": {"oid": "6d65928deb814f3069dbd9154c905f9d3737caf9", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/6d65928deb814f3069dbd9154c905f9d3737caf9", "committedDate": "2020-03-05T23:52:30Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "6d65928deb814f3069dbd9154c905f9d3737caf9", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/6d65928deb814f3069dbd9154c905f9d3737caf9", "committedDate": "2020-03-05T23:52:30Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}, "afterCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/67097b0742239ef5c5ee775e69a27c0b062e1449", "committedDate": "2020-03-06T19:16:36Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcwNjAwOTI2", "url": "https://github.com/apache/beam/pull/11005#pullrequestreview-370600926", "createdAt": "2020-03-06T20:18:20Z", "commit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "state": "COMMENTED", "comments": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDoxODoyMVrOFzGN9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wNlQyMDo0OTo0NlrOFzHCEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyMzU3Mw==", "bodyText": "Why can't we just pass the [None]? Is it because None is not a string?", "url": "https://github.com/apache/beam/pull/11005#discussion_r389123573", "createdAt": "2020-03-06T20:18:21Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/test_stream_impl.py", "diffHunk": "@@ -226,17 +237,53 @@ def expand(self, pcoll):\n   def _infer_output_coder(self, input_type=None, input_coder=None):\n     return self.coder\n \n-  def _events_from_script(self, index):\n-    yield self._events[index]\n-\n-  def events(self, index):\n-    return self._events_from_script(index)\n-\n-  def begin(self):\n-    return 0\n-\n-  def end(self, index):\n-    return index >= len(self._events)\n+  @staticmethod\n+  def events_from_script(events):\n+    \"\"\"Yields the in-memory events.\n+    \"\"\"\n+    return itertools.chain(events)\n \n-  def next(self, index):\n-    return index + 1\n+  @staticmethod\n+  def events_from_rpc(endpoint, output_tags, coder):\n+    \"\"\"Yields the events received from the given endpoint.\n+    \"\"\"\n+    stub_channel = grpc.insecure_channel(endpoint)\n+    stub = beam_runner_api_pb2_grpc.TestStreamServiceStub(stub_channel)\n+\n+    # Request the PCollections that we are looking for from the service.\n+    if list(output_tags) == [None]:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyNjQ1Mg==", "bodyText": "This only holds if the file is not being written to concurrently (just tried it out). Otherwise it may get to EOF half way through a line.", "url": "https://github.com/apache/beam/pull/11005#discussion_r389126452", "createdAt": "2020-03-06T20:25:01Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "diffHunk": "@@ -19,15 +19,298 @@\n \n from __future__ import absolute_import\n \n+import itertools\n+import os\n+import shutil\n+import tempfile\n+import time\n+\n+import apache_beam as beam\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileHeader\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileRecord\n from apache_beam.portability.api.beam_runner_api_pb2 import TestStreamPayload\n+from apache_beam.runners.interactive.cache_manager import CacheManager\n+from apache_beam.runners.interactive.cache_manager import SafeFastPrimitivesCoder\n+from apache_beam.testing.test_stream import ReverseTestStream\n from apache_beam.utils import timestamp\n \n \n-class StreamingCache(object):\n+class StreamingCacheSink(beam.PTransform):\n+  \"\"\"A PTransform that writes TestStreamFile(Header|Records)s to file.\n+\n+  This transform takes in an arbitrary element stream and writes the best-effort\n+  list of TestStream events (as TestStreamFileRecords) to file.\n+\n+  Note that this PTransform is assumed to be only run on a single machine where\n+  the following assumptions are correct: elements come in ordered, no two\n+  transforms are writing to the same file. This PTransform is assumed to only\n+  run correctly with the DirectRunner.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      filename,\n+      sample_resolution_sec,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._filename = filename\n+    self._sample_resolution_sec = sample_resolution_sec\n+    self._coder = coder\n+    self._path = os.path.join(self._cache_dir, self._filename)\n+\n+  @property\n+  def path(self):\n+    \"\"\"Returns the path the sink leads to.\"\"\"\n+    return self._path\n+\n+  def expand(self, pcoll):\n+    class StreamingWriteToText(beam.DoFn):\n+      \"\"\"DoFn that performs the writing.\n+\n+      Note that the other file writing methods cannot be used in streaming\n+      contexts.\n+      \"\"\"\n+      def __init__(self, full_path, coder=SafeFastPrimitivesCoder()):\n+        self._full_path = full_path\n+        self._coder = coder\n+\n+        # Try and make the given path.\n+        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n+\n+      def start_bundle(self):\n+        # Open the file for 'append-mode' and writing 'bytes'.\n+        self._fh = open(self._full_path, 'ab')\n+\n+      def finish_bundle(self):\n+        self._fh.close()\n+\n+      def process(self, e):\n+        \"\"\"Appends the given element to the file.\n+        \"\"\"\n+        self._fh.write(self._coder.encode(e))\n+        self._fh.write(b'\\n')\n+\n+    return (\n+        pcoll\n+        | ReverseTestStream(\n+            output_tag=self._filename,\n+            sample_resolution_sec=self._sample_resolution_sec,\n+            output_format=ReverseTestStream.Format.\n+            SERIALIZED_TEST_STREAM_FILE_RECORDS,\n+            coder=self._coder)\n+        | beam.ParDo(\n+            StreamingWriteToText(full_path=self._path, coder=self._coder)))\n+\n+\n+class StreamingCacheSource:\n+  \"\"\"A class that reads and parses TestStreamFile(Header|Reader)s.\n+\n+  This source operates in the following way:\n+\n+    1. Wait for up to `timeout_secs` for the file to be available.\n+    2. Read, parse, and emit the entire contents of the file\n+    3. Wait for more events to come or until `is_cache_complete` returns True\n+    4. If there are more events, then go to 2\n+    5. Otherwise, stop emitting.\n+\n+  This class is used to read from file and send its to the TestStream via the\n+  StreamingCacheManager.Reader.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      labels,\n+      is_cache_complete=None,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._coder = coder\n+    self._labels = labels\n+    self._is_cache_complete = (\n+        is_cache_complete if is_cache_complete else lambda: True)\n+\n+  def _wait_until_file_exists(self, timeout_secs=30):\n+    \"\"\"Blocks until the file exists for a maximum of timeout_secs.\n+    \"\"\"\n+    f = None\n+    now_secs = time.time()\n+    timeout_timestamp_secs = now_secs + timeout_secs\n+\n+    # Wait for up to `timeout_secs` for the file to be available.\n+    while f is None and now_secs < timeout_timestamp_secs:\n+      now_secs = time.time()\n+      try:\n+        path = os.path.join(self._cache_dir, *self._labels)\n+        f = open(path, mode='r')\n+      except EnvironmentError as e:\n+        # For Python2 and Python3 compatibility, this checks the\n+        # EnvironmentError to see if the file exists.\n+        # TODO: Change this to a FileNotFoundError when Python3 migration is\n+        # complete.\n+        import errno\n+        if e.errno != errno.ENOENT:\n+          # Raise the exception if it is not a FileNotFoundError.\n+          raise\n+        time.sleep(1)\n+    if now_secs >= timeout_timestamp_secs:\n+      raise RuntimeError(\n+          \"Timed out waiting for file '{}' to be available\".format(path))\n+    return f\n+\n+  def _emit_from_file(self, fh, tail):\n+    \"\"\"Emits the TestStreamFile(Header|Record)s from file.\n+\n+    This returns a generator to be able to read all lines from the given file.\n+    If `tail` is True, then it will wait until the cache is complete to exit.\n+    Otherwise, it will read the file only once.\n+    \"\"\"\n+    # Always read at least once to read the whole file.\n+    while True:\n+      pos = fh.tell()\n+      line = fh.readline()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM3MTU2MQ=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 152}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyOTM4Nw==", "bodyText": "No, there should only be one TestStream per pipeline. That's the reason we introduced test streams with multiple outputs. (I also fail to see how multiple TestStreams would work, as they would all seem to share the same global context and so it seems all test stream evaluators would emit all events).", "url": "https://github.com/apache/beam/pull/11005#discussion_r389129387", "createdAt": "2020-03-06T20:32:05Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -214,11 +214,31 @@ class _TestStreamRootBundleProvider(RootBundleProvider):\n   \"\"\"\n   def get_root_bundles(self):\n     test_stream = self._applied_ptransform.transform\n+\n+    # The TestStream specification allows for multiple TestStreams in the same", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMTQ2Mw==", "bodyText": "Why is this passed via the context, instead of accessed directly on the test stream object when it is being processed?", "url": "https://github.com/apache/beam/pull/11005#discussion_r389131463", "createdAt": "2020-03-06T20:37:07Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/direct_runner.py", "diffHunk": "@@ -408,7 +410,8 @@ def visit_transform(self, applied_ptransform):\n         self.consumer_tracking_visitor.value_to_consumers,\n         self.consumer_tracking_visitor.step_names,\n         self.consumer_tracking_visitor.views,\n-        clock)\n+        clock,\n+        test_stream_visitor.endpoint)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMTg0Mg==", "bodyText": "(Regarding the comment above) we should get this from test_stream, rather than plumb it through the context.", "url": "https://github.com/apache/beam/pull/11005#discussion_r389131842", "createdAt": "2020-03-06T20:37:57Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -214,11 +214,31 @@ class _TestStreamRootBundleProvider(RootBundleProvider):\n   \"\"\"\n   def get_root_bundles(self):\n     test_stream = self._applied_ptransform.transform\n+\n+    # The TestStream specification allows for multiple TestStreams in the same\n+    # pipeline (with only one controlling the clock). Here, we use an array in\n+    # the global EvaluationContext state to keep track of the iterator for each\n+    # event stream.\n+    idx = len(self._evaluation_context._test_stream_events)\n+\n+    # If there was an endpoint defined then get the events from the\n+    # TestStreamService.\n+    if self._evaluation_context._test_stream_endpoint:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzNDMyOQ==", "bodyText": "Likewise, why are we plumbing  _test_stream_events through the context instead of creating it in _TestStreamEvaluator?", "url": "https://github.com/apache/beam/pull/11005#discussion_r389134329", "createdAt": "2020-03-06T20:43:44Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -214,11 +214,31 @@ class _TestStreamRootBundleProvider(RootBundleProvider):\n   \"\"\"\n   def get_root_bundles(self):\n     test_stream = self._applied_ptransform.transform\n+\n+    # The TestStream specification allows for multiple TestStreams in the same\n+    # pipeline (with only one controlling the clock). Here, we use an array in\n+    # the global EvaluationContext state to keep track of the iterator for each\n+    # event stream.\n+    idx = len(self._evaluation_context._test_stream_events)\n+\n+    # If there was an endpoint defined then get the events from the\n+    # TestStreamService.\n+    if self._evaluation_context._test_stream_endpoint:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMTg0Mg=="}, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzNjkxNA==", "bodyText": "Why did this have to move out of start_bundle? (Similarly, why do we need is_done now and didn't before?)", "url": "https://github.com/apache/beam/pull/11005#discussion_r389136914", "createdAt": "2020-03-06T20:49:46Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -500,23 +523,21 @@ def __init__(\n         input_committed_bundle,\n         side_inputs)\n     self.test_stream = applied_ptransform.transform\n+    self.event_index = 0", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 53}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcwNzA0NDc5", "url": "https://github.com/apache/beam/pull/11005#pullrequestreview-370704479", "createdAt": "2020-03-07T00:53:52Z", "commit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "state": "COMMENTED", "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wN1QwMDo1Mzo1MlrOFzLaLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0wN1QwMTowNzoyN1rOFzLiIg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTIwODYyMQ==", "bodyText": "What's the convention for this list being empty then? (And if it can be empty, in what cases do we have to provide it? Or should we explicitly be turning None to 'None'?", "url": "https://github.com/apache/beam/pull/11005#discussion_r389208621", "createdAt": "2020-03-07T00:53:52Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/test_stream_impl.py", "diffHunk": "@@ -226,17 +237,53 @@ def expand(self, pcoll):\n   def _infer_output_coder(self, input_type=None, input_coder=None):\n     return self.coder\n \n-  def _events_from_script(self, index):\n-    yield self._events[index]\n-\n-  def events(self, index):\n-    return self._events_from_script(index)\n-\n-  def begin(self):\n-    return 0\n-\n-  def end(self, index):\n-    return index >= len(self._events)\n+  @staticmethod\n+  def events_from_script(events):\n+    \"\"\"Yields the in-memory events.\n+    \"\"\"\n+    return itertools.chain(events)\n \n-  def next(self, index):\n-    return index + 1\n+  @staticmethod\n+  def events_from_rpc(endpoint, output_tags, coder):\n+    \"\"\"Yields the events received from the given endpoint.\n+    \"\"\"\n+    stub_channel = grpc.insecure_channel(endpoint)\n+    stub = beam_runner_api_pb2_grpc.TestStreamServiceStub(stub_channel)\n+\n+    # Request the PCollections that we are looking for from the service.\n+    if list(output_tags) == [None]:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEyMzU3Mw=="}, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 57}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTIwOTAwNQ==", "bodyText": "I'm not convinced that storing it on the global context is an improvement on keeping it on the bundle that gets passed back to the (new) _TestStreamEvaluator instance. It seems harder to track who owns/accesses it (and also that it only gets written to once). I agree the original way of doing things may not have been ideal either.", "url": "https://github.com/apache/beam/pull/11005#discussion_r389209005", "createdAt": "2020-03-07T00:56:17Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -214,11 +214,31 @@ class _TestStreamRootBundleProvider(RootBundleProvider):\n   \"\"\"\n   def get_root_bundles(self):\n     test_stream = self._applied_ptransform.transform\n+\n+    # The TestStream specification allows for multiple TestStreams in the same\n+    # pipeline (with only one controlling the clock). Here, we use an array in\n+    # the global EvaluationContext state to keep track of the iterator for each\n+    # event stream.\n+    idx = len(self._evaluation_context._test_stream_events)\n+\n+    # If there was an endpoint defined then get the events from the\n+    # TestStreamService.\n+    if self._evaluation_context._test_stream_endpoint:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMTg0Mg=="}, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 13}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTIwOTgzNg==", "bodyText": "Please leave the types (it makes it easier to follow).", "url": "https://github.com/apache/beam/pull/11005#discussion_r389209836", "createdAt": "2020-03-07T01:01:24Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/interactive/cache_manager.py", "diffHunk": "@@ -69,8 +69,8 @@ def read(self, *labels):\n       *labels: List of labels for PCollection instance.\n \n     Returns:\n-      Tuple[List[Any], int]: A tuple containing a list of items in the\n-        PCollection and the version number.\n+      A tuple containing an iterator for the items in the PCollection and the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTIwOTk2Nw==", "bodyText": "Not changed?", "url": "https://github.com/apache/beam/pull/11005#discussion_r389209967", "createdAt": "2020-03-07T01:02:25Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/direct_runner.py", "diffHunk": "@@ -408,7 +410,8 @@ def visit_transform(self, applied_ptransform):\n         self.consumer_tracking_visitor.value_to_consumers,\n         self.consumer_tracking_visitor.step_names,\n         self.consumer_tracking_visitor.views,\n-        clock)\n+        clock,\n+        test_stream_visitor.endpoint)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTEzMTQ2Mw=="}, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTIxMDIwMQ==", "bodyText": "Is this ever non-empty? It seems repeated calls would append everything. (Or is it populated elsewhere?)", "url": "https://github.com/apache/beam/pull/11005#discussion_r389210201", "createdAt": "2020-03-07T01:04:09Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/direct/transform_evaluator.py", "diffHunk": "@@ -214,11 +214,31 @@ class _TestStreamRootBundleProvider(RootBundleProvider):\n   \"\"\"\n   def get_root_bundles(self):\n     test_stream = self._applied_ptransform.transform\n+\n+    # The TestStream specification allows for multiple TestStreams in the same\n+    # pipeline (with only one controlling the clock). Here, we use an array in\n+    # the global EvaluationContext state to keep track of the iterator for each\n+    # event stream.\n+    idx = len(self._evaluation_context._test_stream_events)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449"}, "originalPosition": 9}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4OTIxMDY1OA==", "bodyText": "This was not fixed.", "url": "https://github.com/apache/beam/pull/11005#discussion_r389210658", "createdAt": "2020-03-07T01:07:27Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "diffHunk": "@@ -19,15 +19,298 @@\n \n from __future__ import absolute_import\n \n+import itertools\n+import os\n+import shutil\n+import tempfile\n+import time\n+\n+import apache_beam as beam\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileHeader\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileRecord\n from apache_beam.portability.api.beam_runner_api_pb2 import TestStreamPayload\n+from apache_beam.runners.interactive.cache_manager import CacheManager\n+from apache_beam.runners.interactive.cache_manager import SafeFastPrimitivesCoder\n+from apache_beam.testing.test_stream import ReverseTestStream\n from apache_beam.utils import timestamp\n \n \n-class StreamingCache(object):\n+class StreamingCacheSink(beam.PTransform):\n+  \"\"\"A PTransform that writes TestStreamFile(Header|Records)s to file.\n+\n+  This transform takes in an arbitrary element stream and writes the best-effort\n+  list of TestStream events (as TestStreamFileRecords) to file.\n+\n+  Note that this PTransform is assumed to be only run on a single machine where\n+  the following assumptions are correct: elements come in ordered, no two\n+  transforms are writing to the same file. This PTransform is assumed to only\n+  run correctly with the DirectRunner.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      filename,\n+      sample_resolution_sec,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._filename = filename\n+    self._sample_resolution_sec = sample_resolution_sec\n+    self._coder = coder\n+    self._path = os.path.join(self._cache_dir, self._filename)\n+\n+  @property\n+  def path(self):\n+    \"\"\"Returns the path the sink leads to.\"\"\"\n+    return self._path\n+\n+  def expand(self, pcoll):\n+    class StreamingWriteToText(beam.DoFn):\n+      \"\"\"DoFn that performs the writing.\n+\n+      Note that the other file writing methods cannot be used in streaming\n+      contexts.\n+      \"\"\"\n+      def __init__(self, full_path, coder=SafeFastPrimitivesCoder()):\n+        self._full_path = full_path\n+        self._coder = coder\n+\n+        # Try and make the given path.\n+        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n+\n+      def start_bundle(self):\n+        # Open the file for 'append-mode' and writing 'bytes'.\n+        self._fh = open(self._full_path, 'ab')\n+\n+      def finish_bundle(self):\n+        self._fh.close()\n+\n+      def process(self, e):\n+        \"\"\"Appends the given element to the file.\n+        \"\"\"\n+        self._fh.write(self._coder.encode(e))\n+        self._fh.write(b'\\n')\n+\n+    return (\n+        pcoll\n+        | ReverseTestStream(\n+            output_tag=self._filename,\n+            sample_resolution_sec=self._sample_resolution_sec,\n+            output_format=ReverseTestStream.Format.\n+            SERIALIZED_TEST_STREAM_FILE_RECORDS,\n+            coder=self._coder)\n+        | beam.ParDo(\n+            StreamingWriteToText(full_path=self._path, coder=self._coder)))\n+\n+\n+class StreamingCacheSource:\n+  \"\"\"A class that reads and parses TestStreamFile(Header|Reader)s.\n+\n+  This source operates in the following way:\n+\n+    1. Wait for up to `timeout_secs` for the file to be available.\n+    2. Read, parse, and emit the entire contents of the file\n+    3. Wait for more events to come or until `is_cache_complete` returns True\n+    4. If there are more events, then go to 2\n+    5. Otherwise, stop emitting.\n+\n+  This class is used to read from file and send its to the TestStream via the\n+  StreamingCacheManager.Reader.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      labels,\n+      is_cache_complete=None,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._coder = coder\n+    self._labels = labels\n+    self._is_cache_complete = (\n+        is_cache_complete if is_cache_complete else lambda: True)\n+\n+  def _wait_until_file_exists(self, timeout_secs=30):\n+    \"\"\"Blocks until the file exists for a maximum of timeout_secs.\n+    \"\"\"\n+    f = None\n+    now_secs = time.time()\n+    timeout_timestamp_secs = now_secs + timeout_secs\n+\n+    # Wait for up to `timeout_secs` for the file to be available.\n+    while f is None and now_secs < timeout_timestamp_secs:\n+      now_secs = time.time()\n+      try:\n+        path = os.path.join(self._cache_dir, *self._labels)\n+        f = open(path, mode='r')\n+      except EnvironmentError as e:\n+        # For Python2 and Python3 compatibility, this checks the\n+        # EnvironmentError to see if the file exists.\n+        # TODO: Change this to a FileNotFoundError when Python3 migration is\n+        # complete.\n+        import errno\n+        if e.errno != errno.ENOENT:\n+          # Raise the exception if it is not a FileNotFoundError.\n+          raise\n+        time.sleep(1)\n+    if now_secs >= timeout_timestamp_secs:\n+      raise RuntimeError(\n+          \"Timed out waiting for file '{}' to be available\".format(path))\n+    return f\n+\n+  def _emit_from_file(self, fh, tail):\n+    \"\"\"Emits the TestStreamFile(Header|Record)s from file.\n+\n+    This returns a generator to be able to read all lines from the given file.\n+    If `tail` is True, then it will wait until the cache is complete to exit.\n+    Otherwise, it will read the file only once.\n+    \"\"\"\n+    # Always read at least once to read the whole file.\n+    while True:\n+      pos = fh.tell()\n+      line = fh.readline()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM3MTU2MQ=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 152}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "67097b0742239ef5c5ee775e69a27c0b062e1449", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/67097b0742239ef5c5ee775e69a27c0b062e1449", "committedDate": "2020-03-06T19:16:36Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}, "afterCommit": {"oid": "7b0565167699db8be58cf15c047db1c0e28832a7", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/7b0565167699db8be58cf15c047db1c0e28832a7", "committedDate": "2020-03-09T18:39:59Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7b0565167699db8be58cf15c047db1c0e28832a7", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/7b0565167699db8be58cf15c047db1c0e28832a7", "committedDate": "2020-03-09T18:39:59Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}, "afterCommit": {"oid": "8ff732323d2816b6d2e2968a37371eb897e01249", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/8ff732323d2816b6d2e2968a37371eb897e01249", "committedDate": "2020-03-10T18:16:21Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "8ff732323d2816b6d2e2968a37371eb897e01249", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/8ff732323d2816b6d2e2968a37371eb897e01249", "committedDate": "2020-03-10T18:16:21Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}, "afterCommit": {"oid": "3f520baf7ab531d3b913e92079affb2319ada1f3", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/3f520baf7ab531d3b913e92079affb2319ada1f3", "committedDate": "2020-03-10T18:31:41Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3f520baf7ab531d3b913e92079affb2319ada1f3", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/3f520baf7ab531d3b913e92079affb2319ada1f3", "committedDate": "2020-03-10T18:31:41Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}, "afterCommit": {"oid": "7a6a4c6e9b6cb19bd5e21d2a5156a77e3b3c9b1c", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/7a6a4c6e9b6cb19bd5e21d2a5156a77e3b3c9b1c", "committedDate": "2020-03-10T20:04:44Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3MzcyOTcyOTk3", "url": "https://github.com/apache/beam/pull/11005#pullrequestreview-372972997", "createdAt": "2020-03-11T17:26:29Z", "commit": {"oid": "7a6a4c6e9b6cb19bd5e21d2a5156a77e3b3c9b1c"}, "state": "COMMENTED", "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxNzoyNjozMFrOF1BPxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0xMVQxNzo1MjozNVrOF1CU4A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEzOTI3MQ==", "bodyText": "it seems like you can just return the result of self._reader_class(...) all the way. This is a PTransform, so it is consistent with the itnerface (instead of re-wrapping the source / sink)", "url": "https://github.com/apache/beam/pull/11005#discussion_r391139271", "createdAt": "2020-03-11T17:26:30Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/runners/interactive/cache_manager.py", "diffHunk": "@@ -167,20 +196,38 @@ def load_pcoder(self, *labels):\n         self._saved_pcoders[self._path(*labels)])\n \n   def read(self, *labels):\n+    # Return an iterator to an empty list if it doesn't exist.\n     if not self.exists(*labels):\n-      return [], -1\n+      return iter([]), -1\n \n-    source = self.source(*labels)\n+    # Otherwise, return a generator to the cached PCollection.\n+    source = self._source(*labels)\n     range_tracker = source.get_range_tracker(None, None)\n-    result = list(source.read(range_tracker))\n+    reader = source.read(range_tracker)\n     version = self._latest_version(*labels)\n-    return result, version\n+    return reader, version\n+\n+  def write(self, values, *labels):\n+    sink = self._sink(*labels)\n+    path = self._path(*labels)\n+\n+    init_result = sink.initialize_write()\n+    writer = sink.open_writer(init_result, path)\n+    for v in values:\n+      writer.write(v)\n+    writer.close()\n \n   def source(self, *labels):\n+    return beam.io.Read(self._source(*labels))\n+\n+  def sink(self, labels):\n+    return beam.io.Write(self._sink(*labels))\n+\n+  def _source(self, *labels):\n     return self._reader_class(\n         self._glob_path(*labels), coder=self.load_pcoder(*labels))._source", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a6a4c6e9b6cb19bd5e21d2a5156a77e3b3c9b1c"}, "originalPosition": 125}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTEzOTQyNQ==", "bodyText": "it seems like you can just return the result of self._writer_class(...) all the way. This is a PTransform, so it is consistent with the itnerface (instead of re-wrapping the source / sink)", "url": "https://github.com/apache/beam/pull/11005#discussion_r391139425", "createdAt": "2020-03-11T17:26:42Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/runners/interactive/cache_manager.py", "diffHunk": "@@ -167,20 +196,38 @@ def load_pcoder(self, *labels):\n         self._saved_pcoders[self._path(*labels)])\n \n   def read(self, *labels):\n+    # Return an iterator to an empty list if it doesn't exist.\n     if not self.exists(*labels):\n-      return [], -1\n+      return iter([]), -1\n \n-    source = self.source(*labels)\n+    # Otherwise, return a generator to the cached PCollection.\n+    source = self._source(*labels)\n     range_tracker = source.get_range_tracker(None, None)\n-    result = list(source.read(range_tracker))\n+    reader = source.read(range_tracker)\n     version = self._latest_version(*labels)\n-    return result, version\n+    return reader, version\n+\n+  def write(self, values, *labels):\n+    sink = self._sink(*labels)\n+    path = self._path(*labels)\n+\n+    init_result = sink.initialize_write()\n+    writer = sink.open_writer(init_result, path)\n+    for v in values:\n+      writer.write(v)\n+    writer.close()\n \n   def source(self, *labels):\n+    return beam.io.Read(self._source(*labels))\n+\n+  def sink(self, labels):\n+    return beam.io.Write(self._sink(*labels))\n+\n+  def _source(self, *labels):\n     return self._reader_class(\n         self._glob_path(*labels), coder=self.load_pcoder(*labels))._source\n \n-  def sink(self, *labels):\n+  def _sink(self, *labels):\n     return self._writer_class(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "7a6a4c6e9b6cb19bd5e21d2a5156a77e3b3c9b1c"}, "originalPosition": 129}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5MTE1Njk2MA==", "bodyText": "We've discussed this. Sam will add the fix by re-reading from the initial position.", "url": "https://github.com/apache/beam/pull/11005#discussion_r391156960", "createdAt": "2020-03-11T17:52:35Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/runners/interactive/caching/streaming_cache.py", "diffHunk": "@@ -19,15 +19,298 @@\n \n from __future__ import absolute_import\n \n+import itertools\n+import os\n+import shutil\n+import tempfile\n+import time\n+\n+import apache_beam as beam\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileHeader\n+from apache_beam.portability.api.beam_interactive_api_pb2 import TestStreamFileRecord\n from apache_beam.portability.api.beam_runner_api_pb2 import TestStreamPayload\n+from apache_beam.runners.interactive.cache_manager import CacheManager\n+from apache_beam.runners.interactive.cache_manager import SafeFastPrimitivesCoder\n+from apache_beam.testing.test_stream import ReverseTestStream\n from apache_beam.utils import timestamp\n \n \n-class StreamingCache(object):\n+class StreamingCacheSink(beam.PTransform):\n+  \"\"\"A PTransform that writes TestStreamFile(Header|Records)s to file.\n+\n+  This transform takes in an arbitrary element stream and writes the best-effort\n+  list of TestStream events (as TestStreamFileRecords) to file.\n+\n+  Note that this PTransform is assumed to be only run on a single machine where\n+  the following assumptions are correct: elements come in ordered, no two\n+  transforms are writing to the same file. This PTransform is assumed to only\n+  run correctly with the DirectRunner.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      filename,\n+      sample_resolution_sec,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._filename = filename\n+    self._sample_resolution_sec = sample_resolution_sec\n+    self._coder = coder\n+    self._path = os.path.join(self._cache_dir, self._filename)\n+\n+  @property\n+  def path(self):\n+    \"\"\"Returns the path the sink leads to.\"\"\"\n+    return self._path\n+\n+  def expand(self, pcoll):\n+    class StreamingWriteToText(beam.DoFn):\n+      \"\"\"DoFn that performs the writing.\n+\n+      Note that the other file writing methods cannot be used in streaming\n+      contexts.\n+      \"\"\"\n+      def __init__(self, full_path, coder=SafeFastPrimitivesCoder()):\n+        self._full_path = full_path\n+        self._coder = coder\n+\n+        # Try and make the given path.\n+        os.makedirs(os.path.dirname(full_path), exist_ok=True)\n+\n+      def start_bundle(self):\n+        # Open the file for 'append-mode' and writing 'bytes'.\n+        self._fh = open(self._full_path, 'ab')\n+\n+      def finish_bundle(self):\n+        self._fh.close()\n+\n+      def process(self, e):\n+        \"\"\"Appends the given element to the file.\n+        \"\"\"\n+        self._fh.write(self._coder.encode(e))\n+        self._fh.write(b'\\n')\n+\n+    return (\n+        pcoll\n+        | ReverseTestStream(\n+            output_tag=self._filename,\n+            sample_resolution_sec=self._sample_resolution_sec,\n+            output_format=ReverseTestStream.Format.\n+            SERIALIZED_TEST_STREAM_FILE_RECORDS,\n+            coder=self._coder)\n+        | beam.ParDo(\n+            StreamingWriteToText(full_path=self._path, coder=self._coder)))\n+\n+\n+class StreamingCacheSource:\n+  \"\"\"A class that reads and parses TestStreamFile(Header|Reader)s.\n+\n+  This source operates in the following way:\n+\n+    1. Wait for up to `timeout_secs` for the file to be available.\n+    2. Read, parse, and emit the entire contents of the file\n+    3. Wait for more events to come or until `is_cache_complete` returns True\n+    4. If there are more events, then go to 2\n+    5. Otherwise, stop emitting.\n+\n+  This class is used to read from file and send its to the TestStream via the\n+  StreamingCacheManager.Reader.\n+  \"\"\"\n+  def __init__(\n+      self,\n+      cache_dir,\n+      labels,\n+      is_cache_complete=None,\n+      coder=SafeFastPrimitivesCoder()):\n+    self._cache_dir = cache_dir\n+    self._coder = coder\n+    self._labels = labels\n+    self._is_cache_complete = (\n+        is_cache_complete if is_cache_complete else lambda: True)\n+\n+  def _wait_until_file_exists(self, timeout_secs=30):\n+    \"\"\"Blocks until the file exists for a maximum of timeout_secs.\n+    \"\"\"\n+    f = None\n+    now_secs = time.time()\n+    timeout_timestamp_secs = now_secs + timeout_secs\n+\n+    # Wait for up to `timeout_secs` for the file to be available.\n+    while f is None and now_secs < timeout_timestamp_secs:\n+      now_secs = time.time()\n+      try:\n+        path = os.path.join(self._cache_dir, *self._labels)\n+        f = open(path, mode='r')\n+      except EnvironmentError as e:\n+        # For Python2 and Python3 compatibility, this checks the\n+        # EnvironmentError to see if the file exists.\n+        # TODO: Change this to a FileNotFoundError when Python3 migration is\n+        # complete.\n+        import errno\n+        if e.errno != errno.ENOENT:\n+          # Raise the exception if it is not a FileNotFoundError.\n+          raise\n+        time.sleep(1)\n+    if now_secs >= timeout_timestamp_secs:\n+      raise RuntimeError(\n+          \"Timed out waiting for file '{}' to be available\".format(path))\n+    return f\n+\n+  def _emit_from_file(self, fh, tail):\n+    \"\"\"Emits the TestStreamFile(Header|Record)s from file.\n+\n+    This returns a generator to be able to read all lines from the given file.\n+    If `tail` is True, then it will wait until the cache is complete to exit.\n+    Otherwise, it will read the file only once.\n+    \"\"\"\n+    # Always read at least once to read the whole file.\n+    while True:\n+      pos = fh.tell()\n+      line = fh.readline()", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NzM3MTU2MQ=="}, "originalCommit": {"oid": "62cb7a971fb8cda89f7c0c8987786ef3a170ef71"}, "originalPosition": 152}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "7a6a4c6e9b6cb19bd5e21d2a5156a77e3b3c9b1c", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/7a6a4c6e9b6cb19bd5e21d2a5156a77e3b3c9b1c", "committedDate": "2020-03-10T20:04:44Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}, "afterCommit": {"oid": "af683924b87a0338f751a3a2ff38cf04aff4cad9", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/af683924b87a0338f751a3a2ff38cf04aff4cad9", "committedDate": "2020-03-11T18:48:01Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "af683924b87a0338f751a3a2ff38cf04aff4cad9", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/af683924b87a0338f751a3a2ff38cf04aff4cad9", "committedDate": "2020-03-11T18:48:01Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}, "afterCommit": {"oid": "3b38f88649f86ae3fccff554e57eaf2dde897536", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/3b38f88649f86ae3fccff554e57eaf2dde897536", "committedDate": "2020-03-11T21:18:57Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c69d409429cf4dd3234667150458f7777b5b7f4b", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/c69d409429cf4dd3234667150458f7777b5b7f4b", "committedDate": "2020-03-11T23:10:55Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "3b38f88649f86ae3fccff554e57eaf2dde897536", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/3b38f88649f86ae3fccff554e57eaf2dde897536", "committedDate": "2020-03-11T21:18:57Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}, "afterCommit": {"oid": "c69d409429cf4dd3234667150458f7777b5b7f4b", "author": {"user": {"login": "rohdesamuel", "name": "Sam sam"}}, "url": "https://github.com/apache/beam/commit/c69d409429cf4dd3234667150458f7777b5b7f4b", "committedDate": "2020-03-11T23:10:55Z", "message": "[BEAM-8335] Modify the StreamingCache to subclass the CacheManager\n\nChange-Id: Ib61aa3fac53d9109178744e11eeebe5c5da0929c"}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3012, "cost": 1, "resetAt": "2021-10-28T17:48:14Z"}}}