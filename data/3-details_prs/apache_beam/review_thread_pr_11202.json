{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzkyNzA1OTk5", "number": 11202, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQxODoyOTo1OVrODrWFtw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQxODoyOTo1OVrODrWFtw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ2Nzc3MjcxOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/portability/fn_api_runner/execution.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQxODoyOTo1OVrOF7o2Lw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQxODo1NDoxNFrOF7pxkQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODA3OTUzNQ==", "bodyText": "can we rename it to get_input_coder_impl? coder and coder_impl are different, with current name, it would introduce some confusions.", "url": "https://github.com/apache/beam/pull/11202#discussion_r398079535", "createdAt": "2020-03-25T18:29:59Z", "author": {"login": "Hannah-Jiang"}, "path": "sdks/python/apache_beam/runners/portability/fn_api_runner/execution.py", "diffHunk": "@@ -0,0 +1,337 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"Set of utilities for execution of a pipeline by the FnApiRunner.\"\"\"\n+\n+from __future__ import absolute_import\n+\n+import collections\n+import itertools\n+\n+from typing_extensions import Protocol\n+\n+from apache_beam import coders\n+from apache_beam.coders.coder_impl import create_InputStream\n+from apache_beam.coders.coder_impl import create_OutputStream\n+from apache_beam.portability import common_urns\n+from apache_beam.portability.api import beam_fn_api_pb2\n+from apache_beam.runners.portability.fn_api_runner.translations import only_element\n+from apache_beam.runners.portability.fn_api_runner.translations import split_buffer_id\n+from apache_beam.runners.worker import bundle_processor\n+from apache_beam.transforms import trigger\n+from apache_beam.transforms.window import GlobalWindow\n+from apache_beam.transforms.window import GlobalWindows\n+from apache_beam.utils import windowed_value\n+\n+\n+class Buffer(Protocol):\n+  def __iter__(self):\n+    # type: () -> Iterator[bytes]\n+    pass\n+\n+  def append(self, item):\n+    # type: (bytes) -> None\n+    pass\n+\n+\n+class PartitionableBuffer(Buffer, Protocol):\n+  def partition(self, n):\n+    # type: (int) -> List[List[bytes]]\n+    pass\n+\n+\n+class ListBuffer(object):\n+  \"\"\"Used to support parititioning of a list.\"\"\"\n+  def __init__(self, coder_impl):\n+    self._coder_impl = coder_impl\n+    self._inputs = []  # type: List[bytes]\n+    self._grouped_output = None\n+    self.cleared = False\n+\n+  def append(self, element):\n+    # type: (bytes) -> None\n+    if self.cleared:\n+      raise RuntimeError('Trying to append to a cleared ListBuffer.')\n+    if self._grouped_output:\n+      raise RuntimeError('ListBuffer append after read.')\n+    self._inputs.append(element)\n+\n+  def partition(self, n):\n+    # type: (int) -> List[List[bytes]]\n+    if self.cleared:\n+      raise RuntimeError('Trying to partition a cleared ListBuffer.')\n+    if len(self._inputs) >= n or len(self._inputs) == 0:\n+      return [self._inputs[k::n] for k in range(n)]\n+    else:\n+      if not self._grouped_output:\n+        output_stream_list = [create_OutputStream() for _ in range(n)]\n+        idx = 0\n+        for input in self._inputs:\n+          input_stream = create_InputStream(input)\n+          while input_stream.size() > 0:\n+            decoded_value = self._coder_impl.decode_from_stream(\n+                input_stream, True)\n+            self._coder_impl.encode_to_stream(\n+                decoded_value, output_stream_list[idx], True)\n+            idx = (idx + 1) % n\n+        self._grouped_output = [[output_stream.get()]\n+                                for output_stream in output_stream_list]\n+      return self._grouped_output\n+\n+  def __iter__(self):\n+    # type: () -> Iterator[bytes]\n+    if self.cleared:\n+      raise RuntimeError('Trying to iterate through a cleared ListBuffer.')\n+    return iter(self._inputs)\n+\n+  def clear(self):\n+    # type: () -> None\n+    self.cleared = True\n+    self._inputs = []\n+    self._grouped_output = None\n+\n+\n+class GroupingBuffer(object):\n+  \"\"\"Used to accumulate groupded (shuffled) results.\"\"\"\n+  def __init__(self,\n+               pre_grouped_coder,  # type: coders.Coder\n+               post_grouped_coder,  # type: coders.Coder\n+               windowing\n+              ):\n+    # type: (...) -> None\n+    self._key_coder = pre_grouped_coder.key_coder()\n+    self._pre_grouped_coder = pre_grouped_coder\n+    self._post_grouped_coder = post_grouped_coder\n+    self._table = collections.defaultdict(\n+        list)  # type: DefaultDict[bytes, List[Any]]\n+    self._windowing = windowing\n+    self._grouped_output = None  # type: Optional[List[List[bytes]]]\n+\n+  def append(self, elements_data):\n+    # type: (bytes) -> None\n+    if self._grouped_output:\n+      raise RuntimeError('Grouping table append after read.')\n+    input_stream = create_InputStream(elements_data)\n+    coder_impl = self._pre_grouped_coder.get_impl()\n+    key_coder_impl = self._key_coder.get_impl()\n+    # TODO(robertwb): We could optimize this even more by using a\n+    # window-dropping coder for the data plane.\n+    is_trivial_windowing = self._windowing.is_default()\n+    while input_stream.size() > 0:\n+      windowed_key_value = coder_impl.decode_from_stream(input_stream, True)\n+      key, value = windowed_key_value.value\n+      self._table[key_coder_impl.encode(key)].append(\n+          value if is_trivial_windowing else windowed_key_value.\n+          with_value(value))\n+\n+  def partition(self, n):\n+    # type: (int) -> List[List[bytes]]\n+\n+    \"\"\" It is used to partition _GroupingBuffer to N parts. Once it is\n+    partitioned, it would not be re-partitioned with diff N. Re-partition\n+    is not supported now.\n+    \"\"\"\n+    if not self._grouped_output:\n+      if self._windowing.is_default():\n+        globally_window = GlobalWindows.windowed_value(\n+            None,\n+            timestamp=GlobalWindow().max_timestamp(),\n+            pane_info=windowed_value.PaneInfo(\n+                is_first=True,\n+                is_last=True,\n+                timing=windowed_value.PaneInfoTiming.ON_TIME,\n+                index=0,\n+                nonspeculative_index=0)).with_value\n+        windowed_key_values = lambda key, values: [\n+            globally_window((key, values))]\n+      else:\n+        # TODO(pabloem, BEAM-7514): Trigger driver needs access to the clock\n+        #   note that this only comes through if windowing is default - but what\n+        #   about having multiple firings on the global window.\n+        #   May need to revise.\n+        trigger_driver = trigger.create_trigger_driver(self._windowing, True)\n+        windowed_key_values = trigger_driver.process_entire_key\n+      coder_impl = self._post_grouped_coder.get_impl()\n+      key_coder_impl = self._key_coder.get_impl()\n+      self._grouped_output = [[] for _ in range(n)]\n+      output_stream_list = [create_OutputStream() for _ in range(n)]\n+      for idx, (encoded_key, windowed_values) in enumerate(self._table.items()):\n+        key = key_coder_impl.decode(encoded_key)\n+        for wkvs in windowed_key_values(key, windowed_values):\n+          coder_impl.encode_to_stream(wkvs, output_stream_list[idx % n], True)\n+      for ix, output_stream in enumerate(output_stream_list):\n+        self._grouped_output[ix] = [output_stream.get()]\n+      self._table.clear()\n+    return self._grouped_output\n+\n+  def __iter__(self):\n+    # type: () -> Iterator[bytes]\n+\n+    \"\"\" Since partition() returns a list of lists, add this __iter__ to return\n+    a list to simplify code when we need to iterate through ALL elements of\n+    _GroupingBuffer.\n+    \"\"\"\n+    return itertools.chain(*self.partition(1))\n+\n+\n+class WindowGroupingBuffer(object):\n+  \"\"\"Used to partition windowed side inputs.\"\"\"\n+  def __init__(\n+      self,\n+      access_pattern,\n+      coder  # type: coders.WindowedValueCoder\n+  ):\n+    # type: (...) -> None\n+    # Here's where we would use a different type of partitioning\n+    # (e.g. also by key) for a different access pattern.\n+    if access_pattern.urn == common_urns.side_inputs.ITERABLE.urn:\n+      self._kv_extractor = lambda value: ('', value)\n+      self._key_coder = coders.SingletonCoder('')  # type: coders.Coder\n+      self._value_coder = coder.wrapped_value_coder\n+    elif access_pattern.urn == common_urns.side_inputs.MULTIMAP.urn:\n+      self._kv_extractor = lambda value: value\n+      self._key_coder = coder.wrapped_value_coder.key_coder()\n+      self._value_coder = (coder.wrapped_value_coder.value_coder())\n+    else:\n+      raise ValueError(\"Unknown access pattern: '%s'\" % access_pattern.urn)\n+    self._windowed_value_coder = coder\n+    self._window_coder = coder.window_coder\n+    self._values_by_window = collections.defaultdict(\n+        list)  # type: DefaultDict[Tuple[str, BoundedWindow], List[Any]]\n+\n+  def append(self, elements_data):\n+    # type: (bytes) -> None\n+    input_stream = create_InputStream(elements_data)\n+    while input_stream.size() > 0:\n+      windowed_val_coder_impl = self._windowed_value_coder.get_impl(\n+      )  # type: WindowedValueCoderImpl\n+      windowed_value = windowed_val_coder_impl.decode_from_stream(\n+          input_stream, True)\n+      key, value = self._kv_extractor(windowed_value.value)\n+      for window in windowed_value.windows:\n+        self._values_by_window[key, window].append(value)\n+\n+  def encoded_items(self):\n+    # type: () -> Iterator[Tuple[bytes, bytes, bytes]]\n+    value_coder_impl = self._value_coder.get_impl()\n+    key_coder_impl = self._key_coder.get_impl()\n+    for (key, window), values in self._values_by_window.items():\n+      encoded_window = self._window_coder.encode(window)\n+      encoded_key = key_coder_impl.encode_nested(key)\n+      output_stream = create_OutputStream()\n+      for value in values:\n+        value_coder_impl.encode_to_stream(value, output_stream, True)\n+      yield encoded_key, encoded_window, output_stream.get()\n+\n+\n+class FnApiRunnerExecutionContext(object):\n+  \"\"\"\n+ :var pcoll_buffers: (collections.defaultdict of str: list): Mapping of\n+       PCollection IDs to list that functions as buffer for the\n+       ``beam.PCollection``.\n+ \"\"\"\n+  def __init__(self,\n+      worker_handler_factory,  # type: Callable[[Optional[str], int], List[WorkerHandler]]\n+      pipeline_components,  # type: beam_runner_api_pb2.Components\n+      safe_coders,\n+               ):\n+    \"\"\"\n+    :param worker_handler_factory: A ``callable`` that takes in an environment\n+        id and a number of workers, and returns a list of ``WorkerHandler``s.\n+    :param pipeline_components:  (beam_runner_api_pb2.Components): TODO\n+    :param safe_coders:\n+    \"\"\"\n+    self.pcoll_buffers = {}  # type: MutableMapping[bytes, PartitionableBuffer]\n+    self.worker_handler_factory = worker_handler_factory\n+    self.pipeline_components = pipeline_components\n+    self.safe_coders = safe_coders\n+\n+\n+class BundleContextManager(object):\n+\n+  def __init__(self,\n+      execution_context, # type: FnApiRunnerExecutionContext\n+      process_bundle_descriptor,  # type: beam_fn_api_pb2.ProcessBundleDescriptor\n+      worker_handler,  # type: fn_runner.WorkerHandler\n+      p_context,  # type: pipeline_context.PipelineContext\n+               ):\n+    self.execution_context = execution_context\n+    self.process_bundle_descriptor = process_bundle_descriptor\n+    self.worker_handler = worker_handler\n+    self.pipeline_context = p_context\n+\n+  def get_input_coder(self, transform_id):", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9d62c9a6b2e2f4863ab2829017033cb4522f1aed"}, "originalPosition": 277}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODA5NDczNw==", "bodyText": "Done", "url": "https://github.com/apache/beam/pull/11202#discussion_r398094737", "createdAt": "2020-03-25T18:54:14Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/runners/portability/fn_api_runner/execution.py", "diffHunk": "@@ -0,0 +1,337 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"Set of utilities for execution of a pipeline by the FnApiRunner.\"\"\"\n+\n+from __future__ import absolute_import\n+\n+import collections\n+import itertools\n+\n+from typing_extensions import Protocol\n+\n+from apache_beam import coders\n+from apache_beam.coders.coder_impl import create_InputStream\n+from apache_beam.coders.coder_impl import create_OutputStream\n+from apache_beam.portability import common_urns\n+from apache_beam.portability.api import beam_fn_api_pb2\n+from apache_beam.runners.portability.fn_api_runner.translations import only_element\n+from apache_beam.runners.portability.fn_api_runner.translations import split_buffer_id\n+from apache_beam.runners.worker import bundle_processor\n+from apache_beam.transforms import trigger\n+from apache_beam.transforms.window import GlobalWindow\n+from apache_beam.transforms.window import GlobalWindows\n+from apache_beam.utils import windowed_value\n+\n+\n+class Buffer(Protocol):\n+  def __iter__(self):\n+    # type: () -> Iterator[bytes]\n+    pass\n+\n+  def append(self, item):\n+    # type: (bytes) -> None\n+    pass\n+\n+\n+class PartitionableBuffer(Buffer, Protocol):\n+  def partition(self, n):\n+    # type: (int) -> List[List[bytes]]\n+    pass\n+\n+\n+class ListBuffer(object):\n+  \"\"\"Used to support parititioning of a list.\"\"\"\n+  def __init__(self, coder_impl):\n+    self._coder_impl = coder_impl\n+    self._inputs = []  # type: List[bytes]\n+    self._grouped_output = None\n+    self.cleared = False\n+\n+  def append(self, element):\n+    # type: (bytes) -> None\n+    if self.cleared:\n+      raise RuntimeError('Trying to append to a cleared ListBuffer.')\n+    if self._grouped_output:\n+      raise RuntimeError('ListBuffer append after read.')\n+    self._inputs.append(element)\n+\n+  def partition(self, n):\n+    # type: (int) -> List[List[bytes]]\n+    if self.cleared:\n+      raise RuntimeError('Trying to partition a cleared ListBuffer.')\n+    if len(self._inputs) >= n or len(self._inputs) == 0:\n+      return [self._inputs[k::n] for k in range(n)]\n+    else:\n+      if not self._grouped_output:\n+        output_stream_list = [create_OutputStream() for _ in range(n)]\n+        idx = 0\n+        for input in self._inputs:\n+          input_stream = create_InputStream(input)\n+          while input_stream.size() > 0:\n+            decoded_value = self._coder_impl.decode_from_stream(\n+                input_stream, True)\n+            self._coder_impl.encode_to_stream(\n+                decoded_value, output_stream_list[idx], True)\n+            idx = (idx + 1) % n\n+        self._grouped_output = [[output_stream.get()]\n+                                for output_stream in output_stream_list]\n+      return self._grouped_output\n+\n+  def __iter__(self):\n+    # type: () -> Iterator[bytes]\n+    if self.cleared:\n+      raise RuntimeError('Trying to iterate through a cleared ListBuffer.')\n+    return iter(self._inputs)\n+\n+  def clear(self):\n+    # type: () -> None\n+    self.cleared = True\n+    self._inputs = []\n+    self._grouped_output = None\n+\n+\n+class GroupingBuffer(object):\n+  \"\"\"Used to accumulate groupded (shuffled) results.\"\"\"\n+  def __init__(self,\n+               pre_grouped_coder,  # type: coders.Coder\n+               post_grouped_coder,  # type: coders.Coder\n+               windowing\n+              ):\n+    # type: (...) -> None\n+    self._key_coder = pre_grouped_coder.key_coder()\n+    self._pre_grouped_coder = pre_grouped_coder\n+    self._post_grouped_coder = post_grouped_coder\n+    self._table = collections.defaultdict(\n+        list)  # type: DefaultDict[bytes, List[Any]]\n+    self._windowing = windowing\n+    self._grouped_output = None  # type: Optional[List[List[bytes]]]\n+\n+  def append(self, elements_data):\n+    # type: (bytes) -> None\n+    if self._grouped_output:\n+      raise RuntimeError('Grouping table append after read.')\n+    input_stream = create_InputStream(elements_data)\n+    coder_impl = self._pre_grouped_coder.get_impl()\n+    key_coder_impl = self._key_coder.get_impl()\n+    # TODO(robertwb): We could optimize this even more by using a\n+    # window-dropping coder for the data plane.\n+    is_trivial_windowing = self._windowing.is_default()\n+    while input_stream.size() > 0:\n+      windowed_key_value = coder_impl.decode_from_stream(input_stream, True)\n+      key, value = windowed_key_value.value\n+      self._table[key_coder_impl.encode(key)].append(\n+          value if is_trivial_windowing else windowed_key_value.\n+          with_value(value))\n+\n+  def partition(self, n):\n+    # type: (int) -> List[List[bytes]]\n+\n+    \"\"\" It is used to partition _GroupingBuffer to N parts. Once it is\n+    partitioned, it would not be re-partitioned with diff N. Re-partition\n+    is not supported now.\n+    \"\"\"\n+    if not self._grouped_output:\n+      if self._windowing.is_default():\n+        globally_window = GlobalWindows.windowed_value(\n+            None,\n+            timestamp=GlobalWindow().max_timestamp(),\n+            pane_info=windowed_value.PaneInfo(\n+                is_first=True,\n+                is_last=True,\n+                timing=windowed_value.PaneInfoTiming.ON_TIME,\n+                index=0,\n+                nonspeculative_index=0)).with_value\n+        windowed_key_values = lambda key, values: [\n+            globally_window((key, values))]\n+      else:\n+        # TODO(pabloem, BEAM-7514): Trigger driver needs access to the clock\n+        #   note that this only comes through if windowing is default - but what\n+        #   about having multiple firings on the global window.\n+        #   May need to revise.\n+        trigger_driver = trigger.create_trigger_driver(self._windowing, True)\n+        windowed_key_values = trigger_driver.process_entire_key\n+      coder_impl = self._post_grouped_coder.get_impl()\n+      key_coder_impl = self._key_coder.get_impl()\n+      self._grouped_output = [[] for _ in range(n)]\n+      output_stream_list = [create_OutputStream() for _ in range(n)]\n+      for idx, (encoded_key, windowed_values) in enumerate(self._table.items()):\n+        key = key_coder_impl.decode(encoded_key)\n+        for wkvs in windowed_key_values(key, windowed_values):\n+          coder_impl.encode_to_stream(wkvs, output_stream_list[idx % n], True)\n+      for ix, output_stream in enumerate(output_stream_list):\n+        self._grouped_output[ix] = [output_stream.get()]\n+      self._table.clear()\n+    return self._grouped_output\n+\n+  def __iter__(self):\n+    # type: () -> Iterator[bytes]\n+\n+    \"\"\" Since partition() returns a list of lists, add this __iter__ to return\n+    a list to simplify code when we need to iterate through ALL elements of\n+    _GroupingBuffer.\n+    \"\"\"\n+    return itertools.chain(*self.partition(1))\n+\n+\n+class WindowGroupingBuffer(object):\n+  \"\"\"Used to partition windowed side inputs.\"\"\"\n+  def __init__(\n+      self,\n+      access_pattern,\n+      coder  # type: coders.WindowedValueCoder\n+  ):\n+    # type: (...) -> None\n+    # Here's where we would use a different type of partitioning\n+    # (e.g. also by key) for a different access pattern.\n+    if access_pattern.urn == common_urns.side_inputs.ITERABLE.urn:\n+      self._kv_extractor = lambda value: ('', value)\n+      self._key_coder = coders.SingletonCoder('')  # type: coders.Coder\n+      self._value_coder = coder.wrapped_value_coder\n+    elif access_pattern.urn == common_urns.side_inputs.MULTIMAP.urn:\n+      self._kv_extractor = lambda value: value\n+      self._key_coder = coder.wrapped_value_coder.key_coder()\n+      self._value_coder = (coder.wrapped_value_coder.value_coder())\n+    else:\n+      raise ValueError(\"Unknown access pattern: '%s'\" % access_pattern.urn)\n+    self._windowed_value_coder = coder\n+    self._window_coder = coder.window_coder\n+    self._values_by_window = collections.defaultdict(\n+        list)  # type: DefaultDict[Tuple[str, BoundedWindow], List[Any]]\n+\n+  def append(self, elements_data):\n+    # type: (bytes) -> None\n+    input_stream = create_InputStream(elements_data)\n+    while input_stream.size() > 0:\n+      windowed_val_coder_impl = self._windowed_value_coder.get_impl(\n+      )  # type: WindowedValueCoderImpl\n+      windowed_value = windowed_val_coder_impl.decode_from_stream(\n+          input_stream, True)\n+      key, value = self._kv_extractor(windowed_value.value)\n+      for window in windowed_value.windows:\n+        self._values_by_window[key, window].append(value)\n+\n+  def encoded_items(self):\n+    # type: () -> Iterator[Tuple[bytes, bytes, bytes]]\n+    value_coder_impl = self._value_coder.get_impl()\n+    key_coder_impl = self._key_coder.get_impl()\n+    for (key, window), values in self._values_by_window.items():\n+      encoded_window = self._window_coder.encode(window)\n+      encoded_key = key_coder_impl.encode_nested(key)\n+      output_stream = create_OutputStream()\n+      for value in values:\n+        value_coder_impl.encode_to_stream(value, output_stream, True)\n+      yield encoded_key, encoded_window, output_stream.get()\n+\n+\n+class FnApiRunnerExecutionContext(object):\n+  \"\"\"\n+ :var pcoll_buffers: (collections.defaultdict of str: list): Mapping of\n+       PCollection IDs to list that functions as buffer for the\n+       ``beam.PCollection``.\n+ \"\"\"\n+  def __init__(self,\n+      worker_handler_factory,  # type: Callable[[Optional[str], int], List[WorkerHandler]]\n+      pipeline_components,  # type: beam_runner_api_pb2.Components\n+      safe_coders,\n+               ):\n+    \"\"\"\n+    :param worker_handler_factory: A ``callable`` that takes in an environment\n+        id and a number of workers, and returns a list of ``WorkerHandler``s.\n+    :param pipeline_components:  (beam_runner_api_pb2.Components): TODO\n+    :param safe_coders:\n+    \"\"\"\n+    self.pcoll_buffers = {}  # type: MutableMapping[bytes, PartitionableBuffer]\n+    self.worker_handler_factory = worker_handler_factory\n+    self.pipeline_components = pipeline_components\n+    self.safe_coders = safe_coders\n+\n+\n+class BundleContextManager(object):\n+\n+  def __init__(self,\n+      execution_context, # type: FnApiRunnerExecutionContext\n+      process_bundle_descriptor,  # type: beam_fn_api_pb2.ProcessBundleDescriptor\n+      worker_handler,  # type: fn_runner.WorkerHandler\n+      p_context,  # type: pipeline_context.PipelineContext\n+               ):\n+    self.execution_context = execution_context\n+    self.process_bundle_descriptor = process_bundle_descriptor\n+    self.worker_handler = worker_handler\n+    self.pipeline_context = p_context\n+\n+  def get_input_coder(self, transform_id):", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5ODA3OTUzNQ=="}, "originalCommit": {"oid": "9d62c9a6b2e2f4863ab2829017033cb4522f1aed"}, "originalPosition": 277}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1493, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}