{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIxNzE2NzEw", "number": 13353, "title": "[BEAM-11267] Remove unnecessary reshuffle for stateful ParDo after key\u2026", "bodyText": "https://issues.apache.org/jira/browse/BEAM-11267\n\nThank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:\n\n Choose reviewer(s) and mention them in a comment (R: @username).\n Format the pull request title like [BEAM-XXX] Fixes bug in ApproximateQuantiles, where you replace BEAM-XXX with the appropriate JIRA issue, if applicable. This will automatically link the pull request to the issue.\n Update CHANGES.md with noteworthy changes.\n If this contribution is large, please file an Apache Individual Contributor License Agreement.\n\nSee the Contributor Guide for more tips on how to make review process smoother.\nPost-Commit Tests Status (on master branch)\n\n\n\nLang\nSDK\nDataflow\nFlink\nSamza\nSpark\nTwister2\n\n\n\n\nGo\n\n---\n\n---\n\n---\n\n\nJava\n\n\n\n\n\n\n\n\nPython\n\n\n\n---\n\n---\n\n\nXLang\n\n\n\n---\n\n---\n\n\n\nPre-Commit Tests Status (on master branch)\n\n\n\n---\nJava\nPython\nGo\nWebsite\nWhitespace\nTypescript\n\n\n\n\nNon-portable\n\n \n\n\n\n\n\n\nPortable\n---\n\n---\n---\n---\n---\n\n\n\nSee .test-infra/jenkins/README for trigger phrase, status and link of all Jenkins jobs.\nGitHub Actions Tests Status (on master branch)\n\n\n\nSee CI.md for more information about GitHub Actions CI.", "createdAt": "2020-11-16T14:56:16Z", "url": "https://github.com/apache/beam/pull/13353", "merged": true, "mergeCommit": {"oid": "76c6129161f9a2178823b2648eb0110bed305640"}, "closed": true, "closedAt": "2020-12-16T16:34:40Z", "author": {"login": "dmvk"}, "timelineItems": {"totalCount": 8, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABddGWG4gFqTUzMTQwNTYzMg==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdmsGv0gBqjQxMTg4NzMxMTA=", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMxNDA1NjMy", "url": "https://github.com/apache/beam/pull/13353#pullrequestreview-531405632", "createdAt": "2020-11-16T15:04:37Z", "commit": {"oid": "66a81e0ae4e7235c79445afb0f56a8e289c0ad41"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxNTowNDozN1rOH0C22g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxNTowNDozN1rOH0C22g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDMzNDgxMA==", "bodyText": "Need to check Kryo internals whether this is breaking change \ud83e\udd14", "url": "https://github.com/apache/beam/pull/13353#discussion_r524334810", "createdAt": "2020-11-16T15:04:37Z", "author": {"login": "dmvk"}, "path": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/WorkItemKeySelector.java", "diffHunk": "@@ -49,6 +52,6 @@ public ByteBuffer getKey(WindowedValue<SingletonKeyedWorkItem<K, V>> value) thro\n \n   @Override\n   public TypeInformation<ByteBuffer> getProducedType() {\n-    return new GenericTypeInfo<>(ByteBuffer.class);\n+    return new CoderTypeInformation<>(FlinkKeyUtils.ByteBufferCoder.of(), pipelineOptions.get());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66a81e0ae4e7235c79445afb0f56a8e289c0ad41"}, "originalPosition": 32}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMxNDA0MDEx", "url": "https://github.com/apache/beam/pull/13353#pullrequestreview-531404011", "createdAt": "2020-11-16T15:02:52Z", "commit": {"oid": "66a81e0ae4e7235c79445afb0f56a8e289c0ad41"}, "state": "CHANGES_REQUESTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxNTowMjo1MlrOH0CyAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxNTowNTo1OVrOH0C60w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDMzMzU3MA==", "bodyText": "Should this be turned to Precondition.checkState?", "url": "https://github.com/apache/beam/pull/13353#discussion_r524333570", "createdAt": "2020-11-16T15:02:52Z", "author": {"login": "je-ik"}, "path": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTranslationContext.java", "diffHunk": "@@ -84,6 +85,17 @@ public void setOutputDataStream(PValue value, DataStream<?> set) {\n     }\n   }\n \n+  <T extends PValue> void setProducer(T value, PTransform<?, T> producer) {\n+    if (!producers.containsKey(value)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66a81e0ae4e7235c79445afb0f56a8e289c0ad41"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDMzNTgyNw==", "bodyText": "This looks unrelated, can you just explain this modification? I suppose it is correct, just wonder why it was GenericTypeInfo before.", "url": "https://github.com/apache/beam/pull/13353#discussion_r524335827", "createdAt": "2020-11-16T15:05:59Z", "author": {"login": "je-ik"}, "path": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/WorkItemKeySelector.java", "diffHunk": "@@ -49,6 +52,6 @@ public ByteBuffer getKey(WindowedValue<SingletonKeyedWorkItem<K, V>> value) thro\n \n   @Override\n   public TypeInformation<ByteBuffer> getProducedType() {\n-    return new GenericTypeInfo<>(ByteBuffer.class);\n+    return new CoderTypeInformation<>(FlinkKeyUtils.ByteBufferCoder.of(), pipelineOptions.get());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66a81e0ae4e7235c79445afb0f56a8e289c0ad41"}, "originalPosition": 32}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMxODE3NDQy", "url": "https://github.com/apache/beam/pull/13353#pullrequestreview-531817442", "createdAt": "2020-11-16T21:52:23Z", "commit": {"oid": "66a81e0ae4e7235c79445afb0f56a8e289c0ad41"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMTo1MjoyNFrOH0VJ2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMTo1MjozN1rOH0VLOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDYzNDU4NA==", "bodyText": "It would be nice if these explicit calls wouldn't be required. I believe context.setOutputDataStream internally has the current transform available. So we could update the producer internally in the context.", "url": "https://github.com/apache/beam/pull/13353#discussion_r524634584", "createdAt": "2020-11-16T21:52:24Z", "author": {"login": "mxm"}, "path": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTransformTranslators.java", "diffHunk": "@@ -971,7 +987,9 @@ public void translateNode(\n               .transform(fullName, outputTypeInfo, (OneInputStreamOperator) doFnOperator)\n               .uid(fullName);\n \n-      context.setOutputDataStream(context.getOutput(transform), outDataStream);\n+      final PCollection<KV<K, Iterable<InputT>>> output = context.getOutput(transform);\n+      context.setOutputDataStream(output, outDataStream);\n+      context.setProducer(output, transform);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66a81e0ae4e7235c79445afb0f56a8e289c0ad41"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDYzNDkzNw==", "bodyText": "Same here.", "url": "https://github.com/apache/beam/pull/13353#discussion_r524634937", "createdAt": "2020-11-16T21:52:37Z", "author": {"login": "mxm"}, "path": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTransformTranslators.java", "diffHunk": "@@ -1127,7 +1148,9 @@ public void translateNode(\n \n         keyedWorkItemStream.getExecutionEnvironment().addOperator(rawFlinkTransform);\n \n-        context.setOutputDataStream(context.getOutput(transform), outDataStream);\n+        final PCollection<KV<K, OutputT>> output = context.getOutput(transform);\n+        context.setOutputDataStream(output, outDataStream);\n+        context.setProducer(output, transform);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66a81e0ae4e7235c79445afb0f56a8e289c0ad41"}, "originalPosition": 83}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTMzOTQxNDk5", "url": "https://github.com/apache/beam/pull/13353#pullrequestreview-533941499", "createdAt": "2020-11-18T22:41:26Z", "commit": {"oid": "0d44f87259aac9a088b9e014c50e1429acfb42f0"}, "state": "APPROVED", "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMjo0MToyNlrOH2FSxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMjo0NjowOFrOH2FblA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3MTg3Ng==", "bodyText": "I would call this parameter forcesShuffle or something similar.", "url": "https://github.com/apache/beam/pull/13353#discussion_r526471876", "createdAt": "2020-11-18T22:41:26Z", "author": {"login": "je-ik"}, "path": "runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslatorTest.java", "diffHunk": "@@ -120,4 +138,95 @@ public void testAutoBalanceShardKeyCacheMaxSize() throws Exception {\n     assertThat(\n         fn.getCache().size(), equalTo(FlinkAutoBalancedShardKeyShardingFunction.CACHE_MAX_SIZE));\n   }\n+\n+  @Test\n+  public void testStatefulParDoAfterCombineChaining() {\n+    final JobGraph stablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(true);\n+    final JobGraph unstablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(false);\n+    // We expect an extra shuffle stage for unstable partitioning.\n+    Assert.assertEquals(\n+        1,\n+        Iterables.size(unstablePartitioning.getVertices())\n+            - Iterables.size(stablePartitioning.getVertices()));\n+  }\n+\n+  private JobGraph getStatefulParDoAfterCombineChainingJobGraph(boolean stablePartitioning) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d44f87259aac9a088b9e014c50e1429acfb42f0"}, "originalPosition": 47}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3MjUyNQ==", "bodyText": "Instead of the StatelessIdentityDoFn we could use MapElements.into(...).via(e -> KV.of(\"\", e.getValue()), which would enforce shuffle semantically. That might improve readability a bit.", "url": "https://github.com/apache/beam/pull/13353#discussion_r526472525", "createdAt": "2020-11-18T22:42:49Z", "author": {"login": "je-ik"}, "path": "runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslatorTest.java", "diffHunk": "@@ -120,4 +138,95 @@ public void testAutoBalanceShardKeyCacheMaxSize() throws Exception {\n     assertThat(\n         fn.getCache().size(), equalTo(FlinkAutoBalancedShardKeyShardingFunction.CACHE_MAX_SIZE));\n   }\n+\n+  @Test\n+  public void testStatefulParDoAfterCombineChaining() {\n+    final JobGraph stablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(true);\n+    final JobGraph unstablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(false);\n+    // We expect an extra shuffle stage for unstable partitioning.\n+    Assert.assertEquals(\n+        1,\n+        Iterables.size(unstablePartitioning.getVertices())\n+            - Iterables.size(stablePartitioning.getVertices()));\n+  }\n+\n+  private JobGraph getStatefulParDoAfterCombineChainingJobGraph(boolean stablePartitioning) {\n+    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    final FlinkStreamingPipelineTranslator translator =\n+        new FlinkStreamingPipelineTranslator(env, PipelineOptionsFactory.create());\n+    final PipelineOptions pipelineOptions = PipelineOptionsFactory.create();\n+    pipelineOptions.setRunner(FlinkRunner.class);\n+    final Pipeline pipeline = Pipeline.create(pipelineOptions);\n+    PCollection<KV<String, Long>> aggregate =\n+        pipeline\n+            .apply(Create.of(\"foo\", \"bar\").withCoder(StringUtf8Coder.of()))\n+            .apply(Count.perElement());\n+    if (!stablePartitioning) {\n+      // When we insert any element-wise \"map\" operation between aggregation and stateful ParDo, we\n+      // can no longer assume that partitioning did not change, therefore we need an extra shuffle\n+      aggregate = aggregate.apply(ParDo.of(new StatelessIdentityDoFn<>()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d44f87259aac9a088b9e014c50e1429acfb42f0"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3MzMwMg==", "bodyText": "Do we need to set runner if we enforce the translator?", "url": "https://github.com/apache/beam/pull/13353#discussion_r526473302", "createdAt": "2020-11-18T22:44:25Z", "author": {"login": "je-ik"}, "path": "runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslatorTest.java", "diffHunk": "@@ -120,4 +138,95 @@ public void testAutoBalanceShardKeyCacheMaxSize() throws Exception {\n     assertThat(\n         fn.getCache().size(), equalTo(FlinkAutoBalancedShardKeyShardingFunction.CACHE_MAX_SIZE));\n   }\n+\n+  @Test\n+  public void testStatefulParDoAfterCombineChaining() {\n+    final JobGraph stablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(true);\n+    final JobGraph unstablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(false);\n+    // We expect an extra shuffle stage for unstable partitioning.\n+    Assert.assertEquals(\n+        1,\n+        Iterables.size(unstablePartitioning.getVertices())\n+            - Iterables.size(stablePartitioning.getVertices()));\n+  }\n+\n+  private JobGraph getStatefulParDoAfterCombineChainingJobGraph(boolean stablePartitioning) {\n+    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    final FlinkStreamingPipelineTranslator translator =\n+        new FlinkStreamingPipelineTranslator(env, PipelineOptionsFactory.create());\n+    final PipelineOptions pipelineOptions = PipelineOptionsFactory.create();\n+    pipelineOptions.setRunner(FlinkRunner.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d44f87259aac9a088b9e014c50e1429acfb42f0"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3NDEzMg==", "bodyText": "Can we merge the two methods, that seem to differ only by this PTransform applied here?", "url": "https://github.com/apache/beam/pull/13353#discussion_r526474132", "createdAt": "2020-11-18T22:46:08Z", "author": {"login": "je-ik"}, "path": "runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslatorTest.java", "diffHunk": "@@ -120,4 +138,95 @@ public void testAutoBalanceShardKeyCacheMaxSize() throws Exception {\n     assertThat(\n         fn.getCache().size(), equalTo(FlinkAutoBalancedShardKeyShardingFunction.CACHE_MAX_SIZE));\n   }\n+\n+  @Test\n+  public void testStatefulParDoAfterCombineChaining() {\n+    final JobGraph stablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(true);\n+    final JobGraph unstablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(false);\n+    // We expect an extra shuffle stage for unstable partitioning.\n+    Assert.assertEquals(\n+        1,\n+        Iterables.size(unstablePartitioning.getVertices())\n+            - Iterables.size(stablePartitioning.getVertices()));\n+  }\n+\n+  private JobGraph getStatefulParDoAfterCombineChainingJobGraph(boolean stablePartitioning) {\n+    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    final FlinkStreamingPipelineTranslator translator =\n+        new FlinkStreamingPipelineTranslator(env, PipelineOptionsFactory.create());\n+    final PipelineOptions pipelineOptions = PipelineOptionsFactory.create();\n+    pipelineOptions.setRunner(FlinkRunner.class);\n+    final Pipeline pipeline = Pipeline.create(pipelineOptions);\n+    PCollection<KV<String, Long>> aggregate =\n+        pipeline\n+            .apply(Create.of(\"foo\", \"bar\").withCoder(StringUtf8Coder.of()))\n+            .apply(Count.perElement());\n+    if (!stablePartitioning) {\n+      // When we insert any element-wise \"map\" operation between aggregation and stateful ParDo, we\n+      // can no longer assume that partitioning did not change, therefore we need an extra shuffle\n+      aggregate = aggregate.apply(ParDo.of(new StatelessIdentityDoFn<>()));\n+    }\n+    aggregate.apply(ParDo.of(new StatefulNoopDoFn<>()));\n+    translator.translate(pipeline);\n+    return env.getStreamGraph().getJobGraph();\n+  }\n+\n+  @Test\n+  public void testStatefulParDoAfterGroupByKeyChaining() {\n+    final JobGraph stablePartitioning = getStatefulParDoAfterGroupByKeyChainingJobGraph(true);\n+    final JobGraph unstablePartitioning = getStatefulParDoAfterGroupByKeyChainingJobGraph(false);\n+    // We expect an extra shuffle stage for unstable partitioning.\n+    Assert.assertEquals(\n+        1,\n+        Iterables.size(unstablePartitioning.getVertices())\n+            - Iterables.size(stablePartitioning.getVertices()));\n+  }\n+\n+  private JobGraph getStatefulParDoAfterGroupByKeyChainingJobGraph(boolean stablePartitioning) {\n+    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    final FlinkStreamingPipelineTranslator translator =\n+        new FlinkStreamingPipelineTranslator(env, PipelineOptionsFactory.create());\n+    final PipelineOptions pipelineOptions = PipelineOptionsFactory.create();\n+    pipelineOptions.setRunner(FlinkRunner.class);\n+    final Pipeline pipeline = Pipeline.create(pipelineOptions);\n+    PCollection<KV<String, Iterable<Long>>> aggregate =\n+        pipeline\n+            .apply(\n+                Create.of(KV.of(\"foo\", 1L), KV.of(\"bar\", 1L))\n+                    .withCoder(KvCoder.of(StringUtf8Coder.of(), VarLongCoder.of())))\n+            .apply(GroupByKey.create());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d44f87259aac9a088b9e014c50e1429acfb42f0"}, "originalPosition": 91}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NTM0MzU3MjQw", "url": "https://github.com/apache/beam/pull/13353#pullrequestreview-534357240", "createdAt": "2020-11-19T12:01:33Z", "commit": {"oid": "0d44f87259aac9a088b9e014c50e1429acfb42f0"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "0d44f87259aac9a088b9e014c50e1429acfb42f0", "author": {"user": {"login": "dmvk", "name": "David Moravek"}}, "url": "https://github.com/apache/beam/commit/0d44f87259aac9a088b9e014c50e1429acfb42f0", "committedDate": "2020-11-18T20:03:52Z", "message": "[BEAM-11267] Code review."}, "afterCommit": {"oid": "d7cf990cbe88680d6fbbbfb5087c0375aadfe89b", "author": {"user": {"login": "dmvk", "name": "David Moravek"}}, "url": "https://github.com/apache/beam/commit/d7cf990cbe88680d6fbbbfb5087c0375aadfe89b", "committedDate": "2020-12-16T09:22:10Z", "message": "[BEAM-11267] Remove unecessary reshuffle for stateful ParDo after keyed operation."}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f1c1514dc7b33841d59da22e00980163dd7e0474", "author": {"user": {"login": "dmvk", "name": "David Moravek"}}, "url": "https://github.com/apache/beam/commit/f1c1514dc7b33841d59da22e00980163dd7e0474", "committedDate": "2020-12-16T10:09:19Z", "message": "[BEAM-11267] Remove unecessary reshuffle for stateful ParDo after keyed operation."}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "d7cf990cbe88680d6fbbbfb5087c0375aadfe89b", "author": {"user": {"login": "dmvk", "name": "David Moravek"}}, "url": "https://github.com/apache/beam/commit/d7cf990cbe88680d6fbbbfb5087c0375aadfe89b", "committedDate": "2020-12-16T09:22:10Z", "message": "[BEAM-11267] Remove unecessary reshuffle for stateful ParDo after keyed operation."}, "afterCommit": {"oid": "f1c1514dc7b33841d59da22e00980163dd7e0474", "author": {"user": {"login": "dmvk", "name": "David Moravek"}}, "url": "https://github.com/apache/beam/commit/f1c1514dc7b33841d59da22e00980163dd7e0474", "committedDate": "2020-12-16T10:09:19Z", "message": "[BEAM-11267] Remove unecessary reshuffle for stateful ParDo after keyed operation."}}]}}}, "rateLimit": {"limit": 5000, "remaining": 4900, "cost": 1, "resetAt": "2021-10-28T18:00:02Z"}}}