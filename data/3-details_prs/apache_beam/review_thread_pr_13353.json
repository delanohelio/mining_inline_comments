{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTIxNzE2NzEw", "number": 13353, "reviewThreads": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxNTowMjo1MlrOE5h1Ig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMjo0NjowOFrOE6zIJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4NzU4NTYyOnYy", "diffSide": "RIGHT", "path": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTranslationContext.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxNTowMjo1MlrOH0CyAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMDoxNzoxOFrOH0PvxQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDMzMzU3MA==", "bodyText": "Should this be turned to Precondition.checkState?", "url": "https://github.com/apache/beam/pull/13353#discussion_r524333570", "createdAt": "2020-11-16T15:02:52Z", "author": {"login": "je-ik"}, "path": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTranslationContext.java", "diffHunk": "@@ -84,6 +85,17 @@ public void setOutputDataStream(PValue value, DataStream<?> set) {\n     }\n   }\n \n+  <T extends PValue> void setProducer(T value, PTransform<?, T> producer) {\n+    if (!producers.containsKey(value)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66a81e0ae4e7235c79445afb0f56a8e289c0ad41"}, "originalPosition": 23}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDU0NTk4OQ==", "bodyText": "\ud83d\udc4d makes sense", "url": "https://github.com/apache/beam/pull/13353#discussion_r524545989", "createdAt": "2020-11-16T20:17:18Z", "author": {"login": "dmvk"}, "path": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTranslationContext.java", "diffHunk": "@@ -84,6 +85,17 @@ public void setOutputDataStream(PValue value, DataStream<?> set) {\n     }\n   }\n \n+  <T extends PValue> void setProducer(T value, PTransform<?, T> producer) {\n+    if (!producers.containsKey(value)) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDMzMzU3MA=="}, "originalCommit": {"oid": "66a81e0ae4e7235c79445afb0f56a8e289c0ad41"}, "originalPosition": 23}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4NzU5MzMyOnYy", "diffSide": "RIGHT", "path": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/WorkItemKeySelector.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxNTowNDozN1rOH0C22g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxNTowNDozN1rOH0C22g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDMzNDgxMA==", "bodyText": "Need to check Kryo internals whether this is breaking change \ud83e\udd14", "url": "https://github.com/apache/beam/pull/13353#discussion_r524334810", "createdAt": "2020-11-16T15:04:37Z", "author": {"login": "dmvk"}, "path": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/WorkItemKeySelector.java", "diffHunk": "@@ -49,6 +52,6 @@ public ByteBuffer getKey(WindowedValue<SingletonKeyedWorkItem<K, V>> value) thro\n \n   @Override\n   public TypeInformation<ByteBuffer> getProducedType() {\n-    return new GenericTypeInfo<>(ByteBuffer.class);\n+    return new CoderTypeInformation<>(FlinkKeyUtils.ByteBufferCoder.of(), pipelineOptions.get());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66a81e0ae4e7235c79445afb0f56a8e289c0ad41"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4NzU5OTU4OnYy", "diffSide": "RIGHT", "path": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/WorkItemKeySelector.java", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQxNTowNTo1OVrOH0C60w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMDowMzowNVrOH1_5tg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDMzNTgyNw==", "bodyText": "This looks unrelated, can you just explain this modification? I suppose it is correct, just wonder why it was GenericTypeInfo before.", "url": "https://github.com/apache/beam/pull/13353#discussion_r524335827", "createdAt": "2020-11-16T15:05:59Z", "author": {"login": "je-ik"}, "path": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/WorkItemKeySelector.java", "diffHunk": "@@ -49,6 +52,6 @@ public ByteBuffer getKey(WindowedValue<SingletonKeyedWorkItem<K, V>> value) thro\n \n   @Override\n   public TypeInformation<ByteBuffer> getProducedType() {\n-    return new GenericTypeInfo<>(ByteBuffer.class);\n+    return new CoderTypeInformation<>(FlinkKeyUtils.ByteBufferCoder.of(), pipelineOptions.get());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66a81e0ae4e7235c79445afb0f56a8e289c0ad41"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDU0NTMxNg==", "bodyText": "Not sure if this is necessary. I wanted to ensure that the new \"reinterpreted partitioning\" is compatible with the one used by GBK / Combine.\nThe idea was if partitioning is not compatible, it may result in some state partitioning related glitches (eg. you wouldn't have local state for a key-group you need).\nSecond thoughts, flink selects target partition (key group) based on \"pojo hash code\" (not based on binary representation), so the previous version was probably compatible enough \ud83e\udd14\n@mxm WDYT?", "url": "https://github.com/apache/beam/pull/13353#discussion_r524545316", "createdAt": "2020-11-16T20:15:57Z", "author": {"login": "dmvk"}, "path": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/WorkItemKeySelector.java", "diffHunk": "@@ -49,6 +52,6 @@ public ByteBuffer getKey(WindowedValue<SingletonKeyedWorkItem<K, V>> value) thro\n \n   @Override\n   public TypeInformation<ByteBuffer> getProducedType() {\n-    return new GenericTypeInfo<>(ByteBuffer.class);\n+    return new CoderTypeInformation<>(FlinkKeyUtils.ByteBufferCoder.of(), pipelineOptions.get());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDMzNTgyNw=="}, "originalCommit": {"oid": "66a81e0ae4e7235c79445afb0f56a8e289c0ad41"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDU5NzUzMg==", "bodyText": "I would think that the partitioning is ensured by the reinterpretAsKeyedStream and will therefore be preserved from the previous shuffle phase. Is this not enough?", "url": "https://github.com/apache/beam/pull/13353#discussion_r524597532", "createdAt": "2020-11-16T21:22:18Z", "author": {"login": "je-ik"}, "path": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/WorkItemKeySelector.java", "diffHunk": "@@ -49,6 +52,6 @@ public ByteBuffer getKey(WindowedValue<SingletonKeyedWorkItem<K, V>> value) thro\n \n   @Override\n   public TypeInformation<ByteBuffer> getProducedType() {\n-    return new GenericTypeInfo<>(ByteBuffer.class);\n+    return new CoderTypeInformation<>(FlinkKeyUtils.ByteBufferCoder.of(), pipelineOptions.get());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDMzNTgyNw=="}, "originalCommit": {"oid": "66a81e0ae4e7235c79445afb0f56a8e289c0ad41"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjM4MzU0Mg==", "bodyText": "I think that reinterpretAsKeyedStream just trusts the user and there are no safety nets in place.", "url": "https://github.com/apache/beam/pull/13353#discussion_r526383542", "createdAt": "2020-11-18T20:03:05Z", "author": {"login": "dmvk"}, "path": "runners/flink/src/main/java/org/apache/beam/runners/flink/translation/wrappers/streaming/WorkItemKeySelector.java", "diffHunk": "@@ -49,6 +52,6 @@ public ByteBuffer getKey(WindowedValue<SingletonKeyedWorkItem<K, V>> value) thro\n \n   @Override\n   public TypeInformation<ByteBuffer> getProducedType() {\n-    return new GenericTypeInfo<>(ByteBuffer.class);\n+    return new CoderTypeInformation<>(FlinkKeyUtils.ByteBufferCoder.of(), pipelineOptions.get());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDMzNTgyNw=="}, "originalCommit": {"oid": "66a81e0ae4e7235c79445afb0f56a8e289c0ad41"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4OTQ2NTYwOnYy", "diffSide": "RIGHT", "path": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTransformTranslators.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMTo1MjoyNFrOH0VJ2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMDowMDo1N1rOH1_06w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDYzNDU4NA==", "bodyText": "It would be nice if these explicit calls wouldn't be required. I believe context.setOutputDataStream internally has the current transform available. So we could update the producer internally in the context.", "url": "https://github.com/apache/beam/pull/13353#discussion_r524634584", "createdAt": "2020-11-16T21:52:24Z", "author": {"login": "mxm"}, "path": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTransformTranslators.java", "diffHunk": "@@ -971,7 +987,9 @@ public void translateNode(\n               .transform(fullName, outputTypeInfo, (OneInputStreamOperator) doFnOperator)\n               .uid(fullName);\n \n-      context.setOutputDataStream(context.getOutput(transform), outDataStream);\n+      final PCollection<KV<K, Iterable<InputT>>> output = context.getOutput(transform);\n+      context.setOutputDataStream(output, outDataStream);\n+      context.setProducer(output, transform);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66a81e0ae4e7235c79445afb0f56a8e289c0ad41"}, "originalPosition": 60}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjM4MjMxNQ==", "bodyText": "nice catch \ud83d\udc4d", "url": "https://github.com/apache/beam/pull/13353#discussion_r526382315", "createdAt": "2020-11-18T20:00:57Z", "author": {"login": "dmvk"}, "path": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTransformTranslators.java", "diffHunk": "@@ -971,7 +987,9 @@ public void translateNode(\n               .transform(fullName, outputTypeInfo, (OneInputStreamOperator) doFnOperator)\n               .uid(fullName);\n \n-      context.setOutputDataStream(context.getOutput(transform), outDataStream);\n+      final PCollection<KV<K, Iterable<InputT>>> output = context.getOutput(transform);\n+      context.setOutputDataStream(output, outDataStream);\n+      context.setProducer(output, transform);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDYzNDU4NA=="}, "originalCommit": {"oid": "66a81e0ae4e7235c79445afb0f56a8e289c0ad41"}, "originalPosition": 60}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4OTQ2NzY1OnYy", "diffSide": "RIGHT", "path": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTransformTranslators.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMTo1MjozN1rOH0VLOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQyMTo1MjozN1rOH0VLOQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNDYzNDkzNw==", "bodyText": "Same here.", "url": "https://github.com/apache/beam/pull/13353#discussion_r524634937", "createdAt": "2020-11-16T21:52:37Z", "author": {"login": "mxm"}, "path": "runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkStreamingTransformTranslators.java", "diffHunk": "@@ -1127,7 +1148,9 @@ public void translateNode(\n \n         keyedWorkItemStream.getExecutionEnvironment().addOperator(rawFlinkTransform);\n \n-        context.setOutputDataStream(context.getOutput(transform), outDataStream);\n+        final PCollection<KV<K, OutputT>> output = context.getOutput(transform);\n+        context.setOutputDataStream(output, outDataStream);\n+        context.setProducer(output, transform);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "66a81e0ae4e7235c79445afb0f56a8e289c0ad41"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwMDg5MDgwOnYy", "diffSide": "RIGHT", "path": "runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslatorTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMjo0MToyNlrOH2FSxA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMjo0MToyNlrOH2FSxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3MTg3Ng==", "bodyText": "I would call this parameter forcesShuffle or something similar.", "url": "https://github.com/apache/beam/pull/13353#discussion_r526471876", "createdAt": "2020-11-18T22:41:26Z", "author": {"login": "je-ik"}, "path": "runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslatorTest.java", "diffHunk": "@@ -120,4 +138,95 @@ public void testAutoBalanceShardKeyCacheMaxSize() throws Exception {\n     assertThat(\n         fn.getCache().size(), equalTo(FlinkAutoBalancedShardKeyShardingFunction.CACHE_MAX_SIZE));\n   }\n+\n+  @Test\n+  public void testStatefulParDoAfterCombineChaining() {\n+    final JobGraph stablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(true);\n+    final JobGraph unstablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(false);\n+    // We expect an extra shuffle stage for unstable partitioning.\n+    Assert.assertEquals(\n+        1,\n+        Iterables.size(unstablePartitioning.getVertices())\n+            - Iterables.size(stablePartitioning.getVertices()));\n+  }\n+\n+  private JobGraph getStatefulParDoAfterCombineChainingJobGraph(boolean stablePartitioning) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d44f87259aac9a088b9e014c50e1429acfb42f0"}, "originalPosition": 47}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwMDg5NDk2OnYy", "diffSide": "RIGHT", "path": "runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslatorTest.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMjo0Mjo0OVrOH2FVTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMjo0Mjo0OVrOH2FVTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3MjUyNQ==", "bodyText": "Instead of the StatelessIdentityDoFn we could use MapElements.into(...).via(e -> KV.of(\"\", e.getValue()), which would enforce shuffle semantically. That might improve readability a bit.", "url": "https://github.com/apache/beam/pull/13353#discussion_r526472525", "createdAt": "2020-11-18T22:42:49Z", "author": {"login": "je-ik"}, "path": "runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslatorTest.java", "diffHunk": "@@ -120,4 +138,95 @@ public void testAutoBalanceShardKeyCacheMaxSize() throws Exception {\n     assertThat(\n         fn.getCache().size(), equalTo(FlinkAutoBalancedShardKeyShardingFunction.CACHE_MAX_SIZE));\n   }\n+\n+  @Test\n+  public void testStatefulParDoAfterCombineChaining() {\n+    final JobGraph stablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(true);\n+    final JobGraph unstablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(false);\n+    // We expect an extra shuffle stage for unstable partitioning.\n+    Assert.assertEquals(\n+        1,\n+        Iterables.size(unstablePartitioning.getVertices())\n+            - Iterables.size(stablePartitioning.getVertices()));\n+  }\n+\n+  private JobGraph getStatefulParDoAfterCombineChainingJobGraph(boolean stablePartitioning) {\n+    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    final FlinkStreamingPipelineTranslator translator =\n+        new FlinkStreamingPipelineTranslator(env, PipelineOptionsFactory.create());\n+    final PipelineOptions pipelineOptions = PipelineOptionsFactory.create();\n+    pipelineOptions.setRunner(FlinkRunner.class);\n+    final Pipeline pipeline = Pipeline.create(pipelineOptions);\n+    PCollection<KV<String, Long>> aggregate =\n+        pipeline\n+            .apply(Create.of(\"foo\", \"bar\").withCoder(StringUtf8Coder.of()))\n+            .apply(Count.perElement());\n+    if (!stablePartitioning) {\n+      // When we insert any element-wise \"map\" operation between aggregation and stateful ParDo, we\n+      // can no longer assume that partitioning did not change, therefore we need an extra shuffle\n+      aggregate = aggregate.apply(ParDo.of(new StatelessIdentityDoFn<>()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d44f87259aac9a088b9e014c50e1429acfb42f0"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwMDg5OTkzOnYy", "diffSide": "RIGHT", "path": "runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslatorTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMjo0NDoyNVrOH2FYVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQwNzoxNjozMlrOH2PkQg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3MzMwMg==", "bodyText": "Do we need to set runner if we enforce the translator?", "url": "https://github.com/apache/beam/pull/13353#discussion_r526473302", "createdAt": "2020-11-18T22:44:25Z", "author": {"login": "je-ik"}, "path": "runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslatorTest.java", "diffHunk": "@@ -120,4 +138,95 @@ public void testAutoBalanceShardKeyCacheMaxSize() throws Exception {\n     assertThat(\n         fn.getCache().size(), equalTo(FlinkAutoBalancedShardKeyShardingFunction.CACHE_MAX_SIZE));\n   }\n+\n+  @Test\n+  public void testStatefulParDoAfterCombineChaining() {\n+    final JobGraph stablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(true);\n+    final JobGraph unstablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(false);\n+    // We expect an extra shuffle stage for unstable partitioning.\n+    Assert.assertEquals(\n+        1,\n+        Iterables.size(unstablePartitioning.getVertices())\n+            - Iterables.size(stablePartitioning.getVertices()));\n+  }\n+\n+  private JobGraph getStatefulParDoAfterCombineChainingJobGraph(boolean stablePartitioning) {\n+    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    final FlinkStreamingPipelineTranslator translator =\n+        new FlinkStreamingPipelineTranslator(env, PipelineOptionsFactory.create());\n+    final PipelineOptions pipelineOptions = PipelineOptionsFactory.create();\n+    pipelineOptions.setRunner(FlinkRunner.class);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d44f87259aac9a088b9e014c50e1429acfb42f0"}, "originalPosition": 52}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjY0MDE5NA==", "bodyText": "unfortunately yes, otherwise pipeline.create() would fail", "url": "https://github.com/apache/beam/pull/13353#discussion_r526640194", "createdAt": "2020-11-19T07:16:32Z", "author": {"login": "dmvk"}, "path": "runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslatorTest.java", "diffHunk": "@@ -120,4 +138,95 @@ public void testAutoBalanceShardKeyCacheMaxSize() throws Exception {\n     assertThat(\n         fn.getCache().size(), equalTo(FlinkAutoBalancedShardKeyShardingFunction.CACHE_MAX_SIZE));\n   }\n+\n+  @Test\n+  public void testStatefulParDoAfterCombineChaining() {\n+    final JobGraph stablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(true);\n+    final JobGraph unstablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(false);\n+    // We expect an extra shuffle stage for unstable partitioning.\n+    Assert.assertEquals(\n+        1,\n+        Iterables.size(unstablePartitioning.getVertices())\n+            - Iterables.size(stablePartitioning.getVertices()));\n+  }\n+\n+  private JobGraph getStatefulParDoAfterCombineChainingJobGraph(boolean stablePartitioning) {\n+    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    final FlinkStreamingPipelineTranslator translator =\n+        new FlinkStreamingPipelineTranslator(env, PipelineOptionsFactory.create());\n+    final PipelineOptions pipelineOptions = PipelineOptionsFactory.create();\n+    pipelineOptions.setRunner(FlinkRunner.class);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3MzMwMg=="}, "originalCommit": {"oid": "0d44f87259aac9a088b9e014c50e1429acfb42f0"}, "originalPosition": 52}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMwMDkwNTM1OnYy", "diffSide": "RIGHT", "path": "runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslatorTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQyMjo0NjowOFrOH2FblA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOVQwNzoxODozOFrOH2Pnnw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3NDEzMg==", "bodyText": "Can we merge the two methods, that seem to differ only by this PTransform applied here?", "url": "https://github.com/apache/beam/pull/13353#discussion_r526474132", "createdAt": "2020-11-18T22:46:08Z", "author": {"login": "je-ik"}, "path": "runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslatorTest.java", "diffHunk": "@@ -120,4 +138,95 @@ public void testAutoBalanceShardKeyCacheMaxSize() throws Exception {\n     assertThat(\n         fn.getCache().size(), equalTo(FlinkAutoBalancedShardKeyShardingFunction.CACHE_MAX_SIZE));\n   }\n+\n+  @Test\n+  public void testStatefulParDoAfterCombineChaining() {\n+    final JobGraph stablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(true);\n+    final JobGraph unstablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(false);\n+    // We expect an extra shuffle stage for unstable partitioning.\n+    Assert.assertEquals(\n+        1,\n+        Iterables.size(unstablePartitioning.getVertices())\n+            - Iterables.size(stablePartitioning.getVertices()));\n+  }\n+\n+  private JobGraph getStatefulParDoAfterCombineChainingJobGraph(boolean stablePartitioning) {\n+    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    final FlinkStreamingPipelineTranslator translator =\n+        new FlinkStreamingPipelineTranslator(env, PipelineOptionsFactory.create());\n+    final PipelineOptions pipelineOptions = PipelineOptionsFactory.create();\n+    pipelineOptions.setRunner(FlinkRunner.class);\n+    final Pipeline pipeline = Pipeline.create(pipelineOptions);\n+    PCollection<KV<String, Long>> aggregate =\n+        pipeline\n+            .apply(Create.of(\"foo\", \"bar\").withCoder(StringUtf8Coder.of()))\n+            .apply(Count.perElement());\n+    if (!stablePartitioning) {\n+      // When we insert any element-wise \"map\" operation between aggregation and stateful ParDo, we\n+      // can no longer assume that partitioning did not change, therefore we need an extra shuffle\n+      aggregate = aggregate.apply(ParDo.of(new StatelessIdentityDoFn<>()));\n+    }\n+    aggregate.apply(ParDo.of(new StatefulNoopDoFn<>()));\n+    translator.translate(pipeline);\n+    return env.getStreamGraph().getJobGraph();\n+  }\n+\n+  @Test\n+  public void testStatefulParDoAfterGroupByKeyChaining() {\n+    final JobGraph stablePartitioning = getStatefulParDoAfterGroupByKeyChainingJobGraph(true);\n+    final JobGraph unstablePartitioning = getStatefulParDoAfterGroupByKeyChainingJobGraph(false);\n+    // We expect an extra shuffle stage for unstable partitioning.\n+    Assert.assertEquals(\n+        1,\n+        Iterables.size(unstablePartitioning.getVertices())\n+            - Iterables.size(stablePartitioning.getVertices()));\n+  }\n+\n+  private JobGraph getStatefulParDoAfterGroupByKeyChainingJobGraph(boolean stablePartitioning) {\n+    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    final FlinkStreamingPipelineTranslator translator =\n+        new FlinkStreamingPipelineTranslator(env, PipelineOptionsFactory.create());\n+    final PipelineOptions pipelineOptions = PipelineOptionsFactory.create();\n+    pipelineOptions.setRunner(FlinkRunner.class);\n+    final Pipeline pipeline = Pipeline.create(pipelineOptions);\n+    PCollection<KV<String, Iterable<Long>>> aggregate =\n+        pipeline\n+            .apply(\n+                Create.of(KV.of(\"foo\", 1L), KV.of(\"bar\", 1L))\n+                    .withCoder(KvCoder.of(StringUtf8Coder.of(), VarLongCoder.of())))\n+            .apply(GroupByKey.create());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "0d44f87259aac9a088b9e014c50e1429acfb42f0"}, "originalPosition": 91}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjY0MTA1NQ==", "bodyText": "I thought about it, but the methods are really small so it's worth decreasing readability here", "url": "https://github.com/apache/beam/pull/13353#discussion_r526641055", "createdAt": "2020-11-19T07:18:38Z", "author": {"login": "dmvk"}, "path": "runners/flink/src/test/java/org/apache/beam/runners/flink/FlinkStreamingPipelineTranslatorTest.java", "diffHunk": "@@ -120,4 +138,95 @@ public void testAutoBalanceShardKeyCacheMaxSize() throws Exception {\n     assertThat(\n         fn.getCache().size(), equalTo(FlinkAutoBalancedShardKeyShardingFunction.CACHE_MAX_SIZE));\n   }\n+\n+  @Test\n+  public void testStatefulParDoAfterCombineChaining() {\n+    final JobGraph stablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(true);\n+    final JobGraph unstablePartitioning = getStatefulParDoAfterCombineChainingJobGraph(false);\n+    // We expect an extra shuffle stage for unstable partitioning.\n+    Assert.assertEquals(\n+        1,\n+        Iterables.size(unstablePartitioning.getVertices())\n+            - Iterables.size(stablePartitioning.getVertices()));\n+  }\n+\n+  private JobGraph getStatefulParDoAfterCombineChainingJobGraph(boolean stablePartitioning) {\n+    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    final FlinkStreamingPipelineTranslator translator =\n+        new FlinkStreamingPipelineTranslator(env, PipelineOptionsFactory.create());\n+    final PipelineOptions pipelineOptions = PipelineOptionsFactory.create();\n+    pipelineOptions.setRunner(FlinkRunner.class);\n+    final Pipeline pipeline = Pipeline.create(pipelineOptions);\n+    PCollection<KV<String, Long>> aggregate =\n+        pipeline\n+            .apply(Create.of(\"foo\", \"bar\").withCoder(StringUtf8Coder.of()))\n+            .apply(Count.perElement());\n+    if (!stablePartitioning) {\n+      // When we insert any element-wise \"map\" operation between aggregation and stateful ParDo, we\n+      // can no longer assume that partitioning did not change, therefore we need an extra shuffle\n+      aggregate = aggregate.apply(ParDo.of(new StatelessIdentityDoFn<>()));\n+    }\n+    aggregate.apply(ParDo.of(new StatefulNoopDoFn<>()));\n+    translator.translate(pipeline);\n+    return env.getStreamGraph().getJobGraph();\n+  }\n+\n+  @Test\n+  public void testStatefulParDoAfterGroupByKeyChaining() {\n+    final JobGraph stablePartitioning = getStatefulParDoAfterGroupByKeyChainingJobGraph(true);\n+    final JobGraph unstablePartitioning = getStatefulParDoAfterGroupByKeyChainingJobGraph(false);\n+    // We expect an extra shuffle stage for unstable partitioning.\n+    Assert.assertEquals(\n+        1,\n+        Iterables.size(unstablePartitioning.getVertices())\n+            - Iterables.size(stablePartitioning.getVertices()));\n+  }\n+\n+  private JobGraph getStatefulParDoAfterGroupByKeyChainingJobGraph(boolean stablePartitioning) {\n+    final StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n+    final FlinkStreamingPipelineTranslator translator =\n+        new FlinkStreamingPipelineTranslator(env, PipelineOptionsFactory.create());\n+    final PipelineOptions pipelineOptions = PipelineOptionsFactory.create();\n+    pipelineOptions.setRunner(FlinkRunner.class);\n+    final Pipeline pipeline = Pipeline.create(pipelineOptions);\n+    PCollection<KV<String, Iterable<Long>>> aggregate =\n+        pipeline\n+            .apply(\n+                Create.of(KV.of(\"foo\", 1L), KV.of(\"bar\", 1L))\n+                    .withCoder(KvCoder.of(StringUtf8Coder.of(), VarLongCoder.of())))\n+            .apply(GroupByKey.create());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNjQ3NDEzMg=="}, "originalCommit": {"oid": "0d44f87259aac9a088b9e014c50e1429acfb42f0"}, "originalPosition": 91}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2816, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}