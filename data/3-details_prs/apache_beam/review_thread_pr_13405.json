{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTI1MzYzODM4", "number": 13405, "reviewThreads": {"totalCount": 7, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQyMzowMDo0MVrOE_2J9g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwMDo0NDo1M1rOFCqz-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1MzgzMDMwOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/dataflow/dataflow_runner.py", "isResolved": false, "comments": {"totalCount": 10, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMlQyMzowMDo0MVrOH9xutg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMjozMDowM1rOIA9upA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUzOTk1OA==", "bodyText": "Are you going to take care of portable job submission as well? If fo, maybe we need a TODO here.", "url": "https://github.com/apache/beam/pull/13405#discussion_r534539958", "createdAt": "2020-12-02T23:00:41Z", "author": {"login": "boyuanzz"}, "path": "sdks/python/apache_beam/runners/dataflow/dataflow_runner.py", "diffHunk": "@@ -1112,6 +1128,17 @@ def run_ParDo(self, transform_node, options):\n       if is_stateful_dofn:\n         step.add_property(PropertyNames.USES_KEYED_STATE, 'true')\n \n+        # Also checks whether the step allows shardable keyed states.\n+        for pcoll in transform_node.outputs.values():", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e1aafbf5c360738f5c042bba61855055ab92588b"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc3MTYzNA==", "bodyText": "Yes but supposedly the changes for portable job submission would be on backend side. What's to be done here?", "url": "https://github.com/apache/beam/pull/13405#discussion_r535771634", "createdAt": "2020-12-04T01:37:16Z", "author": {"login": "nehsyc"}, "path": "sdks/python/apache_beam/runners/dataflow/dataflow_runner.py", "diffHunk": "@@ -1112,6 +1128,17 @@ def run_ParDo(self, transform_node, options):\n       if is_stateful_dofn:\n         step.add_property(PropertyNames.USES_KEYED_STATE, 'true')\n \n+        # Also checks whether the step allows shardable keyed states.\n+        for pcoll in transform_node.outputs.values():", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUzOTk1OA=="}, "originalCommit": {"oid": "e1aafbf5c360738f5c042bba61855055ab92588b"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ2MDg3OA==", "bodyText": "We might be able to clean this up when we migrate to portable job submission. @chamikaramj", "url": "https://github.com/apache/beam/pull/13405#discussion_r536460878", "createdAt": "2020-12-05T00:32:31Z", "author": {"login": "boyuanzz"}, "path": "sdks/python/apache_beam/runners/dataflow/dataflow_runner.py", "diffHunk": "@@ -1112,6 +1128,17 @@ def run_ParDo(self, transform_node, options):\n       if is_stateful_dofn:\n         step.add_property(PropertyNames.USES_KEYED_STATE, 'true')\n \n+        # Also checks whether the step allows shardable keyed states.\n+        for pcoll in transform_node.outputs.values():", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUzOTk1OA=="}, "originalCommit": {"oid": "e1aafbf5c360738f5c042bba61855055ab92588b"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ4MTEzNQ==", "bodyText": "Is there a JIRA issue I can link here? Assuming that there are quite a few places in dataflow_runner.py that need to be cleaned up when we migrate to portable job submission.", "url": "https://github.com/apache/beam/pull/13405#discussion_r536481135", "createdAt": "2020-12-05T02:04:39Z", "author": {"login": "nehsyc"}, "path": "sdks/python/apache_beam/runners/dataflow/dataflow_runner.py", "diffHunk": "@@ -1112,6 +1128,17 @@ def run_ParDo(self, transform_node, options):\n       if is_stateful_dofn:\n         step.add_property(PropertyNames.USES_KEYED_STATE, 'true')\n \n+        # Also checks whether the step allows shardable keyed states.\n+        for pcoll in transform_node.outputs.values():", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUzOTk1OA=="}, "originalCommit": {"oid": "e1aafbf5c360738f5c042bba61855055ab92588b"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgxMjc3Nw==", "bodyText": "@chamikaramj Do we have a JIRA tracking cleanup task in dataflow_runner.py and DataflowRunner.java after we have portable job submission?", "url": "https://github.com/apache/beam/pull/13405#discussion_r537812777", "createdAt": "2020-12-07T20:33:12Z", "author": {"login": "boyuanzz"}, "path": "sdks/python/apache_beam/runners/dataflow/dataflow_runner.py", "diffHunk": "@@ -1112,6 +1128,17 @@ def run_ParDo(self, transform_node, options):\n       if is_stateful_dofn:\n         step.add_property(PropertyNames.USES_KEYED_STATE, 'true')\n \n+        # Also checks whether the step allows shardable keyed states.\n+        for pcoll in transform_node.outputs.values():", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUzOTk1OA=="}, "originalCommit": {"oid": "e1aafbf5c360738f5c042bba61855055ab92588b"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgxNTExMg==", "bodyText": "https://issues.apache.org/jira/browse/BEAM-11360 for Python. Java does not need cleanups but need to be updated to use portable job submission which is tracked separately.", "url": "https://github.com/apache/beam/pull/13405#discussion_r537815112", "createdAt": "2020-12-07T20:37:06Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/runners/dataflow/dataflow_runner.py", "diffHunk": "@@ -1112,6 +1128,17 @@ def run_ParDo(self, transform_node, options):\n       if is_stateful_dofn:\n         step.add_property(PropertyNames.USES_KEYED_STATE, 'true')\n \n+        # Also checks whether the step allows shardable keyed states.\n+        for pcoll in transform_node.outputs.values():", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUzOTk1OA=="}, "originalCommit": {"oid": "e1aafbf5c360738f5c042bba61855055ab92588b"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgzNzM1MA==", "bodyText": "Thanks. Added a TODO here.", "url": "https://github.com/apache/beam/pull/13405#discussion_r537837350", "createdAt": "2020-12-07T21:14:19Z", "author": {"login": "nehsyc"}, "path": "sdks/python/apache_beam/runners/dataflow/dataflow_runner.py", "diffHunk": "@@ -1112,6 +1128,17 @@ def run_ParDo(self, transform_node, options):\n       if is_stateful_dofn:\n         step.add_property(PropertyNames.USES_KEYED_STATE, 'true')\n \n+        # Also checks whether the step allows shardable keyed states.\n+        for pcoll in transform_node.outputs.values():", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUzOTk1OA=="}, "originalCommit": {"oid": "e1aafbf5c360738f5c042bba61855055ab92588b"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg0MzI0MA==", "bodyText": "Thanks. Please note that all Runner v2 jobs will use portable job submission soon. So this change has to be done for portable job submission in the service side. We won't be necessary cleaning this up due to old SDKs and Runner v1 jobs (so don't think we need a TODO).", "url": "https://github.com/apache/beam/pull/13405#discussion_r537843240", "createdAt": "2020-12-07T21:24:14Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/runners/dataflow/dataflow_runner.py", "diffHunk": "@@ -1112,6 +1128,17 @@ def run_ParDo(self, transform_node, options):\n       if is_stateful_dofn:\n         step.add_property(PropertyNames.USES_KEYED_STATE, 'true')\n \n+        # Also checks whether the step allows shardable keyed states.\n+        for pcoll in transform_node.outputs.values():", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUzOTk1OA=="}, "originalCommit": {"oid": "e1aafbf5c360738f5c042bba61855055ab92588b"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg0ODk4MA==", "bodyText": "Basically, you need to able to configure the Dataflow v1beta3 step directly from the portable job description (runner API proto) without relying on the state set in intermediate pipeline object graph.", "url": "https://github.com/apache/beam/pull/13405#discussion_r537848980", "createdAt": "2020-12-07T21:34:02Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/runners/dataflow/dataflow_runner.py", "diffHunk": "@@ -1112,6 +1128,17 @@ def run_ParDo(self, transform_node, options):\n       if is_stateful_dofn:\n         step.add_property(PropertyNames.USES_KEYED_STATE, 'true')\n \n+        # Also checks whether the step allows shardable keyed states.\n+        for pcoll in transform_node.outputs.values():", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUzOTk1OA=="}, "originalCommit": {"oid": "e1aafbf5c360738f5c042bba61855055ab92588b"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg4MjI3Ng==", "bodyText": "Got it. Yes I have a cl pending for adding translation to the service which is waiting for this PR :) We are not considering adding this feature patch for python runner v1 jobs though; it is disabled for non-unified worker:\n\n  \n    \n      beam/sdks/python/apache_beam/runners/dataflow/ptransform_overrides.py\n    \n    \n         Line 365\n      in\n      c154179\n    \n    \n    \n    \n\n        \n          \n           if not apiclient._use_unified_worker(self.options):", "url": "https://github.com/apache/beam/pull/13405#discussion_r537882276", "createdAt": "2020-12-07T22:30:03Z", "author": {"login": "nehsyc"}, "path": "sdks/python/apache_beam/runners/dataflow/dataflow_runner.py", "diffHunk": "@@ -1112,6 +1128,17 @@ def run_ParDo(self, transform_node, options):\n       if is_stateful_dofn:\n         step.add_property(PropertyNames.USES_KEYED_STATE, 'true')\n \n+        # Also checks whether the step allows shardable keyed states.\n+        for pcoll in transform_node.outputs.values():", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDUzOTk1OA=="}, "originalCommit": {"oid": "e1aafbf5c360738f5c042bba61855055ab92588b"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM1NDA0OTU0OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/dataflow/ptransform_overrides.py", "isResolved": true, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wM1QwMDoyMTowNFrOH9ztPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMjo1MTo0OVrOIA-cNA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDU3MjM1MA==", "bodyText": "I think we also want to check use_runner_v2 as well since it's only supported in runner v2", "url": "https://github.com/apache/beam/pull/13405#discussion_r534572350", "createdAt": "2020-12-03T00:21:04Z", "author": {"login": "boyuanzz"}, "path": "sdks/python/apache_beam/runners/dataflow/ptransform_overrides.py", "diffHunk": "@@ -329,3 +330,44 @@ def expand(self, pcoll):\n         return {key: out for key in self.outputs}\n \n     return WriteToBigQuery(ptransform, self.outputs)\n+\n+\n+class GroupIntoBatchesWithShardedKeyPTransformOverride(PTransformOverride):\n+  \"\"\"A ``PTransformOverride`` for ``GroupIntoBatches.WithShardedKey``.\n+\n+  This override simply returns the original transform but additionally records\n+  the output PCollection in order to append required step properties during\n+  graph translation.\n+  \"\"\"\n+  def __init__(self, dataflow_runner, options):\n+    self.dataflow_runner = dataflow_runner\n+    self.options = options\n+\n+  def matches(self, applied_ptransform):\n+    # Imported here to avoid circular dependencies.\n+    # pylint: disable=wrong-import-order, wrong-import-position\n+    from apache_beam import util\n+\n+    transform = applied_ptransform.transform\n+\n+    if not isinstance(transform, util.GroupIntoBatches.WithShardedKey):\n+      return False\n+\n+    # The replacement is only valid for portable Streaming Engine jobs.\n+    standard_options = self.options.view_as(StandardOptions)\n+    if not standard_options.streaming:\n+      return False\n+    google_cloud_options = self.options.view_as(GoogleCloudOptions)\n+    if not google_cloud_options.enable_streaming_engine:\n+      return False\n+    experiments = self.options.view_as(DebugOptions).experiments or []\n+    if 'beam_fn_api' not in experiments:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e1aafbf5c360738f5c042bba61855055ab92588b"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc3MjE4NA==", "bodyText": "Good catch! Actually we should disable the feature when using beam_fn_api without use_unified_worker (JRH). Updated the logic and tests according in the latest commit.", "url": "https://github.com/apache/beam/pull/13405#discussion_r535772184", "createdAt": "2020-12-04T01:38:47Z", "author": {"login": "nehsyc"}, "path": "sdks/python/apache_beam/runners/dataflow/ptransform_overrides.py", "diffHunk": "@@ -329,3 +330,44 @@ def expand(self, pcoll):\n         return {key: out for key in self.outputs}\n \n     return WriteToBigQuery(ptransform, self.outputs)\n+\n+\n+class GroupIntoBatchesWithShardedKeyPTransformOverride(PTransformOverride):\n+  \"\"\"A ``PTransformOverride`` for ``GroupIntoBatches.WithShardedKey``.\n+\n+  This override simply returns the original transform but additionally records\n+  the output PCollection in order to append required step properties during\n+  graph translation.\n+  \"\"\"\n+  def __init__(self, dataflow_runner, options):\n+    self.dataflow_runner = dataflow_runner\n+    self.options = options\n+\n+  def matches(self, applied_ptransform):\n+    # Imported here to avoid circular dependencies.\n+    # pylint: disable=wrong-import-order, wrong-import-position\n+    from apache_beam import util\n+\n+    transform = applied_ptransform.transform\n+\n+    if not isinstance(transform, util.GroupIntoBatches.WithShardedKey):\n+      return False\n+\n+    # The replacement is only valid for portable Streaming Engine jobs.\n+    standard_options = self.options.view_as(StandardOptions)\n+    if not standard_options.streaming:\n+      return False\n+    google_cloud_options = self.options.view_as(GoogleCloudOptions)\n+    if not google_cloud_options.enable_streaming_engine:\n+      return False\n+    experiments = self.options.view_as(DebugOptions).experiments or []\n+    if 'beam_fn_api' not in experiments:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDU3MjM1MA=="}, "originalCommit": {"oid": "e1aafbf5c360738f5c042bba61855055ab92588b"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNTc3MjU4NQ==", "bodyText": "A second thought, would it just work for JRH as well?", "url": "https://github.com/apache/beam/pull/13405#discussion_r535772585", "createdAt": "2020-12-04T01:40:03Z", "author": {"login": "nehsyc"}, "path": "sdks/python/apache_beam/runners/dataflow/ptransform_overrides.py", "diffHunk": "@@ -329,3 +330,44 @@ def expand(self, pcoll):\n         return {key: out for key in self.outputs}\n \n     return WriteToBigQuery(ptransform, self.outputs)\n+\n+\n+class GroupIntoBatchesWithShardedKeyPTransformOverride(PTransformOverride):\n+  \"\"\"A ``PTransformOverride`` for ``GroupIntoBatches.WithShardedKey``.\n+\n+  This override simply returns the original transform but additionally records\n+  the output PCollection in order to append required step properties during\n+  graph translation.\n+  \"\"\"\n+  def __init__(self, dataflow_runner, options):\n+    self.dataflow_runner = dataflow_runner\n+    self.options = options\n+\n+  def matches(self, applied_ptransform):\n+    # Imported here to avoid circular dependencies.\n+    # pylint: disable=wrong-import-order, wrong-import-position\n+    from apache_beam import util\n+\n+    transform = applied_ptransform.transform\n+\n+    if not isinstance(transform, util.GroupIntoBatches.WithShardedKey):\n+      return False\n+\n+    # The replacement is only valid for portable Streaming Engine jobs.\n+    standard_options = self.options.view_as(StandardOptions)\n+    if not standard_options.streaming:\n+      return False\n+    google_cloud_options = self.options.view_as(GoogleCloudOptions)\n+    if not google_cloud_options.enable_streaming_engine:\n+      return False\n+    experiments = self.options.view_as(DebugOptions).experiments or []\n+    if 'beam_fn_api' not in experiments:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDU3MjM1MA=="}, "originalCommit": {"oid": "e1aafbf5c360738f5c042bba61855055ab92588b"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ2MTM4OQ==", "bodyText": "Yeah it should just work on JRH. My question is that do we want to do the expansion even though we know that we are not going to shard it?", "url": "https://github.com/apache/beam/pull/13405#discussion_r536461389", "createdAt": "2020-12-05T00:34:24Z", "author": {"login": "boyuanzz"}, "path": "sdks/python/apache_beam/runners/dataflow/ptransform_overrides.py", "diffHunk": "@@ -329,3 +330,44 @@ def expand(self, pcoll):\n         return {key: out for key in self.outputs}\n \n     return WriteToBigQuery(ptransform, self.outputs)\n+\n+\n+class GroupIntoBatchesWithShardedKeyPTransformOverride(PTransformOverride):\n+  \"\"\"A ``PTransformOverride`` for ``GroupIntoBatches.WithShardedKey``.\n+\n+  This override simply returns the original transform but additionally records\n+  the output PCollection in order to append required step properties during\n+  graph translation.\n+  \"\"\"\n+  def __init__(self, dataflow_runner, options):\n+    self.dataflow_runner = dataflow_runner\n+    self.options = options\n+\n+  def matches(self, applied_ptransform):\n+    # Imported here to avoid circular dependencies.\n+    # pylint: disable=wrong-import-order, wrong-import-position\n+    from apache_beam import util\n+\n+    transform = applied_ptransform.transform\n+\n+    if not isinstance(transform, util.GroupIntoBatches.WithShardedKey):\n+      return False\n+\n+    # The replacement is only valid for portable Streaming Engine jobs.\n+    standard_options = self.options.view_as(StandardOptions)\n+    if not standard_options.streaming:\n+      return False\n+    google_cloud_options = self.options.view_as(GoogleCloudOptions)\n+    if not google_cloud_options.enable_streaming_engine:\n+      return False\n+    experiments = self.options.view_as(DebugOptions).experiments or []\n+    if 'beam_fn_api' not in experiments:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDU3MjM1MA=="}, "originalCommit": {"oid": "e1aafbf5c360738f5c042bba61855055ab92588b"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNjQ4MTg3Ng==", "bodyText": "Sorry I didn't get the question. If the feature is not enabled, the properties will not be appended to the step therefore no expansion. Also the SDK implementation applies a default sharding which != no sharding.\n\n  \n    \n      beam/sdks/python/apache_beam/transforms/util.py\n    \n    \n         Line 822\n      in\n      30f9a60\n    \n    \n    \n    \n\n        \n          \n           # Use [uuid, thread id] as the shard id.", "url": "https://github.com/apache/beam/pull/13405#discussion_r536481876", "createdAt": "2020-12-05T02:08:49Z", "author": {"login": "nehsyc"}, "path": "sdks/python/apache_beam/runners/dataflow/ptransform_overrides.py", "diffHunk": "@@ -329,3 +330,44 @@ def expand(self, pcoll):\n         return {key: out for key in self.outputs}\n \n     return WriteToBigQuery(ptransform, self.outputs)\n+\n+\n+class GroupIntoBatchesWithShardedKeyPTransformOverride(PTransformOverride):\n+  \"\"\"A ``PTransformOverride`` for ``GroupIntoBatches.WithShardedKey``.\n+\n+  This override simply returns the original transform but additionally records\n+  the output PCollection in order to append required step properties during\n+  graph translation.\n+  \"\"\"\n+  def __init__(self, dataflow_runner, options):\n+    self.dataflow_runner = dataflow_runner\n+    self.options = options\n+\n+  def matches(self, applied_ptransform):\n+    # Imported here to avoid circular dependencies.\n+    # pylint: disable=wrong-import-order, wrong-import-position\n+    from apache_beam import util\n+\n+    transform = applied_ptransform.transform\n+\n+    if not isinstance(transform, util.GroupIntoBatches.WithShardedKey):\n+      return False\n+\n+    # The replacement is only valid for portable Streaming Engine jobs.\n+    standard_options = self.options.view_as(StandardOptions)\n+    if not standard_options.streaming:\n+      return False\n+    google_cloud_options = self.options.view_as(GoogleCloudOptions)\n+    if not google_cloud_options.enable_streaming_engine:\n+      return False\n+    experiments = self.options.view_as(DebugOptions).experiments or []\n+    if 'beam_fn_api' not in experiments:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDU3MjM1MA=="}, "originalCommit": {"oid": "e1aafbf5c360738f5c042bba61855055ab92588b"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg5Mzk0MA==", "bodyText": "I think your latest changes have addressed my comment. Basically, I don't think we want to the transform override when only with beam_fn_api", "url": "https://github.com/apache/beam/pull/13405#discussion_r537893940", "createdAt": "2020-12-07T22:51:49Z", "author": {"login": "boyuanzz"}, "path": "sdks/python/apache_beam/runners/dataflow/ptransform_overrides.py", "diffHunk": "@@ -329,3 +330,44 @@ def expand(self, pcoll):\n         return {key: out for key in self.outputs}\n \n     return WriteToBigQuery(ptransform, self.outputs)\n+\n+\n+class GroupIntoBatchesWithShardedKeyPTransformOverride(PTransformOverride):\n+  \"\"\"A ``PTransformOverride`` for ``GroupIntoBatches.WithShardedKey``.\n+\n+  This override simply returns the original transform but additionally records\n+  the output PCollection in order to append required step properties during\n+  graph translation.\n+  \"\"\"\n+  def __init__(self, dataflow_runner, options):\n+    self.dataflow_runner = dataflow_runner\n+    self.options = options\n+\n+  def matches(self, applied_ptransform):\n+    # Imported here to avoid circular dependencies.\n+    # pylint: disable=wrong-import-order, wrong-import-position\n+    from apache_beam import util\n+\n+    transform = applied_ptransform.transform\n+\n+    if not isinstance(transform, util.GroupIntoBatches.WithShardedKey):\n+      return False\n+\n+    # The replacement is only valid for portable Streaming Engine jobs.\n+    standard_options = self.options.view_as(StandardOptions)\n+    if not standard_options.streaming:\n+      return False\n+    google_cloud_options = self.options.view_as(GoogleCloudOptions)\n+    if not google_cloud_options.enable_streaming_engine:\n+      return False\n+    experiments = self.options.view_as(DebugOptions).experiments or []\n+    if 'beam_fn_api' not in experiments:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNDU3MjM1MA=="}, "originalCommit": {"oid": "e1aafbf5c360738f5c042bba61855055ab92588b"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NjU5MTkzOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/transforms/util.py", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMjoyMjo1MFrOIA9eZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwMDo0MToxM1rOIB82Bg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg3ODExOA==", "bodyText": "We can get ride of GroupIntoBatchesParams by creating GroupIntoBatchesPayload  directly here and accessing the payload directly below.", "url": "https://github.com/apache/beam/pull/13405#discussion_r537878118", "createdAt": "2020-12-07T22:22:50Z", "author": {"login": "boyuanzz"}, "path": "sdks/python/apache_beam/transforms/util.py", "diffHunk": "@@ -825,7 +827,60 @@ def expand(self, pcoll):\n               key_value[1]))\n       return (\n           sharded_pcoll\n-          | GroupIntoBatches(self.batch_size, self.max_buffering_duration_secs))\n+          | GroupIntoBatches(\n+              self.params.batch_size, self.params.max_buffering_duration_secs))\n+\n+    def to_runner_api_parameter(\n+        self,\n+        unused_context  # type: PipelineContext\n+    ):  # type: (...) -> Tuple[str, beam_runner_api_pb2.GroupIntoBatchesPayload]\n+      return (\n+          common_urns.composites.GROUP_INTO_BATCHES_WITH_SHARDED_KEY.urn,\n+          self.params.get_payload())", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1541796435cbd2ee775b6c89ec628f8564a3bcd"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODAyNzEzNg==", "bodyText": "The main reason I extracted the params to a class was to reuse the code as much as possible since the params of GroupIntoBatches and WithShardedKey are exactly the same. The class also does parameter validation other than conversion from/to runner api, and perhaps it would make it easier to add more params in the future. But let me know if you have opinions.", "url": "https://github.com/apache/beam/pull/13405#discussion_r538027136", "createdAt": "2020-12-08T04:32:30Z", "author": {"login": "nehsyc"}, "path": "sdks/python/apache_beam/transforms/util.py", "diffHunk": "@@ -825,7 +827,60 @@ def expand(self, pcoll):\n               key_value[1]))\n       return (\n           sharded_pcoll\n-          | GroupIntoBatches(self.batch_size, self.max_buffering_duration_secs))\n+          | GroupIntoBatches(\n+              self.params.batch_size, self.params.max_buffering_duration_secs))\n+\n+    def to_runner_api_parameter(\n+        self,\n+        unused_context  # type: PipelineContext\n+    ):  # type: (...) -> Tuple[str, beam_runner_api_pb2.GroupIntoBatchesPayload]\n+      return (\n+          common_urns.composites.GROUP_INTO_BATCHES_WITH_SHARDED_KEY.urn,\n+          self.params.get_payload())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg3ODExOA=="}, "originalCommit": {"oid": "c1541796435cbd2ee775b6c89ec628f8564a3bcd"}, "originalPosition": 97}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODkxNjM1OA==", "bodyText": "I see. Thanks for the explanation.", "url": "https://github.com/apache/beam/pull/13405#discussion_r538916358", "createdAt": "2020-12-09T00:41:13Z", "author": {"login": "boyuanzz"}, "path": "sdks/python/apache_beam/transforms/util.py", "diffHunk": "@@ -825,7 +827,60 @@ def expand(self, pcoll):\n               key_value[1]))\n       return (\n           sharded_pcoll\n-          | GroupIntoBatches(self.batch_size, self.max_buffering_duration_secs))\n+          | GroupIntoBatches(\n+              self.params.batch_size, self.params.max_buffering_duration_secs))\n+\n+    def to_runner_api_parameter(\n+        self,\n+        unused_context  # type: PipelineContext\n+    ):  # type: (...) -> Tuple[str, beam_runner_api_pb2.GroupIntoBatchesPayload]\n+      return (\n+          common_urns.composites.GROUP_INTO_BATCHES_WITH_SHARDED_KEY.urn,\n+          self.params.get_payload())", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg3ODExOA=="}, "originalCommit": {"oid": "c1541796435cbd2ee775b6c89ec628f8564a3bcd"}, "originalPosition": 97}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NjY0MzQ4OnYy", "diffSide": "RIGHT", "path": "model/pipeline/src/main/proto/beam_runner_api.proto", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMjozNTo1NFrOIA97TQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQwNDozMjoxNFrOIBGkCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg4NTUxNw==", "bodyText": "It seems like max_buffering_duration_millis is None and max_buffering_duration_millis  == 0 means the same. right? You may want to add a comment about  what will happen if max_buffering_duration_millis is not specified or max_buffering_duration_millis is set to 0. Someone might think max_buffering_duration_millis = 0 or max_buffering_duration_millis is None will make the transform output elements as soon as possible.", "url": "https://github.com/apache/beam/pull/13405#discussion_r537885517", "createdAt": "2020-12-07T22:35:54Z", "author": {"login": "boyuanzz"}, "path": "model/pipeline/src/main/proto/beam_runner_api.proto", "diffHunk": "@@ -706,6 +714,16 @@ message PubSubWritePayload {\n   string id_attribute = 3;\n }\n \n+// Payload for GroupIntoBatches composite transform.\n+message GroupIntoBatchesPayload {\n+\n+  // (Required) Max size of a batch.\n+  int64 batch_size = 1;\n+\n+  // (Optional) Max duration a batch is allowed to be cached in states.\n+  int64 max_buffering_duration_millis = 2;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1541796435cbd2ee775b6c89ec628f8564a3bcd"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODAyNzAxOQ==", "bodyText": "Were you suggesting adding a comment here in the runner proto or in GroupIntoBatches transform? I added some comments around here when changing the API: \n  \n    \n      beam/sdks/python/apache_beam/transforms/util.py\n    \n    \n         Line 767\n      in\n      c154179\n    \n    \n    \n    \n\n        \n          \n                   an int or float. Setting this parameter to zero effectively means no", "url": "https://github.com/apache/beam/pull/13405#discussion_r538027019", "createdAt": "2020-12-08T04:32:14Z", "author": {"login": "nehsyc"}, "path": "model/pipeline/src/main/proto/beam_runner_api.proto", "diffHunk": "@@ -706,6 +714,16 @@ message PubSubWritePayload {\n   string id_attribute = 3;\n }\n \n+// Payload for GroupIntoBatches composite transform.\n+message GroupIntoBatchesPayload {\n+\n+  // (Required) Max size of a batch.\n+  int64 batch_size = 1;\n+\n+  // (Optional) Max duration a batch is allowed to be cached in states.\n+  int64 max_buffering_duration_millis = 2;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg4NTUxNw=="}, "originalCommit": {"oid": "c1541796435cbd2ee775b6c89ec628f8564a3bcd"}, "originalPosition": 33}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NjY3MTc3OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/dataflow/internal/names.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMjo0MzowOFrOIA-LTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQwNDozMjoxNlrOIBGkHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg4OTYxNQ==", "bodyText": "Could you please add some explanations on what ALLOWS_SHARDABLE_STATE  and PRESERVES_KEYS mean?", "url": "https://github.com/apache/beam/pull/13405#discussion_r537889615", "createdAt": "2020-12-07T22:43:08Z", "author": {"login": "boyuanzz"}, "path": "sdks/python/apache_beam/runners/dataflow/internal/names.py", "diffHunk": "@@ -69,6 +69,7 @@ class PropertyNames(object):\n \n   Property strings as they are expected in the CloudWorkflow protos.\n   \"\"\"\n+  ALLOWS_SHARDABLE_STATE = 'allows_shardable_state'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1541796435cbd2ee775b6c89ec628f8564a3bcd"}, "originalPosition": 4}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODAyNzAzNw==", "bodyText": "Done.", "url": "https://github.com/apache/beam/pull/13405#discussion_r538027037", "createdAt": "2020-12-08T04:32:16Z", "author": {"login": "nehsyc"}, "path": "sdks/python/apache_beam/runners/dataflow/internal/names.py", "diffHunk": "@@ -69,6 +69,7 @@ class PropertyNames(object):\n \n   Property strings as they are expected in the CloudWorkflow protos.\n   \"\"\"\n+  ALLOWS_SHARDABLE_STATE = 'allows_shardable_state'", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg4OTYxNQ=="}, "originalCommit": {"oid": "c1541796435cbd2ee775b6c89ec628f8564a3bcd"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM3NjY5NDQ5OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/dataflow/ptransform_overrides.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wN1QyMjo0OTo0NFrOIA-YOw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQwNDozMjoyNFrOIBGkPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg5MjkyMw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                # The replacement is only valid for portable Streaming Engine jobs.\n          \n          \n            \n                # The replacement is only valid for portable Streaming Engine jobs with runner_v2.", "url": "https://github.com/apache/beam/pull/13405#discussion_r537892923", "createdAt": "2020-12-07T22:49:44Z", "author": {"login": "boyuanzz"}, "path": "sdks/python/apache_beam/runners/dataflow/ptransform_overrides.py", "diffHunk": "@@ -329,3 +330,46 @@ def expand(self, pcoll):\n         return {key: out for key in self.outputs}\n \n     return WriteToBigQuery(ptransform, self.outputs)\n+\n+\n+class GroupIntoBatchesWithShardedKeyPTransformOverride(PTransformOverride):\n+  \"\"\"A ``PTransformOverride`` for ``GroupIntoBatches.WithShardedKey``.\n+\n+  This override simply returns the original transform but additionally records\n+  the output PCollection in order to append required step properties during\n+  graph translation.\n+  \"\"\"\n+  def __init__(self, dataflow_runner, options):\n+    self.dataflow_runner = dataflow_runner\n+    self.options = options\n+\n+  def matches(self, applied_ptransform):\n+    # Imported here to avoid circular dependencies.\n+    # pylint: disable=wrong-import-order, wrong-import-position\n+    from apache_beam import util\n+\n+    transform = applied_ptransform.transform\n+\n+    if not isinstance(transform, util.GroupIntoBatches.WithShardedKey):\n+      return False\n+\n+    # The replacement is only valid for portable Streaming Engine jobs.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c1541796435cbd2ee775b6c89ec628f8564a3bcd"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODAyNzA2OQ==", "bodyText": "Done.", "url": "https://github.com/apache/beam/pull/13405#discussion_r538027069", "createdAt": "2020-12-08T04:32:24Z", "author": {"login": "nehsyc"}, "path": "sdks/python/apache_beam/runners/dataflow/ptransform_overrides.py", "diffHunk": "@@ -329,3 +330,46 @@ def expand(self, pcoll):\n         return {key: out for key in self.outputs}\n \n     return WriteToBigQuery(ptransform, self.outputs)\n+\n+\n+class GroupIntoBatchesWithShardedKeyPTransformOverride(PTransformOverride):\n+  \"\"\"A ``PTransformOverride`` for ``GroupIntoBatches.WithShardedKey``.\n+\n+  This override simply returns the original transform but additionally records\n+  the output PCollection in order to append required step properties during\n+  graph translation.\n+  \"\"\"\n+  def __init__(self, dataflow_runner, options):\n+    self.dataflow_runner = dataflow_runner\n+    self.options = options\n+\n+  def matches(self, applied_ptransform):\n+    # Imported here to avoid circular dependencies.\n+    # pylint: disable=wrong-import-order, wrong-import-position\n+    from apache_beam import util\n+\n+    transform = applied_ptransform.transform\n+\n+    if not isinstance(transform, util.GroupIntoBatches.WithShardedKey):\n+      return False\n+\n+    # The replacement is only valid for portable Streaming Engine jobs.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzg5MjkyMw=="}, "originalCommit": {"oid": "c1541796435cbd2ee775b6c89ec628f8564a3bcd"}, "originalPosition": 35}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4MzQyOTA3OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/transforms/util.py", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwMDo0NDo1M1rOIB88Fg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQwMTowMjo0NFrOIB9X9Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODkxNzkxMA==", "bodyText": "Please add some pydoc to describe this class. Other than this, this PR is ready to go.", "url": "https://github.com/apache/beam/pull/13405#discussion_r538917910", "createdAt": "2020-12-09T00:44:53Z", "author": {"login": "boyuanzz"}, "path": "sdks/python/apache_beam/transforms/util.py", "diffHunk": "@@ -825,7 +827,60 @@ def expand(self, pcoll):\n               key_value[1]))\n       return (\n           sharded_pcoll\n-          | GroupIntoBatches(self.batch_size, self.max_buffering_duration_secs))\n+          | GroupIntoBatches(\n+              self.params.batch_size, self.params.max_buffering_duration_secs))\n+\n+    def to_runner_api_parameter(\n+        self,\n+        unused_context  # type: PipelineContext\n+    ):  # type: (...) -> Tuple[str, beam_runner_api_pb2.GroupIntoBatchesPayload]\n+      return (\n+          common_urns.composites.GROUP_INTO_BATCHES_WITH_SHARDED_KEY.urn,\n+          self.params.get_payload())\n+\n+    @staticmethod\n+    @PTransform.register_urn(\n+        common_urns.composites.GROUP_INTO_BATCHES_WITH_SHARDED_KEY.urn,\n+        beam_runner_api_pb2.GroupIntoBatchesPayload)\n+    def from_runner_api_parameter(unused_ptransform, proto, unused_context):\n+      return GroupIntoBatches.WithShardedKey(\n+          *GroupIntoBatchesParams.parse_payload(proto))\n+\n+\n+class GroupIntoBatchesParams:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c764e7bca1ab6d30df56ee7162bfa84f317eb064"}, "originalPosition": 108}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODkyNTA0NQ==", "bodyText": "Done. I also changed the class name to _GroupIntoBatchesParams; it's not intended to be \"public\" and referenced outside of this file.", "url": "https://github.com/apache/beam/pull/13405#discussion_r538925045", "createdAt": "2020-12-09T01:02:44Z", "author": {"login": "nehsyc"}, "path": "sdks/python/apache_beam/transforms/util.py", "diffHunk": "@@ -825,7 +827,60 @@ def expand(self, pcoll):\n               key_value[1]))\n       return (\n           sharded_pcoll\n-          | GroupIntoBatches(self.batch_size, self.max_buffering_duration_secs))\n+          | GroupIntoBatches(\n+              self.params.batch_size, self.params.max_buffering_duration_secs))\n+\n+    def to_runner_api_parameter(\n+        self,\n+        unused_context  # type: PipelineContext\n+    ):  # type: (...) -> Tuple[str, beam_runner_api_pb2.GroupIntoBatchesPayload]\n+      return (\n+          common_urns.composites.GROUP_INTO_BATCHES_WITH_SHARDED_KEY.urn,\n+          self.params.get_payload())\n+\n+    @staticmethod\n+    @PTransform.register_urn(\n+        common_urns.composites.GROUP_INTO_BATCHES_WITH_SHARDED_KEY.urn,\n+        beam_runner_api_pb2.GroupIntoBatchesPayload)\n+    def from_runner_api_parameter(unused_ptransform, proto, unused_context):\n+      return GroupIntoBatches.WithShardedKey(\n+          *GroupIntoBatchesParams.parse_payload(proto))\n+\n+\n+class GroupIntoBatchesParams:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODkxNzkxMA=="}, "originalCommit": {"oid": "c764e7bca1ab6d30df56ee7162bfa84f317eb064"}, "originalPosition": 108}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2634, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}