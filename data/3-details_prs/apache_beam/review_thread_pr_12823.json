{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDg0ODI3ODQ2", "number": 12823, "reviewThreads": {"totalCount": 17, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQxNToxMDowNVrOEjo8Jw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMjowMDozN1rOEmzDPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA1ODA2Mzc1OnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0xNVQxNToxMDowNVrOHSGrzw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yMlQwNzo1NjozNVrOHVuSVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc0NTkzNQ==", "bodyText": "I wonder if this should be called 'Using SnowflakeIO in Python SDK' - what do you think? Users may not care if it's cross-lang, but rather that it works in their language of choice. Thoughts?", "url": "https://github.com/apache/beam/pull/12823#discussion_r488745935", "createdAt": "2020-09-15T15:10:05Z", "author": {"login": "pabloem"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,206 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Cross Language", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e3d9c16f223c79763e99bea6b8ca06082e79eae9"}, "originalPosition": 414}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc3NjU3MA==", "bodyText": "+1\nIt's worth having a note somewhere like \"SnowflakeIO in Python can only be used on runners that support cross-language transforms\". I'm wary of listing specific runners here since it will get out of date. It seems we should have a central location that lists runners that support cross-language, seems like a good thing to have on the compatibility matrix.", "url": "https://github.com/apache/beam/pull/12823#discussion_r488776570", "createdAt": "2020-09-15T15:51:20Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,206 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Cross Language", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc0NTkzNQ=="}, "originalCommit": {"oid": "e3d9c16f223c79763e99bea6b8ca06082e79eae9"}, "originalPosition": 414}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4OTQ2MDEwNQ==", "bodyText": "I changed to Using SnowflakeIO in Python SDK' and updated a little intro.\nAbout the second, maybe lets create an issue for this?", "url": "https://github.com/apache/beam/pull/12823#discussion_r489460105", "createdAt": "2020-09-16T13:59:28Z", "author": {"login": "purbanow"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,206 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Cross Language", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc0NTkzNQ=="}, "originalCommit": {"oid": "e3d9c16f223c79763e99bea6b8ca06082e79eae9"}, "originalPosition": 414}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MjU0MDUwMQ==", "bodyText": "@pabloem @TheNeuralBit WDYT?", "url": "https://github.com/apache/beam/pull/12823#discussion_r492540501", "createdAt": "2020-09-22T07:56:35Z", "author": {"login": "purbanow"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,206 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Cross Language", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4ODc0NTkzNQ=="}, "originalCommit": {"oid": "e3d9c16f223c79763e99bea6b8ca06082e79eae9"}, "originalPosition": 414}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTA5OTcwOnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMTozMjo1MFrOHXBvmg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yOFQyMjo1MDo0NVrOHZSa0w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwNzg2Ng==", "bodyText": "Have you considered using Beam Schemas and Rows here instead of having users provide a CSV mapper? If the SnowflakeIO Read transform produces a PCollection with a schema, then on the Python side we will generate a class that lets users access each field by attribute.", "url": "https://github.com/apache/beam/pull/12823#discussion_r493907866", "createdAt": "2020-09-23T21:32:50Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+\n+#### Required parameters\n+- `server_name` Full Snowflake server name with an account, zone, and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Name of the Google Cloud Storage bucket. Bucket will be used as a temporary location for storing CSV files. Those temporary directories will be named `sf_copy_csv_DATE_TIME_RANDOMSUFFIX` and they will be removed automatically once Read operation finishes.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 471}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkyMTI2OQ==", "bodyText": "Similarly on the WriteToSnowflake side we can inspect user types and infer a schema from it", "url": "https://github.com/apache/beam/pull/12823#discussion_r493921269", "createdAt": "2020-09-23T22:03:41Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+\n+#### Required parameters\n+- `server_name` Full Snowflake server name with an account, zone, and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Name of the Google Cloud Storage bucket. Bucket will be used as a temporary location for storing CSV files. Those temporary directories will be named `sf_copy_csv_DATE_TIME_RANDOMSUFFIX` and they will be removed automatically once Read operation finishes.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwNzg2Ng=="}, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 471}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc5MTA1OQ==", "bodyText": "I think it was considered but at the time of writing this connector Rows and Schemas were not well known and the csv + user mapper were already implemented. It would definitely be a nice feature to be done in the future.", "url": "https://github.com/apache/beam/pull/12823#discussion_r494791059", "createdAt": "2020-09-25T07:10:10Z", "author": {"login": "piotr-szuberski"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+\n+#### Required parameters\n+- `server_name` Full Snowflake server name with an account, zone, and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Name of the Google Cloud Storage bucket. Bucket will be used as a temporary location for storing CSV files. Those temporary directories will be named `sf_copy_csv_DATE_TIME_RANDOMSUFFIX` and they will be removed automatically once Read operation finishes.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwNzg2Ng=="}, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 471}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NjI3ODIyNw==", "bodyText": "+1, thanks", "url": "https://github.com/apache/beam/pull/12823#discussion_r496278227", "createdAt": "2020-09-28T22:50:45Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+\n+#### Required parameters\n+- `server_name` Full Snowflake server name with an account, zone, and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Name of the Google Cloud Storage bucket. Bucket will be used as a temporary location for storing CSV files. Those temporary directories will be named `sf_copy_csv_DATE_TIME_RANDOMSUFFIX` and they will be removed automatically once Read operation finishes.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwNzg2Ng=="}, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 471}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTEwMTcyOnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMTozMzozMVrOHXBwyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwNjowMToxMVrOHX2CDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwODE3MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.\n          \n          \n            \n            - `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/current/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.\n          \n      \n    \n    \n  \n\nAll of the javadoc and pydoc links should refer to current instead of a specific version number", "url": "https://github.com/apache/beam/pull/12823#discussion_r493908171", "createdAt": "2020-09-23T21:33:31Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+\n+#### Required parameters\n+- `server_name` Full Snowflake server name with an account, zone, and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Name of the Google Cloud Storage bucket. Bucket will be used as a temporary location for storing CSV files. Those temporary directories will be named `sf_copy_csv_DATE_TIME_RANDOMSUFFIX` and they will be removed automatically once Read operation finishes.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 471}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc2NDU1Ng==", "bodyText": "Good to know that! Done.", "url": "https://github.com/apache/beam/pull/12823#discussion_r494764556", "createdAt": "2020-09-25T06:01:11Z", "author": {"login": "piotr-szuberski"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+\n+#### Required parameters\n+- `server_name` Full Snowflake server name with an account, zone, and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Name of the Google Cloud Storage bucket. Bucket will be used as a temporary location for storing CSV files. Those temporary directories will be named `sf_copy_csv_DATE_TIME_RANDOMSUFFIX` and they will be removed automatically once Read operation finishes.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwODE3MQ=="}, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 471}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTEwMjIwOnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMTozMzo0NFrOHXBxEg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwNjowMToxM1rOHX2CHQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwODI0Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n          \n          \n            \n            One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/current/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.", "url": "https://github.com/apache/beam/pull/12823#discussion_r493908242", "createdAt": "2020-09-23T21:33:44Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 428}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc2NDU3Mw==", "bodyText": "Done.", "url": "https://github.com/apache/beam/pull/12823#discussion_r494764573", "createdAt": "2020-09-25T06:01:13Z", "author": {"login": "piotr-szuberski"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwODI0Mg=="}, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 428}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTExMjQ3OnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMTozNzoyOVrOHXB3TQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwNjoxMzoxMlrOHX2RXA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwOTgzNw==", "bodyText": "This is optional right? In other cross-language transforms we will download the appropriate expansion service jar and start it for you if a URL isn't specified.", "url": "https://github.com/apache/beam/pull/12823#discussion_r493909837", "createdAt": "2020-09-23T21:37:29Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+\n+#### Required parameters\n+- `server_name` Full Snowflake server name with an account, zone, and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Name of the Google Cloud Storage bucket. Bucket will be used as a temporary location for storing CSV files. Those temporary directories will be named `sf_copy_csv_DATE_TIME_RANDOMSUFFIX` and they will be removed automatically once Read operation finishes.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.\n+Example:\n+{{< highlight >}}\n+def csv_mapper(strings_array):\n+    return User(strings_array[0], int(strings_array[1])))\n+{{< /highlight >}}\n+\n+- `table or query` Specifies a Snowflake table name or custom SQL query\n+\n+- `expansion_service`: specifies URL of expansion service.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 480}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc2ODQ3Ng==", "bodyText": "Of course it is. Done for both read and write.", "url": "https://github.com/apache/beam/pull/12823#discussion_r494768476", "createdAt": "2020-09-25T06:13:12Z", "author": {"login": "piotr-szuberski"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+\n+#### Required parameters\n+- `server_name` Full Snowflake server name with an account, zone, and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Name of the Google Cloud Storage bucket. Bucket will be used as a temporary location for storing CSV files. Those temporary directories will be named `sf_copy_csv_DATE_TIME_RANDOMSUFFIX` and they will be removed automatically once Read operation finishes.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.\n+Example:\n+{{< highlight >}}\n+def csv_mapper(strings_array):\n+    return User(strings_array[0], int(strings_array[1])))\n+{{< /highlight >}}\n+\n+- `table or query` Specifies a Snowflake table name or custom SQL query\n+\n+- `expansion_service`: specifies URL of expansion service.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwOTgzNw=="}, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 480}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTExMzIzOnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMTozNzo0NFrOHXB3uQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwNjoxODo1MFrOHX2YgQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwOTk0NQ==", "bodyText": "nit: please make sure only the argument names are code formatted when you are referencing two different args, e.g.:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - `table or query` Specifies a Snowflake table name or custom SQL query\n          \n          \n            \n            - `table` or `query` Specifies a Snowflake table name or custom SQL query", "url": "https://github.com/apache/beam/pull/12823#discussion_r493909945", "createdAt": "2020-09-23T21:37:44Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+\n+#### Required parameters\n+- `server_name` Full Snowflake server name with an account, zone, and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Name of the Google Cloud Storage bucket. Bucket will be used as a temporary location for storing CSV files. Those temporary directories will be named `sf_copy_csv_DATE_TIME_RANDOMSUFFIX` and they will be removed automatically once Read operation finishes.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.\n+Example:\n+{{< highlight >}}\n+def csv_mapper(strings_array):\n+    return User(strings_array[0], int(strings_array[1])))\n+{{< /highlight >}}\n+\n+- `table or query` Specifies a Snowflake table name or custom SQL query", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 478}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc3MDMwNQ==", "bodyText": "Done for this and the rest of such cases.", "url": "https://github.com/apache/beam/pull/12823#discussion_r494770305", "createdAt": "2020-09-25T06:18:50Z", "author": {"login": "piotr-szuberski"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+\n+#### Required parameters\n+- `server_name` Full Snowflake server name with an account, zone, and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Name of the Google Cloud Storage bucket. Bucket will be used as a temporary location for storing CSV files. Those temporary directories will be named `sf_copy_csv_DATE_TIME_RANDOMSUFFIX` and they will be removed automatically once Read operation finishes.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.\n+Example:\n+{{< highlight >}}\n+def csv_mapper(strings_array):\n+    return User(strings_array[0], int(strings_array[1])))\n+{{< /highlight >}}\n+\n+- `table or query` Specifies a Snowflake table name or custom SQL query", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkwOTk0NQ=="}, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 478}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTExNTkxOnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMTozODo0NVrOHXB5WQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwNjoxOTo1M1rOHX2Z-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkxMDM2MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - `role`: specifies Snowflake role. If not specified then the user's default will be used.\n          \n          \n            \n            - `role` specifies Snowflake role. If not specified the user's default will be used.", "url": "https://github.com/apache/beam/pull/12823#discussion_r493910361", "createdAt": "2020-09-23T21:38:45Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+\n+#### Required parameters\n+- `server_name` Full Snowflake server name with an account, zone, and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Name of the Google Cloud Storage bucket. Bucket will be used as a temporary location for storing CSV files. Those temporary directories will be named `sf_copy_csv_DATE_TIME_RANDOMSUFFIX` and they will be removed automatically once Read operation finishes.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.\n+Example:\n+{{< highlight >}}\n+def csv_mapper(strings_array):\n+    return User(strings_array[0], int(strings_array[1])))\n+{{< /highlight >}}\n+\n+- `table or query` Specifies a Snowflake table name or custom SQL query\n+\n+- `expansion_service`: specifies URL of expansion service.\n+\n+#### Authentication parameters\n+It\u2019s required to pass one of the following combinations of valid parameters for authentication:\n+- `username and password` Specifies username and password for username/password authentication method.\n+\n+- `private_key_path and private_key_passphrase` Specifies a path to private key and passphrase for key/pair authentication method.\n+\n+- `raw_private_key and private_key_passphrase` Specifies a private key and passphrase for key/pair authentication method.\n+\n+- `o_auth_token` Specifies access token for OAuth authentication method.\n+\n+#### Additional parameters\n+- `role`: specifies Snowflake role. If not specified then the user's default will be used.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 493}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc3MDY4Mg==", "bodyText": "Done.", "url": "https://github.com/apache/beam/pull/12823#discussion_r494770682", "createdAt": "2020-09-25T06:19:53Z", "author": {"login": "piotr-szuberski"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+\n+#### Required parameters\n+- `server_name` Full Snowflake server name with an account, zone, and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Name of the Google Cloud Storage bucket. Bucket will be used as a temporary location for storing CSV files. Those temporary directories will be named `sf_copy_csv_DATE_TIME_RANDOMSUFFIX` and they will be removed automatically once Read operation finishes.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.\n+Example:\n+{{< highlight >}}\n+def csv_mapper(strings_array):\n+    return User(strings_array[0], int(strings_array[1])))\n+{{< /highlight >}}\n+\n+- `table or query` Specifies a Snowflake table name or custom SQL query\n+\n+- `expansion_service`: specifies URL of expansion service.\n+\n+#### Authentication parameters\n+It\u2019s required to pass one of the following combinations of valid parameters for authentication:\n+- `username and password` Specifies username and password for username/password authentication method.\n+\n+- `private_key_path and private_key_passphrase` Specifies a path to private key and passphrase for key/pair authentication method.\n+\n+- `raw_private_key and private_key_passphrase` Specifies a private key and passphrase for key/pair authentication method.\n+\n+- `o_auth_token` Specifies access token for OAuth authentication method.\n+\n+#### Additional parameters\n+- `role`: specifies Snowflake role. If not specified then the user's default will be used.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkxMDM2MQ=="}, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 493}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTExNjMxOnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMTozODo1M1rOHXB5kw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwNjoxOTo1OFrOHX2aEg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkxMDQxOQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - `warehouse`: specifies Snowflake warehouse name. If not specified then the user's default will be used.\n          \n          \n            \n            - `warehouse` specifies Snowflake warehouse name. If not specified the user's default will be used.", "url": "https://github.com/apache/beam/pull/12823#discussion_r493910419", "createdAt": "2020-09-23T21:38:53Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+\n+#### Required parameters\n+- `server_name` Full Snowflake server name with an account, zone, and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Name of the Google Cloud Storage bucket. Bucket will be used as a temporary location for storing CSV files. Those temporary directories will be named `sf_copy_csv_DATE_TIME_RANDOMSUFFIX` and they will be removed automatically once Read operation finishes.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.\n+Example:\n+{{< highlight >}}\n+def csv_mapper(strings_array):\n+    return User(strings_array[0], int(strings_array[1])))\n+{{< /highlight >}}\n+\n+- `table or query` Specifies a Snowflake table name or custom SQL query\n+\n+- `expansion_service`: specifies URL of expansion service.\n+\n+#### Authentication parameters\n+It\u2019s required to pass one of the following combinations of valid parameters for authentication:\n+- `username and password` Specifies username and password for username/password authentication method.\n+\n+- `private_key_path and private_key_passphrase` Specifies a path to private key and passphrase for key/pair authentication method.\n+\n+- `raw_private_key and private_key_passphrase` Specifies a private key and passphrase for key/pair authentication method.\n+\n+- `o_auth_token` Specifies access token for OAuth authentication method.\n+\n+#### Additional parameters\n+- `role`: specifies Snowflake role. If not specified then the user's default will be used.\n+\n+- `warehouse`: specifies Snowflake warehouse name. If not specified then the user's default will be used.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 495}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc3MDcwNg==", "bodyText": "Done.", "url": "https://github.com/apache/beam/pull/12823#discussion_r494770706", "createdAt": "2020-09-25T06:19:58Z", "author": {"login": "piotr-szuberski"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+\n+#### Required parameters\n+- `server_name` Full Snowflake server name with an account, zone, and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Name of the Google Cloud Storage bucket. Bucket will be used as a temporary location for storing CSV files. Those temporary directories will be named `sf_copy_csv_DATE_TIME_RANDOMSUFFIX` and they will be removed automatically once Read operation finishes.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.\n+Example:\n+{{< highlight >}}\n+def csv_mapper(strings_array):\n+    return User(strings_array[0], int(strings_array[1])))\n+{{< /highlight >}}\n+\n+- `table or query` Specifies a Snowflake table name or custom SQL query\n+\n+- `expansion_service`: specifies URL of expansion service.\n+\n+#### Authentication parameters\n+It\u2019s required to pass one of the following combinations of valid parameters for authentication:\n+- `username and password` Specifies username and password for username/password authentication method.\n+\n+- `private_key_path and private_key_passphrase` Specifies a path to private key and passphrase for key/pair authentication method.\n+\n+- `raw_private_key and private_key_passphrase` Specifies a private key and passphrase for key/pair authentication method.\n+\n+- `o_auth_token` Specifies access token for OAuth authentication method.\n+\n+#### Additional parameters\n+- `role`: specifies Snowflake role. If not specified then the user's default will be used.\n+\n+- `warehouse`: specifies Snowflake warehouse name. If not specified then the user's default will be used.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkxMDQxOQ=="}, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 495}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTEyMzgzOnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMTo0MToyNVrOHXB-Lw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwNjo0MzowM1rOHX28xg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkxMTU5OQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            {{< highlight >}}\n          \n          \n            \n            {{< highlight py >}}\n          \n      \n    \n    \n  \n\nPlease specify py or java in all the relevant code blocks so that syntax is properly highlighted", "url": "https://github.com/apache/beam/pull/12823#discussion_r493911599", "createdAt": "2020-09-23T21:41:25Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+\n+#### Required parameters\n+- `server_name` Full Snowflake server name with an account, zone, and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Name of the Google Cloud Storage bucket. Bucket will be used as a temporary location for storing CSV files. Those temporary directories will be named `sf_copy_csv_DATE_TIME_RANDOMSUFFIX` and they will be removed automatically once Read operation finishes.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.\n+Example:\n+{{< highlight >}}\n+def csv_mapper(strings_array):\n+    return User(strings_array[0], int(strings_array[1])))\n+{{< /highlight >}}\n+\n+- `table or query` Specifies a Snowflake table name or custom SQL query\n+\n+- `expansion_service`: specifies URL of expansion service.\n+\n+#### Authentication parameters\n+It\u2019s required to pass one of the following combinations of valid parameters for authentication:\n+- `username and password` Specifies username and password for username/password authentication method.\n+\n+- `private_key_path and private_key_passphrase` Specifies a path to private key and passphrase for key/pair authentication method.\n+\n+- `raw_private_key and private_key_passphrase` Specifies a private key and passphrase for key/pair authentication method.\n+\n+- `o_auth_token` Specifies access token for OAuth authentication method.\n+\n+#### Additional parameters\n+- `role`: specifies Snowflake role. If not specified then the user's default will be used.\n+\n+- `warehouse`: specifies Snowflake warehouse name. If not specified then the user's default will be used.\n+\n+### Writing to Snowflake\n+One of the functions of SnowflakeIO is writing to Snowflake tables. This transformation enables you to finish the Beam pipeline with an output operation that sends the user's [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) to your Snowflake database.\n+#### General usage\n+{{< highlight >}}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 500}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc3OTU5MA==", "bodyText": "Done.", "url": "https://github.com/apache/beam/pull/12823#discussion_r494779590", "createdAt": "2020-09-25T06:43:03Z", "author": {"login": "piotr-szuberski"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+\n+#### Required parameters\n+- `server_name` Full Snowflake server name with an account, zone, and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Name of the Google Cloud Storage bucket. Bucket will be used as a temporary location for storing CSV files. Those temporary directories will be named `sf_copy_csv_DATE_TIME_RANDOMSUFFIX` and they will be removed automatically once Read operation finishes.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.\n+Example:\n+{{< highlight >}}\n+def csv_mapper(strings_array):\n+    return User(strings_array[0], int(strings_array[1])))\n+{{< /highlight >}}\n+\n+- `table or query` Specifies a Snowflake table name or custom SQL query\n+\n+- `expansion_service`: specifies URL of expansion service.\n+\n+#### Authentication parameters\n+It\u2019s required to pass one of the following combinations of valid parameters for authentication:\n+- `username and password` Specifies username and password for username/password authentication method.\n+\n+- `private_key_path and private_key_passphrase` Specifies a path to private key and passphrase for key/pair authentication method.\n+\n+- `raw_private_key and private_key_passphrase` Specifies a private key and passphrase for key/pair authentication method.\n+\n+- `o_auth_token` Specifies access token for OAuth authentication method.\n+\n+#### Additional parameters\n+- `role`: specifies Snowflake role. If not specified then the user's default will be used.\n+\n+- `warehouse`: specifies Snowflake warehouse name. If not specified then the user's default will be used.\n+\n+### Writing to Snowflake\n+One of the functions of SnowflakeIO is writing to Snowflake tables. This transformation enables you to finish the Beam pipeline with an output operation that sends the user's [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) to your Snowflake database.\n+#### General usage\n+{{< highlight >}}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkxMTU5OQ=="}, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 500}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTE0NDE4OnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMTo0ODoyN1rOHXCKgA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwNjo0ODo1MFrOHX3F_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkxNDc1Mg==", "bodyText": "This page shouldn't state Flink is the only runner the supports cross-language, instead please refer users to https://beam.apache.org/roadmap/connectors-multi-sdk/#cross-language-transforms-api-and-expansion-service for information about which runners support it.", "url": "https://github.com/apache/beam/pull/12823#discussion_r493914752", "createdAt": "2020-09-23T21:48:27Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 422}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc4MTk0OQ==", "bodyText": "Done.", "url": "https://github.com/apache/beam/pull/12823#discussion_r494781949", "createdAt": "2020-09-25T06:48:50Z", "author": {"login": "piotr-szuberski"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkxNDc1Mg=="}, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 422}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTE0Njg1OnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMTo0OTozMFrOHXCMKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwNjo0OTo0N1rOHX3Hgg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkxNTE3Ng==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ### Set up\n          \n          \n            \n            Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n          \n      \n    \n    \n  \n\nI don't think this is necessary", "url": "https://github.com/apache/beam/pull/12823#discussion_r493915176", "createdAt": "2020-09-23T21:49:30Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 426}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc4MjMzOA==", "bodyText": "Yeah, it used to be when the expansion services weren't picked up automatically. Removed.", "url": "https://github.com/apache/beam/pull/12823#discussion_r494782338", "createdAt": "2020-09-25T06:49:47Z", "author": {"login": "piotr-szuberski"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkxNTE3Ng=="}, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 426}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTE1NDM0OnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMTo1MjowNlrOHXCQrg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwNjo1MTo0OFrOHX3KqQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkxNjMzNA==", "bodyText": "These exampels can drop the pipeline options and instead just focus on applying the transform to the pipeline, e.g.:\n(p | <SOURCE>\n   | WriteToSnowflake(...)", "url": "https://github.com/apache/beam/pull/12823#discussion_r493916334", "createdAt": "2020-09-23T21:52:06Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+\n+#### Required parameters\n+- `server_name` Full Snowflake server name with an account, zone, and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Name of the Google Cloud Storage bucket. Bucket will be used as a temporary location for storing CSV files. Those temporary directories will be named `sf_copy_csv_DATE_TIME_RANDOMSUFFIX` and they will be removed automatically once Read operation finishes.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.\n+Example:\n+{{< highlight >}}\n+def csv_mapper(strings_array):\n+    return User(strings_array[0], int(strings_array[1])))\n+{{< /highlight >}}\n+\n+- `table or query` Specifies a Snowflake table name or custom SQL query\n+\n+- `expansion_service`: specifies URL of expansion service.\n+\n+#### Authentication parameters\n+It\u2019s required to pass one of the following combinations of valid parameters for authentication:\n+- `username and password` Specifies username and password for username/password authentication method.\n+\n+- `private_key_path and private_key_passphrase` Specifies a path to private key and passphrase for key/pair authentication method.\n+\n+- `raw_private_key and private_key_passphrase` Specifies a private key and passphrase for key/pair authentication method.\n+\n+- `o_auth_token` Specifies access token for OAuth authentication method.\n+\n+#### Additional parameters\n+- `role`: specifies Snowflake role. If not specified then the user's default will be used.\n+\n+- `warehouse`: specifies Snowflake warehouse name. If not specified then the user's default will be used.\n+\n+### Writing to Snowflake\n+One of the functions of SnowflakeIO is writing to Snowflake tables. This transformation enables you to finish the Beam pipeline with an output operation that sends the user's [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) to your Snowflake database.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | <SOURCE OF DATA>\n+       | WriteToSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           create_disposition=<CREATE DISPOSITION>,\n+           write_disposition=<WRITE DISPOSITION>,\n+           table_schema=<SNOWFLAKE TABLE SCHEMA>,\n+           user_data_mapper=<USER DATA MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 532}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc4MzE0NQ==", "bodyText": "Done.", "url": "https://github.com/apache/beam/pull/12823#discussion_r494783145", "createdAt": "2020-09-25T06:51:48Z", "author": {"login": "piotr-szuberski"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+\n+#### Required parameters\n+- `server_name` Full Snowflake server name with an account, zone, and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Name of the Google Cloud Storage bucket. Bucket will be used as a temporary location for storing CSV files. Those temporary directories will be named `sf_copy_csv_DATE_TIME_RANDOMSUFFIX` and they will be removed automatically once Read operation finishes.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.\n+Example:\n+{{< highlight >}}\n+def csv_mapper(strings_array):\n+    return User(strings_array[0], int(strings_array[1])))\n+{{< /highlight >}}\n+\n+- `table or query` Specifies a Snowflake table name or custom SQL query\n+\n+- `expansion_service`: specifies URL of expansion service.\n+\n+#### Authentication parameters\n+It\u2019s required to pass one of the following combinations of valid parameters for authentication:\n+- `username and password` Specifies username and password for username/password authentication method.\n+\n+- `private_key_path and private_key_passphrase` Specifies a path to private key and passphrase for key/pair authentication method.\n+\n+- `raw_private_key and private_key_passphrase` Specifies a private key and passphrase for key/pair authentication method.\n+\n+- `o_auth_token` Specifies access token for OAuth authentication method.\n+\n+#### Additional parameters\n+- `role`: specifies Snowflake role. If not specified then the user's default will be used.\n+\n+- `warehouse`: specifies Snowflake warehouse name. If not specified then the user's default will be used.\n+\n+### Writing to Snowflake\n+One of the functions of SnowflakeIO is writing to Snowflake tables. This transformation enables you to finish the Beam pipeline with an output operation that sends the user's [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) to your Snowflake database.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | <SOURCE OF DATA>\n+       | WriteToSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           create_disposition=<CREATE DISPOSITION>,\n+           write_disposition=<WRITE DISPOSITION>,\n+           table_schema=<SNOWFLAKE TABLE SCHEMA>,\n+           user_data_mapper=<USER DATA MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkxNjMzNA=="}, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 532}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTE1NzA2OnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMTo1MzowMVrOHXCSSw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwNjo1NDowNFrOHX3ONQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkxNjc0Nw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                       table=<SNOWFALKE TABLE>,\n          \n          \n            \n                       table=<SNOWFLAKE TABLE>,", "url": "https://github.com/apache/beam/pull/12823#discussion_r493916747", "createdAt": "2020-09-23T21:53:01Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+\n+#### Required parameters\n+- `server_name` Full Snowflake server name with an account, zone, and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Name of the Google Cloud Storage bucket. Bucket will be used as a temporary location for storing CSV files. Those temporary directories will be named `sf_copy_csv_DATE_TIME_RANDOMSUFFIX` and they will be removed automatically once Read operation finishes.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.\n+Example:\n+{{< highlight >}}\n+def csv_mapper(strings_array):\n+    return User(strings_array[0], int(strings_array[1])))\n+{{< /highlight >}}\n+\n+- `table or query` Specifies a Snowflake table name or custom SQL query\n+\n+- `expansion_service`: specifies URL of expansion service.\n+\n+#### Authentication parameters\n+It\u2019s required to pass one of the following combinations of valid parameters for authentication:\n+- `username and password` Specifies username and password for username/password authentication method.\n+\n+- `private_key_path and private_key_passphrase` Specifies a path to private key and passphrase for key/pair authentication method.\n+\n+- `raw_private_key and private_key_passphrase` Specifies a private key and passphrase for key/pair authentication method.\n+\n+- `o_auth_token` Specifies access token for OAuth authentication method.\n+\n+#### Additional parameters\n+- `role`: specifies Snowflake role. If not specified then the user's default will be used.\n+\n+- `warehouse`: specifies Snowflake warehouse name. If not specified then the user's default will be used.\n+\n+### Writing to Snowflake\n+One of the functions of SnowflakeIO is writing to Snowflake tables. This transformation enables you to finish the Beam pipeline with an output operation that sends the user's [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) to your Snowflake database.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | <SOURCE OF DATA>\n+       | WriteToSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           create_disposition=<CREATE DISPOSITION>,\n+           write_disposition=<WRITE DISPOSITION>,\n+           table_schema=<SNOWFLAKE TABLE SCHEMA>,\n+           user_data_mapper=<USER DATA MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 527}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc4NDA1Mw==", "bodyText": "Done.", "url": "https://github.com/apache/beam/pull/12823#discussion_r494784053", "createdAt": "2020-09-25T06:54:04Z", "author": {"login": "piotr-szuberski"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+\n+#### Required parameters\n+- `server_name` Full Snowflake server name with an account, zone, and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Name of the Google Cloud Storage bucket. Bucket will be used as a temporary location for storing CSV files. Those temporary directories will be named `sf_copy_csv_DATE_TIME_RANDOMSUFFIX` and they will be removed automatically once Read operation finishes.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.\n+Example:\n+{{< highlight >}}\n+def csv_mapper(strings_array):\n+    return User(strings_array[0], int(strings_array[1])))\n+{{< /highlight >}}\n+\n+- `table or query` Specifies a Snowflake table name or custom SQL query\n+\n+- `expansion_service`: specifies URL of expansion service.\n+\n+#### Authentication parameters\n+It\u2019s required to pass one of the following combinations of valid parameters for authentication:\n+- `username and password` Specifies username and password for username/password authentication method.\n+\n+- `private_key_path and private_key_passphrase` Specifies a path to private key and passphrase for key/pair authentication method.\n+\n+- `raw_private_key and private_key_passphrase` Specifies a private key and passphrase for key/pair authentication method.\n+\n+- `o_auth_token` Specifies access token for OAuth authentication method.\n+\n+#### Additional parameters\n+- `role`: specifies Snowflake role. If not specified then the user's default will be used.\n+\n+- `warehouse`: specifies Snowflake warehouse name. If not specified then the user's default will be used.\n+\n+### Writing to Snowflake\n+One of the functions of SnowflakeIO is writing to Snowflake tables. This transformation enables you to finish the Beam pipeline with an output operation that sends the user's [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) to your Snowflake database.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | <SOURCE OF DATA>\n+       | WriteToSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           create_disposition=<CREATE DISPOSITION>,\n+           write_disposition=<WRITE DISPOSITION>,\n+           table_schema=<SNOWFLAKE TABLE SCHEMA>,\n+           user_data_mapper=<USER DATA MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkxNjc0Nw=="}, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 527}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTE1NzYwOnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMTo1MzoxN1rOHXCSpg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwNjo1NDowOVrOHX3OZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkxNjgzOA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                       table=<SNOWFALKE TABLE>,\n          \n          \n            \n                       table=<SNOWFLAKE TABLE>,", "url": "https://github.com/apache/beam/pull/12823#discussion_r493916838", "createdAt": "2020-09-23T21:53:17Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 453}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc4NDEwMw==", "bodyText": "Done.", "url": "https://github.com/apache/beam/pull/12823#discussion_r494784103", "createdAt": "2020-09-25T06:54:09Z", "author": {"login": "piotr-szuberski"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkxNjgzOA=="}, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 453}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTE2NTg4OnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMTo1NjowM1rOHXCX9Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwNzowMzoxMFrOHX3dyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkxODE5Nw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            - `table_schema` When the create_disposition parameter is set to CREATE_IF_NEEDED, the table_schema  parameter  enables specifying the schema for the created target table. A table schema is as JSON with the following structure:\n          \n          \n            \n            - `table_schema` When the `create_disposition` parameter is set to CREATE_IF_NEEDED, the table_schema  parameter  enables specifying the schema for the created target table. A table schema is a JSON array with the following structure:", "url": "https://github.com/apache/beam/pull/12823#discussion_r493918197", "createdAt": "2020-09-23T21:56:03Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+\n+#### Required parameters\n+- `server_name` Full Snowflake server name with an account, zone, and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Name of the Google Cloud Storage bucket. Bucket will be used as a temporary location for storing CSV files. Those temporary directories will be named `sf_copy_csv_DATE_TIME_RANDOMSUFFIX` and they will be removed automatically once Read operation finishes.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.\n+Example:\n+{{< highlight >}}\n+def csv_mapper(strings_array):\n+    return User(strings_array[0], int(strings_array[1])))\n+{{< /highlight >}}\n+\n+- `table or query` Specifies a Snowflake table name or custom SQL query\n+\n+- `expansion_service`: specifies URL of expansion service.\n+\n+#### Authentication parameters\n+It\u2019s required to pass one of the following combinations of valid parameters for authentication:\n+- `username and password` Specifies username and password for username/password authentication method.\n+\n+- `private_key_path and private_key_passphrase` Specifies a path to private key and passphrase for key/pair authentication method.\n+\n+- `raw_private_key and private_key_passphrase` Specifies a private key and passphrase for key/pair authentication method.\n+\n+- `o_auth_token` Specifies access token for OAuth authentication method.\n+\n+#### Additional parameters\n+- `role`: specifies Snowflake role. If not specified then the user's default will be used.\n+\n+- `warehouse`: specifies Snowflake warehouse name. If not specified then the user's default will be used.\n+\n+### Writing to Snowflake\n+One of the functions of SnowflakeIO is writing to Snowflake tables. This transformation enables you to finish the Beam pipeline with an output operation that sends the user's [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) to your Snowflake database.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | <SOURCE OF DATA>\n+       | WriteToSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           create_disposition=<CREATE DISPOSITION>,\n+           write_disposition=<WRITE DISPOSITION>,\n+           table_schema=<SNOWFLAKE TABLE SCHEMA>,\n+           user_data_mapper=<USER DATA MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+#### Required parameters\n+\n+- `server_name` Full Snowflake server name with account, zone and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Path to Google Cloud Storage bucket ended with slash. Bucket will be used to save CSV files which will end up in Snowflake. Those CSV files will be saved under \u201cstaging_bucket_name\u201d path.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `user_data_mapper` Specifies a function which  maps data from a PCollection to an array of String values before the write operation saves the data to temporary .csv files.\n+Example:\n+{{< highlight >}}\n+def user_data_mapper(user):\n+    return [user.name, str(user.age)]\n+{{< /highlight >}}\n+\n+- `table or query` Specifies a Snowflake table name or custom SQL query\n+\n+- `expansion_service` Specifies URL of expansion service.\n+\n+#### Authentication parameters\n+It\u2019s required to pass one of the following combination of valid parameters for authentication:\n+\n+- `username and password` Specifies username/password authentication method.\n+\n+- `private_key_path and private_key_passphrase` Specifies a path to private key and passphrase for key/pair authentication method.\n+\n+- `raw_private_key and private_key_passphrase` Specifies a private key and passphrase for key/pair authentication method.\n+\n+- `o_auth_token` Specifies access token for OAuth authentication method.\n+\n+#### Additional parameters\n+- `role`: specifies Snowflake role. If not specified then the user's default will be used.\n+\n+- `warehouse`: specifies Snowflake warehouse name. If not specified then the user's default will be used.\n+\n+- `create_disposition` Defines the behaviour of the write operation if the target table does not exist. The following values are supported:\n+  - CREATE_IF_NEEDED - default behaviour. The write operation checks whether the specified target table exists; if it does not, the write operation attempts to create the table Specify the schema for the target table using the table_schema parameter.\n+  - CREATE_NEVER -  The write operation fails if the target table does not exist.\n+\n+- `write_disposition` Defines the write behaviour based on the table where data will be written to. The following values are supported:\n+  - APPEND - Default behaviour. Written data is added to the existing rows in the table,\n+  - EMPTY - The target table must be empty;  otherwise, the write operation fails,\n+  - TRUNCATE - The write operation deletes all rows from the target table before writing to it.\n+\n+- `table_schema` When the create_disposition parameter is set to CREATE_IF_NEEDED, the table_schema  parameter  enables specifying the schema for the created target table. A table schema is as JSON with the following structure:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 581}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkxODQ4OQ==", "bodyText": "Is there documentation we can refer to about how all of these types are encoded in CSV?", "url": "https://github.com/apache/beam/pull/12823#discussion_r493918489", "createdAt": "2020-09-23T21:56:47Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+\n+#### Required parameters\n+- `server_name` Full Snowflake server name with an account, zone, and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Name of the Google Cloud Storage bucket. Bucket will be used as a temporary location for storing CSV files. Those temporary directories will be named `sf_copy_csv_DATE_TIME_RANDOMSUFFIX` and they will be removed automatically once Read operation finishes.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.\n+Example:\n+{{< highlight >}}\n+def csv_mapper(strings_array):\n+    return User(strings_array[0], int(strings_array[1])))\n+{{< /highlight >}}\n+\n+- `table or query` Specifies a Snowflake table name or custom SQL query\n+\n+- `expansion_service`: specifies URL of expansion service.\n+\n+#### Authentication parameters\n+It\u2019s required to pass one of the following combinations of valid parameters for authentication:\n+- `username and password` Specifies username and password for username/password authentication method.\n+\n+- `private_key_path and private_key_passphrase` Specifies a path to private key and passphrase for key/pair authentication method.\n+\n+- `raw_private_key and private_key_passphrase` Specifies a private key and passphrase for key/pair authentication method.\n+\n+- `o_auth_token` Specifies access token for OAuth authentication method.\n+\n+#### Additional parameters\n+- `role`: specifies Snowflake role. If not specified then the user's default will be used.\n+\n+- `warehouse`: specifies Snowflake warehouse name. If not specified then the user's default will be used.\n+\n+### Writing to Snowflake\n+One of the functions of SnowflakeIO is writing to Snowflake tables. This transformation enables you to finish the Beam pipeline with an output operation that sends the user's [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) to your Snowflake database.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | <SOURCE OF DATA>\n+       | WriteToSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           create_disposition=<CREATE DISPOSITION>,\n+           write_disposition=<WRITE DISPOSITION>,\n+           table_schema=<SNOWFLAKE TABLE SCHEMA>,\n+           user_data_mapper=<USER DATA MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+#### Required parameters\n+\n+- `server_name` Full Snowflake server name with account, zone and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Path to Google Cloud Storage bucket ended with slash. Bucket will be used to save CSV files which will end up in Snowflake. Those CSV files will be saved under \u201cstaging_bucket_name\u201d path.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `user_data_mapper` Specifies a function which  maps data from a PCollection to an array of String values before the write operation saves the data to temporary .csv files.\n+Example:\n+{{< highlight >}}\n+def user_data_mapper(user):\n+    return [user.name, str(user.age)]\n+{{< /highlight >}}\n+\n+- `table or query` Specifies a Snowflake table name or custom SQL query\n+\n+- `expansion_service` Specifies URL of expansion service.\n+\n+#### Authentication parameters\n+It\u2019s required to pass one of the following combination of valid parameters for authentication:\n+\n+- `username and password` Specifies username/password authentication method.\n+\n+- `private_key_path and private_key_passphrase` Specifies a path to private key and passphrase for key/pair authentication method.\n+\n+- `raw_private_key and private_key_passphrase` Specifies a private key and passphrase for key/pair authentication method.\n+\n+- `o_auth_token` Specifies access token for OAuth authentication method.\n+\n+#### Additional parameters\n+- `role`: specifies Snowflake role. If not specified then the user's default will be used.\n+\n+- `warehouse`: specifies Snowflake warehouse name. If not specified then the user's default will be used.\n+\n+- `create_disposition` Defines the behaviour of the write operation if the target table does not exist. The following values are supported:\n+  - CREATE_IF_NEEDED - default behaviour. The write operation checks whether the specified target table exists; if it does not, the write operation attempts to create the table Specify the schema for the target table using the table_schema parameter.\n+  - CREATE_NEVER -  The write operation fails if the target table does not exist.\n+\n+- `write_disposition` Defines the write behaviour based on the table where data will be written to. The following values are supported:\n+  - APPEND - Default behaviour. Written data is added to the existing rows in the table,\n+  - EMPTY - The target table must be empty;  otherwise, the write operation fails,\n+  - TRUNCATE - The write operation deletes all rows from the target table before writing to it.\n+\n+- `table_schema` When the create_disposition parameter is set to CREATE_IF_NEEDED, the table_schema  parameter  enables specifying the schema for the created target table. A table schema is as JSON with the following structure:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkxODE5Nw=="}, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 581}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc4ODA0MQ==", "bodyText": "Done. I added a link to https://docs.snowflake.com/en/sql-reference/data-types.html", "url": "https://github.com/apache/beam/pull/12823#discussion_r494788041", "createdAt": "2020-09-25T07:03:10Z", "author": {"login": "piotr-szuberski"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+\n+#### Required parameters\n+- `server_name` Full Snowflake server name with an account, zone, and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Name of the Google Cloud Storage bucket. Bucket will be used as a temporary location for storing CSV files. Those temporary directories will be named `sf_copy_csv_DATE_TIME_RANDOMSUFFIX` and they will be removed automatically once Read operation finishes.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.\n+Example:\n+{{< highlight >}}\n+def csv_mapper(strings_array):\n+    return User(strings_array[0], int(strings_array[1])))\n+{{< /highlight >}}\n+\n+- `table or query` Specifies a Snowflake table name or custom SQL query\n+\n+- `expansion_service`: specifies URL of expansion service.\n+\n+#### Authentication parameters\n+It\u2019s required to pass one of the following combinations of valid parameters for authentication:\n+- `username and password` Specifies username and password for username/password authentication method.\n+\n+- `private_key_path and private_key_passphrase` Specifies a path to private key and passphrase for key/pair authentication method.\n+\n+- `raw_private_key and private_key_passphrase` Specifies a private key and passphrase for key/pair authentication method.\n+\n+- `o_auth_token` Specifies access token for OAuth authentication method.\n+\n+#### Additional parameters\n+- `role`: specifies Snowflake role. If not specified then the user's default will be used.\n+\n+- `warehouse`: specifies Snowflake warehouse name. If not specified then the user's default will be used.\n+\n+### Writing to Snowflake\n+One of the functions of SnowflakeIO is writing to Snowflake tables. This transformation enables you to finish the Beam pipeline with an output operation that sends the user's [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) to your Snowflake database.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | <SOURCE OF DATA>\n+       | WriteToSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           create_disposition=<CREATE DISPOSITION>,\n+           write_disposition=<WRITE DISPOSITION>,\n+           table_schema=<SNOWFLAKE TABLE SCHEMA>,\n+           user_data_mapper=<USER DATA MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+#### Required parameters\n+\n+- `server_name` Full Snowflake server name with account, zone and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Path to Google Cloud Storage bucket ended with slash. Bucket will be used to save CSV files which will end up in Snowflake. Those CSV files will be saved under \u201cstaging_bucket_name\u201d path.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `user_data_mapper` Specifies a function which  maps data from a PCollection to an array of String values before the write operation saves the data to temporary .csv files.\n+Example:\n+{{< highlight >}}\n+def user_data_mapper(user):\n+    return [user.name, str(user.age)]\n+{{< /highlight >}}\n+\n+- `table or query` Specifies a Snowflake table name or custom SQL query\n+\n+- `expansion_service` Specifies URL of expansion service.\n+\n+#### Authentication parameters\n+It\u2019s required to pass one of the following combination of valid parameters for authentication:\n+\n+- `username and password` Specifies username/password authentication method.\n+\n+- `private_key_path and private_key_passphrase` Specifies a path to private key and passphrase for key/pair authentication method.\n+\n+- `raw_private_key and private_key_passphrase` Specifies a private key and passphrase for key/pair authentication method.\n+\n+- `o_auth_token` Specifies access token for OAuth authentication method.\n+\n+#### Additional parameters\n+- `role`: specifies Snowflake role. If not specified then the user's default will be used.\n+\n+- `warehouse`: specifies Snowflake warehouse name. If not specified then the user's default will be used.\n+\n+- `create_disposition` Defines the behaviour of the write operation if the target table does not exist. The following values are supported:\n+  - CREATE_IF_NEEDED - default behaviour. The write operation checks whether the specified target table exists; if it does not, the write operation attempts to create the table Specify the schema for the target table using the table_schema parameter.\n+  - CREATE_NEVER -  The write operation fails if the target table does not exist.\n+\n+- `write_disposition` Defines the write behaviour based on the table where data will be written to. The following values are supported:\n+  - APPEND - Default behaviour. Written data is added to the existing rows in the table,\n+  - EMPTY - The target table must be empty;  otherwise, the write operation fails,\n+  - TRUNCATE - The write operation deletes all rows from the target table before writing to it.\n+\n+- `table_schema` When the create_disposition parameter is set to CREATE_IF_NEEDED, the table_schema  parameter  enables specifying the schema for the created target table. A table schema is as JSON with the following structure:", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkxODE5Nw=="}, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 581}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTE3MzYyOnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMTo1OTowOFrOHXCc0Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwNzowNDowMlrOHX3fKA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkxOTQ0MQ==", "bodyText": "Does the fact that all of these have \"nullable\":false indicate nulls aren't supported? This could be more concise if you just include the \"dataType\" object, e.g.:\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            {\"dataType\":{\"type\":\"varchar\",\"length\":100},\"name\":\"\",\"nullable\":false}]\n          \n          \n            \n            {\"type\":\"varchar\",\"length\":100}", "url": "https://github.com/apache/beam/pull/12823#discussion_r493919441", "createdAt": "2020-09-23T21:59:08Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+\n+#### Required parameters\n+- `server_name` Full Snowflake server name with an account, zone, and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Name of the Google Cloud Storage bucket. Bucket will be used as a temporary location for storing CSV files. Those temporary directories will be named `sf_copy_csv_DATE_TIME_RANDOMSUFFIX` and they will be removed automatically once Read operation finishes.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.\n+Example:\n+{{< highlight >}}\n+def csv_mapper(strings_array):\n+    return User(strings_array[0], int(strings_array[1])))\n+{{< /highlight >}}\n+\n+- `table or query` Specifies a Snowflake table name or custom SQL query\n+\n+- `expansion_service`: specifies URL of expansion service.\n+\n+#### Authentication parameters\n+It\u2019s required to pass one of the following combinations of valid parameters for authentication:\n+- `username and password` Specifies username and password for username/password authentication method.\n+\n+- `private_key_path and private_key_passphrase` Specifies a path to private key and passphrase for key/pair authentication method.\n+\n+- `raw_private_key and private_key_passphrase` Specifies a private key and passphrase for key/pair authentication method.\n+\n+- `o_auth_token` Specifies access token for OAuth authentication method.\n+\n+#### Additional parameters\n+- `role`: specifies Snowflake role. If not specified then the user's default will be used.\n+\n+- `warehouse`: specifies Snowflake warehouse name. If not specified then the user's default will be used.\n+\n+### Writing to Snowflake\n+One of the functions of SnowflakeIO is writing to Snowflake tables. This transformation enables you to finish the Beam pipeline with an output operation that sends the user's [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) to your Snowflake database.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | <SOURCE OF DATA>\n+       | WriteToSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           create_disposition=<CREATE DISPOSITION>,\n+           write_disposition=<WRITE DISPOSITION>,\n+           table_schema=<SNOWFLAKE TABLE SCHEMA>,\n+           user_data_mapper=<USER DATA MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+#### Required parameters\n+\n+- `server_name` Full Snowflake server name with account, zone and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Path to Google Cloud Storage bucket ended with slash. Bucket will be used to save CSV files which will end up in Snowflake. Those CSV files will be saved under \u201cstaging_bucket_name\u201d path.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `user_data_mapper` Specifies a function which  maps data from a PCollection to an array of String values before the write operation saves the data to temporary .csv files.\n+Example:\n+{{< highlight >}}\n+def user_data_mapper(user):\n+    return [user.name, str(user.age)]\n+{{< /highlight >}}\n+\n+- `table or query` Specifies a Snowflake table name or custom SQL query\n+\n+- `expansion_service` Specifies URL of expansion service.\n+\n+#### Authentication parameters\n+It\u2019s required to pass one of the following combination of valid parameters for authentication:\n+\n+- `username and password` Specifies username/password authentication method.\n+\n+- `private_key_path and private_key_passphrase` Specifies a path to private key and passphrase for key/pair authentication method.\n+\n+- `raw_private_key and private_key_passphrase` Specifies a private key and passphrase for key/pair authentication method.\n+\n+- `o_auth_token` Specifies access token for OAuth authentication method.\n+\n+#### Additional parameters\n+- `role`: specifies Snowflake role. If not specified then the user's default will be used.\n+\n+- `warehouse`: specifies Snowflake warehouse name. If not specified then the user's default will be used.\n+\n+- `create_disposition` Defines the behaviour of the write operation if the target table does not exist. The following values are supported:\n+  - CREATE_IF_NEEDED - default behaviour. The write operation checks whether the specified target table exists; if it does not, the write operation attempts to create the table Specify the schema for the target table using the table_schema parameter.\n+  - CREATE_NEVER -  The write operation fails if the target table does not exist.\n+\n+- `write_disposition` Defines the write behaviour based on the table where data will be written to. The following values are supported:\n+  - APPEND - Default behaviour. Written data is added to the existing rows in the table,\n+  - EMPTY - The target table must be empty;  otherwise, the write operation fails,\n+  - TRUNCATE - The write operation deletes all rows from the target table before writing to it.\n+\n+- `table_schema` When the create_disposition parameter is set to CREATE_IF_NEEDED, the table_schema  parameter  enables specifying the schema for the created target table. A table schema is as JSON with the following structure:\n+{{< highlight >}}\n+{\"schema\":[\n+    {\n+      \"dataType\":{\"type\":\"<COLUMN DATA TYPE>\"},\n+      \"name\":\"<COLUMN  NAME> \",\n+      \"nullable\": <NULLABLE>\n+    },\n+        ...\n+  ]}\n+{{< /highlight >}}\n+All supported data types:\n+{{< highlight >}}\n+{\"dataType\":{\"type\":\"date\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"datetime\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"time\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"timestamp\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"timestamp_ltz\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"timestamp_ntz\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"timestamp_tz\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"boolean\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"decimal\",\"precision\":38,\"scale\":1},\"name\":\"\",\"nullable\":true},\n+{\"dataType\":{\"type\":\"double\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"float\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"integer\",\"precision\":38,\"scale\":0},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"number\",\"precision\":38,\"scale\":1},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"numeric\",\"precision\":40,\"scale\":2},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"real\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"array\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"object\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"variant\"},\"name\":\"\",\"nullable\":true},\n+{\"dataType\":{\"type\":\"binary\",\"size\":null},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"char\",\"length\":1},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"string\",\"length\":null},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"text\",\"length\":null},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"varbinary\",\"size\":null},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"varchar\",\"length\":100},\"name\":\"\",\"nullable\":false}]", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 617}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc4ODM5Mg==", "bodyText": "I agree. I found 2 of nullables to be true but the overall json schema is written above so just the dataType json object will be sufficient and much clearer.", "url": "https://github.com/apache/beam/pull/12823#discussion_r494788392", "createdAt": "2020-09-25T07:04:02Z", "author": {"login": "piotr-szuberski"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -362,3 +635,208 @@ static SnowflakeIO.CsvMapper<GenericRecord> getCsvMapper() {\n            };\n }\n {{< /highlight >}}\n+## Using SnowflakeIO in Python SDK\n+### Intro\n+Snowflake cross-language implementation is supporting both reading and writing operations for Python programming language, thanks to\n+cross-language which is part of [Portability Framework Roadmap](https://beam.apache.org/roadmap/portability/) which aims to provide full interoperability\n+across the Beam ecosystem. From a developer perspective it means the possibility of combining transforms written in different languages(Java/Python/Go).\n+\n+Currently, cross-language is supporting only [Apache Flink](https://flink.apache.org/) as a runner in a stable manner but plans are to support all runners.\n+For more information about cross-language please see [multi sdk efforts](https://beam.apache.org/roadmap/connectors-multi-sdk/)\n+and [Beam on top of Flink](https://flink.apache.org/ecosystem/2020/02/22/apache-beam-how-beam-runs-on-top-of-flink.html) articles.\n+\n+### Set up\n+Please see [Apache Beam with Flink runner](https://beam.apache.org/documentation/runners/flink/) for a setup.\n+\n+### Reading from Snowflake\n+One of the functions of SnowflakeIO is reading Snowflake tables - either full tables via table name or custom data via query. Output of the read transform is a [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) of user-defined data type.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | ReadFromSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           csv_mapper=<CSV MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+\n+#### Required parameters\n+- `server_name` Full Snowflake server name with an account, zone, and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Name of the Google Cloud Storage bucket. Bucket will be used as a temporary location for storing CSV files. Those temporary directories will be named `sf_copy_csv_DATE_TIME_RANDOMSUFFIX` and they will be removed automatically once Read operation finishes.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `csv_mapper` Specifies a function which must translate user-defined object to array of strings. SnowflakeIO uses a [COPY INTO <location>](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-location.html) statement to move data from a Snowflake table to Google Cloud Storage as CSV files. These files are then downloaded via [FileIO](https://beam.apache.org/releases/javadoc/2.3.0/index.html?org/apache/beam/sdk/io/FileIO.html) and processed line by line. Each line is split into an array of Strings using the [OpenCSV](http://opencsv.sourceforge.net/) library. The csv_mapper function job is to give the user the possibility to convert the array of Strings to a user-defined type, ie. GenericRecord for Avro or Parquet files, or custom objects.\n+Example:\n+{{< highlight >}}\n+def csv_mapper(strings_array):\n+    return User(strings_array[0], int(strings_array[1])))\n+{{< /highlight >}}\n+\n+- `table or query` Specifies a Snowflake table name or custom SQL query\n+\n+- `expansion_service`: specifies URL of expansion service.\n+\n+#### Authentication parameters\n+It\u2019s required to pass one of the following combinations of valid parameters for authentication:\n+- `username and password` Specifies username and password for username/password authentication method.\n+\n+- `private_key_path and private_key_passphrase` Specifies a path to private key and passphrase for key/pair authentication method.\n+\n+- `raw_private_key and private_key_passphrase` Specifies a private key and passphrase for key/pair authentication method.\n+\n+- `o_auth_token` Specifies access token for OAuth authentication method.\n+\n+#### Additional parameters\n+- `role`: specifies Snowflake role. If not specified then the user's default will be used.\n+\n+- `warehouse`: specifies Snowflake warehouse name. If not specified then the user's default will be used.\n+\n+### Writing to Snowflake\n+One of the functions of SnowflakeIO is writing to Snowflake tables. This transformation enables you to finish the Beam pipeline with an output operation that sends the user's [PCollection](https://beam.apache.org/releases/pydoc/2.20.0/apache_beam.pvalue.html#apache_beam.pvalue.PCollection) to your Snowflake database.\n+#### General usage\n+{{< highlight >}}\n+OPTIONS = [\n+   \"--runner=FlinkRunner\",\n+   \"--flink_version=1.10\",\n+   \"--flink_master=localhost:8081\",\n+   \"--environment_type=LOOPBACK\"\n+]\n+\n+with TestPipeline(options=PipelineOptions(OPTIONS)) as p:\n+   (p\n+       | <SOURCE OF DATA>\n+       | WriteToSnowflake(\n+           server_name=<SNOWFLAKE SERVER NAME>,\n+           username=<SNOWFLAKE USERNAME>,\n+           password=<SNOWFLAKE PASSWORD>,\n+           o_auth_token=<OAUTH TOKEN>,\n+           private_key_path=<PATH TO P8 FILE>,\n+           raw_private_key=<PRIVATE_KEY>\n+           private_key_passphrase=<PASSWORD FOR KEY>,\n+           schema=<SNOWFLAKE SCHEMA>,\n+           database=<SNOWFLAKE DATABASE>,\n+           staging_bucket_name=<GCS BUCKET NAME>,\n+           storage_integration_name=<SNOWFLAKE STORAGE INTEGRATION NAME>,\n+           create_disposition=<CREATE DISPOSITION>,\n+           write_disposition=<WRITE DISPOSITION>,\n+           table_schema=<SNOWFLAKE TABLE SCHEMA>,\n+           user_data_mapper=<USER DATA MAPPER FUNCTION>,\n+           table=<SNOWFALKE TABLE>,\n+           query=<IF NOT TABLE THEN QUERY>,\n+           role=<SNOWFLAKE ROLE>,\n+           warehouse=<SNOWFLAKE WAREHOUSE>,\n+           expansion_service=<EXPANSION SERVICE ADDRESS>))\n+{{< /highlight >}}\n+#### Required parameters\n+\n+- `server_name` Full Snowflake server name with account, zone and domain.\n+\n+- `schema` Name of the Snowflake schema in the database to use.\n+\n+- `database` Name of the Snowflake database to use.\n+\n+- `staging_bucket_name` Path to Google Cloud Storage bucket ended with slash. Bucket will be used to save CSV files which will end up in Snowflake. Those CSV files will be saved under \u201cstaging_bucket_name\u201d path.\n+\n+- `storage_integration_name` Is the name of a Snowflake storage integration object created according to [Snowflake documentation](https://docs.snowflake.net/manuals/sql-reference/sql/create-storage-integration.html).\n+\n+- `user_data_mapper` Specifies a function which  maps data from a PCollection to an array of String values before the write operation saves the data to temporary .csv files.\n+Example:\n+{{< highlight >}}\n+def user_data_mapper(user):\n+    return [user.name, str(user.age)]\n+{{< /highlight >}}\n+\n+- `table or query` Specifies a Snowflake table name or custom SQL query\n+\n+- `expansion_service` Specifies URL of expansion service.\n+\n+#### Authentication parameters\n+It\u2019s required to pass one of the following combination of valid parameters for authentication:\n+\n+- `username and password` Specifies username/password authentication method.\n+\n+- `private_key_path and private_key_passphrase` Specifies a path to private key and passphrase for key/pair authentication method.\n+\n+- `raw_private_key and private_key_passphrase` Specifies a private key and passphrase for key/pair authentication method.\n+\n+- `o_auth_token` Specifies access token for OAuth authentication method.\n+\n+#### Additional parameters\n+- `role`: specifies Snowflake role. If not specified then the user's default will be used.\n+\n+- `warehouse`: specifies Snowflake warehouse name. If not specified then the user's default will be used.\n+\n+- `create_disposition` Defines the behaviour of the write operation if the target table does not exist. The following values are supported:\n+  - CREATE_IF_NEEDED - default behaviour. The write operation checks whether the specified target table exists; if it does not, the write operation attempts to create the table Specify the schema for the target table using the table_schema parameter.\n+  - CREATE_NEVER -  The write operation fails if the target table does not exist.\n+\n+- `write_disposition` Defines the write behaviour based on the table where data will be written to. The following values are supported:\n+  - APPEND - Default behaviour. Written data is added to the existing rows in the table,\n+  - EMPTY - The target table must be empty;  otherwise, the write operation fails,\n+  - TRUNCATE - The write operation deletes all rows from the target table before writing to it.\n+\n+- `table_schema` When the create_disposition parameter is set to CREATE_IF_NEEDED, the table_schema  parameter  enables specifying the schema for the created target table. A table schema is as JSON with the following structure:\n+{{< highlight >}}\n+{\"schema\":[\n+    {\n+      \"dataType\":{\"type\":\"<COLUMN DATA TYPE>\"},\n+      \"name\":\"<COLUMN  NAME> \",\n+      \"nullable\": <NULLABLE>\n+    },\n+        ...\n+  ]}\n+{{< /highlight >}}\n+All supported data types:\n+{{< highlight >}}\n+{\"dataType\":{\"type\":\"date\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"datetime\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"time\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"timestamp\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"timestamp_ltz\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"timestamp_ntz\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"timestamp_tz\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"boolean\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"decimal\",\"precision\":38,\"scale\":1},\"name\":\"\",\"nullable\":true},\n+{\"dataType\":{\"type\":\"double\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"float\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"integer\",\"precision\":38,\"scale\":0},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"number\",\"precision\":38,\"scale\":1},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"numeric\",\"precision\":40,\"scale\":2},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"real\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"array\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"object\"},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"variant\"},\"name\":\"\",\"nullable\":true},\n+{\"dataType\":{\"type\":\"binary\",\"size\":null},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"char\",\"length\":1},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"string\",\"length\":null},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"text\",\"length\":null},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"varbinary\",\"size\":null},\"name\":\"\",\"nullable\":false},\n+{\"dataType\":{\"type\":\"varchar\",\"length\":100},\"name\":\"\",\"nullable\":false}]", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkxOTQ0MQ=="}, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 617}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzA5MTE3NzU3OnYy", "diffSide": "RIGHT", "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yM1QyMjowMDozN1rOHXCfNA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0yNVQwNzowNToxNlrOHX3hGw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkyMDA1Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              - Limit of rows written to each file staged file\n          \n          \n            \n              - Limit of rows written to each staged file", "url": "https://github.com/apache/beam/pull/12823#discussion_r493920052", "createdAt": "2020-09-23T22:00:37Z", "author": {"login": "TheNeuralBit"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -204,14 +370,121 @@ Then:\n **Note**:\n SnowflakeIO uses COPY statements behind the scenes to write (using [COPY to table](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-table.html)). StagingBucketName will be used to save CSV files which will end up in Snowflake. Those CSV files will be saved under the \u201cstagingBucketName\u201d path.\n \n+**Optional** for batching:\n+- `.withQuotationMark()`\n+  - Default value: `\u2018` (single quotation mark).\n+  - Accepts String with one character. It will surround all text (String) fields saved to CSV. It should be one of the accepted characters by [Snowflake\u2019s](https://docs.snowflake.com/en/sql-reference/sql/create-file-format.html) [FIELD_OPTIONALLY_ENCLOSED_BY](https://docs.snowflake.com/en/sql-reference/sql/create-file-format.html) parameter (double quotation mark, single quotation mark or none).\n+  - Example: `.withQuotationMark(\"'\")`\n+### Streaming write  (from unbounded source)\n+It is required to create a [SnowPipe](https://docs.snowflake.com/en/user-guide/data-load-snowpipe.html) in the Snowflake console. SnowPipe should use the same integration and the same bucket as specified by .withStagingBucketName and .withStorageIntegrationName methods. The write operation might look as follows:\n+{{< highlight java >}}\n+data.apply(\n+   SnowflakeIO.<type>write()\n+      .withStagingBucketName(\"BUCKET NAME\")\n+      .withStorageIntegrationName(\"STORAGE INTEGRATION NAME\")\n+      .withDataSourceConfiguration(dc)\n+      .withUserDataMapper(mapper)\n+      .withSnowPipe(\"MY_SNOW_PIPE\")\n+      .withFlushTimeLimit(Duration.millis(time))\n+      .withFlushRowLimit(rowsNumber)\n+      .withShardsNumber(shardsNumber)\n+)\n+{{< /highlight >}}\n+#### Parameters\n+**Required** for streaming:\n+\n+- ` .withDataSourceConfiguration()`\n+  - Accepts a DatasourceConfiguration object.\n+\n+- `.toTable()`\n+  - Accepts the target Snowflake table name.\n+  - Example: `.toTable(\"MY_TABLE)`\n+\n+- `.withStagingBucketName()`\n+  - Accepts a cloud bucket path ended with slash.\n+  - Example: `.withStagingBucketName(\"gs://mybucket/my/dir/\")`\n+\n+- `.withStorageIntegrationName()`\n+  - Accepts a name of a Snowflake storage integration object created according to Snowflake documentationt.\n+  - Example:\n+{{< highlight >}}\n+CREATE OR REPLACE STORAGE INTEGRATION test_integration\n+TYPE = EXTERNAL_STAGE\n+STORAGE_PROVIDER = GCS\n+ENABLED = TRUE\n+STORAGE_ALLOWED_LOCATIONS = ('gcs://bucket/');\n+{{< /highlight >}}\n+Then:\n+{{< highlight >}}\n+.withStorageIntegrationName(test_integration)\n+{{< /highlight >}}\n+\n+- `.withSnowPipe()`\n+  - Accepts the target SnowPipe name. `.withSnowPipe()` accepts the exact name of snowpipe.\n+Example:\n+{{< highlight >}}\n+CREATE OR REPLACE PIPE test_database.public.test_gcs_pipe\n+AS COPY INTO stream_table from @streamstage;\n+{{< /highlight >}}\n+\n+   - Then:\n+{{< highlight >}}\n+.withSnowPipe(test_gcs_pipe)\n+{{< /highlight >}}\n+\n+**Note**: this is important to provide **schema** and **database** names.\n+- `.withUserDataMapper()`\n+  - Accepts the [UserDataMapper](https://beam.apache.org/documentation/io/built-in/snowflake/#userdatamapper-function) function that will map a user's PCollection to an array of String values `(String[]).`\n+\n+**Note**:\n+\n+SnowflakeIO uses COPY statements behind the scenes to write (using [COPY to table](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-table.html)). StagingBucketName will be used to save CSV files which will end up in Snowflake. Those CSV files will be saved under the \u201cstagingBucketName\u201d path.\n+\n+**Optional** for streaming:\n+- `.withFlushTimeLimit()`\n+  - Default value: 30 seconds\n+  - Accepts Duration objects with the specified time after each the streaming write will be repeated\n+  - Example: `.withFlushTimeLimit(Duration.millis(180000))`\n+\n+- `.withFlushRowLimit()`\n+  - Default value: 10,000 rows\n+  - Limit of rows written to each file staged file", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 324}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5NDc4ODg5MQ==", "bodyText": "Done.", "url": "https://github.com/apache/beam/pull/12823#discussion_r494788891", "createdAt": "2020-09-25T07:05:16Z", "author": {"login": "piotr-szuberski"}, "path": "website/www/site/content/en/documentation/io/built-in/snowflake.md", "diffHunk": "@@ -204,14 +370,121 @@ Then:\n **Note**:\n SnowflakeIO uses COPY statements behind the scenes to write (using [COPY to table](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-table.html)). StagingBucketName will be used to save CSV files which will end up in Snowflake. Those CSV files will be saved under the \u201cstagingBucketName\u201d path.\n \n+**Optional** for batching:\n+- `.withQuotationMark()`\n+  - Default value: `\u2018` (single quotation mark).\n+  - Accepts String with one character. It will surround all text (String) fields saved to CSV. It should be one of the accepted characters by [Snowflake\u2019s](https://docs.snowflake.com/en/sql-reference/sql/create-file-format.html) [FIELD_OPTIONALLY_ENCLOSED_BY](https://docs.snowflake.com/en/sql-reference/sql/create-file-format.html) parameter (double quotation mark, single quotation mark or none).\n+  - Example: `.withQuotationMark(\"'\")`\n+### Streaming write  (from unbounded source)\n+It is required to create a [SnowPipe](https://docs.snowflake.com/en/user-guide/data-load-snowpipe.html) in the Snowflake console. SnowPipe should use the same integration and the same bucket as specified by .withStagingBucketName and .withStorageIntegrationName methods. The write operation might look as follows:\n+{{< highlight java >}}\n+data.apply(\n+   SnowflakeIO.<type>write()\n+      .withStagingBucketName(\"BUCKET NAME\")\n+      .withStorageIntegrationName(\"STORAGE INTEGRATION NAME\")\n+      .withDataSourceConfiguration(dc)\n+      .withUserDataMapper(mapper)\n+      .withSnowPipe(\"MY_SNOW_PIPE\")\n+      .withFlushTimeLimit(Duration.millis(time))\n+      .withFlushRowLimit(rowsNumber)\n+      .withShardsNumber(shardsNumber)\n+)\n+{{< /highlight >}}\n+#### Parameters\n+**Required** for streaming:\n+\n+- ` .withDataSourceConfiguration()`\n+  - Accepts a DatasourceConfiguration object.\n+\n+- `.toTable()`\n+  - Accepts the target Snowflake table name.\n+  - Example: `.toTable(\"MY_TABLE)`\n+\n+- `.withStagingBucketName()`\n+  - Accepts a cloud bucket path ended with slash.\n+  - Example: `.withStagingBucketName(\"gs://mybucket/my/dir/\")`\n+\n+- `.withStorageIntegrationName()`\n+  - Accepts a name of a Snowflake storage integration object created according to Snowflake documentationt.\n+  - Example:\n+{{< highlight >}}\n+CREATE OR REPLACE STORAGE INTEGRATION test_integration\n+TYPE = EXTERNAL_STAGE\n+STORAGE_PROVIDER = GCS\n+ENABLED = TRUE\n+STORAGE_ALLOWED_LOCATIONS = ('gcs://bucket/');\n+{{< /highlight >}}\n+Then:\n+{{< highlight >}}\n+.withStorageIntegrationName(test_integration)\n+{{< /highlight >}}\n+\n+- `.withSnowPipe()`\n+  - Accepts the target SnowPipe name. `.withSnowPipe()` accepts the exact name of snowpipe.\n+Example:\n+{{< highlight >}}\n+CREATE OR REPLACE PIPE test_database.public.test_gcs_pipe\n+AS COPY INTO stream_table from @streamstage;\n+{{< /highlight >}}\n+\n+   - Then:\n+{{< highlight >}}\n+.withSnowPipe(test_gcs_pipe)\n+{{< /highlight >}}\n+\n+**Note**: this is important to provide **schema** and **database** names.\n+- `.withUserDataMapper()`\n+  - Accepts the [UserDataMapper](https://beam.apache.org/documentation/io/built-in/snowflake/#userdatamapper-function) function that will map a user's PCollection to an array of String values `(String[]).`\n+\n+**Note**:\n+\n+SnowflakeIO uses COPY statements behind the scenes to write (using [COPY to table](https://docs.snowflake.net/manuals/sql-reference/sql/copy-into-table.html)). StagingBucketName will be used to save CSV files which will end up in Snowflake. Those CSV files will be saved under the \u201cstagingBucketName\u201d path.\n+\n+**Optional** for streaming:\n+- `.withFlushTimeLimit()`\n+  - Default value: 30 seconds\n+  - Accepts Duration objects with the specified time after each the streaming write will be repeated\n+  - Example: `.withFlushTimeLimit(Duration.millis(180000))`\n+\n+- `.withFlushRowLimit()`\n+  - Default value: 10,000 rows\n+  - Limit of rows written to each file staged file", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ5MzkyMDA1Mg=="}, "originalCommit": {"oid": "181aff39cc5ae0b927efe212bd7a5fd5b182e6cc"}, "originalPosition": 324}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3299, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}