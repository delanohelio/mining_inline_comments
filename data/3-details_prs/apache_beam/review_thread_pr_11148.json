{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzkwMDg0MzU0", "number": 11148, "reviewThreads": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QyMDo1NzozMlrODqjWiA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QyMDo1ODo0M1rODqjYRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ1OTQ1OTkyOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/interactive/interactive_runner_test.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QyMDo1NzozMlrOF6XztA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNFQxNzo1MDowOFrOF68PeA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc1MTc5Ng==", "bodyText": "Does this not trigger the capture duration?", "url": "https://github.com/apache/beam/pull/11148#discussion_r396751796", "createdAt": "2020-03-23T20:57:32Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/interactive/interactive_runner_test.py", "diffHunk": "@@ -147,6 +150,97 @@ def process(self, element):\n     ]\n     self.assertEqual(actual_reified, expected_reified)\n \n+  def test_streaming_wordcount(self):\n+    class WordExtractingDoFn(beam.DoFn):\n+      def process(self, element):\n+        text_line = element.strip()\n+        words = text_line.split()\n+        return words\n+\n+    # Add the TestStream so that it can be cached.\n+    ib.options.capturable_sources.add(TestStream)\n+    ib.options.capture_duration = timedelta(seconds=1)\n+\n+    p = beam.Pipeline(\n+        runner=interactive_runner.InteractiveRunner(),\n+        options=StandardOptions(streaming=True))\n+\n+    data = (\n+        p\n+        | TestStream()\n+            .advance_watermark_to(0)\n+            .advance_processing_time(1)\n+            .add_elements(['to', 'be', 'or', 'not', 'to', 'be'])\n+            .advance_watermark_to(20)\n+            .advance_processing_time(1)\n+            .add_elements(['to', 'be', 'or', 'not', 'to', 'be'])\n+            .advance_watermark_to(40)\n+            .advance_processing_time(1)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "06c365026fc255b6ccc1fbc800b76d3ab6445ba2"}, "originalPosition": 48}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzM0ODcyOA==", "bodyText": "No, because the fake clock is instantiated by the runner, the background caching job only has a handle on the real clock.", "url": "https://github.com/apache/beam/pull/11148#discussion_r397348728", "createdAt": "2020-03-24T17:50:08Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/interactive/interactive_runner_test.py", "diffHunk": "@@ -147,6 +150,97 @@ def process(self, element):\n     ]\n     self.assertEqual(actual_reified, expected_reified)\n \n+  def test_streaming_wordcount(self):\n+    class WordExtractingDoFn(beam.DoFn):\n+      def process(self, element):\n+        text_line = element.strip()\n+        words = text_line.split()\n+        return words\n+\n+    # Add the TestStream so that it can be cached.\n+    ib.options.capturable_sources.add(TestStream)\n+    ib.options.capture_duration = timedelta(seconds=1)\n+\n+    p = beam.Pipeline(\n+        runner=interactive_runner.InteractiveRunner(),\n+        options=StandardOptions(streaming=True))\n+\n+    data = (\n+        p\n+        | TestStream()\n+            .advance_watermark_to(0)\n+            .advance_processing_time(1)\n+            .add_elements(['to', 'be', 'or', 'not', 'to', 'be'])\n+            .advance_watermark_to(20)\n+            .advance_processing_time(1)\n+            .add_elements(['to', 'be', 'or', 'not', 'to', 'be'])\n+            .advance_watermark_to(40)\n+            .advance_processing_time(1)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc1MTc5Ng=="}, "originalCommit": {"oid": "06c365026fc255b6ccc1fbc800b76d3ab6445ba2"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ1OTQ2MTY3OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/interactive/interactive_runner_test.py", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QyMDo1ODowMFrOF6X00Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNFQxNzo1MTozOFrOF68TaQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc1MjA4MQ==", "bodyText": "Why is this need?", "url": "https://github.com/apache/beam/pull/11148#discussion_r396752081", "createdAt": "2020-03-23T20:58:00Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/interactive/interactive_runner_test.py", "diffHunk": "@@ -147,6 +150,97 @@ def process(self, element):\n     ]\n     self.assertEqual(actual_reified, expected_reified)\n \n+  def test_streaming_wordcount(self):\n+    class WordExtractingDoFn(beam.DoFn):\n+      def process(self, element):\n+        text_line = element.strip()\n+        words = text_line.split()\n+        return words\n+\n+    # Add the TestStream so that it can be cached.\n+    ib.options.capturable_sources.add(TestStream)\n+    ib.options.capture_duration = timedelta(seconds=1)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "06c365026fc255b6ccc1fbc800b76d3ab6445ba2"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzM0OTczNw==", "bodyText": "Because the background_caching_job is using the real-time clock instead of the fake clock from the runner, we limit the capture duration to 1s to limit the length of the test.", "url": "https://github.com/apache/beam/pull/11148#discussion_r397349737", "createdAt": "2020-03-24T17:51:38Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/interactive/interactive_runner_test.py", "diffHunk": "@@ -147,6 +150,97 @@ def process(self, element):\n     ]\n     self.assertEqual(actual_reified, expected_reified)\n \n+  def test_streaming_wordcount(self):\n+    class WordExtractingDoFn(beam.DoFn):\n+      def process(self, element):\n+        text_line = element.strip()\n+        words = text_line.split()\n+        return words\n+\n+    # Add the TestStream so that it can be cached.\n+    ib.options.capturable_sources.add(TestStream)\n+    ib.options.capture_duration = timedelta(seconds=1)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc1MjA4MQ=="}, "originalCommit": {"oid": "06c365026fc255b6ccc1fbc800b76d3ab6445ba2"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjQ1OTQ2NDM5OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/runners/interactive/interactive_runner_test.py", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yM1QyMDo1ODo0M1rOF6X2eg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMy0yNVQwMDowOTo1OFrOF7Hsbw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc1MjUwNg==", "bodyText": "It'd be easier to understand the test if there were less data.", "url": "https://github.com/apache/beam/pull/11148#discussion_r396752506", "createdAt": "2020-03-23T20:58:43Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/interactive/interactive_runner_test.py", "diffHunk": "@@ -147,6 +150,97 @@ def process(self, element):\n     ]\n     self.assertEqual(actual_reified, expected_reified)\n \n+  def test_streaming_wordcount(self):\n+    class WordExtractingDoFn(beam.DoFn):\n+      def process(self, element):\n+        text_line = element.strip()\n+        words = text_line.split()\n+        return words\n+\n+    # Add the TestStream so that it can be cached.\n+    ib.options.capturable_sources.add(TestStream)\n+    ib.options.capture_duration = timedelta(seconds=1)\n+\n+    p = beam.Pipeline(\n+        runner=interactive_runner.InteractiveRunner(),\n+        options=StandardOptions(streaming=True))\n+\n+    data = (\n+        p\n+        | TestStream()\n+            .advance_watermark_to(0)\n+            .advance_processing_time(1)\n+            .add_elements(['to', 'be', 'or', 'not', 'to', 'be'])\n+            .advance_watermark_to(20)\n+            .advance_processing_time(1)\n+            .add_elements(['to', 'be', 'or', 'not', 'to', 'be'])\n+            .advance_watermark_to(40)\n+            .advance_processing_time(1)\n+            .add_elements(['to', 'be', 'or', 'not', 'to', 'be'])\n+        | beam.WindowInto(beam.window.FixedWindows(10))) # yapf: disable\n+\n+    counts = (\n+        data\n+        | 'split' >> beam.ParDo(WordExtractingDoFn())\n+        | 'pair_with_one' >> beam.Map(lambda x: (x, 1))\n+        | 'group' >> beam.GroupByKey()\n+        | 'count' >> beam.Map(lambda wordones: (wordones[0], sum(wordones[1]))))\n+\n+    # Watch the local scope for Interactive Beam so that referenced PCollections\n+    # will be cached.\n+    ib.watch(locals())\n+\n+    # This is normally done in the interactive_utils when a transform is\n+    # applied but needs an IPython environment. So we manually run this here.\n+    ie.current_env().track_user_pipelines()\n+\n+    # This tests that the data was correctly cached.\n+    pane_info = PaneInfo(True, True, PaneInfoTiming.UNKNOWN, 0, 0)\n+    expected_data_df = pd.DataFrame(\n+        [('to', 0, [beam.window.IntervalWindow(0, 10)], pane_info),", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "06c365026fc255b6ccc1fbc800b76d3ab6445ba2"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzM1MTcwMQ==", "bodyText": "Initially I had the test to generate the data in code, but I felt that this was more explicit and easier to show what we are expecting. In some ways, the test is about the data and to make that obvious and explicit makes it easier to understand.", "url": "https://github.com/apache/beam/pull/11148#discussion_r397351701", "createdAt": "2020-03-24T17:54:25Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/interactive/interactive_runner_test.py", "diffHunk": "@@ -147,6 +150,97 @@ def process(self, element):\n     ]\n     self.assertEqual(actual_reified, expected_reified)\n \n+  def test_streaming_wordcount(self):\n+    class WordExtractingDoFn(beam.DoFn):\n+      def process(self, element):\n+        text_line = element.strip()\n+        words = text_line.split()\n+        return words\n+\n+    # Add the TestStream so that it can be cached.\n+    ib.options.capturable_sources.add(TestStream)\n+    ib.options.capture_duration = timedelta(seconds=1)\n+\n+    p = beam.Pipeline(\n+        runner=interactive_runner.InteractiveRunner(),\n+        options=StandardOptions(streaming=True))\n+\n+    data = (\n+        p\n+        | TestStream()\n+            .advance_watermark_to(0)\n+            .advance_processing_time(1)\n+            .add_elements(['to', 'be', 'or', 'not', 'to', 'be'])\n+            .advance_watermark_to(20)\n+            .advance_processing_time(1)\n+            .add_elements(['to', 'be', 'or', 'not', 'to', 'be'])\n+            .advance_watermark_to(40)\n+            .advance_processing_time(1)\n+            .add_elements(['to', 'be', 'or', 'not', 'to', 'be'])\n+        | beam.WindowInto(beam.window.FixedWindows(10))) # yapf: disable\n+\n+    counts = (\n+        data\n+        | 'split' >> beam.ParDo(WordExtractingDoFn())\n+        | 'pair_with_one' >> beam.Map(lambda x: (x, 1))\n+        | 'group' >> beam.GroupByKey()\n+        | 'count' >> beam.Map(lambda wordones: (wordones[0], sum(wordones[1]))))\n+\n+    # Watch the local scope for Interactive Beam so that referenced PCollections\n+    # will be cached.\n+    ib.watch(locals())\n+\n+    # This is normally done in the interactive_utils when a transform is\n+    # applied but needs an IPython environment. So we manually run this here.\n+    ie.current_env().track_user_pipelines()\n+\n+    # This tests that the data was correctly cached.\n+    pane_info = PaneInfo(True, True, PaneInfoTiming.UNKNOWN, 0, 0)\n+    expected_data_df = pd.DataFrame(\n+        [('to', 0, [beam.window.IntervalWindow(0, 10)], pane_info),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc1MjUwNg=="}, "originalCommit": {"oid": "06c365026fc255b6ccc1fbc800b76d3ab6445ba2"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzQ4OTM2Nw==", "bodyText": "I agree that making things explicit can make it easier to understand. What I was asking was whether adding six items three times each increased the coverage of adding (say) three items (one duplicate), advancing the watermark, then adding one or two more.", "url": "https://github.com/apache/beam/pull/11148#discussion_r397489367", "createdAt": "2020-03-24T22:03:49Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/interactive/interactive_runner_test.py", "diffHunk": "@@ -147,6 +150,97 @@ def process(self, element):\n     ]\n     self.assertEqual(actual_reified, expected_reified)\n \n+  def test_streaming_wordcount(self):\n+    class WordExtractingDoFn(beam.DoFn):\n+      def process(self, element):\n+        text_line = element.strip()\n+        words = text_line.split()\n+        return words\n+\n+    # Add the TestStream so that it can be cached.\n+    ib.options.capturable_sources.add(TestStream)\n+    ib.options.capture_duration = timedelta(seconds=1)\n+\n+    p = beam.Pipeline(\n+        runner=interactive_runner.InteractiveRunner(),\n+        options=StandardOptions(streaming=True))\n+\n+    data = (\n+        p\n+        | TestStream()\n+            .advance_watermark_to(0)\n+            .advance_processing_time(1)\n+            .add_elements(['to', 'be', 'or', 'not', 'to', 'be'])\n+            .advance_watermark_to(20)\n+            .advance_processing_time(1)\n+            .add_elements(['to', 'be', 'or', 'not', 'to', 'be'])\n+            .advance_watermark_to(40)\n+            .advance_processing_time(1)\n+            .add_elements(['to', 'be', 'or', 'not', 'to', 'be'])\n+        | beam.WindowInto(beam.window.FixedWindows(10))) # yapf: disable\n+\n+    counts = (\n+        data\n+        | 'split' >> beam.ParDo(WordExtractingDoFn())\n+        | 'pair_with_one' >> beam.Map(lambda x: (x, 1))\n+        | 'group' >> beam.GroupByKey()\n+        | 'count' >> beam.Map(lambda wordones: (wordones[0], sum(wordones[1]))))\n+\n+    # Watch the local scope for Interactive Beam so that referenced PCollections\n+    # will be cached.\n+    ib.watch(locals())\n+\n+    # This is normally done in the interactive_utils when a transform is\n+    # applied but needs an IPython environment. So we manually run this here.\n+    ie.current_env().track_user_pipelines()\n+\n+    # This tests that the data was correctly cached.\n+    pane_info = PaneInfo(True, True, PaneInfoTiming.UNKNOWN, 0, 0)\n+    expected_data_df = pd.DataFrame(\n+        [('to', 0, [beam.window.IntervalWindow(0, 10)], pane_info),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc1MjUwNg=="}, "originalCommit": {"oid": "06c365026fc255b6ccc1fbc800b76d3ab6445ba2"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzUzNDUyMw==", "bodyText": "Ohhh I see, I misread your comment. I changed it to a smaller set of data that doesn't have duplicates to test the windowing.", "url": "https://github.com/apache/beam/pull/11148#discussion_r397534523", "createdAt": "2020-03-25T00:04:01Z", "author": {"login": "rohdesamuel"}, "path": "sdks/python/apache_beam/runners/interactive/interactive_runner_test.py", "diffHunk": "@@ -147,6 +150,97 @@ def process(self, element):\n     ]\n     self.assertEqual(actual_reified, expected_reified)\n \n+  def test_streaming_wordcount(self):\n+    class WordExtractingDoFn(beam.DoFn):\n+      def process(self, element):\n+        text_line = element.strip()\n+        words = text_line.split()\n+        return words\n+\n+    # Add the TestStream so that it can be cached.\n+    ib.options.capturable_sources.add(TestStream)\n+    ib.options.capture_duration = timedelta(seconds=1)\n+\n+    p = beam.Pipeline(\n+        runner=interactive_runner.InteractiveRunner(),\n+        options=StandardOptions(streaming=True))\n+\n+    data = (\n+        p\n+        | TestStream()\n+            .advance_watermark_to(0)\n+            .advance_processing_time(1)\n+            .add_elements(['to', 'be', 'or', 'not', 'to', 'be'])\n+            .advance_watermark_to(20)\n+            .advance_processing_time(1)\n+            .add_elements(['to', 'be', 'or', 'not', 'to', 'be'])\n+            .advance_watermark_to(40)\n+            .advance_processing_time(1)\n+            .add_elements(['to', 'be', 'or', 'not', 'to', 'be'])\n+        | beam.WindowInto(beam.window.FixedWindows(10))) # yapf: disable\n+\n+    counts = (\n+        data\n+        | 'split' >> beam.ParDo(WordExtractingDoFn())\n+        | 'pair_with_one' >> beam.Map(lambda x: (x, 1))\n+        | 'group' >> beam.GroupByKey()\n+        | 'count' >> beam.Map(lambda wordones: (wordones[0], sum(wordones[1]))))\n+\n+    # Watch the local scope for Interactive Beam so that referenced PCollections\n+    # will be cached.\n+    ib.watch(locals())\n+\n+    # This is normally done in the interactive_utils when a transform is\n+    # applied but needs an IPython environment. So we manually run this here.\n+    ie.current_env().track_user_pipelines()\n+\n+    # This tests that the data was correctly cached.\n+    pane_info = PaneInfo(True, True, PaneInfoTiming.UNKNOWN, 0, 0)\n+    expected_data_df = pd.DataFrame(\n+        [('to', 0, [beam.window.IntervalWindow(0, 10)], pane_info),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc1MjUwNg=="}, "originalCommit": {"oid": "06c365026fc255b6ccc1fbc800b76d3ab6445ba2"}, "originalPosition": 70}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5NzUzNjM2Nw==", "bodyText": "Thanks, that's much better--now I can read it and know the answer is right. (Duplicates would have been OK, and the data set could be smaller still and still provide the right coverage, but this is fine.)\nLGMT", "url": "https://github.com/apache/beam/pull/11148#discussion_r397536367", "createdAt": "2020-03-25T00:09:58Z", "author": {"login": "robertwb"}, "path": "sdks/python/apache_beam/runners/interactive/interactive_runner_test.py", "diffHunk": "@@ -147,6 +150,97 @@ def process(self, element):\n     ]\n     self.assertEqual(actual_reified, expected_reified)\n \n+  def test_streaming_wordcount(self):\n+    class WordExtractingDoFn(beam.DoFn):\n+      def process(self, element):\n+        text_line = element.strip()\n+        words = text_line.split()\n+        return words\n+\n+    # Add the TestStream so that it can be cached.\n+    ib.options.capturable_sources.add(TestStream)\n+    ib.options.capture_duration = timedelta(seconds=1)\n+\n+    p = beam.Pipeline(\n+        runner=interactive_runner.InteractiveRunner(),\n+        options=StandardOptions(streaming=True))\n+\n+    data = (\n+        p\n+        | TestStream()\n+            .advance_watermark_to(0)\n+            .advance_processing_time(1)\n+            .add_elements(['to', 'be', 'or', 'not', 'to', 'be'])\n+            .advance_watermark_to(20)\n+            .advance_processing_time(1)\n+            .add_elements(['to', 'be', 'or', 'not', 'to', 'be'])\n+            .advance_watermark_to(40)\n+            .advance_processing_time(1)\n+            .add_elements(['to', 'be', 'or', 'not', 'to', 'be'])\n+        | beam.WindowInto(beam.window.FixedWindows(10))) # yapf: disable\n+\n+    counts = (\n+        data\n+        | 'split' >> beam.ParDo(WordExtractingDoFn())\n+        | 'pair_with_one' >> beam.Map(lambda x: (x, 1))\n+        | 'group' >> beam.GroupByKey()\n+        | 'count' >> beam.Map(lambda wordones: (wordones[0], sum(wordones[1]))))\n+\n+    # Watch the local scope for Interactive Beam so that referenced PCollections\n+    # will be cached.\n+    ib.watch(locals())\n+\n+    # This is normally done in the interactive_utils when a transform is\n+    # applied but needs an IPython environment. So we manually run this here.\n+    ie.current_env().track_user_pipelines()\n+\n+    # This tests that the data was correctly cached.\n+    pane_info = PaneInfo(True, True, PaneInfoTiming.UNKNOWN, 0, 0)\n+    expected_data_df = pd.DataFrame(\n+        [('to', 0, [beam.window.IntervalWindow(0, 10)], pane_info),", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM5Njc1MjUwNg=="}, "originalCommit": {"oid": "06c365026fc255b6ccc1fbc800b76d3ab6445ba2"}, "originalPosition": 70}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1664, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}