{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0MzgwNDEzMjcz", "number": 10979, "reviewThreads": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQxNzozMjo0MFrODjK96w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQyMjoxMjo1OFrODjQwCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4MjA2NDQzOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/io/localfilesystem.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQxNzozMjo0MFrOFu1MbQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQxNzozMjo0MFrOFu1MbQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDY1MDM0OQ==", "bodyText": "open doesn't provide io.IOBase methods like writable() in Python 2.", "url": "https://github.com/apache/beam/pull/10979#discussion_r384650349", "createdAt": "2020-02-26T17:32:40Z", "author": {"login": "chunyang"}, "path": "sdks/python/apache_beam/io/localfilesystem.py", "diffHunk": "@@ -139,7 +140,7 @@ def _path_open(\n     \"\"\"Helper functions to open a file in the provided mode.\n     \"\"\"\n     compression_type = FileSystem._get_compression_type(path, compression_type)\n-    raw_file = open(path, mode)\n+    raw_file = io.open(path, mode)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f4d1706016a26e0f411ce3722c9df382a4f9cf06"}, "originalPosition": 13}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4MjA2OTE4OnYy", "diffSide": "RIGHT", "path": "sdks/python/scripts/run_integration_test.sh", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQxNzozNDowM1rOFu1PWA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQxNzozNDowM1rOFu1PWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDY1MTA5Ng==", "bodyText": "This was added because the bigquery_test module imports _ELEMENTS from the bigquery_file_loads_test module, which uses parameterized. Should we refactor _ELEMENTS into its own module?", "url": "https://github.com/apache/beam/pull/10979#discussion_r384651096", "createdAt": "2020-02-26T17:34:03Z", "author": {"login": "chunyang"}, "path": "sdks/python/scripts/run_integration_test.sh", "diffHunk": "@@ -198,6 +198,7 @@ if [[ -z $PIPELINE_OPTS ]]; then\n   # See: https://github.com/hamcrest/PyHamcrest/issues/131.\n   echo \"pyhamcrest!=1.10.0,<2.0.0\" > postcommit_requirements.txt\n   echo \"mock<3.0.0\" >> postcommit_requirements.txt\n+  echo \"parameterized>=0.7.1,<0.8.0\" >> postcommit_requirements.txt", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f4d1706016a26e0f411ce3722c9df382a4f9cf06"}, "originalPosition": 4}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4MjA4MDE3OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/io/gcp/bigquery_avro_tools.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQxNzozNzoxNFrOFu1WCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQxNzozNzoxNFrOFu1WCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDY1MjgwOA==", "bodyText": "The same code in the Java SDK doesn't seem to support logical types, so behavior is a bit different here.", "url": "https://github.com/apache/beam/pull/10979#discussion_r384652808", "createdAt": "2020-02-26T17:37:14Z", "author": {"login": "chunyang"}, "path": "sdks/python/apache_beam/io/gcp/bigquery_avro_tools.py", "diffHunk": "@@ -0,0 +1,135 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"Tools used tool work with Avro files in the context of BigQuery.\n+\n+Classes, constants and functions in this file are experimental and have no\n+backwards compatibility guarantees.\n+\n+NOTHING IN THIS FILE HAS BACKWARDS COMPATIBILITY GUARANTEES.\n+\"\"\"\n+\n+from __future__ import absolute_import\n+from __future__ import division\n+\n+BIG_QUERY_TO_AVRO_TYPES = {\n+  \"RECORD\": \"record\",\n+  \"STRING\": \"string\",\n+  \"BOOLEAN\": \"boolean\",\n+  \"BYTES\": \"bytes\",\n+  \"FLOAT\": \"double\",\n+  \"INTEGER\": \"long\",\n+  \"TIME\": {\n+    \"type\": \"long\",\n+    \"logicalType\": \"time-micros\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f4d1706016a26e0f411ce3722c9df382a4f9cf06"}, "originalPosition": 38}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4MjA4MTg4OnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/io/gcp/bigquery.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQxNzozNzo0NFrOFu1XBQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQxNzozNzo0NFrOFu1XBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDY1MzA2MQ==", "bodyText": "Moved these to avoid a cyclic import.", "url": "https://github.com/apache/beam/pull/10979#discussion_r384653061", "createdAt": "2020-02-26T17:37:44Z", "author": {"login": "chunyang"}, "path": "sdks/python/apache_beam/io/gcp/bigquery.py", "diffHunk": "@@ -1361,87 +1369,18 @@ def __init__(\n     self.triggering_frequency = triggering_frequency\n     self.insert_retry_strategy = insert_retry_strategy\n     self._validate = validate\n+    self._temp_file_format = temp_file_format or bigquery_tools.FileFormat.JSON\n \n     self.additional_bq_parameters = additional_bq_parameters or {}\n     self.table_side_inputs = table_side_inputs or ()\n     self.schema_side_inputs = schema_side_inputs or ()\n \n-  @staticmethod\n-  def get_table_schema_from_string(schema):\n-    \"\"\"Transform the string table schema into a\n-    :class:`~apache_beam.io.gcp.internal.clients.bigquery.\\\n-bigquery_v2_messages.TableSchema` instance.\n-\n-    Args:\n-      schema (str): The sting schema to be used if the BigQuery table to write\n-        has to be created.\n-\n-    Returns:\n-      ~apache_beam.io.gcp.internal.clients.bigquery.\\\n-bigquery_v2_messages.TableSchema:\n-      The schema to be used if the BigQuery table to write has to be created\n-      but in the :class:`~apache_beam.io.gcp.internal.clients.bigquery.\\\n-bigquery_v2_messages.TableSchema` format.\n-    \"\"\"\n-    table_schema = bigquery.TableSchema()\n-    schema_list = [s.strip() for s in schema.split(',')]\n-    for field_and_type in schema_list:\n-      field_name, field_type = field_and_type.split(':')\n-      field_schema = bigquery.TableFieldSchema()\n-      field_schema.name = field_name\n-      field_schema.type = field_type\n-      field_schema.mode = 'NULLABLE'\n-      table_schema.fields.append(field_schema)\n-    return table_schema\n-\n-  @staticmethod\n-  def table_schema_to_dict(table_schema):\n-    \"\"\"Create a dictionary representation of table schema for serialization\n-    \"\"\"\n-    def get_table_field(field):\n-      \"\"\"Create a dictionary representation of a table field\n-      \"\"\"\n-      result = {}\n-      result['name'] = field.name\n-      result['type'] = field.type\n-      result['mode'] = getattr(field, 'mode', 'NULLABLE')\n-      if hasattr(field, 'description') and field.description is not None:\n-        result['description'] = field.description\n-      if hasattr(field, 'fields') and field.fields:\n-        result['fields'] = [get_table_field(f) for f in field.fields]\n-      return result\n-\n-    if not isinstance(table_schema, bigquery.TableSchema):\n-      raise ValueError(\"Table schema must be of the type bigquery.TableSchema\")\n-    schema = {'fields': []}\n-    for field in table_schema.fields:\n-      schema['fields'].append(get_table_field(field))\n-    return schema\n-\n-  @staticmethod\n-  def get_dict_table_schema(schema):\n-    \"\"\"Transform the table schema into a dictionary instance.\n-\n-    Args:\n-      schema (~apache_beam.io.gcp.internal.clients.bigquery.\\\n-bigquery_v2_messages.TableSchema):\n-        The schema to be used if the BigQuery table to write has to be created.\n-        This can either be a dict or string or in the TableSchema format.\n-\n-    Returns:\n-      Dict[str, Any]: The schema to be used if the BigQuery table to write has\n-      to be created but in the dictionary format.\n-    \"\"\"\n-    if (isinstance(schema, (dict, vp.ValueProvider)) or callable(schema) or\n-        schema is None):\n-      return schema\n-    elif isinstance(schema, (str, unicode)):\n-      table_schema = WriteToBigQuery.get_table_schema_from_string(schema)\n-      return WriteToBigQuery.table_schema_to_dict(table_schema)\n-    elif isinstance(schema, bigquery.TableSchema):\n-      return WriteToBigQuery.table_schema_to_dict(schema)\n-    else:\n-      raise TypeError('Unexpected schema argument: %s.' % schema)\n+  # Dict/schema methods were moved to bigquery_tools, but keep references\n+  # here for backward compatibility.\n+  get_table_schema_from_string = \\\n+      staticmethod(bigquery_tools.get_table_schema_from_string)\n+  table_schema_to_dict = staticmethod(bigquery_tools.table_schema_to_dict)\n+  get_dict_table_schema = staticmethod(bigquery_tools.get_dict_table_schema)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "f4d1706016a26e0f411ce3722c9df382a4f9cf06"}, "originalPosition": 133}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4MjgyMzIyOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/io/gcp/bigquery.py", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQyMToxNToyMlrOFu8e0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yOFQxODozNjoxNFrOFv-z-Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDc2OTc0NA==", "bodyText": "I'm happy to make AVRO the default format if possible. I guess the issue is that users need to provide the schema, right? Otherwise we cannot write the avro files.\nWe could make AVRO the default, and add a check that the schema was provided (i.e. is neither None nor autodetect) - and error out if that's the case? What do you think?", "url": "https://github.com/apache/beam/pull/10979#discussion_r384769744", "createdAt": "2020-02-26T21:15:22Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/io/gcp/bigquery.py", "diffHunk": "@@ -1361,87 +1369,18 @@ def __init__(\n     self.triggering_frequency = triggering_frequency\n     self.insert_retry_strategy = insert_retry_strategy\n     self._validate = validate\n+    self._temp_file_format = temp_file_format or bigquery_tools.FileFormat.JSON", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "efa1137f29da8ab0d4607307ab33f8793531aacc"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTQ0MjA0Ng==", "bodyText": "AFAICT using Avro has no disadvantages compared to JSON for loading data into BigQuery, but would requiring a schema constitute a breaking API change for semantic versioning purposes?\nPersonally I'm for using Avro as default. I guess when users update Beam, they'll specify a temp_file_format explicitly to get the old behavior.", "url": "https://github.com/apache/beam/pull/10979#discussion_r385442046", "createdAt": "2020-02-28T00:08:44Z", "author": {"login": "chunyang"}, "path": "sdks/python/apache_beam/io/gcp/bigquery.py", "diffHunk": "@@ -1361,87 +1369,18 @@ def __init__(\n     self.triggering_frequency = triggering_frequency\n     self.insert_retry_strategy = insert_retry_strategy\n     self._validate = validate\n+    self._temp_file_format = temp_file_format or bigquery_tools.FileFormat.JSON", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDc2OTc0NA=="}, "originalCommit": {"oid": "efa1137f29da8ab0d4607307ab33f8793531aacc"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTg0MjY5MA==", "bodyText": "Since this is technically still experimental, and masked behind a flag, I think it makes sense to make avro the main way of doing it (and simply add a check that the schema was passed). cc: @chamikaramj", "url": "https://github.com/apache/beam/pull/10979#discussion_r385842690", "createdAt": "2020-02-28T18:06:04Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/io/gcp/bigquery.py", "diffHunk": "@@ -1361,87 +1369,18 @@ def __init__(\n     self.triggering_frequency = triggering_frequency\n     self.insert_retry_strategy = insert_retry_strategy\n     self._validate = validate\n+    self._temp_file_format = temp_file_format or bigquery_tools.FileFormat.JSON", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDc2OTc0NA=="}, "originalCommit": {"oid": "efa1137f29da8ab0d4607307ab33f8793531aacc"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTg1NDY0NA==", "bodyText": "Oh I didn't realize it was experimental. I'll make the change then!", "url": "https://github.com/apache/beam/pull/10979#discussion_r385854644", "createdAt": "2020-02-28T18:32:21Z", "author": {"login": "chunyang"}, "path": "sdks/python/apache_beam/io/gcp/bigquery.py", "diffHunk": "@@ -1361,87 +1369,18 @@ def __init__(\n     self.triggering_frequency = triggering_frequency\n     self.insert_retry_strategy = insert_retry_strategy\n     self._validate = validate\n+    self._temp_file_format = temp_file_format or bigquery_tools.FileFormat.JSON", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDc2OTc0NA=="}, "originalCommit": {"oid": "efa1137f29da8ab0d4607307ab33f8793531aacc"}, "originalPosition": 46}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NTg1NjUwNQ==", "bodyText": "+1 for making Avro the default for the new sink.", "url": "https://github.com/apache/beam/pull/10979#discussion_r385856505", "createdAt": "2020-02-28T18:36:14Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/io/gcp/bigquery.py", "diffHunk": "@@ -1361,87 +1369,18 @@ def __init__(\n     self.triggering_frequency = triggering_frequency\n     self.insert_retry_strategy = insert_retry_strategy\n     self._validate = validate\n+    self._temp_file_format = temp_file_format or bigquery_tools.FileFormat.JSON", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDc2OTc0NA=="}, "originalCommit": {"oid": "efa1137f29da8ab0d4607307ab33f8793531aacc"}, "originalPosition": 46}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjM4MzAxMTkyOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/io/gcp/bigquery_test.py", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQyMjoxMjo1OFrOFu-SIA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wMi0yNlQyMjoxMjo1OFrOFu-SIA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDM4NDc5OTI2NA==", "bodyText": "Can you add code to delete the dataset after the test runs?", "url": "https://github.com/apache/beam/pull/10979#discussion_r384799264", "createdAt": "2020-02-26T22:12:58Z", "author": {"login": "pabloem"}, "path": "sdks/python/apache_beam/io/gcp/bigquery_test.py", "diffHunk": "@@ -1025,6 +1027,91 @@ def test_file_loads(self):\n         WriteToBigQuery.Method.FILE_LOADS, triggering_frequency=20)\n \n \n+class BigQueryFileLoadsIntegrationTests(unittest.TestCase):\n+  BIG_QUERY_DATASET_ID = 'python_bq_file_loads_'\n+\n+  def setUp(self):\n+    self.test_pipeline = TestPipeline(is_integration_test=True)\n+    self.runner_name = type(self.test_pipeline.runner).__name__\n+    self.project = self.test_pipeline.get_option('project')\n+\n+    self.dataset_id = '%s%s%s' % (\n+        self.BIG_QUERY_DATASET_ID,\n+        str(int(time.time())),\n+        random.randint(0, 10000))\n+    self.bigquery_client = bigquery_tools.BigQueryWrapper()\n+    self.bigquery_client.get_or_create_dataset(self.project, self.dataset_id)\n+    self.output_table = '%s.output_table' % (self.dataset_id)\n+    self.table_ref = bigquery_tools.parse_table_reference(self.output_table)\n+    _LOGGER.info(\n+        'Created dataset %s in project %s', self.dataset_id, self.project)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "efa1137f29da8ab0d4607307ab33f8793531aacc"}, "originalPosition": 37}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 1927, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}