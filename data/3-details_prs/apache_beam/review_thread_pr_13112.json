{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTAzNDU2MzAx", "number": 13112, "reviewThreads": {"totalCount": 19, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwNjo1NToxNlrOE5SiHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzo1ODo1MFrOFCgnYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4NTA3OTM1OnYy", "diffSide": "RIGHT", "path": "examples/templates/java/kafka-to-pubsub/src/test/java/org/apache/beam/templates/KafkaToPubsubTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwNjo1NToxNlrOHzqD8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QxODo1ODoyNFrOH4bNYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkyODU2MQ==", "bodyText": "Can we write a positive test as well ?", "url": "https://github.com/apache/beam/pull/13112#discussion_r523928561", "createdAt": "2020-11-16T06:55:16Z", "author": {"login": "manavgarg"}, "path": "examples/templates/java/kafka-to-pubsub/src/test/java/org/apache/beam/templates/KafkaToPubsubTest.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.templates.transforms.FormatTransform;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+/** Test of KafkaToPubsub. */\n+@RunWith(JUnit4.class)\n+public class KafkaToPubsubTest {\n+\n+  @Rule public final transient TestPipeline pipeline = TestPipeline.create();\n+\n+  @Rule public ExpectedException thrown = ExpectedException.none();\n+\n+  @Test\n+  public void testKafkaReadingFailsWrongBootstrapServer() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODcxNzI3NQ==", "bodyText": "Yes, we've already implemented 5 tests with positive and negative cases.", "url": "https://github.com/apache/beam/pull/13112#discussion_r528717275", "createdAt": "2020-11-23T13:52:24Z", "author": {"login": "KhaninArtur"}, "path": "examples/templates/java/kafka-to-pubsub/src/test/java/org/apache/beam/templates/KafkaToPubsubTest.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.templates.transforms.FormatTransform;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+/** Test of KafkaToPubsub. */\n+@RunWith(JUnit4.class)\n+public class KafkaToPubsubTest {\n+\n+  @Rule public final transient TestPipeline pipeline = TestPipeline.create();\n+\n+  @Rule public ExpectedException thrown = ExpectedException.none();\n+\n+  @Test\n+  public void testKafkaReadingFailsWrongBootstrapServer() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkyODU2MQ=="}, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 42}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkyODA5OA==", "bodyText": "Thanks.", "url": "https://github.com/apache/beam/pull/13112#discussion_r528928098", "createdAt": "2020-11-23T18:58:24Z", "author": {"login": "manavgarg"}, "path": "examples/templates/java/kafka-to-pubsub/src/test/java/org/apache/beam/templates/KafkaToPubsubTest.java", "diffHunk": "@@ -0,0 +1,54 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates;\n+\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.testing.TestPipeline;\n+import org.apache.beam.templates.transforms.FormatTransform;\n+import org.junit.Rule;\n+import org.junit.Test;\n+import org.junit.rules.ExpectedException;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+/** Test of KafkaToPubsub. */\n+@RunWith(JUnit4.class)\n+public class KafkaToPubsubTest {\n+\n+  @Rule public final transient TestPipeline pipeline = TestPipeline.create();\n+\n+  @Rule public ExpectedException thrown = ExpectedException.none();\n+\n+  @Test\n+  public void testKafkaReadingFailsWrongBootstrapServer() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkyODU2MQ=="}, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 42}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI4NTEwOTM1OnYy", "diffSide": "RIGHT", "path": "examples/templates/java/kafka-to-pubsub/src/main/java/org/apache/beam/templates/options/KafkaToPubsubOptions.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwNzowODoyOFrOHzqUuQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xNlQwNzowODoyOFrOHzqUuQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyMzkzMjg1Nw==", "bodyText": "Remove default value.", "url": "https://github.com/apache/beam/pull/13112#discussion_r523932857", "createdAt": "2020-11-16T07:08:28Z", "author": {"login": "manavgarg"}, "path": "examples/templates/java/kafka-to-pubsub/src/main/java/org/apache/beam/templates/options/KafkaToPubsubOptions.java", "diffHunk": "@@ -0,0 +1,89 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates.options;\n+\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.Validation;\n+import org.apache.beam.templates.transforms.FormatTransform;\n+\n+public interface KafkaToPubsubOptions extends PipelineOptions {\n+  @Description(\"Kafka Bootstrap Servers\")\n+  @Validation.Required\n+  String getBootstrapServers();\n+\n+  void setBootstrapServers(String value);\n+\n+  @Description(\"Kafka topics to read the input from\")\n+  @Validation.Required\n+  String getInputTopics();\n+\n+  void setInputTopics(String value);\n+\n+  @Description(\n+      \"The Cloud Pub/Sub topic to publish to. \"\n+          + \"The name should be in the format of \"\n+          + \"projects/<project-id>/topics/<topic-name>.\")\n+  @Validation.Required\n+  String getOutputTopic();\n+\n+  void setOutputTopic(String outputTopic);\n+\n+  @Description(\"\")\n+  @Validation.Required\n+  @Default.Enum(\"PUBSUB\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 50}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5NTU3MDU1OnYy", "diffSide": "RIGHT", "path": "examples/templates/java/README.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQwMDoxOTo0NFrOH1Qz2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QxMzo1MDoxN1rOH4OQxA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYxMTk5NQ==", "bodyText": "Does this work with a local path? Wouldn't the keystore need to get staged for use on a distributed runner?\nFWIW I think that configuring KafkaIO to use SSL is much more difficult than it should be. Here's an SO question that describes how you can create a custom ConsumerFactoryFn that downloads a keystore from GCS at execution time: https://stackoverflow.com/questions/42726011/truststore-and-google-cloud-dataflow\nI think it could be worthwhile to make KafkaIO do this by default", "url": "https://github.com/apache/beam/pull/13112#discussion_r525611995", "createdAt": "2020-11-18T00:19:44Z", "author": {"login": "TheNeuralBit"}, "path": "examples/templates/java/README.md", "diffHunk": "@@ -0,0 +1,252 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam Template to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) Template that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Pub/Sub](https://cloud.google.com/pubsub).\n+\n+Supported data formats:\n+- Serializable plaintext formats, such as JSON\n+- [PubSubMessage](https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage).\n+\n+Supported input source configurations:\n+- Single or multiple Apache Kafka bootstrap servers\n+- Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection\n+- Secrets vault service [HashiCorp Vault](https://www.vaultproject.io/).\n+\n+Supported destination configuration:\n+- Single Google Pub/Sub topic.\n+\n+In a simple scenario, the template will create an Apache Beam pipeline that will read messages from a source Kafka server with a source topic, and stream the text messages into specified Pub/Sub destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed over plain text or SSL encrypted connection. The template supports using a single Kafka user account to authenticate in the provided source Kafka servers and topics. To support SASL authenticaton over SSL the template will need an SSL certificate location and access to a secrets vault service with Kafka username and password, currently supporting HashiCorp Vault.\n+\n+## Requirements\n+\n+- Java 11\n+- Kafka Bootstrap Server(s) up and running\n+- Existing source Kafka topic(s)\n+- An existing Pub/Sub destination output topic\n+- (Optional) An existing HashiCorp Vault\n+- (Optional) A configured secure SSL connection for Kafka\n+\n+## Getting Started\n+\n+This section describes what is needed to get the template up and running.\n+- Assembling the Uber-JAR\n+- Local execution\n+- Google Dataflow Template\n+  - Set up the environment\n+  - Creating the Dataflow Flex Template\n+  - Create a Dataflow job to ingest data using the template.\n+- Avro format transferring.\n+\n+## Assembling the Uber-JAR\n+\n+To run this template the template Java project should be built into\n+an Uber JAR file.\n+\n+Navigate to the Beam folder:\n+\n+```\n+cd /path/to/beam\n+```\n+\n+In order to create Uber JAR with Gradle, [Shadow plugin](https://github.com/johnrengelman/shadow)\n+is used. It creates the `shadowJar` task that builds the Uber JAR:\n+\n+```\n+./gradlew -p examples/templates/java/kafka-to-pubsub clean shadowJar\n+```\n+\n+\u2139\ufe0f An **Uber JAR** - also known as **fat JAR** - is a single JAR file that contains\n+both target package *and* all its dependencies.\n+\n+The result of the `shadowJar` task execution is a `.jar` file that is generated\n+under the `build/libs/` folder in kafka-to-pubsub directory.\n+\n+## Local execution\n+To execute this pipeline locally, specify the parameters:\n+- Kafka Bootstrap servers\n+- Kafka input topics\n+- Pub/Sub output topic\n+in the following format:\n+```bash\n+--bootstrapServers=host:port \\\n+--inputTopics=your-input-topic \\\n+--outputTopic=projects/your-project-id/topics/your-topic-pame\n+```\n+Optionally, to retrieve Kafka credentials for SASL/SCRAM,\n+specify a URL to the credentials in HashiCorp Vault and the vault access token:\n+```bash\n+--secretStoreUrl=http(s)://host:port/path/to/credentials\n+--vaultToken=your-token\n+```\n+Optionally, to configure secure SSL connection between the Beam pipeline and Kafka,\n+specify the parameters:\n+- A local path to a truststore file\n+- A local path to a keystore file", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 105}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODcxNTk3Mg==", "bodyText": "Yes, you're absolutely right, we've made this SSL configuration just as the example for local execution. We updated the code and gave a possibility to store a keystore in GCS and download from it at execution time. If the path starts with gs://, then we try to retrieve from GCS, and from the local file otherwise.", "url": "https://github.com/apache/beam/pull/13112#discussion_r528715972", "createdAt": "2020-11-23T13:50:17Z", "author": {"login": "KhaninArtur"}, "path": "examples/templates/java/README.md", "diffHunk": "@@ -0,0 +1,252 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam Template to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) Template that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Pub/Sub](https://cloud.google.com/pubsub).\n+\n+Supported data formats:\n+- Serializable plaintext formats, such as JSON\n+- [PubSubMessage](https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage).\n+\n+Supported input source configurations:\n+- Single or multiple Apache Kafka bootstrap servers\n+- Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection\n+- Secrets vault service [HashiCorp Vault](https://www.vaultproject.io/).\n+\n+Supported destination configuration:\n+- Single Google Pub/Sub topic.\n+\n+In a simple scenario, the template will create an Apache Beam pipeline that will read messages from a source Kafka server with a source topic, and stream the text messages into specified Pub/Sub destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed over plain text or SSL encrypted connection. The template supports using a single Kafka user account to authenticate in the provided source Kafka servers and topics. To support SASL authenticaton over SSL the template will need an SSL certificate location and access to a secrets vault service with Kafka username and password, currently supporting HashiCorp Vault.\n+\n+## Requirements\n+\n+- Java 11\n+- Kafka Bootstrap Server(s) up and running\n+- Existing source Kafka topic(s)\n+- An existing Pub/Sub destination output topic\n+- (Optional) An existing HashiCorp Vault\n+- (Optional) A configured secure SSL connection for Kafka\n+\n+## Getting Started\n+\n+This section describes what is needed to get the template up and running.\n+- Assembling the Uber-JAR\n+- Local execution\n+- Google Dataflow Template\n+  - Set up the environment\n+  - Creating the Dataflow Flex Template\n+  - Create a Dataflow job to ingest data using the template.\n+- Avro format transferring.\n+\n+## Assembling the Uber-JAR\n+\n+To run this template the template Java project should be built into\n+an Uber JAR file.\n+\n+Navigate to the Beam folder:\n+\n+```\n+cd /path/to/beam\n+```\n+\n+In order to create Uber JAR with Gradle, [Shadow plugin](https://github.com/johnrengelman/shadow)\n+is used. It creates the `shadowJar` task that builds the Uber JAR:\n+\n+```\n+./gradlew -p examples/templates/java/kafka-to-pubsub clean shadowJar\n+```\n+\n+\u2139\ufe0f An **Uber JAR** - also known as **fat JAR** - is a single JAR file that contains\n+both target package *and* all its dependencies.\n+\n+The result of the `shadowJar` task execution is a `.jar` file that is generated\n+under the `build/libs/` folder in kafka-to-pubsub directory.\n+\n+## Local execution\n+To execute this pipeline locally, specify the parameters:\n+- Kafka Bootstrap servers\n+- Kafka input topics\n+- Pub/Sub output topic\n+in the following format:\n+```bash\n+--bootstrapServers=host:port \\\n+--inputTopics=your-input-topic \\\n+--outputTopic=projects/your-project-id/topics/your-topic-pame\n+```\n+Optionally, to retrieve Kafka credentials for SASL/SCRAM,\n+specify a URL to the credentials in HashiCorp Vault and the vault access token:\n+```bash\n+--secretStoreUrl=http(s)://host:port/path/to/credentials\n+--vaultToken=your-token\n+```\n+Optionally, to configure secure SSL connection between the Beam pipeline and Kafka,\n+specify the parameters:\n+- A local path to a truststore file\n+- A local path to a keystore file", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYxMTk5NQ=="}, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 105}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5NTYxODI2OnYy", "diffSide": "RIGHT", "path": "examples/templates/java/README.md", "isResolved": false, "comments": {"totalCount": 4, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQwMDozOTo1OVrOH1RPqw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNlQyMzozNDowMVrOH6pIng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYxOTExNQ==", "bodyText": "I'm not sure I understand the purpose of this, won't this just end up re-serializing to the same byte array? And in that case couldn't we just forward the value byte array directly instead?\nMaybe I'm missing something, could you clarify?", "url": "https://github.com/apache/beam/pull/13112#discussion_r525619115", "createdAt": "2020-11-18T00:39:59Z", "author": {"login": "TheNeuralBit"}, "path": "examples/templates/java/README.md", "diffHunk": "@@ -0,0 +1,252 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam Template to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) Template that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Pub/Sub](https://cloud.google.com/pubsub).\n+\n+Supported data formats:\n+- Serializable plaintext formats, such as JSON\n+- [PubSubMessage](https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage).\n+\n+Supported input source configurations:\n+- Single or multiple Apache Kafka bootstrap servers\n+- Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection\n+- Secrets vault service [HashiCorp Vault](https://www.vaultproject.io/).\n+\n+Supported destination configuration:\n+- Single Google Pub/Sub topic.\n+\n+In a simple scenario, the template will create an Apache Beam pipeline that will read messages from a source Kafka server with a source topic, and stream the text messages into specified Pub/Sub destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed over plain text or SSL encrypted connection. The template supports using a single Kafka user account to authenticate in the provided source Kafka servers and topics. To support SASL authenticaton over SSL the template will need an SSL certificate location and access to a secrets vault service with Kafka username and password, currently supporting HashiCorp Vault.\n+\n+## Requirements\n+\n+- Java 11\n+- Kafka Bootstrap Server(s) up and running\n+- Existing source Kafka topic(s)\n+- An existing Pub/Sub destination output topic\n+- (Optional) An existing HashiCorp Vault\n+- (Optional) A configured secure SSL connection for Kafka\n+\n+## Getting Started\n+\n+This section describes what is needed to get the template up and running.\n+- Assembling the Uber-JAR\n+- Local execution\n+- Google Dataflow Template\n+  - Set up the environment\n+  - Creating the Dataflow Flex Template\n+  - Create a Dataflow job to ingest data using the template.\n+- Avro format transferring.\n+\n+## Assembling the Uber-JAR\n+\n+To run this template the template Java project should be built into\n+an Uber JAR file.\n+\n+Navigate to the Beam folder:\n+\n+```\n+cd /path/to/beam\n+```\n+\n+In order to create Uber JAR with Gradle, [Shadow plugin](https://github.com/johnrengelman/shadow)\n+is used. It creates the `shadowJar` task that builds the Uber JAR:\n+\n+```\n+./gradlew -p examples/templates/java/kafka-to-pubsub clean shadowJar\n+```\n+\n+\u2139\ufe0f An **Uber JAR** - also known as **fat JAR** - is a single JAR file that contains\n+both target package *and* all its dependencies.\n+\n+The result of the `shadowJar` task execution is a `.jar` file that is generated\n+under the `build/libs/` folder in kafka-to-pubsub directory.\n+\n+## Local execution\n+To execute this pipeline locally, specify the parameters:\n+- Kafka Bootstrap servers\n+- Kafka input topics\n+- Pub/Sub output topic\n+in the following format:\n+```bash\n+--bootstrapServers=host:port \\\n+--inputTopics=your-input-topic \\\n+--outputTopic=projects/your-project-id/topics/your-topic-pame\n+```\n+Optionally, to retrieve Kafka credentials for SASL/SCRAM,\n+specify a URL to the credentials in HashiCorp Vault and the vault access token:\n+```bash\n+--secretStoreUrl=http(s)://host:port/path/to/credentials\n+--vaultToken=your-token\n+```\n+Optionally, to configure secure SSL connection between the Beam pipeline and Kafka,\n+specify the parameters:\n+- A local path to a truststore file\n+- A local path to a keystore file\n+- Truststore password\n+- Keystore password\n+- Key password\n+```bash\n+--truststorePath=path/to/kafka.truststore.jks\n+--keystorePath=path/to/kafka.keystore.jks\n+--truststorePassword=your-truststore-password\n+--keystorePassword=your-keystore-password\n+--keyPassword=your-key-password\n+```\n+To change the runner, specify:\n+```bash\n+--runner=YOUR_SELECTED_RUNNER\n+```\n+See examples/java/README.md for steps and examples to configure different runners.\n+\n+## Google Dataflow Template\n+\n+### Setting Up Project Environment\n+\n+#### Pipeline variables:\n+\n+```\n+PROJECT=id-of-my-project\n+BUCKET_NAME=my-bucket\n+REGION=my-region\n+```\n+\n+#### Template Metadata Storage Bucket Creation\n+\n+The Dataflow Flex template has to store its metadata in a bucket in\n+[Google Cloud Storage](https://cloud.google.com/storage), so it can be executed from the Google Cloud Platform.\n+Create the bucket in Google Cloud Storage if it doesn't exist yet:\n+\n+```\n+gsutil mb gs://${BUCKET_NAME}\n+```\n+\n+#### Containerization variables:\n+\n+```\n+IMAGE_NAME=my-image-name\n+TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+BASE_CONTAINER_IMAGE=my-base-container-image\n+TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+```\n+\n+### Creating the Dataflow Flex Template\n+\n+Dataflow Flex Templates package the pipeline as a Docker image and stage these images\n+on your project's [Container Registry](https://cloud.google.com/container-registry).\n+\n+To execute the template you need to create the template spec file containing all\n+the necessary information to run the job. This template already has the following\n+[metadata file](kafka-to-pubsub/src/main/resources/kafka_to_pubsub_metadata.json) in resources.\n+\n+Navigate to the template folder:\n+\n+```\n+cd /path/to/beam/examples/templates/java/kafka-to-pubsub\n+```\n+\n+Build the Dataflow Flex Template:\n+\n+```\n+gcloud dataflow flex-template build ${TEMPLATE_PATH} \\\n+       --image-gcr-path ${TARGET_GCR_IMAGE} \\\n+       --sdk-language \"JAVA\" \\\n+       --flex-template-base-image ${BASE_CONTAINER_IMAGE} \\\n+       --metadata-file \"src/main/resources/kafka_to_pubsub_metadata.json\" \\\n+       --jar \"build/libs/beam-examples-templates-java-kafka-to-pubsub-2.25.0-SNAPSHOT-all.jar\" \\\n+       --env FLEX_TEMPLATE_JAVA_MAIN_CLASS=\"org.apache.beam.templates.KafkaToPubsub\"\n+```\n+\n+### Create Dataflow Job Using the Apache Kafka to Google Pub/Sub Dataflow Flex Template\n+\n+To deploy the pipeline, you should refer to the template file and pass the\n+[parameters](https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-dataflow-pipeline-options)\n+required by the pipeline.\n+\n+You can do this in 3 different ways:\n+1. Using [Dataflow Google Cloud Console](https://console.cloud.google.com/dataflow/jobs)\n+\n+2. Using `gcloud` CLI tool\n+    ```\n+    gcloud dataflow flex-template run \"kafka-to-pubsub-`date +%Y%m%d-%H%M%S`\" \\\n+        --template-file-gcs-location \"${TEMPLATE_PATH}\" \\\n+        --parameters bootstrapServers=\"broker_1:9092,broker_2:9092\" \\\n+        --parameters inputTopics=\"topic1,topic2\" \\\n+        --parameters outputTopic=\"projects/${PROJECT}/topics/your-topic-name\" \\\n+        --parameters outputFormat=\"PLAINTEXT\" \\\n+        --parameters secretStoreUrl=\"http(s)://host:port/path/to/credentials\" \\\n+        --parameters vaultToken=\"your-token\" \\\n+        --region \"${REGION}\"\n+    ```\n+3. With a REST API request\n+    ```\n+    API_ROOT_URL=\"https://dataflow.googleapis.com\"\n+    TEMPLATES_LAUNCH_API=\"${API_ROOT_URL}/v1b3/projects/${PROJECT}/locations/${REGION}/flexTemplates:launch\"\n+    JOB_NAME=\"kafka-to-pubsub-`date +%Y%m%d-%H%M%S-%N`\"\n+\n+    time curl -X POST -H \"Content-Type: application/json\" \\\n+        -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n+        -d '\n+         {\n+             \"launch_parameter\": {\n+                 \"jobName\": \"'$JOB_NAME'\",\n+                 \"containerSpecGcsPath\": \"'$TEMPLATE_PATH'\",\n+                 \"parameters\": {\n+                     \"bootstrapServers\": \"broker_1:9091, broker_2:9092\",\n+                     \"inputTopics\": \"topic1, topic2\",\n+                     \"outputTopic\": \"projects/'$PROJECT'/topics/your-topic-name\",\n+                     \"outputFormat\": \"PLAINTEXT\",\n+                     \"secretStoreUrl\": \"http(s)://host:port/path/to/credentials\",\n+                     \"vaultToken\": \"your-token\"\n+                 }\n+             }\n+         }\n+        '\n+        \"${TEMPLATES_LAUNCH_API}\"\n+    ```\n+\n+## AVRO format transferring.\n+This template contains an example Class to deserialize AVRO from Kafka and serialize it to AVRO in Pub/Sub.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 229}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODcwODU3Nw==", "bodyText": "Yes, as you mentioned we can read and write bytes without any serialization and deserialization. In our case, we want to give an example of Avro usage for users who will work with this template. It helps easily modify the class to deserialize data and, if it is needed, implement any transformation using this class.\nI updated our example to be more abstract. Now it is more obvious that users need to make changes to our class for working with each specific dataset.", "url": "https://github.com/apache/beam/pull/13112#discussion_r528708577", "createdAt": "2020-11-23T13:38:30Z", "author": {"login": "ilya-kozyrev"}, "path": "examples/templates/java/README.md", "diffHunk": "@@ -0,0 +1,252 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam Template to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) Template that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Pub/Sub](https://cloud.google.com/pubsub).\n+\n+Supported data formats:\n+- Serializable plaintext formats, such as JSON\n+- [PubSubMessage](https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage).\n+\n+Supported input source configurations:\n+- Single or multiple Apache Kafka bootstrap servers\n+- Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection\n+- Secrets vault service [HashiCorp Vault](https://www.vaultproject.io/).\n+\n+Supported destination configuration:\n+- Single Google Pub/Sub topic.\n+\n+In a simple scenario, the template will create an Apache Beam pipeline that will read messages from a source Kafka server with a source topic, and stream the text messages into specified Pub/Sub destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed over plain text or SSL encrypted connection. The template supports using a single Kafka user account to authenticate in the provided source Kafka servers and topics. To support SASL authenticaton over SSL the template will need an SSL certificate location and access to a secrets vault service with Kafka username and password, currently supporting HashiCorp Vault.\n+\n+## Requirements\n+\n+- Java 11\n+- Kafka Bootstrap Server(s) up and running\n+- Existing source Kafka topic(s)\n+- An existing Pub/Sub destination output topic\n+- (Optional) An existing HashiCorp Vault\n+- (Optional) A configured secure SSL connection for Kafka\n+\n+## Getting Started\n+\n+This section describes what is needed to get the template up and running.\n+- Assembling the Uber-JAR\n+- Local execution\n+- Google Dataflow Template\n+  - Set up the environment\n+  - Creating the Dataflow Flex Template\n+  - Create a Dataflow job to ingest data using the template.\n+- Avro format transferring.\n+\n+## Assembling the Uber-JAR\n+\n+To run this template the template Java project should be built into\n+an Uber JAR file.\n+\n+Navigate to the Beam folder:\n+\n+```\n+cd /path/to/beam\n+```\n+\n+In order to create Uber JAR with Gradle, [Shadow plugin](https://github.com/johnrengelman/shadow)\n+is used. It creates the `shadowJar` task that builds the Uber JAR:\n+\n+```\n+./gradlew -p examples/templates/java/kafka-to-pubsub clean shadowJar\n+```\n+\n+\u2139\ufe0f An **Uber JAR** - also known as **fat JAR** - is a single JAR file that contains\n+both target package *and* all its dependencies.\n+\n+The result of the `shadowJar` task execution is a `.jar` file that is generated\n+under the `build/libs/` folder in kafka-to-pubsub directory.\n+\n+## Local execution\n+To execute this pipeline locally, specify the parameters:\n+- Kafka Bootstrap servers\n+- Kafka input topics\n+- Pub/Sub output topic\n+in the following format:\n+```bash\n+--bootstrapServers=host:port \\\n+--inputTopics=your-input-topic \\\n+--outputTopic=projects/your-project-id/topics/your-topic-pame\n+```\n+Optionally, to retrieve Kafka credentials for SASL/SCRAM,\n+specify a URL to the credentials in HashiCorp Vault and the vault access token:\n+```bash\n+--secretStoreUrl=http(s)://host:port/path/to/credentials\n+--vaultToken=your-token\n+```\n+Optionally, to configure secure SSL connection between the Beam pipeline and Kafka,\n+specify the parameters:\n+- A local path to a truststore file\n+- A local path to a keystore file\n+- Truststore password\n+- Keystore password\n+- Key password\n+```bash\n+--truststorePath=path/to/kafka.truststore.jks\n+--keystorePath=path/to/kafka.keystore.jks\n+--truststorePassword=your-truststore-password\n+--keystorePassword=your-keystore-password\n+--keyPassword=your-key-password\n+```\n+To change the runner, specify:\n+```bash\n+--runner=YOUR_SELECTED_RUNNER\n+```\n+See examples/java/README.md for steps and examples to configure different runners.\n+\n+## Google Dataflow Template\n+\n+### Setting Up Project Environment\n+\n+#### Pipeline variables:\n+\n+```\n+PROJECT=id-of-my-project\n+BUCKET_NAME=my-bucket\n+REGION=my-region\n+```\n+\n+#### Template Metadata Storage Bucket Creation\n+\n+The Dataflow Flex template has to store its metadata in a bucket in\n+[Google Cloud Storage](https://cloud.google.com/storage), so it can be executed from the Google Cloud Platform.\n+Create the bucket in Google Cloud Storage if it doesn't exist yet:\n+\n+```\n+gsutil mb gs://${BUCKET_NAME}\n+```\n+\n+#### Containerization variables:\n+\n+```\n+IMAGE_NAME=my-image-name\n+TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+BASE_CONTAINER_IMAGE=my-base-container-image\n+TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+```\n+\n+### Creating the Dataflow Flex Template\n+\n+Dataflow Flex Templates package the pipeline as a Docker image and stage these images\n+on your project's [Container Registry](https://cloud.google.com/container-registry).\n+\n+To execute the template you need to create the template spec file containing all\n+the necessary information to run the job. This template already has the following\n+[metadata file](kafka-to-pubsub/src/main/resources/kafka_to_pubsub_metadata.json) in resources.\n+\n+Navigate to the template folder:\n+\n+```\n+cd /path/to/beam/examples/templates/java/kafka-to-pubsub\n+```\n+\n+Build the Dataflow Flex Template:\n+\n+```\n+gcloud dataflow flex-template build ${TEMPLATE_PATH} \\\n+       --image-gcr-path ${TARGET_GCR_IMAGE} \\\n+       --sdk-language \"JAVA\" \\\n+       --flex-template-base-image ${BASE_CONTAINER_IMAGE} \\\n+       --metadata-file \"src/main/resources/kafka_to_pubsub_metadata.json\" \\\n+       --jar \"build/libs/beam-examples-templates-java-kafka-to-pubsub-2.25.0-SNAPSHOT-all.jar\" \\\n+       --env FLEX_TEMPLATE_JAVA_MAIN_CLASS=\"org.apache.beam.templates.KafkaToPubsub\"\n+```\n+\n+### Create Dataflow Job Using the Apache Kafka to Google Pub/Sub Dataflow Flex Template\n+\n+To deploy the pipeline, you should refer to the template file and pass the\n+[parameters](https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-dataflow-pipeline-options)\n+required by the pipeline.\n+\n+You can do this in 3 different ways:\n+1. Using [Dataflow Google Cloud Console](https://console.cloud.google.com/dataflow/jobs)\n+\n+2. Using `gcloud` CLI tool\n+    ```\n+    gcloud dataflow flex-template run \"kafka-to-pubsub-`date +%Y%m%d-%H%M%S`\" \\\n+        --template-file-gcs-location \"${TEMPLATE_PATH}\" \\\n+        --parameters bootstrapServers=\"broker_1:9092,broker_2:9092\" \\\n+        --parameters inputTopics=\"topic1,topic2\" \\\n+        --parameters outputTopic=\"projects/${PROJECT}/topics/your-topic-name\" \\\n+        --parameters outputFormat=\"PLAINTEXT\" \\\n+        --parameters secretStoreUrl=\"http(s)://host:port/path/to/credentials\" \\\n+        --parameters vaultToken=\"your-token\" \\\n+        --region \"${REGION}\"\n+    ```\n+3. With a REST API request\n+    ```\n+    API_ROOT_URL=\"https://dataflow.googleapis.com\"\n+    TEMPLATES_LAUNCH_API=\"${API_ROOT_URL}/v1b3/projects/${PROJECT}/locations/${REGION}/flexTemplates:launch\"\n+    JOB_NAME=\"kafka-to-pubsub-`date +%Y%m%d-%H%M%S-%N`\"\n+\n+    time curl -X POST -H \"Content-Type: application/json\" \\\n+        -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n+        -d '\n+         {\n+             \"launch_parameter\": {\n+                 \"jobName\": \"'$JOB_NAME'\",\n+                 \"containerSpecGcsPath\": \"'$TEMPLATE_PATH'\",\n+                 \"parameters\": {\n+                     \"bootstrapServers\": \"broker_1:9091, broker_2:9092\",\n+                     \"inputTopics\": \"topic1, topic2\",\n+                     \"outputTopic\": \"projects/'$PROJECT'/topics/your-topic-name\",\n+                     \"outputFormat\": \"PLAINTEXT\",\n+                     \"secretStoreUrl\": \"http(s)://host:port/path/to/credentials\",\n+                     \"vaultToken\": \"your-token\"\n+                 }\n+             }\n+         }\n+        '\n+        \"${TEMPLATES_LAUNCH_API}\"\n+    ```\n+\n+## AVRO format transferring.\n+This template contains an example Class to deserialize AVRO from Kafka and serialize it to AVRO in Pub/Sub.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYxOTExNQ=="}, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 229}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDA0ODY3Mg==", "bodyText": "But even with the updated language, the user would just deserialize the byte array to an instance MyAvroClass and then reserialize it back to the same byte array to send to PubSub. If we want to show an example of deserializing and serializing, it should at least do some processing, or convert the message to another format. Otherwise I feel this could just confuse users.", "url": "https://github.com/apache/beam/pull/13112#discussion_r530048672", "createdAt": "2020-11-25T01:25:43Z", "author": {"login": "TheNeuralBit"}, "path": "examples/templates/java/README.md", "diffHunk": "@@ -0,0 +1,252 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam Template to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) Template that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Pub/Sub](https://cloud.google.com/pubsub).\n+\n+Supported data formats:\n+- Serializable plaintext formats, such as JSON\n+- [PubSubMessage](https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage).\n+\n+Supported input source configurations:\n+- Single or multiple Apache Kafka bootstrap servers\n+- Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection\n+- Secrets vault service [HashiCorp Vault](https://www.vaultproject.io/).\n+\n+Supported destination configuration:\n+- Single Google Pub/Sub topic.\n+\n+In a simple scenario, the template will create an Apache Beam pipeline that will read messages from a source Kafka server with a source topic, and stream the text messages into specified Pub/Sub destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed over plain text or SSL encrypted connection. The template supports using a single Kafka user account to authenticate in the provided source Kafka servers and topics. To support SASL authenticaton over SSL the template will need an SSL certificate location and access to a secrets vault service with Kafka username and password, currently supporting HashiCorp Vault.\n+\n+## Requirements\n+\n+- Java 11\n+- Kafka Bootstrap Server(s) up and running\n+- Existing source Kafka topic(s)\n+- An existing Pub/Sub destination output topic\n+- (Optional) An existing HashiCorp Vault\n+- (Optional) A configured secure SSL connection for Kafka\n+\n+## Getting Started\n+\n+This section describes what is needed to get the template up and running.\n+- Assembling the Uber-JAR\n+- Local execution\n+- Google Dataflow Template\n+  - Set up the environment\n+  - Creating the Dataflow Flex Template\n+  - Create a Dataflow job to ingest data using the template.\n+- Avro format transferring.\n+\n+## Assembling the Uber-JAR\n+\n+To run this template the template Java project should be built into\n+an Uber JAR file.\n+\n+Navigate to the Beam folder:\n+\n+```\n+cd /path/to/beam\n+```\n+\n+In order to create Uber JAR with Gradle, [Shadow plugin](https://github.com/johnrengelman/shadow)\n+is used. It creates the `shadowJar` task that builds the Uber JAR:\n+\n+```\n+./gradlew -p examples/templates/java/kafka-to-pubsub clean shadowJar\n+```\n+\n+\u2139\ufe0f An **Uber JAR** - also known as **fat JAR** - is a single JAR file that contains\n+both target package *and* all its dependencies.\n+\n+The result of the `shadowJar` task execution is a `.jar` file that is generated\n+under the `build/libs/` folder in kafka-to-pubsub directory.\n+\n+## Local execution\n+To execute this pipeline locally, specify the parameters:\n+- Kafka Bootstrap servers\n+- Kafka input topics\n+- Pub/Sub output topic\n+in the following format:\n+```bash\n+--bootstrapServers=host:port \\\n+--inputTopics=your-input-topic \\\n+--outputTopic=projects/your-project-id/topics/your-topic-pame\n+```\n+Optionally, to retrieve Kafka credentials for SASL/SCRAM,\n+specify a URL to the credentials in HashiCorp Vault and the vault access token:\n+```bash\n+--secretStoreUrl=http(s)://host:port/path/to/credentials\n+--vaultToken=your-token\n+```\n+Optionally, to configure secure SSL connection between the Beam pipeline and Kafka,\n+specify the parameters:\n+- A local path to a truststore file\n+- A local path to a keystore file\n+- Truststore password\n+- Keystore password\n+- Key password\n+```bash\n+--truststorePath=path/to/kafka.truststore.jks\n+--keystorePath=path/to/kafka.keystore.jks\n+--truststorePassword=your-truststore-password\n+--keystorePassword=your-keystore-password\n+--keyPassword=your-key-password\n+```\n+To change the runner, specify:\n+```bash\n+--runner=YOUR_SELECTED_RUNNER\n+```\n+See examples/java/README.md for steps and examples to configure different runners.\n+\n+## Google Dataflow Template\n+\n+### Setting Up Project Environment\n+\n+#### Pipeline variables:\n+\n+```\n+PROJECT=id-of-my-project\n+BUCKET_NAME=my-bucket\n+REGION=my-region\n+```\n+\n+#### Template Metadata Storage Bucket Creation\n+\n+The Dataflow Flex template has to store its metadata in a bucket in\n+[Google Cloud Storage](https://cloud.google.com/storage), so it can be executed from the Google Cloud Platform.\n+Create the bucket in Google Cloud Storage if it doesn't exist yet:\n+\n+```\n+gsutil mb gs://${BUCKET_NAME}\n+```\n+\n+#### Containerization variables:\n+\n+```\n+IMAGE_NAME=my-image-name\n+TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+BASE_CONTAINER_IMAGE=my-base-container-image\n+TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+```\n+\n+### Creating the Dataflow Flex Template\n+\n+Dataflow Flex Templates package the pipeline as a Docker image and stage these images\n+on your project's [Container Registry](https://cloud.google.com/container-registry).\n+\n+To execute the template you need to create the template spec file containing all\n+the necessary information to run the job. This template already has the following\n+[metadata file](kafka-to-pubsub/src/main/resources/kafka_to_pubsub_metadata.json) in resources.\n+\n+Navigate to the template folder:\n+\n+```\n+cd /path/to/beam/examples/templates/java/kafka-to-pubsub\n+```\n+\n+Build the Dataflow Flex Template:\n+\n+```\n+gcloud dataflow flex-template build ${TEMPLATE_PATH} \\\n+       --image-gcr-path ${TARGET_GCR_IMAGE} \\\n+       --sdk-language \"JAVA\" \\\n+       --flex-template-base-image ${BASE_CONTAINER_IMAGE} \\\n+       --metadata-file \"src/main/resources/kafka_to_pubsub_metadata.json\" \\\n+       --jar \"build/libs/beam-examples-templates-java-kafka-to-pubsub-2.25.0-SNAPSHOT-all.jar\" \\\n+       --env FLEX_TEMPLATE_JAVA_MAIN_CLASS=\"org.apache.beam.templates.KafkaToPubsub\"\n+```\n+\n+### Create Dataflow Job Using the Apache Kafka to Google Pub/Sub Dataflow Flex Template\n+\n+To deploy the pipeline, you should refer to the template file and pass the\n+[parameters](https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-dataflow-pipeline-options)\n+required by the pipeline.\n+\n+You can do this in 3 different ways:\n+1. Using [Dataflow Google Cloud Console](https://console.cloud.google.com/dataflow/jobs)\n+\n+2. Using `gcloud` CLI tool\n+    ```\n+    gcloud dataflow flex-template run \"kafka-to-pubsub-`date +%Y%m%d-%H%M%S`\" \\\n+        --template-file-gcs-location \"${TEMPLATE_PATH}\" \\\n+        --parameters bootstrapServers=\"broker_1:9092,broker_2:9092\" \\\n+        --parameters inputTopics=\"topic1,topic2\" \\\n+        --parameters outputTopic=\"projects/${PROJECT}/topics/your-topic-name\" \\\n+        --parameters outputFormat=\"PLAINTEXT\" \\\n+        --parameters secretStoreUrl=\"http(s)://host:port/path/to/credentials\" \\\n+        --parameters vaultToken=\"your-token\" \\\n+        --region \"${REGION}\"\n+    ```\n+3. With a REST API request\n+    ```\n+    API_ROOT_URL=\"https://dataflow.googleapis.com\"\n+    TEMPLATES_LAUNCH_API=\"${API_ROOT_URL}/v1b3/projects/${PROJECT}/locations/${REGION}/flexTemplates:launch\"\n+    JOB_NAME=\"kafka-to-pubsub-`date +%Y%m%d-%H%M%S-%N`\"\n+\n+    time curl -X POST -H \"Content-Type: application/json\" \\\n+        -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n+        -d '\n+         {\n+             \"launch_parameter\": {\n+                 \"jobName\": \"'$JOB_NAME'\",\n+                 \"containerSpecGcsPath\": \"'$TEMPLATE_PATH'\",\n+                 \"parameters\": {\n+                     \"bootstrapServers\": \"broker_1:9091, broker_2:9092\",\n+                     \"inputTopics\": \"topic1, topic2\",\n+                     \"outputTopic\": \"projects/'$PROJECT'/topics/your-topic-name\",\n+                     \"outputFormat\": \"PLAINTEXT\",\n+                     \"secretStoreUrl\": \"http(s)://host:port/path/to/credentials\",\n+                     \"vaultToken\": \"your-token\"\n+                 }\n+             }\n+         }\n+        '\n+        \"${TEMPLATES_LAUNCH_API}\"\n+    ```\n+\n+## AVRO format transferring.\n+This template contains an example Class to deserialize AVRO from Kafka and serialize it to AVRO in Pub/Sub.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYxOTExNQ=="}, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 229}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTI1MzQwNg==", "bodyText": "I understand where the frustration for someone looking for an example might come from. I included clarity in the readme where a user would plug in their data transforms, calling out those steps with [OPTIONAL TO IMPLEMENT] and providing a link to Beam transforms that go into details of transforms.", "url": "https://github.com/apache/beam/pull/13112#discussion_r531253406", "createdAt": "2020-11-26T23:34:01Z", "author": {"login": "ilya-kozyrev"}, "path": "examples/templates/java/README.md", "diffHunk": "@@ -0,0 +1,252 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam Template to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) Template that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Pub/Sub](https://cloud.google.com/pubsub).\n+\n+Supported data formats:\n+- Serializable plaintext formats, such as JSON\n+- [PubSubMessage](https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage).\n+\n+Supported input source configurations:\n+- Single or multiple Apache Kafka bootstrap servers\n+- Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection\n+- Secrets vault service [HashiCorp Vault](https://www.vaultproject.io/).\n+\n+Supported destination configuration:\n+- Single Google Pub/Sub topic.\n+\n+In a simple scenario, the template will create an Apache Beam pipeline that will read messages from a source Kafka server with a source topic, and stream the text messages into specified Pub/Sub destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed over plain text or SSL encrypted connection. The template supports using a single Kafka user account to authenticate in the provided source Kafka servers and topics. To support SASL authenticaton over SSL the template will need an SSL certificate location and access to a secrets vault service with Kafka username and password, currently supporting HashiCorp Vault.\n+\n+## Requirements\n+\n+- Java 11\n+- Kafka Bootstrap Server(s) up and running\n+- Existing source Kafka topic(s)\n+- An existing Pub/Sub destination output topic\n+- (Optional) An existing HashiCorp Vault\n+- (Optional) A configured secure SSL connection for Kafka\n+\n+## Getting Started\n+\n+This section describes what is needed to get the template up and running.\n+- Assembling the Uber-JAR\n+- Local execution\n+- Google Dataflow Template\n+  - Set up the environment\n+  - Creating the Dataflow Flex Template\n+  - Create a Dataflow job to ingest data using the template.\n+- Avro format transferring.\n+\n+## Assembling the Uber-JAR\n+\n+To run this template the template Java project should be built into\n+an Uber JAR file.\n+\n+Navigate to the Beam folder:\n+\n+```\n+cd /path/to/beam\n+```\n+\n+In order to create Uber JAR with Gradle, [Shadow plugin](https://github.com/johnrengelman/shadow)\n+is used. It creates the `shadowJar` task that builds the Uber JAR:\n+\n+```\n+./gradlew -p examples/templates/java/kafka-to-pubsub clean shadowJar\n+```\n+\n+\u2139\ufe0f An **Uber JAR** - also known as **fat JAR** - is a single JAR file that contains\n+both target package *and* all its dependencies.\n+\n+The result of the `shadowJar` task execution is a `.jar` file that is generated\n+under the `build/libs/` folder in kafka-to-pubsub directory.\n+\n+## Local execution\n+To execute this pipeline locally, specify the parameters:\n+- Kafka Bootstrap servers\n+- Kafka input topics\n+- Pub/Sub output topic\n+in the following format:\n+```bash\n+--bootstrapServers=host:port \\\n+--inputTopics=your-input-topic \\\n+--outputTopic=projects/your-project-id/topics/your-topic-pame\n+```\n+Optionally, to retrieve Kafka credentials for SASL/SCRAM,\n+specify a URL to the credentials in HashiCorp Vault and the vault access token:\n+```bash\n+--secretStoreUrl=http(s)://host:port/path/to/credentials\n+--vaultToken=your-token\n+```\n+Optionally, to configure secure SSL connection between the Beam pipeline and Kafka,\n+specify the parameters:\n+- A local path to a truststore file\n+- A local path to a keystore file\n+- Truststore password\n+- Keystore password\n+- Key password\n+```bash\n+--truststorePath=path/to/kafka.truststore.jks\n+--keystorePath=path/to/kafka.keystore.jks\n+--truststorePassword=your-truststore-password\n+--keystorePassword=your-keystore-password\n+--keyPassword=your-key-password\n+```\n+To change the runner, specify:\n+```bash\n+--runner=YOUR_SELECTED_RUNNER\n+```\n+See examples/java/README.md for steps and examples to configure different runners.\n+\n+## Google Dataflow Template\n+\n+### Setting Up Project Environment\n+\n+#### Pipeline variables:\n+\n+```\n+PROJECT=id-of-my-project\n+BUCKET_NAME=my-bucket\n+REGION=my-region\n+```\n+\n+#### Template Metadata Storage Bucket Creation\n+\n+The Dataflow Flex template has to store its metadata in a bucket in\n+[Google Cloud Storage](https://cloud.google.com/storage), so it can be executed from the Google Cloud Platform.\n+Create the bucket in Google Cloud Storage if it doesn't exist yet:\n+\n+```\n+gsutil mb gs://${BUCKET_NAME}\n+```\n+\n+#### Containerization variables:\n+\n+```\n+IMAGE_NAME=my-image-name\n+TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+BASE_CONTAINER_IMAGE=my-base-container-image\n+TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+```\n+\n+### Creating the Dataflow Flex Template\n+\n+Dataflow Flex Templates package the pipeline as a Docker image and stage these images\n+on your project's [Container Registry](https://cloud.google.com/container-registry).\n+\n+To execute the template you need to create the template spec file containing all\n+the necessary information to run the job. This template already has the following\n+[metadata file](kafka-to-pubsub/src/main/resources/kafka_to_pubsub_metadata.json) in resources.\n+\n+Navigate to the template folder:\n+\n+```\n+cd /path/to/beam/examples/templates/java/kafka-to-pubsub\n+```\n+\n+Build the Dataflow Flex Template:\n+\n+```\n+gcloud dataflow flex-template build ${TEMPLATE_PATH} \\\n+       --image-gcr-path ${TARGET_GCR_IMAGE} \\\n+       --sdk-language \"JAVA\" \\\n+       --flex-template-base-image ${BASE_CONTAINER_IMAGE} \\\n+       --metadata-file \"src/main/resources/kafka_to_pubsub_metadata.json\" \\\n+       --jar \"build/libs/beam-examples-templates-java-kafka-to-pubsub-2.25.0-SNAPSHOT-all.jar\" \\\n+       --env FLEX_TEMPLATE_JAVA_MAIN_CLASS=\"org.apache.beam.templates.KafkaToPubsub\"\n+```\n+\n+### Create Dataflow Job Using the Apache Kafka to Google Pub/Sub Dataflow Flex Template\n+\n+To deploy the pipeline, you should refer to the template file and pass the\n+[parameters](https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-dataflow-pipeline-options)\n+required by the pipeline.\n+\n+You can do this in 3 different ways:\n+1. Using [Dataflow Google Cloud Console](https://console.cloud.google.com/dataflow/jobs)\n+\n+2. Using `gcloud` CLI tool\n+    ```\n+    gcloud dataflow flex-template run \"kafka-to-pubsub-`date +%Y%m%d-%H%M%S`\" \\\n+        --template-file-gcs-location \"${TEMPLATE_PATH}\" \\\n+        --parameters bootstrapServers=\"broker_1:9092,broker_2:9092\" \\\n+        --parameters inputTopics=\"topic1,topic2\" \\\n+        --parameters outputTopic=\"projects/${PROJECT}/topics/your-topic-name\" \\\n+        --parameters outputFormat=\"PLAINTEXT\" \\\n+        --parameters secretStoreUrl=\"http(s)://host:port/path/to/credentials\" \\\n+        --parameters vaultToken=\"your-token\" \\\n+        --region \"${REGION}\"\n+    ```\n+3. With a REST API request\n+    ```\n+    API_ROOT_URL=\"https://dataflow.googleapis.com\"\n+    TEMPLATES_LAUNCH_API=\"${API_ROOT_URL}/v1b3/projects/${PROJECT}/locations/${REGION}/flexTemplates:launch\"\n+    JOB_NAME=\"kafka-to-pubsub-`date +%Y%m%d-%H%M%S-%N`\"\n+\n+    time curl -X POST -H \"Content-Type: application/json\" \\\n+        -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n+        -d '\n+         {\n+             \"launch_parameter\": {\n+                 \"jobName\": \"'$JOB_NAME'\",\n+                 \"containerSpecGcsPath\": \"'$TEMPLATE_PATH'\",\n+                 \"parameters\": {\n+                     \"bootstrapServers\": \"broker_1:9091, broker_2:9092\",\n+                     \"inputTopics\": \"topic1, topic2\",\n+                     \"outputTopic\": \"projects/'$PROJECT'/topics/your-topic-name\",\n+                     \"outputFormat\": \"PLAINTEXT\",\n+                     \"secretStoreUrl\": \"http(s)://host:port/path/to/credentials\",\n+                     \"vaultToken\": \"your-token\"\n+                 }\n+             }\n+         }\n+        '\n+        \"${TEMPLATES_LAUNCH_API}\"\n+    ```\n+\n+## AVRO format transferring.\n+This template contains an example Class to deserialize AVRO from Kafka and serialize it to AVRO in Pub/Sub.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYxOTExNQ=="}, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 229}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5NTYyMzMwOnYy", "diffSide": "RIGHT", "path": "examples/templates/java/README.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQwMDo0MjowMFrOH1RShw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQxNToyNjoxOFrOH3UTkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYxOTg0Nw==", "bodyText": "nit: this shouldn't specify a specific version\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                   --jar \"build/libs/beam-examples-templates-java-kafka-to-pubsub-2.25.0-SNAPSHOT-all.jar\" \\\n          \n          \n            \n                   --jar \"build/libs/beam-examples-templates-java-kafka-to-pubsub-<version>-all.jar\" \\", "url": "https://github.com/apache/beam/pull/13112#discussion_r525619847", "createdAt": "2020-11-18T00:42:00Z", "author": {"login": "TheNeuralBit"}, "path": "examples/templates/java/README.md", "diffHunk": "@@ -0,0 +1,252 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam Template to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) Template that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Pub/Sub](https://cloud.google.com/pubsub).\n+\n+Supported data formats:\n+- Serializable plaintext formats, such as JSON\n+- [PubSubMessage](https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage).\n+\n+Supported input source configurations:\n+- Single or multiple Apache Kafka bootstrap servers\n+- Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection\n+- Secrets vault service [HashiCorp Vault](https://www.vaultproject.io/).\n+\n+Supported destination configuration:\n+- Single Google Pub/Sub topic.\n+\n+In a simple scenario, the template will create an Apache Beam pipeline that will read messages from a source Kafka server with a source topic, and stream the text messages into specified Pub/Sub destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed over plain text or SSL encrypted connection. The template supports using a single Kafka user account to authenticate in the provided source Kafka servers and topics. To support SASL authenticaton over SSL the template will need an SSL certificate location and access to a secrets vault service with Kafka username and password, currently supporting HashiCorp Vault.\n+\n+## Requirements\n+\n+- Java 11\n+- Kafka Bootstrap Server(s) up and running\n+- Existing source Kafka topic(s)\n+- An existing Pub/Sub destination output topic\n+- (Optional) An existing HashiCorp Vault\n+- (Optional) A configured secure SSL connection for Kafka\n+\n+## Getting Started\n+\n+This section describes what is needed to get the template up and running.\n+- Assembling the Uber-JAR\n+- Local execution\n+- Google Dataflow Template\n+  - Set up the environment\n+  - Creating the Dataflow Flex Template\n+  - Create a Dataflow job to ingest data using the template.\n+- Avro format transferring.\n+\n+## Assembling the Uber-JAR\n+\n+To run this template the template Java project should be built into\n+an Uber JAR file.\n+\n+Navigate to the Beam folder:\n+\n+```\n+cd /path/to/beam\n+```\n+\n+In order to create Uber JAR with Gradle, [Shadow plugin](https://github.com/johnrengelman/shadow)\n+is used. It creates the `shadowJar` task that builds the Uber JAR:\n+\n+```\n+./gradlew -p examples/templates/java/kafka-to-pubsub clean shadowJar\n+```\n+\n+\u2139\ufe0f An **Uber JAR** - also known as **fat JAR** - is a single JAR file that contains\n+both target package *and* all its dependencies.\n+\n+The result of the `shadowJar` task execution is a `.jar` file that is generated\n+under the `build/libs/` folder in kafka-to-pubsub directory.\n+\n+## Local execution\n+To execute this pipeline locally, specify the parameters:\n+- Kafka Bootstrap servers\n+- Kafka input topics\n+- Pub/Sub output topic\n+in the following format:\n+```bash\n+--bootstrapServers=host:port \\\n+--inputTopics=your-input-topic \\\n+--outputTopic=projects/your-project-id/topics/your-topic-pame\n+```\n+Optionally, to retrieve Kafka credentials for SASL/SCRAM,\n+specify a URL to the credentials in HashiCorp Vault and the vault access token:\n+```bash\n+--secretStoreUrl=http(s)://host:port/path/to/credentials\n+--vaultToken=your-token\n+```\n+Optionally, to configure secure SSL connection between the Beam pipeline and Kafka,\n+specify the parameters:\n+- A local path to a truststore file\n+- A local path to a keystore file\n+- Truststore password\n+- Keystore password\n+- Key password\n+```bash\n+--truststorePath=path/to/kafka.truststore.jks\n+--keystorePath=path/to/kafka.keystore.jks\n+--truststorePassword=your-truststore-password\n+--keystorePassword=your-keystore-password\n+--keyPassword=your-key-password\n+```\n+To change the runner, specify:\n+```bash\n+--runner=YOUR_SELECTED_RUNNER\n+```\n+See examples/java/README.md for steps and examples to configure different runners.\n+\n+## Google Dataflow Template\n+\n+### Setting Up Project Environment\n+\n+#### Pipeline variables:\n+\n+```\n+PROJECT=id-of-my-project\n+BUCKET_NAME=my-bucket\n+REGION=my-region\n+```\n+\n+#### Template Metadata Storage Bucket Creation\n+\n+The Dataflow Flex template has to store its metadata in a bucket in\n+[Google Cloud Storage](https://cloud.google.com/storage), so it can be executed from the Google Cloud Platform.\n+Create the bucket in Google Cloud Storage if it doesn't exist yet:\n+\n+```\n+gsutil mb gs://${BUCKET_NAME}\n+```\n+\n+#### Containerization variables:\n+\n+```\n+IMAGE_NAME=my-image-name\n+TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+BASE_CONTAINER_IMAGE=my-base-container-image\n+TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+```\n+\n+### Creating the Dataflow Flex Template\n+\n+Dataflow Flex Templates package the pipeline as a Docker image and stage these images\n+on your project's [Container Registry](https://cloud.google.com/container-registry).\n+\n+To execute the template you need to create the template spec file containing all\n+the necessary information to run the job. This template already has the following\n+[metadata file](kafka-to-pubsub/src/main/resources/kafka_to_pubsub_metadata.json) in resources.\n+\n+Navigate to the template folder:\n+\n+```\n+cd /path/to/beam/examples/templates/java/kafka-to-pubsub\n+```\n+\n+Build the Dataflow Flex Template:\n+\n+```\n+gcloud dataflow flex-template build ${TEMPLATE_PATH} \\\n+       --image-gcr-path ${TARGET_GCR_IMAGE} \\\n+       --sdk-language \"JAVA\" \\\n+       --flex-template-base-image ${BASE_CONTAINER_IMAGE} \\\n+       --metadata-file \"src/main/resources/kafka_to_pubsub_metadata.json\" \\\n+       --jar \"build/libs/beam-examples-templates-java-kafka-to-pubsub-2.25.0-SNAPSHOT-all.jar\" \\", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 176}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzc2NjQxOA==", "bodyText": "fixed", "url": "https://github.com/apache/beam/pull/13112#discussion_r527766418", "createdAt": "2020-11-20T15:26:18Z", "author": {"login": "ilya-kozyrev"}, "path": "examples/templates/java/README.md", "diffHunk": "@@ -0,0 +1,252 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam Template to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) Template that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Pub/Sub](https://cloud.google.com/pubsub).\n+\n+Supported data formats:\n+- Serializable plaintext formats, such as JSON\n+- [PubSubMessage](https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage).\n+\n+Supported input source configurations:\n+- Single or multiple Apache Kafka bootstrap servers\n+- Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection\n+- Secrets vault service [HashiCorp Vault](https://www.vaultproject.io/).\n+\n+Supported destination configuration:\n+- Single Google Pub/Sub topic.\n+\n+In a simple scenario, the template will create an Apache Beam pipeline that will read messages from a source Kafka server with a source topic, and stream the text messages into specified Pub/Sub destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed over plain text or SSL encrypted connection. The template supports using a single Kafka user account to authenticate in the provided source Kafka servers and topics. To support SASL authenticaton over SSL the template will need an SSL certificate location and access to a secrets vault service with Kafka username and password, currently supporting HashiCorp Vault.\n+\n+## Requirements\n+\n+- Java 11\n+- Kafka Bootstrap Server(s) up and running\n+- Existing source Kafka topic(s)\n+- An existing Pub/Sub destination output topic\n+- (Optional) An existing HashiCorp Vault\n+- (Optional) A configured secure SSL connection for Kafka\n+\n+## Getting Started\n+\n+This section describes what is needed to get the template up and running.\n+- Assembling the Uber-JAR\n+- Local execution\n+- Google Dataflow Template\n+  - Set up the environment\n+  - Creating the Dataflow Flex Template\n+  - Create a Dataflow job to ingest data using the template.\n+- Avro format transferring.\n+\n+## Assembling the Uber-JAR\n+\n+To run this template the template Java project should be built into\n+an Uber JAR file.\n+\n+Navigate to the Beam folder:\n+\n+```\n+cd /path/to/beam\n+```\n+\n+In order to create Uber JAR with Gradle, [Shadow plugin](https://github.com/johnrengelman/shadow)\n+is used. It creates the `shadowJar` task that builds the Uber JAR:\n+\n+```\n+./gradlew -p examples/templates/java/kafka-to-pubsub clean shadowJar\n+```\n+\n+\u2139\ufe0f An **Uber JAR** - also known as **fat JAR** - is a single JAR file that contains\n+both target package *and* all its dependencies.\n+\n+The result of the `shadowJar` task execution is a `.jar` file that is generated\n+under the `build/libs/` folder in kafka-to-pubsub directory.\n+\n+## Local execution\n+To execute this pipeline locally, specify the parameters:\n+- Kafka Bootstrap servers\n+- Kafka input topics\n+- Pub/Sub output topic\n+in the following format:\n+```bash\n+--bootstrapServers=host:port \\\n+--inputTopics=your-input-topic \\\n+--outputTopic=projects/your-project-id/topics/your-topic-pame\n+```\n+Optionally, to retrieve Kafka credentials for SASL/SCRAM,\n+specify a URL to the credentials in HashiCorp Vault and the vault access token:\n+```bash\n+--secretStoreUrl=http(s)://host:port/path/to/credentials\n+--vaultToken=your-token\n+```\n+Optionally, to configure secure SSL connection between the Beam pipeline and Kafka,\n+specify the parameters:\n+- A local path to a truststore file\n+- A local path to a keystore file\n+- Truststore password\n+- Keystore password\n+- Key password\n+```bash\n+--truststorePath=path/to/kafka.truststore.jks\n+--keystorePath=path/to/kafka.keystore.jks\n+--truststorePassword=your-truststore-password\n+--keystorePassword=your-keystore-password\n+--keyPassword=your-key-password\n+```\n+To change the runner, specify:\n+```bash\n+--runner=YOUR_SELECTED_RUNNER\n+```\n+See examples/java/README.md for steps and examples to configure different runners.\n+\n+## Google Dataflow Template\n+\n+### Setting Up Project Environment\n+\n+#### Pipeline variables:\n+\n+```\n+PROJECT=id-of-my-project\n+BUCKET_NAME=my-bucket\n+REGION=my-region\n+```\n+\n+#### Template Metadata Storage Bucket Creation\n+\n+The Dataflow Flex template has to store its metadata in a bucket in\n+[Google Cloud Storage](https://cloud.google.com/storage), so it can be executed from the Google Cloud Platform.\n+Create the bucket in Google Cloud Storage if it doesn't exist yet:\n+\n+```\n+gsutil mb gs://${BUCKET_NAME}\n+```\n+\n+#### Containerization variables:\n+\n+```\n+IMAGE_NAME=my-image-name\n+TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+BASE_CONTAINER_IMAGE=my-base-container-image\n+TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+```\n+\n+### Creating the Dataflow Flex Template\n+\n+Dataflow Flex Templates package the pipeline as a Docker image and stage these images\n+on your project's [Container Registry](https://cloud.google.com/container-registry).\n+\n+To execute the template you need to create the template spec file containing all\n+the necessary information to run the job. This template already has the following\n+[metadata file](kafka-to-pubsub/src/main/resources/kafka_to_pubsub_metadata.json) in resources.\n+\n+Navigate to the template folder:\n+\n+```\n+cd /path/to/beam/examples/templates/java/kafka-to-pubsub\n+```\n+\n+Build the Dataflow Flex Template:\n+\n+```\n+gcloud dataflow flex-template build ${TEMPLATE_PATH} \\\n+       --image-gcr-path ${TARGET_GCR_IMAGE} \\\n+       --sdk-language \"JAVA\" \\\n+       --flex-template-base-image ${BASE_CONTAINER_IMAGE} \\\n+       --metadata-file \"src/main/resources/kafka_to_pubsub_metadata.json\" \\\n+       --jar \"build/libs/beam-examples-templates-java-kafka-to-pubsub-2.25.0-SNAPSHOT-all.jar\" \\", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYxOTg0Nw=="}, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 176}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5NTYzMDU1OnYy", "diffSide": "RIGHT", "path": "examples/templates/java/README.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQwMDo0NToyMVrOH1RW6Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQxNToyNjowNlrOH3UTCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMDk2OQ==", "bodyText": "nit: Google Pub/Sub -> Google Cloud Pub/Sub\nThere's a couple places where cloud products are referenced as Google X, they should be Google Cloud X, or just X", "url": "https://github.com/apache/beam/pull/13112#discussion_r525620969", "createdAt": "2020-11-18T00:45:21Z", "author": {"login": "TheNeuralBit"}, "path": "examples/templates/java/README.md", "diffHunk": "@@ -0,0 +1,252 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam Template to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) Template that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Pub/Sub](https://cloud.google.com/pubsub).", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 25}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzc2NjI4MA==", "bodyText": "fixed", "url": "https://github.com/apache/beam/pull/13112#discussion_r527766280", "createdAt": "2020-11-20T15:26:06Z", "author": {"login": "ilya-kozyrev"}, "path": "examples/templates/java/README.md", "diffHunk": "@@ -0,0 +1,252 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam Template to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) Template that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Pub/Sub](https://cloud.google.com/pubsub).", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMDk2OQ=="}, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 25}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5NTYzNjY3OnYy", "diffSide": "RIGHT", "path": "examples/templates/java/README.md", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQwMDo0ODowMVrOH1RacQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQxNToyNjowM1rOH3US5A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMTg3Mw==", "bodyText": "Maybe this could point to the GCP docs with more details on launching a flex tempalte from cloud console (if such a page exists)", "url": "https://github.com/apache/beam/pull/13112#discussion_r525621873", "createdAt": "2020-11-18T00:48:01Z", "author": {"login": "TheNeuralBit"}, "path": "examples/templates/java/README.md", "diffHunk": "@@ -0,0 +1,252 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam Template to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) Template that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Pub/Sub](https://cloud.google.com/pubsub).\n+\n+Supported data formats:\n+- Serializable plaintext formats, such as JSON\n+- [PubSubMessage](https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage).\n+\n+Supported input source configurations:\n+- Single or multiple Apache Kafka bootstrap servers\n+- Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection\n+- Secrets vault service [HashiCorp Vault](https://www.vaultproject.io/).\n+\n+Supported destination configuration:\n+- Single Google Pub/Sub topic.\n+\n+In a simple scenario, the template will create an Apache Beam pipeline that will read messages from a source Kafka server with a source topic, and stream the text messages into specified Pub/Sub destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed over plain text or SSL encrypted connection. The template supports using a single Kafka user account to authenticate in the provided source Kafka servers and topics. To support SASL authenticaton over SSL the template will need an SSL certificate location and access to a secrets vault service with Kafka username and password, currently supporting HashiCorp Vault.\n+\n+## Requirements\n+\n+- Java 11\n+- Kafka Bootstrap Server(s) up and running\n+- Existing source Kafka topic(s)\n+- An existing Pub/Sub destination output topic\n+- (Optional) An existing HashiCorp Vault\n+- (Optional) A configured secure SSL connection for Kafka\n+\n+## Getting Started\n+\n+This section describes what is needed to get the template up and running.\n+- Assembling the Uber-JAR\n+- Local execution\n+- Google Dataflow Template\n+  - Set up the environment\n+  - Creating the Dataflow Flex Template\n+  - Create a Dataflow job to ingest data using the template.\n+- Avro format transferring.\n+\n+## Assembling the Uber-JAR\n+\n+To run this template the template Java project should be built into\n+an Uber JAR file.\n+\n+Navigate to the Beam folder:\n+\n+```\n+cd /path/to/beam\n+```\n+\n+In order to create Uber JAR with Gradle, [Shadow plugin](https://github.com/johnrengelman/shadow)\n+is used. It creates the `shadowJar` task that builds the Uber JAR:\n+\n+```\n+./gradlew -p examples/templates/java/kafka-to-pubsub clean shadowJar\n+```\n+\n+\u2139\ufe0f An **Uber JAR** - also known as **fat JAR** - is a single JAR file that contains\n+both target package *and* all its dependencies.\n+\n+The result of the `shadowJar` task execution is a `.jar` file that is generated\n+under the `build/libs/` folder in kafka-to-pubsub directory.\n+\n+## Local execution\n+To execute this pipeline locally, specify the parameters:\n+- Kafka Bootstrap servers\n+- Kafka input topics\n+- Pub/Sub output topic\n+in the following format:\n+```bash\n+--bootstrapServers=host:port \\\n+--inputTopics=your-input-topic \\\n+--outputTopic=projects/your-project-id/topics/your-topic-pame\n+```\n+Optionally, to retrieve Kafka credentials for SASL/SCRAM,\n+specify a URL to the credentials in HashiCorp Vault and the vault access token:\n+```bash\n+--secretStoreUrl=http(s)://host:port/path/to/credentials\n+--vaultToken=your-token\n+```\n+Optionally, to configure secure SSL connection between the Beam pipeline and Kafka,\n+specify the parameters:\n+- A local path to a truststore file\n+- A local path to a keystore file\n+- Truststore password\n+- Keystore password\n+- Key password\n+```bash\n+--truststorePath=path/to/kafka.truststore.jks\n+--keystorePath=path/to/kafka.keystore.jks\n+--truststorePassword=your-truststore-password\n+--keystorePassword=your-keystore-password\n+--keyPassword=your-key-password\n+```\n+To change the runner, specify:\n+```bash\n+--runner=YOUR_SELECTED_RUNNER\n+```\n+See examples/java/README.md for steps and examples to configure different runners.\n+\n+## Google Dataflow Template\n+\n+### Setting Up Project Environment\n+\n+#### Pipeline variables:\n+\n+```\n+PROJECT=id-of-my-project\n+BUCKET_NAME=my-bucket\n+REGION=my-region\n+```\n+\n+#### Template Metadata Storage Bucket Creation\n+\n+The Dataflow Flex template has to store its metadata in a bucket in\n+[Google Cloud Storage](https://cloud.google.com/storage), so it can be executed from the Google Cloud Platform.\n+Create the bucket in Google Cloud Storage if it doesn't exist yet:\n+\n+```\n+gsutil mb gs://${BUCKET_NAME}\n+```\n+\n+#### Containerization variables:\n+\n+```\n+IMAGE_NAME=my-image-name\n+TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+BASE_CONTAINER_IMAGE=my-base-container-image\n+TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+```\n+\n+### Creating the Dataflow Flex Template\n+\n+Dataflow Flex Templates package the pipeline as a Docker image and stage these images\n+on your project's [Container Registry](https://cloud.google.com/container-registry).\n+\n+To execute the template you need to create the template spec file containing all\n+the necessary information to run the job. This template already has the following\n+[metadata file](kafka-to-pubsub/src/main/resources/kafka_to_pubsub_metadata.json) in resources.\n+\n+Navigate to the template folder:\n+\n+```\n+cd /path/to/beam/examples/templates/java/kafka-to-pubsub\n+```\n+\n+Build the Dataflow Flex Template:\n+\n+```\n+gcloud dataflow flex-template build ${TEMPLATE_PATH} \\\n+       --image-gcr-path ${TARGET_GCR_IMAGE} \\\n+       --sdk-language \"JAVA\" \\\n+       --flex-template-base-image ${BASE_CONTAINER_IMAGE} \\\n+       --metadata-file \"src/main/resources/kafka_to_pubsub_metadata.json\" \\\n+       --jar \"build/libs/beam-examples-templates-java-kafka-to-pubsub-2.25.0-SNAPSHOT-all.jar\" \\\n+       --env FLEX_TEMPLATE_JAVA_MAIN_CLASS=\"org.apache.beam.templates.KafkaToPubsub\"\n+```\n+\n+### Create Dataflow Job Using the Apache Kafka to Google Pub/Sub Dataflow Flex Template\n+\n+To deploy the pipeline, you should refer to the template file and pass the\n+[parameters](https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-dataflow-pipeline-options)\n+required by the pipeline.\n+\n+You can do this in 3 different ways:\n+1. Using [Dataflow Google Cloud Console](https://console.cloud.google.com/dataflow/jobs)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 187}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzc2NjI0NA==", "bodyText": "Added link to part of Google Docs related to flex templates", "url": "https://github.com/apache/beam/pull/13112#discussion_r527766244", "createdAt": "2020-11-20T15:26:03Z", "author": {"login": "ilya-kozyrev"}, "path": "examples/templates/java/README.md", "diffHunk": "@@ -0,0 +1,252 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam Template to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) Template that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Pub/Sub](https://cloud.google.com/pubsub).\n+\n+Supported data formats:\n+- Serializable plaintext formats, such as JSON\n+- [PubSubMessage](https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage).\n+\n+Supported input source configurations:\n+- Single or multiple Apache Kafka bootstrap servers\n+- Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection\n+- Secrets vault service [HashiCorp Vault](https://www.vaultproject.io/).\n+\n+Supported destination configuration:\n+- Single Google Pub/Sub topic.\n+\n+In a simple scenario, the template will create an Apache Beam pipeline that will read messages from a source Kafka server with a source topic, and stream the text messages into specified Pub/Sub destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed over plain text or SSL encrypted connection. The template supports using a single Kafka user account to authenticate in the provided source Kafka servers and topics. To support SASL authenticaton over SSL the template will need an SSL certificate location and access to a secrets vault service with Kafka username and password, currently supporting HashiCorp Vault.\n+\n+## Requirements\n+\n+- Java 11\n+- Kafka Bootstrap Server(s) up and running\n+- Existing source Kafka topic(s)\n+- An existing Pub/Sub destination output topic\n+- (Optional) An existing HashiCorp Vault\n+- (Optional) A configured secure SSL connection for Kafka\n+\n+## Getting Started\n+\n+This section describes what is needed to get the template up and running.\n+- Assembling the Uber-JAR\n+- Local execution\n+- Google Dataflow Template\n+  - Set up the environment\n+  - Creating the Dataflow Flex Template\n+  - Create a Dataflow job to ingest data using the template.\n+- Avro format transferring.\n+\n+## Assembling the Uber-JAR\n+\n+To run this template the template Java project should be built into\n+an Uber JAR file.\n+\n+Navigate to the Beam folder:\n+\n+```\n+cd /path/to/beam\n+```\n+\n+In order to create Uber JAR with Gradle, [Shadow plugin](https://github.com/johnrengelman/shadow)\n+is used. It creates the `shadowJar` task that builds the Uber JAR:\n+\n+```\n+./gradlew -p examples/templates/java/kafka-to-pubsub clean shadowJar\n+```\n+\n+\u2139\ufe0f An **Uber JAR** - also known as **fat JAR** - is a single JAR file that contains\n+both target package *and* all its dependencies.\n+\n+The result of the `shadowJar` task execution is a `.jar` file that is generated\n+under the `build/libs/` folder in kafka-to-pubsub directory.\n+\n+## Local execution\n+To execute this pipeline locally, specify the parameters:\n+- Kafka Bootstrap servers\n+- Kafka input topics\n+- Pub/Sub output topic\n+in the following format:\n+```bash\n+--bootstrapServers=host:port \\\n+--inputTopics=your-input-topic \\\n+--outputTopic=projects/your-project-id/topics/your-topic-pame\n+```\n+Optionally, to retrieve Kafka credentials for SASL/SCRAM,\n+specify a URL to the credentials in HashiCorp Vault and the vault access token:\n+```bash\n+--secretStoreUrl=http(s)://host:port/path/to/credentials\n+--vaultToken=your-token\n+```\n+Optionally, to configure secure SSL connection between the Beam pipeline and Kafka,\n+specify the parameters:\n+- A local path to a truststore file\n+- A local path to a keystore file\n+- Truststore password\n+- Keystore password\n+- Key password\n+```bash\n+--truststorePath=path/to/kafka.truststore.jks\n+--keystorePath=path/to/kafka.keystore.jks\n+--truststorePassword=your-truststore-password\n+--keystorePassword=your-keystore-password\n+--keyPassword=your-key-password\n+```\n+To change the runner, specify:\n+```bash\n+--runner=YOUR_SELECTED_RUNNER\n+```\n+See examples/java/README.md for steps and examples to configure different runners.\n+\n+## Google Dataflow Template\n+\n+### Setting Up Project Environment\n+\n+#### Pipeline variables:\n+\n+```\n+PROJECT=id-of-my-project\n+BUCKET_NAME=my-bucket\n+REGION=my-region\n+```\n+\n+#### Template Metadata Storage Bucket Creation\n+\n+The Dataflow Flex template has to store its metadata in a bucket in\n+[Google Cloud Storage](https://cloud.google.com/storage), so it can be executed from the Google Cloud Platform.\n+Create the bucket in Google Cloud Storage if it doesn't exist yet:\n+\n+```\n+gsutil mb gs://${BUCKET_NAME}\n+```\n+\n+#### Containerization variables:\n+\n+```\n+IMAGE_NAME=my-image-name\n+TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+BASE_CONTAINER_IMAGE=my-base-container-image\n+TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+```\n+\n+### Creating the Dataflow Flex Template\n+\n+Dataflow Flex Templates package the pipeline as a Docker image and stage these images\n+on your project's [Container Registry](https://cloud.google.com/container-registry).\n+\n+To execute the template you need to create the template spec file containing all\n+the necessary information to run the job. This template already has the following\n+[metadata file](kafka-to-pubsub/src/main/resources/kafka_to_pubsub_metadata.json) in resources.\n+\n+Navigate to the template folder:\n+\n+```\n+cd /path/to/beam/examples/templates/java/kafka-to-pubsub\n+```\n+\n+Build the Dataflow Flex Template:\n+\n+```\n+gcloud dataflow flex-template build ${TEMPLATE_PATH} \\\n+       --image-gcr-path ${TARGET_GCR_IMAGE} \\\n+       --sdk-language \"JAVA\" \\\n+       --flex-template-base-image ${BASE_CONTAINER_IMAGE} \\\n+       --metadata-file \"src/main/resources/kafka_to_pubsub_metadata.json\" \\\n+       --jar \"build/libs/beam-examples-templates-java-kafka-to-pubsub-2.25.0-SNAPSHOT-all.jar\" \\\n+       --env FLEX_TEMPLATE_JAVA_MAIN_CLASS=\"org.apache.beam.templates.KafkaToPubsub\"\n+```\n+\n+### Create Dataflow Job Using the Apache Kafka to Google Pub/Sub Dataflow Flex Template\n+\n+To deploy the pipeline, you should refer to the template file and pass the\n+[parameters](https://cloud.google.com/dataflow/docs/guides/specifying-exec-params#setting-other-cloud-dataflow-pipeline-options)\n+required by the pipeline.\n+\n+You can do this in 3 different ways:\n+1. Using [Dataflow Google Cloud Console](https://console.cloud.google.com/dataflow/jobs)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMTg3Mw=="}, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 187}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5NTY0MzM3OnYy", "diffSide": "RIGHT", "path": "examples/templates/java/kafka-to-pubsub/build.gradle", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQwMDo1MDo1OVrOH1RegA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QxMjozODowMVrOH4LqQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMjkxMg==", "bodyText": "Please enable the checker and try to fix any nullness issues it detects. If there are confusing/tricky issues you can supress these warnings at the class or function level with @SuppressWarnings(\"nullness\")", "url": "https://github.com/apache/beam/pull/13112#discussion_r525622912", "createdAt": "2020-11-18T00:50:59Z", "author": {"login": "TheNeuralBit"}, "path": "examples/templates/java/kafka-to-pubsub/build.gradle", "diffHunk": "@@ -0,0 +1,53 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * License); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an AS IS BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+plugins {\n+    id 'java'\n+    id 'org.apache.beam.module'\n+    id 'com.github.johnrengelman.shadow'\n+}\n+applyJavaNature(\n+        exportJavadoc: false,\n+        enableChecker: false,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 26}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODY3MzM0NQ==", "bodyText": "fixed", "url": "https://github.com/apache/beam/pull/13112#discussion_r528673345", "createdAt": "2020-11-23T12:38:01Z", "author": {"login": "ilya-kozyrev"}, "path": "examples/templates/java/kafka-to-pubsub/build.gradle", "diffHunk": "@@ -0,0 +1,53 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * License); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an AS IS BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+plugins {\n+    id 'java'\n+    id 'org.apache.beam.module'\n+    id 'com.github.johnrengelman.shadow'\n+}\n+applyJavaNature(\n+        exportJavadoc: false,\n+        enableChecker: false,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMjkxMg=="}, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5NTY0NDMwOnYy", "diffSide": "RIGHT", "path": "examples/templates/java/build.gradle", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQwMDo1MToyN1rOH1RfFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yMFQxNToxNToyN1rOH3T3FA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMzA2MA==", "bodyText": "Is this file necessary?", "url": "https://github.com/apache/beam/pull/13112#discussion_r525623060", "createdAt": "2020-11-18T00:51:27Z", "author": {"login": "TheNeuralBit"}, "path": "examples/templates/java/build.gradle", "diffHunk": "@@ -0,0 +1,31 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * License); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an AS IS BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+plugins {\n+    id 'java'\n+}\n+\n+version '2.25.0-SNAPSHOT'\n+\n+repositories {\n+    mavenCentral()\n+}\n+\n+dependencies {\n+    testCompile group: 'junit', name: 'junit', version: '4.12'\n+}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 31}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNzc1OTEyNA==", "bodyText": "removed", "url": "https://github.com/apache/beam/pull/13112#discussion_r527759124", "createdAt": "2020-11-20T15:15:27Z", "author": {"login": "ilya-kozyrev"}, "path": "examples/templates/java/build.gradle", "diffHunk": "@@ -0,0 +1,31 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * License); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an AS IS BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+\n+plugins {\n+    id 'java'\n+}\n+\n+version '2.25.0-SNAPSHOT'\n+\n+repositories {\n+    mavenCentral()\n+}\n+\n+dependencies {\n+    testCompile group: 'junit', name: 'junit', version: '4.12'\n+}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMzA2MA=="}, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 31}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzI5NTY0Nzk5OnYy", "diffSide": "RIGHT", "path": "examples/templates/java/kafka-to-pubsub/src/main/java/org/apache/beam/templates/KafkaToPubsub.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0xOFQwMDo1MzowNFrOH1RhOQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QxNDowMTowMlrOH4OrWA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMzYwOQ==", "bodyText": "This looks like a dupe of the README, could the javadoc just refer to that instead?", "url": "https://github.com/apache/beam/pull/13112#discussion_r525623609", "createdAt": "2020-11-18T00:53:04Z", "author": {"login": "TheNeuralBit"}, "path": "examples/templates/java/kafka-to-pubsub/src/main/java/org/apache/beam/templates/KafkaToPubsub.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.gson.JsonObject;\n+import com.google.gson.JsonParser;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult;\n+import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.Values;\n+import org.apache.beam.templates.avro.TaxiRide;\n+import org.apache.beam.templates.options.KafkaToPubsubOptions;\n+import org.apache.beam.templates.transforms.FormatTransform;\n+import org.apache.http.HttpResponse;\n+import org.apache.http.client.HttpClient;\n+import org.apache.http.client.methods.HttpGet;\n+import org.apache.http.impl.client.HttpClientBuilder;\n+import org.apache.http.util.EntityUtils;\n+import org.apache.kafka.common.config.SaslConfigs;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.common.security.scram.ScramMechanism;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The {@link KafkaToPubsub} pipeline is a streaming pipeline which ingests data in JSON format from\n+ * Kafka, and outputs the resulting records to PubSub. Input topics, output topic, Bootstrap servers\n+ * are specified by the user as template parameters. <br>\n+ * Kafka may be configured with SASL/SCRAM security mechanism, in this case a Vault secret storage\n+ * with credentials should be provided. URL to credentials and Vault token are specified by the user\n+ * as template parameters.\n+ *\n+ * <p><b>Pipeline Requirements</b>\n+ *\n+ * <ul>\n+ *   <li>Kafka Bootstrap Server(s).\n+ *   <li>Kafka Topic(s) exists.\n+ *   <li>The PubSub output topic exists.\n+ *   <li>(Optional) An existing HashiCorp Vault secret storage\n+ * </ul>\n+ *\n+ * <p><b>Example Usage</b>\n+ *\n+ * <pre>\n+ * # Set the pipeline vars\n+ * PROJECT=id-of-my-project\n+ * BUCKET_NAME=my-bucket\n+ *\n+ * # Set containerization vars\n+ * IMAGE_NAME=my-image-name\n+ * TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+ * BASE_CONTAINER_IMAGE=my-base-container-image\n+ * TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+ *\n+ * # Create bucket in the cloud storage\n+ * gsutil mb gs://${BUCKET_NAME}\n+ *\n+ * # Go to the beam folder\n+ * cd /path/to/beam\n+ *\n+ * <b>FLEX TEMPLATE</b>\n+ * # Assemble uber-jar\n+ * ./gradlew -p templates/kafka-to-pubsub clean shadowJar\n+ *\n+ * # Go to the template folder\n+ * cd /path/to/beam/templates/kafka-to-pubsub\n+ *\n+ * # Build the flex template\n+ * gcloud dataflow flex-template build ${TEMPLATE_PATH} \\\n+ *       --image-gcr-path \"${TARGET_GCR_IMAGE}\" \\\n+ *       --sdk-language \"JAVA\" \\\n+ *       --flex-template-base-image ${BASE_CONTAINER_IMAGE} \\\n+ *       --metadata-file \"src/main/resources/kafka_to_pubsub_metadata.json\" \\\n+ *       --jar \"build/libs/beam-templates-kafka-to-pubsub-2.25.0-SNAPSHOT-all.jar\" \\\n+ *       --env FLEX_TEMPLATE_JAVA_MAIN_CLASS=\"org.apache.beam.templates.KafkaToPubsub\"\n+ *\n+ * # Execute template:\n+ *    API_ROOT_URL=\"https://dataflow.googleapis.com\"\n+ *    TEMPLATES_LAUNCH_API=\"${API_ROOT_URL}/v1b3/projects/${PROJECT}/locations/${REGION}/flexTemplates:launch\"\n+ *    JOB_NAME=\"kafka-to-pubsub-`date +%Y%m%d-%H%M%S-%N`\"\n+ *\n+ *    time curl -X POST -H \"Content-Type: application/json\" \\\n+ *            -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n+ *            -d '\n+ *             {\n+ *                 \"launch_parameter\": {\n+ *                     \"jobName\": \"'$JOB_NAME'\",\n+ *                     \"containerSpecGcsPath\": \"'$TEMPLATE_PATH'\",\n+ *                     \"parameters\": {\n+ *                         \"bootstrapServers\": \"broker_1:9091, broker_2:9092\",\n+ *                         \"inputTopics\": \"topic1, topic2\",\n+ *                         \"outputTopic\": \"projects/'$PROJECT'/topics/your-topic-name\",\n+ *                         \"secretStoreUrl\": \"http(s)://host:port/path/to/credentials\",\n+ *                         \"vaultToken\": \"your-token\"\n+ *                     }\n+ *                 }\n+ *             }\n+ *            '\n+ *            \"${TEMPLATES_LAUNCH_API}\"\n+ * </pre>\n+ *\n+ * <p><b>Example Avro usage</b>\n+ *\n+ * <pre>\n+ * This template contains an example Class to deserialize AVRO from Kafka and serialize it to AVRO in Pub/Sub.\n+ *\n+ * To use this example in the specific case, follow the few steps:\n+ * <ul>\n+ * <li> Create your own class to describe AVRO schema. As an example use {@link TaxiRide}. Just define necessary fields.\n+ * <li> Create your own Avro Deserializer class. As an example use {@link org.apache.beam.templates.avro.TaxiRidesKafkaAvroDeserializer}. Just rename it, and put your own Schema class as the necessary types.\n+ * <li> Modify the {@link FormatTransform}. Put your Schema class and Deserializer to the related parameter.\n+ * <li> Modify write step in the {@link KafkaToPubsub} by put your Schema class to \"writeAvrosToPubSub\" step.\n+ * </ul>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 138}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODcyMjc3Ng==", "bodyText": "I think README and javadoc have a bit different use-cases. When we work in IDE or text editors javadoc is a good place to look at class or module documentation, but when we work with GitHub, it provides us awesome approaches to render MD files and we can just open the template folder and look at the rendered readme.", "url": "https://github.com/apache/beam/pull/13112#discussion_r528722776", "createdAt": "2020-11-23T14:01:02Z", "author": {"login": "ilya-kozyrev"}, "path": "examples/templates/java/kafka-to-pubsub/src/main/java/org/apache/beam/templates/KafkaToPubsub.java", "diffHunk": "@@ -0,0 +1,319 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.gson.JsonObject;\n+import com.google.gson.JsonParser;\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult;\n+import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.Values;\n+import org.apache.beam.templates.avro.TaxiRide;\n+import org.apache.beam.templates.options.KafkaToPubsubOptions;\n+import org.apache.beam.templates.transforms.FormatTransform;\n+import org.apache.http.HttpResponse;\n+import org.apache.http.client.HttpClient;\n+import org.apache.http.client.methods.HttpGet;\n+import org.apache.http.impl.client.HttpClientBuilder;\n+import org.apache.http.util.EntityUtils;\n+import org.apache.kafka.common.config.SaslConfigs;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.apache.kafka.common.security.scram.ScramMechanism;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The {@link KafkaToPubsub} pipeline is a streaming pipeline which ingests data in JSON format from\n+ * Kafka, and outputs the resulting records to PubSub. Input topics, output topic, Bootstrap servers\n+ * are specified by the user as template parameters. <br>\n+ * Kafka may be configured with SASL/SCRAM security mechanism, in this case a Vault secret storage\n+ * with credentials should be provided. URL to credentials and Vault token are specified by the user\n+ * as template parameters.\n+ *\n+ * <p><b>Pipeline Requirements</b>\n+ *\n+ * <ul>\n+ *   <li>Kafka Bootstrap Server(s).\n+ *   <li>Kafka Topic(s) exists.\n+ *   <li>The PubSub output topic exists.\n+ *   <li>(Optional) An existing HashiCorp Vault secret storage\n+ * </ul>\n+ *\n+ * <p><b>Example Usage</b>\n+ *\n+ * <pre>\n+ * # Set the pipeline vars\n+ * PROJECT=id-of-my-project\n+ * BUCKET_NAME=my-bucket\n+ *\n+ * # Set containerization vars\n+ * IMAGE_NAME=my-image-name\n+ * TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+ * BASE_CONTAINER_IMAGE=my-base-container-image\n+ * TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+ *\n+ * # Create bucket in the cloud storage\n+ * gsutil mb gs://${BUCKET_NAME}\n+ *\n+ * # Go to the beam folder\n+ * cd /path/to/beam\n+ *\n+ * <b>FLEX TEMPLATE</b>\n+ * # Assemble uber-jar\n+ * ./gradlew -p templates/kafka-to-pubsub clean shadowJar\n+ *\n+ * # Go to the template folder\n+ * cd /path/to/beam/templates/kafka-to-pubsub\n+ *\n+ * # Build the flex template\n+ * gcloud dataflow flex-template build ${TEMPLATE_PATH} \\\n+ *       --image-gcr-path \"${TARGET_GCR_IMAGE}\" \\\n+ *       --sdk-language \"JAVA\" \\\n+ *       --flex-template-base-image ${BASE_CONTAINER_IMAGE} \\\n+ *       --metadata-file \"src/main/resources/kafka_to_pubsub_metadata.json\" \\\n+ *       --jar \"build/libs/beam-templates-kafka-to-pubsub-2.25.0-SNAPSHOT-all.jar\" \\\n+ *       --env FLEX_TEMPLATE_JAVA_MAIN_CLASS=\"org.apache.beam.templates.KafkaToPubsub\"\n+ *\n+ * # Execute template:\n+ *    API_ROOT_URL=\"https://dataflow.googleapis.com\"\n+ *    TEMPLATES_LAUNCH_API=\"${API_ROOT_URL}/v1b3/projects/${PROJECT}/locations/${REGION}/flexTemplates:launch\"\n+ *    JOB_NAME=\"kafka-to-pubsub-`date +%Y%m%d-%H%M%S-%N`\"\n+ *\n+ *    time curl -X POST -H \"Content-Type: application/json\" \\\n+ *            -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n+ *            -d '\n+ *             {\n+ *                 \"launch_parameter\": {\n+ *                     \"jobName\": \"'$JOB_NAME'\",\n+ *                     \"containerSpecGcsPath\": \"'$TEMPLATE_PATH'\",\n+ *                     \"parameters\": {\n+ *                         \"bootstrapServers\": \"broker_1:9091, broker_2:9092\",\n+ *                         \"inputTopics\": \"topic1, topic2\",\n+ *                         \"outputTopic\": \"projects/'$PROJECT'/topics/your-topic-name\",\n+ *                         \"secretStoreUrl\": \"http(s)://host:port/path/to/credentials\",\n+ *                         \"vaultToken\": \"your-token\"\n+ *                     }\n+ *                 }\n+ *             }\n+ *            '\n+ *            \"${TEMPLATES_LAUNCH_API}\"\n+ * </pre>\n+ *\n+ * <p><b>Example Avro usage</b>\n+ *\n+ * <pre>\n+ * This template contains an example Class to deserialize AVRO from Kafka and serialize it to AVRO in Pub/Sub.\n+ *\n+ * To use this example in the specific case, follow the few steps:\n+ * <ul>\n+ * <li> Create your own class to describe AVRO schema. As an example use {@link TaxiRide}. Just define necessary fields.\n+ * <li> Create your own Avro Deserializer class. As an example use {@link org.apache.beam.templates.avro.TaxiRidesKafkaAvroDeserializer}. Just rename it, and put your own Schema class as the necessary types.\n+ * <li> Modify the {@link FormatTransform}. Put your Schema class and Deserializer to the related parameter.\n+ * <li> Modify write step in the {@link KafkaToPubsub} by put your Schema class to \"writeAvrosToPubSub\" step.\n+ * </ul>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyNTYyMzYwOQ=="}, "originalCommit": {"oid": "a22c6966012dfbfed51aa0e23c8e8ca2e74b2245"}, "originalPosition": 138}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMxNzI0MjE4OnYy", "diffSide": "RIGHT", "path": "examples/templates/java/kafka-to-pubsub/src/test/java/org/apache/beam/templates/KafkaToPubsubTest.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yM1QxOTowNzowNlrOH4bgnw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxOTowMDo0M1rOIBxVLw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMzAyMw==", "bodyText": "Maybe I am missing something here but these tests only validate the config.\nIs it possible to have a more e2e test where we create & execute a pipeline like what we had before ?", "url": "https://github.com/apache/beam/pull/13112#discussion_r528933023", "createdAt": "2020-11-23T19:07:06Z", "author": {"login": "manavgarg"}, "path": "examples/templates/java/kafka-to-pubsub/src/test/java/org/apache/beam/templates/KafkaToPubsubTest.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates;\n+\n+import static org.apache.beam.templates.KafkaPubsubConstants.PASSWORD;\n+import static org.apache.beam.templates.KafkaPubsubConstants.USERNAME;\n+import static org.apache.beam.templates.kafka.consumer.Utils.getKafkaCredentialsFromVault;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.beam.templates.kafka.consumer.Utils;\n+import org.apache.kafka.common.config.SaslConfigs;\n+import org.apache.kafka.common.security.scram.ScramMechanism;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+/** Test of KafkaToPubsub. */\n+@RunWith(JUnit4.class)\n+public class KafkaToPubsubTest {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a3d85d19e45613b28ad5bc0042921a0f961598c"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzMyOTE5Nw==", "bodyText": "We've made an investigation towards e2e tests and for such tests we need Kafka and Pub/Sub being set up in some environment or in a GCP account with billing. That is why seems like there is no way to do it properly.", "url": "https://github.com/apache/beam/pull/13112#discussion_r533329197", "createdAt": "2020-12-01T11:13:42Z", "author": {"login": "KhaninArtur"}, "path": "examples/templates/java/kafka-to-pubsub/src/test/java/org/apache/beam/templates/KafkaToPubsubTest.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates;\n+\n+import static org.apache.beam.templates.KafkaPubsubConstants.PASSWORD;\n+import static org.apache.beam.templates.KafkaPubsubConstants.USERNAME;\n+import static org.apache.beam.templates.kafka.consumer.Utils.getKafkaCredentialsFromVault;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.beam.templates.kafka.consumer.Utils;\n+import org.apache.kafka.common.config.SaslConfigs;\n+import org.apache.kafka.common.security.scram.ScramMechanism;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+/** Test of KafkaToPubsub. */\n+@RunWith(JUnit4.class)\n+public class KafkaToPubsubTest {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMzAyMw=="}, "originalCommit": {"oid": "8a3d85d19e45613b28ad5bc0042921a0f961598c"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzU1OTM3NQ==", "bodyText": "You might take a look at how our tests for PubsubIO and KafkaIO work as they have to contend with this same problem.\nFor KafkaIO, we use testcontainers to stand up a fake Kafka service to test against.\nFor E2E tests of PubsubIO and other GCP-specific features we have the apache-beam-testing project. We could probably give you access to this. Alternatively, there is now a PubSub emulator that you could use to stand up a fake PubSub service, just like the Kafka testcontainer. I don't think there are any examples using it in Beam yet though.", "url": "https://github.com/apache/beam/pull/13112#discussion_r533559375", "createdAt": "2020-12-01T16:42:27Z", "author": {"login": "TheNeuralBit"}, "path": "examples/templates/java/kafka-to-pubsub/src/test/java/org/apache/beam/templates/KafkaToPubsubTest.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates;\n+\n+import static org.apache.beam.templates.KafkaPubsubConstants.PASSWORD;\n+import static org.apache.beam.templates.KafkaPubsubConstants.USERNAME;\n+import static org.apache.beam.templates.kafka.consumer.Utils.getKafkaCredentialsFromVault;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.beam.templates.kafka.consumer.Utils;\n+import org.apache.kafka.common.config.SaslConfigs;\n+import org.apache.kafka.common.security.scram.ScramMechanism;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+/** Test of KafkaToPubsub. */\n+@RunWith(JUnit4.class)\n+public class KafkaToPubsubTest {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMzAyMw=="}, "originalCommit": {"oid": "8a3d85d19e45613b28ad5bc0042921a0f961598c"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzgwMjY5MA==", "bodyText": "Thank you for bringing up the requirement of the e2e tests, we understand the solution needs it.\nOur focus is to get this example available to the community and be able to collect any feedback from customers using this example in their scenarios and the approach to the examples. We propose to keep this PR scope, create a JIRA ticket (for Kafka and Pub/Sub) to extend this example with e2e tests, and set customer expectations on e2e testing in the readme for this PR.", "url": "https://github.com/apache/beam/pull/13112#discussion_r537802690", "createdAt": "2020-12-07T20:17:09Z", "author": {"login": "ilya-kozyrev"}, "path": "examples/templates/java/kafka-to-pubsub/src/test/java/org/apache/beam/templates/KafkaToPubsubTest.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates;\n+\n+import static org.apache.beam.templates.KafkaPubsubConstants.PASSWORD;\n+import static org.apache.beam.templates.KafkaPubsubConstants.USERNAME;\n+import static org.apache.beam.templates.kafka.consumer.Utils.getKafkaCredentialsFromVault;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.beam.templates.kafka.consumer.Utils;\n+import org.apache.kafka.common.config.SaslConfigs;\n+import org.apache.kafka.common.security.scram.ScramMechanism;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+/** Test of KafkaToPubsub. */\n+@RunWith(JUnit4.class)\n+public class KafkaToPubsubTest {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMzAyMw=="}, "originalCommit": {"oid": "8a3d85d19e45613b28ad5bc0042921a0f961598c"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzNzkzMDEzMQ==", "bodyText": "I'm a +1 for @ilya-kozyrev's proposal.\nIf the solution passes all tests, I'm supportive of merging the solution so we entice users to test it according to their use cases. The consideration here is to complete the two steps he suggests: adding the note on e2e tests needed and creating Jira tickets to create these.", "url": "https://github.com/apache/beam/pull/13112#discussion_r537930131", "createdAt": "2020-12-08T00:07:11Z", "author": {"login": "griscz"}, "path": "examples/templates/java/kafka-to-pubsub/src/test/java/org/apache/beam/templates/KafkaToPubsubTest.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates;\n+\n+import static org.apache.beam.templates.KafkaPubsubConstants.PASSWORD;\n+import static org.apache.beam.templates.KafkaPubsubConstants.USERNAME;\n+import static org.apache.beam.templates.kafka.consumer.Utils.getKafkaCredentialsFromVault;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.beam.templates.kafka.consumer.Utils;\n+import org.apache.kafka.common.config.SaslConfigs;\n+import org.apache.kafka.common.security.scram.ScramMechanism;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+/** Test of KafkaToPubsub. */\n+@RunWith(JUnit4.class)\n+public class KafkaToPubsubTest {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMzAyMw=="}, "originalCommit": {"oid": "8a3d85d19e45613b28ad5bc0042921a0f961598c"}, "originalPosition": 36}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODcyNzcyNw==", "bodyText": "Sounds good. Please add the note and the link the JIRA tickets in that case.", "url": "https://github.com/apache/beam/pull/13112#discussion_r538727727", "createdAt": "2020-12-08T19:00:43Z", "author": {"login": "manavgarg"}, "path": "examples/templates/java/kafka-to-pubsub/src/test/java/org/apache/beam/templates/KafkaToPubsubTest.java", "diffHunk": "@@ -0,0 +1,90 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates;\n+\n+import static org.apache.beam.templates.KafkaPubsubConstants.PASSWORD;\n+import static org.apache.beam.templates.KafkaPubsubConstants.USERNAME;\n+import static org.apache.beam.templates.kafka.consumer.Utils.getKafkaCredentialsFromVault;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import org.apache.beam.templates.kafka.consumer.Utils;\n+import org.apache.kafka.common.config.SaslConfigs;\n+import org.apache.kafka.common.security.scram.ScramMechanism;\n+import org.junit.Assert;\n+import org.junit.Test;\n+import org.junit.runner.RunWith;\n+import org.junit.runners.JUnit4;\n+\n+/** Test of KafkaToPubsub. */\n+@RunWith(JUnit4.class)\n+public class KafkaToPubsubTest {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUyODkzMzAyMw=="}, "originalCommit": {"oid": "8a3d85d19e45613b28ad5bc0042921a0f961598c"}, "originalPosition": 36}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMyNDE3ODQwOnYy", "diffSide": "RIGHT", "path": "examples/templates/java/kafka-to-pubsub/src/main/java/org/apache/beam/templates/kafka/consumer/SslConsumerFactoryFn.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNVQwMDo1NDoyNlrOH5fBLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wMVQxNTo0Nzo1NlrOH8zTMg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDAzOTA4NQ==", "bodyText": "Since your implementation for getGcsFileAsLocal relies on FileSystems.matchSingleFileSpec I think it will actually work for the else path here as well (It could also pull a truststore from AWS s3 if you include the dependency).\nIt's worth noting that the local file option will fail at execution time for a distributed runner, we may want to catch that and raise a more helpful error - e.g. suggest that they stage the file on cloud storage", "url": "https://github.com/apache/beam/pull/13112#discussion_r530039085", "createdAt": "2020-11-25T00:54:26Z", "author": {"login": "TheNeuralBit"}, "path": "examples/templates/java/kafka-to-pubsub/src/main/java/org/apache/beam/templates/kafka/consumer/SslConsumerFactoryFn.java", "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates.kafka.consumer;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** Class to create Kafka Consumer with configured SSL. */\n+public class SslConsumerFactoryFn\n+    implements SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> {\n+  private final Map<String, String> sslConfig;\n+  private static final String TRUSTSTORE_LOCAL_PATH = \"/tmp/kafka.truststore.jks\";\n+  private static final String KEYSTORE_LOCAL_PATH = \"/tmp/kafka.keystore.jks\";\n+\n+  /* Logger for class.*/\n+  private static final Logger LOG = LoggerFactory.getLogger(SslConsumerFactoryFn.class);\n+\n+  public SslConsumerFactoryFn(Map<String, String> sslConfig) {\n+    this.sslConfig = sslConfig;\n+  }\n+\n+  @SuppressWarnings(\"nullness\")\n+  @Override\n+  public Consumer<byte[], byte[]> apply(Map<String, Object> config) {\n+    try {\n+      String truststoreLocation = sslConfig.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG);\n+      if (truststoreLocation.startsWith(\"gs://\")) {\n+        getGcsFileAsLocal(truststoreLocation, TRUSTSTORE_LOCAL_PATH);\n+      } else {\n+        checkFileExists(truststoreLocation);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a3d85d19e45613b28ad5bc0042921a0f961598c"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMzUxNzEwNg==", "bodyText": "Thank you for the great idea to use different filesystems. We will add it to future plans.\nI added a new message to mention what we cant use local paths when we using distribute runners.", "url": "https://github.com/apache/beam/pull/13112#discussion_r533517106", "createdAt": "2020-12-01T15:47:56Z", "author": {"login": "ilya-kozyrev"}, "path": "examples/templates/java/kafka-to-pubsub/src/main/java/org/apache/beam/templates/kafka/consumer/SslConsumerFactoryFn.java", "diffHunk": "@@ -0,0 +1,125 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates.kafka.consumer;\n+\n+import java.io.File;\n+import java.io.IOException;\n+import java.nio.channels.FileChannel;\n+import java.nio.channels.ReadableByteChannel;\n+import java.nio.file.Paths;\n+import java.nio.file.StandardOpenOption;\n+import java.util.HashSet;\n+import java.util.Map;\n+import java.util.Set;\n+import org.apache.beam.sdk.io.FileSystems;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.kafka.clients.CommonClientConfigs;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.config.SslConfigs;\n+import org.apache.kafka.common.security.auth.SecurityProtocol;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/** Class to create Kafka Consumer with configured SSL. */\n+public class SslConsumerFactoryFn\n+    implements SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> {\n+  private final Map<String, String> sslConfig;\n+  private static final String TRUSTSTORE_LOCAL_PATH = \"/tmp/kafka.truststore.jks\";\n+  private static final String KEYSTORE_LOCAL_PATH = \"/tmp/kafka.keystore.jks\";\n+\n+  /* Logger for class.*/\n+  private static final Logger LOG = LoggerFactory.getLogger(SslConsumerFactoryFn.class);\n+\n+  public SslConsumerFactoryFn(Map<String, String> sslConfig) {\n+    this.sslConfig = sslConfig;\n+  }\n+\n+  @SuppressWarnings(\"nullness\")\n+  @Override\n+  public Consumer<byte[], byte[]> apply(Map<String, Object> config) {\n+    try {\n+      String truststoreLocation = sslConfig.get(SslConfigs.SSL_TRUSTSTORE_LOCATION_CONFIG);\n+      if (truststoreLocation.startsWith(\"gs://\")) {\n+        getGcsFileAsLocal(truststoreLocation, TRUSTSTORE_LOCAL_PATH);\n+      } else {\n+        checkFileExists(truststoreLocation);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDAzOTA4NQ=="}, "originalCommit": {"oid": "8a3d85d19e45613b28ad5bc0042921a0f961598c"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMyNDE4NTA5OnYy", "diffSide": "RIGHT", "path": "examples/templates/java/kafka-to-pubsub/src/main/java/org/apache/beam/templates/KafkaToPubsub.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNVQwMDo1NzoyOFrOH5fE5Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNlQyMzozNDoyMFrOH6pIww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDA0MDAzNw==", "bodyText": "This will never be triggered its the same condition as the if", "url": "https://github.com/apache/beam/pull/13112#discussion_r530040037", "createdAt": "2020-11-25T00:57:28Z", "author": {"login": "TheNeuralBit"}, "path": "examples/templates/java/kafka-to-pubsub/src/main/java/org/apache/beam/templates/KafkaToPubsub.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates;\n+\n+import static org.apache.beam.templates.kafka.consumer.Utils.configureKafka;\n+import static org.apache.beam.templates.kafka.consumer.Utils.configureSsl;\n+import static org.apache.beam.templates.kafka.consumer.Utils.getKafkaCredentialsFromVault;\n+import static org.apache.beam.templates.kafka.consumer.Utils.isSslSpecified;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult;\n+import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.Values;\n+import org.apache.beam.templates.avro.AvroDataClass;\n+import org.apache.beam.templates.avro.AvroDataClassKafkaAvroDeserializer;\n+import org.apache.beam.templates.options.KafkaToPubsubOptions;\n+import org.apache.beam.templates.transforms.FormatTransform;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The {@link KafkaToPubsub} pipeline is a streaming pipeline which ingests data in JSON format from\n+ * Kafka, and outputs the resulting records to PubSub. Input topics, output topic, Bootstrap servers\n+ * are specified by the user as template parameters. <br>\n+ * Kafka may be configured with SASL/SCRAM security mechanism, in this case a Vault secret storage\n+ * with credentials should be provided. URL to credentials and Vault token are specified by the user\n+ * as template parameters.\n+ *\n+ * <p><b>Pipeline Requirements</b>\n+ *\n+ * <ul>\n+ *   <li>Kafka Bootstrap Server(s).\n+ *   <li>Kafka Topic(s) exists.\n+ *   <li>The PubSub output topic exists.\n+ *   <li>(Optional) An existing HashiCorp Vault secret storage\n+ * </ul>\n+ *\n+ * <p><b>Example Usage</b>\n+ *\n+ * <pre>\n+ * # Set the pipeline vars\n+ * PROJECT=id-of-my-project\n+ * BUCKET_NAME=my-bucket\n+ *\n+ * # Set containerization vars\n+ * IMAGE_NAME=my-image-name\n+ * TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+ * BASE_CONTAINER_IMAGE=my-base-container-image\n+ * TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+ *\n+ * # Create bucket in the cloud storage\n+ * gsutil mb gs://${BUCKET_NAME}\n+ *\n+ * # Go to the beam folder\n+ * cd /path/to/beam\n+ *\n+ * <b>FLEX TEMPLATE</b>\n+ * # Assemble uber-jar\n+ * ./gradlew -p templates/kafka-to-pubsub clean shadowJar\n+ *\n+ * # Go to the template folder\n+ * cd /path/to/beam/templates/kafka-to-pubsub\n+ *\n+ * # Build the flex template\n+ * gcloud dataflow flex-template build ${TEMPLATE_PATH} \\\n+ *       --image-gcr-path \"${TARGET_GCR_IMAGE}\" \\\n+ *       --sdk-language \"JAVA\" \\\n+ *       --flex-template-base-image ${BASE_CONTAINER_IMAGE} \\\n+ *       --metadata-file \"src/main/resources/kafka_to_pubsub_metadata.json\" \\\n+ *       --jar \"build/libs/beam-templates-kafka-to-pubsub-<version>-all.jar\" \\\n+ *       --env FLEX_TEMPLATE_JAVA_MAIN_CLASS=\"org.apache.beam.templates.KafkaToPubsub\"\n+ *\n+ * # Execute template:\n+ *    API_ROOT_URL=\"https://dataflow.googleapis.com\"\n+ *    TEMPLATES_LAUNCH_API=\"${API_ROOT_URL}/v1b3/projects/${PROJECT}/locations/${REGION}/flexTemplates:launch\"\n+ *    JOB_NAME=\"kafka-to-pubsub-`date +%Y%m%d-%H%M%S-%N`\"\n+ *\n+ *    time curl -X POST -H \"Content-Type: application/json\" \\\n+ *            -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n+ *            -d '\n+ *             {\n+ *                 \"launch_parameter\": {\n+ *                     \"jobName\": \"'$JOB_NAME'\",\n+ *                     \"containerSpecGcsPath\": \"'$TEMPLATE_PATH'\",\n+ *                     \"parameters\": {\n+ *                         \"bootstrapServers\": \"broker_1:9091, broker_2:9092\",\n+ *                         \"inputTopics\": \"topic1, topic2\",\n+ *                         \"outputTopic\": \"projects/'$PROJECT'/topics/your-topic-name\",\n+ *                         \"secretStoreUrl\": \"http(s)://host:port/path/to/credentials\",\n+ *                         \"vaultToken\": \"your-token\"\n+ *                     }\n+ *                 }\n+ *             }\n+ *            '\n+ *            \"${TEMPLATES_LAUNCH_API}\"\n+ * </pre>\n+ *\n+ * <p><b>Example Avro usage</b>\n+ *\n+ * <pre>\n+ * This template contains an example Class to deserialize AVRO from Kafka and serialize it to AVRO in Pub/Sub.\n+ *\n+ * To use this example in the specific case, follow the few steps:\n+ * <ul>\n+ * <li> Create your own class to describe AVRO schema. As an example use {@link AvroDataClass}. Just define necessary fields.\n+ * <li> Create your own Avro Deserializer class. As an example use {@link AvroDataClassKafkaAvroDeserializer}. Just rename it, and put your own Schema class as the necessary types.\n+ * <li> Modify the {@link FormatTransform}. Put your Schema class and Deserializer to the related parameter.\n+ * <li> Modify write step in the {@link KafkaToPubsub} by put your Schema class to \"writeAvrosToPubSub\" step.\n+ * </ul>\n+ * </pre>\n+ */\n+public class KafkaToPubsub {\n+\n+  /* Logger for class.*/\n+  private static final Logger LOG = LoggerFactory.getLogger(KafkaToPubsub.class);\n+\n+  /**\n+   * Main entry point for pipeline execution.\n+   *\n+   * @param args Command line arguments to the pipeline.\n+   */\n+  public static void main(String[] args) {\n+    KafkaToPubsubOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(KafkaToPubsubOptions.class);\n+\n+    run(options);\n+  }\n+\n+  /**\n+   * Runs a pipeline which reads message from Kafka and writes it to GCS.\n+   *\n+   * @param options arguments to the pipeline\n+   */\n+  public static PipelineResult run(KafkaToPubsubOptions options) {\n+    // Configure Kafka consumer properties\n+    Map<String, Object> kafkaConfig = new HashMap<>();\n+    if (options.getSecretStoreUrl() != null && options.getVaultToken() != null) {\n+      Map<String, Map<String, String>> credentials =\n+          getKafkaCredentialsFromVault(options.getSecretStoreUrl(), options.getVaultToken());\n+      kafkaConfig = configureKafka(credentials.get(KafkaPubsubConstants.KAFKA_CREDENTIALS));\n+    } else {\n+      LOG.warn(\n+          \"No information to retrieve Kafka credentials was provided. \"\n+              + \"Trying to initiate an unauthorized connection.\");\n+    }\n+\n+    Map<String, String> sslConfig = new HashMap<>();\n+    if (isSslSpecified(options)) {\n+      sslConfig.putAll(configureSsl(options));\n+    } else {\n+      LOG.info(\n+          \"No information to retrieve SSL certificate was provided. \"\n+              + \"Trying to initiate a plain text connection.\");\n+    }\n+\n+    List<String> topicsList = new ArrayList<>(Arrays.asList(options.getInputTopics().split(\",\")));\n+\n+    checkArgument(\n+        topicsList.size() > 0 && topicsList.get(0).length() > 0,\n+        \"inputTopics cannot be an empty string.\");\n+\n+    List<String> bootstrapServersList =\n+        new ArrayList<>(Arrays.asList(options.getBootstrapServers().split(\",\")));\n+\n+    checkArgument(\n+        bootstrapServersList.size() > 0 && topicsList.get(0).length() > 0,\n+        \"bootstrapServers cannot be an empty string.\");\n+\n+    // Create the pipeline\n+    Pipeline pipeline = Pipeline.create(options);\n+    LOG.info(\n+        \"Starting Kafka-To-PubSub pipeline with parameters bootstrap servers:\"\n+            + options.getBootstrapServers()\n+            + \" input topics: \"\n+            + options.getInputTopics()\n+            + \" output pubsub topic: \"\n+            + options.getOutputTopic());\n+\n+    /*\n+     * Steps:\n+     *  1) Read messages in from Kafka\n+     *  2) Extract values only\n+     *  3) Write successful records to PubSub\n+     */\n+\n+    if (options.getOutputFormat() == FormatTransform.FORMAT.AVRO) {\n+      pipeline\n+          .apply(\n+              \"readAvrosFromKafka\",\n+              FormatTransform.readAvrosFromKafka(\n+                  options.getBootstrapServers(), topicsList, kafkaConfig, sslConfig))\n+          .apply(\"createValues\", Values.create())\n+          .apply(\"writeAvrosToPubSub\", PubsubIO.writeAvros(AvroDataClass.class));\n+\n+    } else if (options.getOutputFormat() == FormatTransform.FORMAT.AVRO) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a3d85d19e45613b28ad5bc0042921a0f961598c"}, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTI1MzQ0Mw==", "bodyText": "fixed", "url": "https://github.com/apache/beam/pull/13112#discussion_r531253443", "createdAt": "2020-11-26T23:34:20Z", "author": {"login": "ilya-kozyrev"}, "path": "examples/templates/java/kafka-to-pubsub/src/main/java/org/apache/beam/templates/KafkaToPubsub.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.templates;\n+\n+import static org.apache.beam.templates.kafka.consumer.Utils.configureKafka;\n+import static org.apache.beam.templates.kafka.consumer.Utils.configureSsl;\n+import static org.apache.beam.templates.kafka.consumer.Utils.getKafkaCredentialsFromVault;\n+import static org.apache.beam.templates.kafka.consumer.Utils.isSslSpecified;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult;\n+import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.Values;\n+import org.apache.beam.templates.avro.AvroDataClass;\n+import org.apache.beam.templates.avro.AvroDataClassKafkaAvroDeserializer;\n+import org.apache.beam.templates.options.KafkaToPubsubOptions;\n+import org.apache.beam.templates.transforms.FormatTransform;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The {@link KafkaToPubsub} pipeline is a streaming pipeline which ingests data in JSON format from\n+ * Kafka, and outputs the resulting records to PubSub. Input topics, output topic, Bootstrap servers\n+ * are specified by the user as template parameters. <br>\n+ * Kafka may be configured with SASL/SCRAM security mechanism, in this case a Vault secret storage\n+ * with credentials should be provided. URL to credentials and Vault token are specified by the user\n+ * as template parameters.\n+ *\n+ * <p><b>Pipeline Requirements</b>\n+ *\n+ * <ul>\n+ *   <li>Kafka Bootstrap Server(s).\n+ *   <li>Kafka Topic(s) exists.\n+ *   <li>The PubSub output topic exists.\n+ *   <li>(Optional) An existing HashiCorp Vault secret storage\n+ * </ul>\n+ *\n+ * <p><b>Example Usage</b>\n+ *\n+ * <pre>\n+ * # Set the pipeline vars\n+ * PROJECT=id-of-my-project\n+ * BUCKET_NAME=my-bucket\n+ *\n+ * # Set containerization vars\n+ * IMAGE_NAME=my-image-name\n+ * TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+ * BASE_CONTAINER_IMAGE=my-base-container-image\n+ * TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+ *\n+ * # Create bucket in the cloud storage\n+ * gsutil mb gs://${BUCKET_NAME}\n+ *\n+ * # Go to the beam folder\n+ * cd /path/to/beam\n+ *\n+ * <b>FLEX TEMPLATE</b>\n+ * # Assemble uber-jar\n+ * ./gradlew -p templates/kafka-to-pubsub clean shadowJar\n+ *\n+ * # Go to the template folder\n+ * cd /path/to/beam/templates/kafka-to-pubsub\n+ *\n+ * # Build the flex template\n+ * gcloud dataflow flex-template build ${TEMPLATE_PATH} \\\n+ *       --image-gcr-path \"${TARGET_GCR_IMAGE}\" \\\n+ *       --sdk-language \"JAVA\" \\\n+ *       --flex-template-base-image ${BASE_CONTAINER_IMAGE} \\\n+ *       --metadata-file \"src/main/resources/kafka_to_pubsub_metadata.json\" \\\n+ *       --jar \"build/libs/beam-templates-kafka-to-pubsub-<version>-all.jar\" \\\n+ *       --env FLEX_TEMPLATE_JAVA_MAIN_CLASS=\"org.apache.beam.templates.KafkaToPubsub\"\n+ *\n+ * # Execute template:\n+ *    API_ROOT_URL=\"https://dataflow.googleapis.com\"\n+ *    TEMPLATES_LAUNCH_API=\"${API_ROOT_URL}/v1b3/projects/${PROJECT}/locations/${REGION}/flexTemplates:launch\"\n+ *    JOB_NAME=\"kafka-to-pubsub-`date +%Y%m%d-%H%M%S-%N`\"\n+ *\n+ *    time curl -X POST -H \"Content-Type: application/json\" \\\n+ *            -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n+ *            -d '\n+ *             {\n+ *                 \"launch_parameter\": {\n+ *                     \"jobName\": \"'$JOB_NAME'\",\n+ *                     \"containerSpecGcsPath\": \"'$TEMPLATE_PATH'\",\n+ *                     \"parameters\": {\n+ *                         \"bootstrapServers\": \"broker_1:9091, broker_2:9092\",\n+ *                         \"inputTopics\": \"topic1, topic2\",\n+ *                         \"outputTopic\": \"projects/'$PROJECT'/topics/your-topic-name\",\n+ *                         \"secretStoreUrl\": \"http(s)://host:port/path/to/credentials\",\n+ *                         \"vaultToken\": \"your-token\"\n+ *                     }\n+ *                 }\n+ *             }\n+ *            '\n+ *            \"${TEMPLATES_LAUNCH_API}\"\n+ * </pre>\n+ *\n+ * <p><b>Example Avro usage</b>\n+ *\n+ * <pre>\n+ * This template contains an example Class to deserialize AVRO from Kafka and serialize it to AVRO in Pub/Sub.\n+ *\n+ * To use this example in the specific case, follow the few steps:\n+ * <ul>\n+ * <li> Create your own class to describe AVRO schema. As an example use {@link AvroDataClass}. Just define necessary fields.\n+ * <li> Create your own Avro Deserializer class. As an example use {@link AvroDataClassKafkaAvroDeserializer}. Just rename it, and put your own Schema class as the necessary types.\n+ * <li> Modify the {@link FormatTransform}. Put your Schema class and Deserializer to the related parameter.\n+ * <li> Modify write step in the {@link KafkaToPubsub} by put your Schema class to \"writeAvrosToPubSub\" step.\n+ * </ul>\n+ * </pre>\n+ */\n+public class KafkaToPubsub {\n+\n+  /* Logger for class.*/\n+  private static final Logger LOG = LoggerFactory.getLogger(KafkaToPubsub.class);\n+\n+  /**\n+   * Main entry point for pipeline execution.\n+   *\n+   * @param args Command line arguments to the pipeline.\n+   */\n+  public static void main(String[] args) {\n+    KafkaToPubsubOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(KafkaToPubsubOptions.class);\n+\n+    run(options);\n+  }\n+\n+  /**\n+   * Runs a pipeline which reads message from Kafka and writes it to GCS.\n+   *\n+   * @param options arguments to the pipeline\n+   */\n+  public static PipelineResult run(KafkaToPubsubOptions options) {\n+    // Configure Kafka consumer properties\n+    Map<String, Object> kafkaConfig = new HashMap<>();\n+    if (options.getSecretStoreUrl() != null && options.getVaultToken() != null) {\n+      Map<String, Map<String, String>> credentials =\n+          getKafkaCredentialsFromVault(options.getSecretStoreUrl(), options.getVaultToken());\n+      kafkaConfig = configureKafka(credentials.get(KafkaPubsubConstants.KAFKA_CREDENTIALS));\n+    } else {\n+      LOG.warn(\n+          \"No information to retrieve Kafka credentials was provided. \"\n+              + \"Trying to initiate an unauthorized connection.\");\n+    }\n+\n+    Map<String, String> sslConfig = new HashMap<>();\n+    if (isSslSpecified(options)) {\n+      sslConfig.putAll(configureSsl(options));\n+    } else {\n+      LOG.info(\n+          \"No information to retrieve SSL certificate was provided. \"\n+              + \"Trying to initiate a plain text connection.\");\n+    }\n+\n+    List<String> topicsList = new ArrayList<>(Arrays.asList(options.getInputTopics().split(\",\")));\n+\n+    checkArgument(\n+        topicsList.size() > 0 && topicsList.get(0).length() > 0,\n+        \"inputTopics cannot be an empty string.\");\n+\n+    List<String> bootstrapServersList =\n+        new ArrayList<>(Arrays.asList(options.getBootstrapServers().split(\",\")));\n+\n+    checkArgument(\n+        bootstrapServersList.size() > 0 && topicsList.get(0).length() > 0,\n+        \"bootstrapServers cannot be an empty string.\");\n+\n+    // Create the pipeline\n+    Pipeline pipeline = Pipeline.create(options);\n+    LOG.info(\n+        \"Starting Kafka-To-PubSub pipeline with parameters bootstrap servers:\"\n+            + options.getBootstrapServers()\n+            + \" input topics: \"\n+            + options.getInputTopics()\n+            + \" output pubsub topic: \"\n+            + options.getOutputTopic());\n+\n+    /*\n+     * Steps:\n+     *  1) Read messages in from Kafka\n+     *  2) Extract values only\n+     *  3) Write successful records to PubSub\n+     */\n+\n+    if (options.getOutputFormat() == FormatTransform.FORMAT.AVRO) {\n+      pipeline\n+          .apply(\n+              \"readAvrosFromKafka\",\n+              FormatTransform.readAvrosFromKafka(\n+                  options.getBootstrapServers(), topicsList, kafkaConfig, sslConfig))\n+          .apply(\"createValues\", Values.create())\n+          .apply(\"writeAvrosToPubSub\", PubsubIO.writeAvros(AvroDataClass.class));\n+\n+    } else if (options.getOutputFormat() == FormatTransform.FORMAT.AVRO) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDA0MDAzNw=="}, "originalCommit": {"oid": "8a3d85d19e45613b28ad5bc0042921a0f961598c"}, "originalPosition": 217}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzMyNDIzOTg0OnYy", "diffSide": "RIGHT", "path": "examples/templates/java/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNVQwMToyNDowNlrOH5fkqQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMS0yNlQyMzo0NzozOFrOH6pQ9g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDA0ODE2OQ==", "bodyText": "I'm not sure it makes sense to discuss Dataflow Flex templates here, we should leave that to GoogleCloudPlatform/DataflowTemplates#176\nI think this README should instead discuss how to run directly on Dataflow and/or how to run on some other Beam runners", "url": "https://github.com/apache/beam/pull/13112#discussion_r530048169", "createdAt": "2020-11-25T01:24:06Z", "author": {"login": "TheNeuralBit"}, "path": "examples/templates/java/README.md", "diffHunk": "@@ -0,0 +1,254 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam Template to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) Template that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Cloud Pub/Sub](https://cloud.google.com/pubsub).\n+\n+Supported data formats:\n+- Serializable plaintext formats, such as JSON\n+- [PubSubMessage](https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage).\n+\n+Supported input source configurations:\n+- Single or multiple Apache Kafka bootstrap servers\n+- Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection\n+- Secrets vault service [HashiCorp Vault](https://www.vaultproject.io/).\n+\n+Supported destination configuration:\n+- Single Google Cloud Pub/Sub topic.\n+\n+In a simple scenario, the template will create an Apache Beam pipeline that will read messages from a source Kafka server with a source topic, and stream the text messages into specified Pub/Sub destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed over plain text or SSL encrypted connection. The template supports using a single Kafka user account to authenticate in the provided source Kafka servers and topics. To support SASL authenticaton over SSL the template will need an SSL certificate location and access to a secrets vault service with Kafka username and password, currently supporting HashiCorp Vault.\n+\n+## Requirements\n+\n+- Java 8\n+- Kafka Bootstrap Server(s) up and running\n+- Existing source Kafka topic(s)\n+- An existing Pub/Sub destination output topic\n+- (Optional) An existing HashiCorp Vault\n+- (Optional) A configured secure SSL connection for Kafka\n+\n+## Getting Started\n+\n+This section describes what is needed to get the template up and running.\n+- Assembling the Uber-JAR\n+- Local execution\n+- Google Dataflow Template\n+  - Set up the environment\n+  - Creating the Dataflow Flex Template\n+  - Create a Dataflow job to ingest data using the template.\n+- Avro format transferring.\n+\n+## Assembling the Uber-JAR\n+\n+To run this template the template Java project should be built into\n+an Uber JAR file.\n+\n+Navigate to the Beam folder:\n+\n+```\n+cd /path/to/beam\n+```\n+\n+In order to create Uber JAR with Gradle, [Shadow plugin](https://github.com/johnrengelman/shadow)\n+is used. It creates the `shadowJar` task that builds the Uber JAR:\n+\n+```\n+./gradlew -p examples/templates/java/kafka-to-pubsub clean shadowJar\n+```\n+\n+\u2139\ufe0f An **Uber JAR** - also known as **fat JAR** - is a single JAR file that contains\n+both target package *and* all its dependencies.\n+\n+The result of the `shadowJar` task execution is a `.jar` file that is generated\n+under the `build/libs/` folder in kafka-to-pubsub directory.\n+\n+## Local execution\n+To execute this pipeline locally, specify the parameters:\n+- Kafka Bootstrap servers\n+- Kafka input topics\n+- Pub/Sub output topic\n+in the following format:\n+```bash\n+--bootstrapServers=host:port \\\n+--inputTopics=your-input-topic \\\n+--outputTopic=projects/your-project-id/topics/your-topic-pame\n+```\n+Optionally, to retrieve Kafka credentials for SASL/SCRAM,\n+specify a URL to the credentials in HashiCorp Vault and the vault access token:\n+```bash\n+--secretStoreUrl=http(s)://host:port/path/to/credentials\n+--vaultToken=your-token\n+```\n+Optionally, to configure secure SSL connection between the Beam pipeline and Kafka,\n+specify the parameters:\n+- A path to a truststore file (it can be a local path or a GCS path, which should start with `gs://`)\n+- A path to a keystore file (it can be a local path or a GCS path, which should start with `gs://`)\n+- Truststore password\n+- Keystore password\n+- Key password\n+```bash\n+--truststorePath=path/to/kafka.truststore.jks\n+--keystorePath=path/to/kafka.keystore.jks\n+--truststorePassword=your-truststore-password\n+--keystorePassword=your-keystore-password\n+--keyPassword=your-key-password\n+```\n+To change the runner, specify:\n+```bash\n+--runner=YOUR_SELECTED_RUNNER\n+```\n+See examples/java/README.md for steps and examples to configure different runners.\n+\n+## Google Dataflow Template\n+\n+### Setting Up Project Environment\n+\n+#### Pipeline variables:\n+\n+```\n+PROJECT=id-of-my-project\n+BUCKET_NAME=my-bucket\n+REGION=my-region\n+```\n+\n+#### Template Metadata Storage Bucket Creation\n+\n+The Dataflow Flex template has to store its metadata in a bucket in", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "8a3d85d19e45613b28ad5bc0042921a0f961598c"}, "originalPosition": 136}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMTI1NTU0Mg==", "bodyText": "In our case, we describe in Readme how to run the template on Direct runner and with Dataflow. In the readme, we have the \"Local execution\" part firstly. In this part of README, we discuss Direct runner.\nTo run the template using Dataflow we need to build it first. Flex template is the better and newest way to do it. The built template could be used in GCP Console or CLI as a \"custom template\".  Based on this documentation (I added to the readme)\nPR which you mentioned targets the repository which contained  Google Provided Templates. But in this PR we planning to give options for Beam users to make custom templates based on our one or using it directly if we cover necessary use-cases.", "url": "https://github.com/apache/beam/pull/13112#discussion_r531255542", "createdAt": "2020-11-26T23:47:38Z", "author": {"login": "ilya-kozyrev"}, "path": "examples/templates/java/README.md", "diffHunk": "@@ -0,0 +1,254 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam Template to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) Template that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Cloud Pub/Sub](https://cloud.google.com/pubsub).\n+\n+Supported data formats:\n+- Serializable plaintext formats, such as JSON\n+- [PubSubMessage](https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage).\n+\n+Supported input source configurations:\n+- Single or multiple Apache Kafka bootstrap servers\n+- Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection\n+- Secrets vault service [HashiCorp Vault](https://www.vaultproject.io/).\n+\n+Supported destination configuration:\n+- Single Google Cloud Pub/Sub topic.\n+\n+In a simple scenario, the template will create an Apache Beam pipeline that will read messages from a source Kafka server with a source topic, and stream the text messages into specified Pub/Sub destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed over plain text or SSL encrypted connection. The template supports using a single Kafka user account to authenticate in the provided source Kafka servers and topics. To support SASL authenticaton over SSL the template will need an SSL certificate location and access to a secrets vault service with Kafka username and password, currently supporting HashiCorp Vault.\n+\n+## Requirements\n+\n+- Java 8\n+- Kafka Bootstrap Server(s) up and running\n+- Existing source Kafka topic(s)\n+- An existing Pub/Sub destination output topic\n+- (Optional) An existing HashiCorp Vault\n+- (Optional) A configured secure SSL connection for Kafka\n+\n+## Getting Started\n+\n+This section describes what is needed to get the template up and running.\n+- Assembling the Uber-JAR\n+- Local execution\n+- Google Dataflow Template\n+  - Set up the environment\n+  - Creating the Dataflow Flex Template\n+  - Create a Dataflow job to ingest data using the template.\n+- Avro format transferring.\n+\n+## Assembling the Uber-JAR\n+\n+To run this template the template Java project should be built into\n+an Uber JAR file.\n+\n+Navigate to the Beam folder:\n+\n+```\n+cd /path/to/beam\n+```\n+\n+In order to create Uber JAR with Gradle, [Shadow plugin](https://github.com/johnrengelman/shadow)\n+is used. It creates the `shadowJar` task that builds the Uber JAR:\n+\n+```\n+./gradlew -p examples/templates/java/kafka-to-pubsub clean shadowJar\n+```\n+\n+\u2139\ufe0f An **Uber JAR** - also known as **fat JAR** - is a single JAR file that contains\n+both target package *and* all its dependencies.\n+\n+The result of the `shadowJar` task execution is a `.jar` file that is generated\n+under the `build/libs/` folder in kafka-to-pubsub directory.\n+\n+## Local execution\n+To execute this pipeline locally, specify the parameters:\n+- Kafka Bootstrap servers\n+- Kafka input topics\n+- Pub/Sub output topic\n+in the following format:\n+```bash\n+--bootstrapServers=host:port \\\n+--inputTopics=your-input-topic \\\n+--outputTopic=projects/your-project-id/topics/your-topic-pame\n+```\n+Optionally, to retrieve Kafka credentials for SASL/SCRAM,\n+specify a URL to the credentials in HashiCorp Vault and the vault access token:\n+```bash\n+--secretStoreUrl=http(s)://host:port/path/to/credentials\n+--vaultToken=your-token\n+```\n+Optionally, to configure secure SSL connection between the Beam pipeline and Kafka,\n+specify the parameters:\n+- A path to a truststore file (it can be a local path or a GCS path, which should start with `gs://`)\n+- A path to a keystore file (it can be a local path or a GCS path, which should start with `gs://`)\n+- Truststore password\n+- Keystore password\n+- Key password\n+```bash\n+--truststorePath=path/to/kafka.truststore.jks\n+--keystorePath=path/to/kafka.keystore.jks\n+--truststorePassword=your-truststore-password\n+--keystorePassword=your-keystore-password\n+--keyPassword=your-key-password\n+```\n+To change the runner, specify:\n+```bash\n+--runner=YOUR_SELECTED_RUNNER\n+```\n+See examples/java/README.md for steps and examples to configure different runners.\n+\n+## Google Dataflow Template\n+\n+### Setting Up Project Environment\n+\n+#### Pipeline variables:\n+\n+```\n+PROJECT=id-of-my-project\n+BUCKET_NAME=my-bucket\n+REGION=my-region\n+```\n+\n+#### Template Metadata Storage Bucket Creation\n+\n+The Dataflow Flex template has to store its metadata in a bucket in", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzMDA0ODE2OQ=="}, "originalCommit": {"oid": "8a3d85d19e45613b28ad5bc0042921a0f961598c"}, "originalPosition": 136}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4MTYyOTI1OnYy", "diffSide": "RIGHT", "path": "examples/kafka-to-pubsub/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzozOTowMFrOIBsVoQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzozOTowMFrOIBsVoQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY0NTkyMQ==", "bodyText": "\ud83d\udc4d I think this is a great way to make the connection to the related Dataflow template while still making this example useful for Beam users using other runners. Thank you!", "url": "https://github.com/apache/beam/pull/13112#discussion_r538645921", "createdAt": "2020-12-08T17:39:00Z", "author": {"login": "TheNeuralBit"}, "path": "examples/kafka-to-pubsub/README.md", "diffHunk": "@@ -0,0 +1,163 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam pipeline example to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) pipeline example that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Cloud Pub/Sub](https://cloud.google.com/pubsub).\n+\n+Supported data formats:\n+- Serializable plaintext formats, such as JSON\n+- [PubSubMessage](https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage).\n+\n+Supported input source configurations:\n+- Single or multiple Apache Kafka bootstrap servers\n+- Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection\n+- Secrets vault service [HashiCorp Vault](https://www.vaultproject.io/).\n+\n+Supported destination configuration:\n+- Single Google Cloud Pub/Sub topic.\n+\n+In a simple scenario, the example will create an Apache Beam pipeline that will read messages from a source Kafka server with a source topic, and stream the text messages into specified Pub/Sub destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed over plain text or SSL encrypted connection. The example supports using a single Kafka user account to authenticate in the provided source Kafka servers and topics. To support SASL authenticaton over SSL the example will need an SSL certificate location and access to a secrets vault service with Kafka username and password, currently supporting HashiCorp Vault.\n+\n+## Requirements\n+\n+- Java 8\n+- Kafka Bootstrap Server(s) up and running\n+- Existing source Kafka topic(s)\n+- An existing Pub/Sub destination output topic\n+- (Optional) An existing HashiCorp Vault\n+- (Optional) A configured secure SSL connection for Kafka\n+\n+## Getting Started\n+\n+This section describes what is needed to get the exaple up and running.\n+- Assembling the Uber-JAR\n+- Local execution\n+- Google Dataflow Template\n+  - Set up the environment\n+  - Creating the Dataflow Flex Template\n+  - Create a Dataflow job to ingest data using the template\n+- Avro format transferring.\n+- E2E tests (TBD)\n+\n+## Assembling the Uber-JAR\n+\n+To run this example the Java project should be built into\n+an Uber JAR file.\n+\n+Navigate to the Beam folder:\n+\n+```\n+cd /path/to/beam\n+```\n+\n+In order to create Uber JAR with Gradle, [Shadow plugin](https://github.com/johnrengelman/shadow)\n+is used. It creates the `shadowJar` task that builds the Uber JAR:\n+\n+```\n+./gradlew -p examples/kafka-to-pubsub clean shadowJar\n+```\n+\n+\u2139\ufe0f An **Uber JAR** - also known as **fat JAR** - is a single JAR file that contains\n+both target package *and* all its dependencies.\n+\n+The result of the `shadowJar` task execution is a `.jar` file that is generated\n+under the `build/libs/` folder in kafka-to-pubsub directory.\n+\n+## Local execution\n+To execute this pipeline locally, specify the parameters:\n+- Kafka Bootstrap servers\n+- Kafka input topics\n+- Pub/Sub output topic\n+- Output format\n+\n+in the following format:\n+```bash\n+--bootstrapServers=host:port \\\n+--inputTopics=your-input-topic \\\n+--outputTopic=projects/your-project-id/topics/your-topic-pame \\\n+--outputFormat=AVRO|PUBSUB\n+```\n+Optionally, to retrieve Kafka credentials for SASL/SCRAM,\n+specify a URL to the credentials in HashiCorp Vault and the vault access token:\n+```bash\n+--secretStoreUrl=http(s)://host:port/path/to/credentials\n+--vaultToken=your-token\n+```\n+Optionally, to configure secure SSL connection between the Beam pipeline and Kafka,\n+specify the parameters:\n+- A path to a truststore file (it can be a local path or a GCS path, which should start with `gs://`)\n+- A path to a keystore file (it can be a local path or a GCS path, which should start with `gs://`)\n+- Truststore password\n+- Keystore password\n+- Key password\n+```bash\n+--truststorePath=path/to/kafka.truststore.jks\n+--keystorePath=path/to/kafka.keystore.jks\n+--truststorePassword=your-truststore-password\n+--keystorePassword=your-keystore-password\n+--keyPassword=your-key-password\n+```\n+To change the runner, specify:\n+```bash\n+--runner=YOUR_SELECTED_RUNNER\n+```\n+See examples/java/README.md for steps and examples to configure different runners.\n+\n+## Google Dataflow Execution\n+\n+This example also exists as Google Dataflow Template, see its [README.md](https://github.com/GoogleCloudPlatform/DataflowTemplates/blob/master/v2/kafka-to-pubsub/README.md) for more information.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4372d6ff56bdef40e106f358adb1881cd3032e6f"}, "originalPosition": 128}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4MTY0NDc1OnYy", "diffSide": "RIGHT", "path": "examples/kafka-to-pubsub/README.md", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzo0MToyNFrOIBsf8Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzo0MToyNFrOIBsf8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY0ODU2MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            To change the runner, specify:\n          \n          \n            \n            By default this will run the pipeline locally with the DirectRunner. To change the runner, specify:", "url": "https://github.com/apache/beam/pull/13112#discussion_r538648561", "createdAt": "2020-12-08T17:41:24Z", "author": {"login": "TheNeuralBit"}, "path": "examples/kafka-to-pubsub/README.md", "diffHunk": "@@ -0,0 +1,163 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam pipeline example to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) pipeline example that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Cloud Pub/Sub](https://cloud.google.com/pubsub).\n+\n+Supported data formats:\n+- Serializable plaintext formats, such as JSON\n+- [PubSubMessage](https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage).\n+\n+Supported input source configurations:\n+- Single or multiple Apache Kafka bootstrap servers\n+- Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection\n+- Secrets vault service [HashiCorp Vault](https://www.vaultproject.io/).\n+\n+Supported destination configuration:\n+- Single Google Cloud Pub/Sub topic.\n+\n+In a simple scenario, the example will create an Apache Beam pipeline that will read messages from a source Kafka server with a source topic, and stream the text messages into specified Pub/Sub destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed over plain text or SSL encrypted connection. The example supports using a single Kafka user account to authenticate in the provided source Kafka servers and topics. To support SASL authenticaton over SSL the example will need an SSL certificate location and access to a secrets vault service with Kafka username and password, currently supporting HashiCorp Vault.\n+\n+## Requirements\n+\n+- Java 8\n+- Kafka Bootstrap Server(s) up and running\n+- Existing source Kafka topic(s)\n+- An existing Pub/Sub destination output topic\n+- (Optional) An existing HashiCorp Vault\n+- (Optional) A configured secure SSL connection for Kafka\n+\n+## Getting Started\n+\n+This section describes what is needed to get the exaple up and running.\n+- Assembling the Uber-JAR\n+- Local execution\n+- Google Dataflow Template\n+  - Set up the environment\n+  - Creating the Dataflow Flex Template\n+  - Create a Dataflow job to ingest data using the template\n+- Avro format transferring.\n+- E2E tests (TBD)\n+\n+## Assembling the Uber-JAR\n+\n+To run this example the Java project should be built into\n+an Uber JAR file.\n+\n+Navigate to the Beam folder:\n+\n+```\n+cd /path/to/beam\n+```\n+\n+In order to create Uber JAR with Gradle, [Shadow plugin](https://github.com/johnrengelman/shadow)\n+is used. It creates the `shadowJar` task that builds the Uber JAR:\n+\n+```\n+./gradlew -p examples/kafka-to-pubsub clean shadowJar\n+```\n+\n+\u2139\ufe0f An **Uber JAR** - also known as **fat JAR** - is a single JAR file that contains\n+both target package *and* all its dependencies.\n+\n+The result of the `shadowJar` task execution is a `.jar` file that is generated\n+under the `build/libs/` folder in kafka-to-pubsub directory.\n+\n+## Local execution\n+To execute this pipeline locally, specify the parameters:\n+- Kafka Bootstrap servers\n+- Kafka input topics\n+- Pub/Sub output topic\n+- Output format\n+\n+in the following format:\n+```bash\n+--bootstrapServers=host:port \\\n+--inputTopics=your-input-topic \\\n+--outputTopic=projects/your-project-id/topics/your-topic-pame \\\n+--outputFormat=AVRO|PUBSUB\n+```\n+Optionally, to retrieve Kafka credentials for SASL/SCRAM,\n+specify a URL to the credentials in HashiCorp Vault and the vault access token:\n+```bash\n+--secretStoreUrl=http(s)://host:port/path/to/credentials\n+--vaultToken=your-token\n+```\n+Optionally, to configure secure SSL connection between the Beam pipeline and Kafka,\n+specify the parameters:\n+- A path to a truststore file (it can be a local path or a GCS path, which should start with `gs://`)\n+- A path to a keystore file (it can be a local path or a GCS path, which should start with `gs://`)\n+- Truststore password\n+- Keystore password\n+- Key password\n+```bash\n+--truststorePath=path/to/kafka.truststore.jks\n+--keystorePath=path/to/kafka.keystore.jks\n+--truststorePassword=your-truststore-password\n+--keystorePassword=your-keystore-password\n+--keyPassword=your-key-password\n+```\n+To change the runner, specify:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4372d6ff56bdef40e106f358adb1881cd3032e6f"}, "originalPosition": 120}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4MTY4NjY3OnYy", "diffSide": "RIGHT", "path": "examples/kafka-to-pubsub/README.md", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzo0Nzo1M1rOIBs66Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxNjo1ODo0MFrOICfORw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY1NTQ2NQ==", "bodyText": "Let's call this just \"Running the pipeline\", since it also describes how to run on other runners, not just locally.\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            ## Local execution\n          \n          \n            \n            ## Running the pipeline\n          \n      \n    \n    \n  \n\nTo be clear, users would normally use this approach to run on Dataflow just like any other runner, but the way this is written it looks like you have to use the approach in the next section. In fact the next section is for running the pipeline as a Dataflow template. For that reason please rename the \"Google Dataflow Execution\" section to \"Running as a Dataflow Template\".", "url": "https://github.com/apache/beam/pull/13112#discussion_r538655465", "createdAt": "2020-12-08T17:47:53Z", "author": {"login": "TheNeuralBit"}, "path": "examples/kafka-to-pubsub/README.md", "diffHunk": "@@ -0,0 +1,163 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam pipeline example to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) pipeline example that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Cloud Pub/Sub](https://cloud.google.com/pubsub).\n+\n+Supported data formats:\n+- Serializable plaintext formats, such as JSON\n+- [PubSubMessage](https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage).\n+\n+Supported input source configurations:\n+- Single or multiple Apache Kafka bootstrap servers\n+- Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection\n+- Secrets vault service [HashiCorp Vault](https://www.vaultproject.io/).\n+\n+Supported destination configuration:\n+- Single Google Cloud Pub/Sub topic.\n+\n+In a simple scenario, the example will create an Apache Beam pipeline that will read messages from a source Kafka server with a source topic, and stream the text messages into specified Pub/Sub destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed over plain text or SSL encrypted connection. The example supports using a single Kafka user account to authenticate in the provided source Kafka servers and topics. To support SASL authenticaton over SSL the example will need an SSL certificate location and access to a secrets vault service with Kafka username and password, currently supporting HashiCorp Vault.\n+\n+## Requirements\n+\n+- Java 8\n+- Kafka Bootstrap Server(s) up and running\n+- Existing source Kafka topic(s)\n+- An existing Pub/Sub destination output topic\n+- (Optional) An existing HashiCorp Vault\n+- (Optional) A configured secure SSL connection for Kafka\n+\n+## Getting Started\n+\n+This section describes what is needed to get the exaple up and running.\n+- Assembling the Uber-JAR\n+- Local execution\n+- Google Dataflow Template\n+  - Set up the environment\n+  - Creating the Dataflow Flex Template\n+  - Create a Dataflow job to ingest data using the template\n+- Avro format transferring.\n+- E2E tests (TBD)\n+\n+## Assembling the Uber-JAR\n+\n+To run this example the Java project should be built into\n+an Uber JAR file.\n+\n+Navigate to the Beam folder:\n+\n+```\n+cd /path/to/beam\n+```\n+\n+In order to create Uber JAR with Gradle, [Shadow plugin](https://github.com/johnrengelman/shadow)\n+is used. It creates the `shadowJar` task that builds the Uber JAR:\n+\n+```\n+./gradlew -p examples/kafka-to-pubsub clean shadowJar\n+```\n+\n+\u2139\ufe0f An **Uber JAR** - also known as **fat JAR** - is a single JAR file that contains\n+both target package *and* all its dependencies.\n+\n+The result of the `shadowJar` task execution is a `.jar` file that is generated\n+under the `build/libs/` folder in kafka-to-pubsub directory.\n+\n+## Local execution", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4372d6ff56bdef40e106f358adb1881cd3032e6f"}, "originalPosition": 86}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ3OTYyMw==", "bodyText": "Renamed both sections", "url": "https://github.com/apache/beam/pull/13112#discussion_r539479623", "createdAt": "2020-12-09T16:58:40Z", "author": {"login": "KhaninArtur"}, "path": "examples/kafka-to-pubsub/README.md", "diffHunk": "@@ -0,0 +1,163 @@\n+<!--\n+    Licensed to the Apache Software Foundation (ASF) under one\n+    or more contributor license agreements.  See the NOTICE file\n+    distributed with this work for additional information\n+    regarding copyright ownership.  The ASF licenses this file\n+    to you under the Apache License, Version 2.0 (the\n+    \"License\"); you may not use this file except in compliance\n+    with the License.  You may obtain a copy of the License at\n+\n+      http://www.apache.org/licenses/LICENSE-2.0\n+\n+    Unless required by applicable law or agreed to in writing,\n+    software distributed under the License is distributed on an\n+    \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n+    KIND, either express or implied.  See the License for the\n+    specific language governing permissions and limitations\n+    under the License.\n+-->\n+\n+# Apache Beam pipeline example to ingest data from Apache Kafka to Google Cloud Pub/Sub\n+\n+This directory contains an [Apache Beam](https://beam.apache.org/) pipeline example that creates a pipeline\n+to read data from a single or multiple topics from\n+[Apache Kafka](https://kafka.apache.org/) and write data into a single topic\n+in [Google Cloud Pub/Sub](https://cloud.google.com/pubsub).\n+\n+Supported data formats:\n+- Serializable plaintext formats, such as JSON\n+- [PubSubMessage](https://cloud.google.com/pubsub/docs/reference/rest/v1/PubsubMessage).\n+\n+Supported input source configurations:\n+- Single or multiple Apache Kafka bootstrap servers\n+- Apache Kafka SASL/SCRAM authentication over plaintext or SSL connection\n+- Secrets vault service [HashiCorp Vault](https://www.vaultproject.io/).\n+\n+Supported destination configuration:\n+- Single Google Cloud Pub/Sub topic.\n+\n+In a simple scenario, the example will create an Apache Beam pipeline that will read messages from a source Kafka server with a source topic, and stream the text messages into specified Pub/Sub destination topic. Other scenarios may need Kafka SASL/SCRAM authentication, that can be performed over plain text or SSL encrypted connection. The example supports using a single Kafka user account to authenticate in the provided source Kafka servers and topics. To support SASL authenticaton over SSL the example will need an SSL certificate location and access to a secrets vault service with Kafka username and password, currently supporting HashiCorp Vault.\n+\n+## Requirements\n+\n+- Java 8\n+- Kafka Bootstrap Server(s) up and running\n+- Existing source Kafka topic(s)\n+- An existing Pub/Sub destination output topic\n+- (Optional) An existing HashiCorp Vault\n+- (Optional) A configured secure SSL connection for Kafka\n+\n+## Getting Started\n+\n+This section describes what is needed to get the exaple up and running.\n+- Assembling the Uber-JAR\n+- Local execution\n+- Google Dataflow Template\n+  - Set up the environment\n+  - Creating the Dataflow Flex Template\n+  - Create a Dataflow job to ingest data using the template\n+- Avro format transferring.\n+- E2E tests (TBD)\n+\n+## Assembling the Uber-JAR\n+\n+To run this example the Java project should be built into\n+an Uber JAR file.\n+\n+Navigate to the Beam folder:\n+\n+```\n+cd /path/to/beam\n+```\n+\n+In order to create Uber JAR with Gradle, [Shadow plugin](https://github.com/johnrengelman/shadow)\n+is used. It creates the `shadowJar` task that builds the Uber JAR:\n+\n+```\n+./gradlew -p examples/kafka-to-pubsub clean shadowJar\n+```\n+\n+\u2139\ufe0f An **Uber JAR** - also known as **fat JAR** - is a single JAR file that contains\n+both target package *and* all its dependencies.\n+\n+The result of the `shadowJar` task execution is a `.jar` file that is generated\n+under the `build/libs/` folder in kafka-to-pubsub directory.\n+\n+## Local execution", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY1NTQ2NQ=="}, "originalCommit": {"oid": "4372d6ff56bdef40e106f358adb1881cd3032e6f"}, "originalPosition": 86}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4MTc0MjkwOnYy", "diffSide": "RIGHT", "path": "examples/kafka-to-pubsub/src/main/java/org/apache/beam/examples/KafkaToPubsub.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzo1NjozNVrOIBtfVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxNjo1ODo1OVrOICfPYA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY2NDc4OA==", "bodyText": "Please remove the Dataflow template specific parts from this javadoc", "url": "https://github.com/apache/beam/pull/13112#discussion_r538664788", "createdAt": "2020-12-08T17:56:35Z", "author": {"login": "TheNeuralBit"}, "path": "examples/kafka-to-pubsub/src/main/java/org/apache/beam/examples/KafkaToPubsub.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.examples;\n+\n+import static org.apache.beam.examples.kafka.consumer.Utils.configureKafka;\n+import static org.apache.beam.examples.kafka.consumer.Utils.configureSsl;\n+import static org.apache.beam.examples.kafka.consumer.Utils.getKafkaCredentialsFromVault;\n+import static org.apache.beam.examples.kafka.consumer.Utils.isSslSpecified;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.beam.examples.avro.AvroDataClass;\n+import org.apache.beam.examples.avro.AvroDataClassKafkaAvroDeserializer;\n+import org.apache.beam.examples.options.KafkaToPubsubOptions;\n+import org.apache.beam.examples.transforms.FormatTransform;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult;\n+import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.Values;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The {@link KafkaToPubsub} pipeline is a streaming pipeline which ingests data in JSON format from\n+ * Kafka, and outputs the resulting records to PubSub. Input topics, output topic, Bootstrap servers\n+ * are specified by the user as template parameters. <br>\n+ * Kafka may be configured with SASL/SCRAM security mechanism, in this case a Vault secret storage\n+ * with credentials should be provided. URL to credentials and Vault token are specified by the user\n+ * as template parameters.\n+ *\n+ * <p><b>Pipeline Requirements</b>\n+ *\n+ * <ul>\n+ *   <li>Kafka Bootstrap Server(s).\n+ *   <li>Kafka Topic(s) exists.\n+ *   <li>The PubSub output topic exists.\n+ *   <li>(Optional) An existing HashiCorp Vault secret storage\n+ * </ul>\n+ *\n+ * <p><b>Example Usage</b>\n+ *\n+ * <pre>\n+ * # Set the pipeline vars\n+ * PROJECT=id-of-my-project\n+ * BUCKET_NAME=my-bucket\n+ *\n+ * # Set containerization vars\n+ * IMAGE_NAME=my-image-name\n+ * TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+ * BASE_CONTAINER_IMAGE=my-base-container-image\n+ * TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+ *\n+ * # Create bucket in the cloud storage\n+ * gsutil mb gs://${BUCKET_NAME}\n+ *\n+ * # Go to the beam folder\n+ * cd /path/to/beam\n+ *\n+ * <b>FLEX TEMPLATE</b>\n+ * # Assemble uber-jar\n+ * ./gradlew -p templates/kafka-to-pubsub clean shadowJar\n+ *\n+ * # Go to the template folder\n+ * cd /path/to/beam/templates/kafka-to-pubsub\n+ *\n+ * # Build the flex template\n+ * gcloud dataflow flex-template build ${TEMPLATE_PATH} \\", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4372d6ff56bdef40e106f358adb1881cd3032e6f"}, "originalPosition": 87}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ3OTkwNA==", "bodyText": "Removed", "url": "https://github.com/apache/beam/pull/13112#discussion_r539479904", "createdAt": "2020-12-09T16:58:59Z", "author": {"login": "KhaninArtur"}, "path": "examples/kafka-to-pubsub/src/main/java/org/apache/beam/examples/KafkaToPubsub.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.examples;\n+\n+import static org.apache.beam.examples.kafka.consumer.Utils.configureKafka;\n+import static org.apache.beam.examples.kafka.consumer.Utils.configureSsl;\n+import static org.apache.beam.examples.kafka.consumer.Utils.getKafkaCredentialsFromVault;\n+import static org.apache.beam.examples.kafka.consumer.Utils.isSslSpecified;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.beam.examples.avro.AvroDataClass;\n+import org.apache.beam.examples.avro.AvroDataClassKafkaAvroDeserializer;\n+import org.apache.beam.examples.options.KafkaToPubsubOptions;\n+import org.apache.beam.examples.transforms.FormatTransform;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult;\n+import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.Values;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The {@link KafkaToPubsub} pipeline is a streaming pipeline which ingests data in JSON format from\n+ * Kafka, and outputs the resulting records to PubSub. Input topics, output topic, Bootstrap servers\n+ * are specified by the user as template parameters. <br>\n+ * Kafka may be configured with SASL/SCRAM security mechanism, in this case a Vault secret storage\n+ * with credentials should be provided. URL to credentials and Vault token are specified by the user\n+ * as template parameters.\n+ *\n+ * <p><b>Pipeline Requirements</b>\n+ *\n+ * <ul>\n+ *   <li>Kafka Bootstrap Server(s).\n+ *   <li>Kafka Topic(s) exists.\n+ *   <li>The PubSub output topic exists.\n+ *   <li>(Optional) An existing HashiCorp Vault secret storage\n+ * </ul>\n+ *\n+ * <p><b>Example Usage</b>\n+ *\n+ * <pre>\n+ * # Set the pipeline vars\n+ * PROJECT=id-of-my-project\n+ * BUCKET_NAME=my-bucket\n+ *\n+ * # Set containerization vars\n+ * IMAGE_NAME=my-image-name\n+ * TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+ * BASE_CONTAINER_IMAGE=my-base-container-image\n+ * TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+ *\n+ * # Create bucket in the cloud storage\n+ * gsutil mb gs://${BUCKET_NAME}\n+ *\n+ * # Go to the beam folder\n+ * cd /path/to/beam\n+ *\n+ * <b>FLEX TEMPLATE</b>\n+ * # Assemble uber-jar\n+ * ./gradlew -p templates/kafka-to-pubsub clean shadowJar\n+ *\n+ * # Go to the template folder\n+ * cd /path/to/beam/templates/kafka-to-pubsub\n+ *\n+ * # Build the flex template\n+ * gcloud dataflow flex-template build ${TEMPLATE_PATH} \\", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY2NDc4OA=="}, "originalCommit": {"oid": "4372d6ff56bdef40e106f358adb1881cd3032e6f"}, "originalPosition": 87}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzM4MTc1ODQxOnYy", "diffSide": "RIGHT", "path": "examples/kafka-to-pubsub/src/main/java/org/apache/beam/examples/KafkaToPubsub.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOFQxNzo1ODo1MFrOIBtpAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0wOVQxNzowNzozNFrOICfqJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY2NzI2NQ==", "bodyText": "Is it worth having this PUBSUB path? The README and javadoc only discuss the AVRO path. I think we should just have that one and remove the enum", "url": "https://github.com/apache/beam/pull/13112#discussion_r538667265", "createdAt": "2020-12-08T17:58:50Z", "author": {"login": "TheNeuralBit"}, "path": "examples/kafka-to-pubsub/src/main/java/org/apache/beam/examples/KafkaToPubsub.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.examples;\n+\n+import static org.apache.beam.examples.kafka.consumer.Utils.configureKafka;\n+import static org.apache.beam.examples.kafka.consumer.Utils.configureSsl;\n+import static org.apache.beam.examples.kafka.consumer.Utils.getKafkaCredentialsFromVault;\n+import static org.apache.beam.examples.kafka.consumer.Utils.isSslSpecified;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.beam.examples.avro.AvroDataClass;\n+import org.apache.beam.examples.avro.AvroDataClassKafkaAvroDeserializer;\n+import org.apache.beam.examples.options.KafkaToPubsubOptions;\n+import org.apache.beam.examples.transforms.FormatTransform;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult;\n+import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.Values;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The {@link KafkaToPubsub} pipeline is a streaming pipeline which ingests data in JSON format from\n+ * Kafka, and outputs the resulting records to PubSub. Input topics, output topic, Bootstrap servers\n+ * are specified by the user as template parameters. <br>\n+ * Kafka may be configured with SASL/SCRAM security mechanism, in this case a Vault secret storage\n+ * with credentials should be provided. URL to credentials and Vault token are specified by the user\n+ * as template parameters.\n+ *\n+ * <p><b>Pipeline Requirements</b>\n+ *\n+ * <ul>\n+ *   <li>Kafka Bootstrap Server(s).\n+ *   <li>Kafka Topic(s) exists.\n+ *   <li>The PubSub output topic exists.\n+ *   <li>(Optional) An existing HashiCorp Vault secret storage\n+ * </ul>\n+ *\n+ * <p><b>Example Usage</b>\n+ *\n+ * <pre>\n+ * # Set the pipeline vars\n+ * PROJECT=id-of-my-project\n+ * BUCKET_NAME=my-bucket\n+ *\n+ * # Set containerization vars\n+ * IMAGE_NAME=my-image-name\n+ * TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+ * BASE_CONTAINER_IMAGE=my-base-container-image\n+ * TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+ *\n+ * # Create bucket in the cloud storage\n+ * gsutil mb gs://${BUCKET_NAME}\n+ *\n+ * # Go to the beam folder\n+ * cd /path/to/beam\n+ *\n+ * <b>FLEX TEMPLATE</b>\n+ * # Assemble uber-jar\n+ * ./gradlew -p templates/kafka-to-pubsub clean shadowJar\n+ *\n+ * # Go to the template folder\n+ * cd /path/to/beam/templates/kafka-to-pubsub\n+ *\n+ * # Build the flex template\n+ * gcloud dataflow flex-template build ${TEMPLATE_PATH} \\\n+ *       --image-gcr-path \"${TARGET_GCR_IMAGE}\" \\\n+ *       --sdk-language \"JAVA\" \\\n+ *       --flex-template-base-image ${BASE_CONTAINER_IMAGE} \\\n+ *       --metadata-file \"src/main/resources/kafka_to_pubsub_metadata.json\" \\\n+ *       --jar \"build/libs/beam-templates-kafka-to-pubsub-<version>-all.jar\" \\\n+ *       --env FLEX_TEMPLATE_JAVA_MAIN_CLASS=\"org.apache.beam.templates.KafkaToPubsub\"\n+ *\n+ * # Execute template:\n+ *    API_ROOT_URL=\"https://dataflow.googleapis.com\"\n+ *    TEMPLATES_LAUNCH_API=\"${API_ROOT_URL}/v1b3/projects/${PROJECT}/locations/${REGION}/flexTemplates:launch\"\n+ *    JOB_NAME=\"kafka-to-pubsub-`date +%Y%m%d-%H%M%S-%N`\"\n+ *\n+ *    time curl -X POST -H \"Content-Type: application/json\" \\\n+ *            -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n+ *            -d '\n+ *             {\n+ *                 \"launch_parameter\": {\n+ *                     \"jobName\": \"'$JOB_NAME'\",\n+ *                     \"containerSpecGcsPath\": \"'$TEMPLATE_PATH'\",\n+ *                     \"parameters\": {\n+ *                         \"bootstrapServers\": \"broker_1:9091, broker_2:9092\",\n+ *                         \"inputTopics\": \"topic1, topic2\",\n+ *                         \"outputTopic\": \"projects/'$PROJECT'/topics/your-topic-name\",\n+ *                         \"secretStoreUrl\": \"http(s)://host:port/path/to/credentials\",\n+ *                         \"vaultToken\": \"your-token\"\n+ *                     }\n+ *                 }\n+ *             }\n+ *            '\n+ *            \"${TEMPLATES_LAUNCH_API}\"\n+ * </pre>\n+ *\n+ * <p><b>Example Avro usage</b>\n+ *\n+ * <pre>\n+ * This template contains an example Class to deserialize AVRO from Kafka and serialize it to AVRO in Pub/Sub.\n+ *\n+ * To use this example in the specific case, follow the few steps:\n+ * <ul>\n+ * <li> Create your own class to describe AVRO schema. As an example use {@link AvroDataClass}. Just define necessary fields.\n+ * <li> Create your own Avro Deserializer class. As an example use {@link AvroDataClassKafkaAvroDeserializer}. Just rename it, and put your own Schema class as the necessary types.\n+ * <li> Modify the {@link FormatTransform}. Put your Schema class and Deserializer to the related parameter.\n+ * <li> Modify write step in the {@link KafkaToPubsub} by put your Schema class to \"writeAvrosToPubSub\" step.\n+ * </ul>\n+ * </pre>\n+ */\n+public class KafkaToPubsub {\n+\n+  /* Logger for class.*/\n+  private static final Logger LOG = LoggerFactory.getLogger(KafkaToPubsub.class);\n+\n+  /**\n+   * Main entry point for pipeline execution.\n+   *\n+   * @param args Command line arguments to the pipeline.\n+   */\n+  public static void main(String[] args) {\n+    KafkaToPubsubOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(KafkaToPubsubOptions.class);\n+\n+    run(options);\n+  }\n+\n+  /**\n+   * Runs a pipeline which reads message from Kafka and writes it to GCS.\n+   *\n+   * @param options arguments to the pipeline\n+   */\n+  public static PipelineResult run(KafkaToPubsubOptions options) {\n+    // Configure Kafka consumer properties\n+    Map<String, Object> kafkaConfig = new HashMap<>();\n+    Map<String, String> sslConfig = new HashMap<>();\n+    if (options.getSecretStoreUrl() != null && options.getVaultToken() != null) {\n+      Map<String, Map<String, String>> credentials =\n+          getKafkaCredentialsFromVault(options.getSecretStoreUrl(), options.getVaultToken());\n+      kafkaConfig = configureKafka(credentials.get(KafkaPubsubConstants.KAFKA_CREDENTIALS));\n+    } else {\n+      LOG.warn(\n+          \"No information to retrieve Kafka credentials was provided. \"\n+              + \"Trying to initiate an unauthorized connection.\");\n+    }\n+\n+    if (isSslSpecified(options)) {\n+      sslConfig.putAll(configureSsl(options));\n+    } else {\n+      LOG.info(\n+          \"No information to retrieve SSL certificate was provided by parameters.\"\n+              + \"Trying to initiate a plain text connection.\");\n+    }\n+\n+    List<String> topicsList = new ArrayList<>(Arrays.asList(options.getInputTopics().split(\",\")));\n+\n+    checkArgument(\n+        topicsList.size() > 0 && topicsList.get(0).length() > 0,\n+        \"inputTopics cannot be an empty string.\");\n+\n+    List<String> bootstrapServersList =\n+        new ArrayList<>(Arrays.asList(options.getBootstrapServers().split(\",\")));\n+\n+    checkArgument(\n+        bootstrapServersList.size() > 0 && topicsList.get(0).length() > 0,\n+        \"bootstrapServers cannot be an empty string.\");\n+\n+    // Create the pipeline\n+    Pipeline pipeline = Pipeline.create(options);\n+    LOG.info(\n+        \"Starting Kafka-To-PubSub pipeline with parameters bootstrap servers:\"\n+            + options.getBootstrapServers()\n+            + \" input topics: \"\n+            + options.getInputTopics()\n+            + \" output pubsub topic: \"\n+            + options.getOutputTopic());\n+\n+    /*\n+     * Steps:\n+     *  1) Read messages in from Kafka\n+     *  2) Extract values only\n+     *  3) Write successful records to PubSub\n+     */\n+\n+    if (options.getOutputFormat() == FormatTransform.FORMAT.AVRO) {\n+      pipeline\n+          .apply(\n+              \"readAvrosFromKafka\",\n+              FormatTransform.readAvrosFromKafka(\n+                  options.getBootstrapServers(), topicsList, kafkaConfig, sslConfig))\n+          .apply(\"createValues\", Values.create())\n+          .apply(\"writeAvrosToPubSub\", PubsubIO.writeAvros(AvroDataClass.class));\n+\n+    } else {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "4372d6ff56bdef40e106f358adb1881cd3032e6f"}, "originalPosition": 217}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzOTQ4Njc1OQ==", "bodyText": "I see why it may seem invaluable for the example, thank you for noticing this!\nI suppose it is worth having the PUBSUB path because this example works out-of-the-box with it. For the AVRO path, the user has to add some code to make it work and also to understand what and how should be changed - the PUBSUB path doesn't require it.\nI also updated the README file to highlight the value of it.", "url": "https://github.com/apache/beam/pull/13112#discussion_r539486759", "createdAt": "2020-12-09T17:07:34Z", "author": {"login": "KhaninArtur"}, "path": "examples/kafka-to-pubsub/src/main/java/org/apache/beam/examples/KafkaToPubsub.java", "diffHunk": "@@ -0,0 +1,229 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.examples;\n+\n+import static org.apache.beam.examples.kafka.consumer.Utils.configureKafka;\n+import static org.apache.beam.examples.kafka.consumer.Utils.configureSsl;\n+import static org.apache.beam.examples.kafka.consumer.Utils.getKafkaCredentialsFromVault;\n+import static org.apache.beam.examples.kafka.consumer.Utils.isSslSpecified;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.ArrayList;\n+import java.util.Arrays;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import org.apache.beam.examples.avro.AvroDataClass;\n+import org.apache.beam.examples.avro.AvroDataClassKafkaAvroDeserializer;\n+import org.apache.beam.examples.options.KafkaToPubsubOptions;\n+import org.apache.beam.examples.transforms.FormatTransform;\n+import org.apache.beam.sdk.Pipeline;\n+import org.apache.beam.sdk.PipelineResult;\n+import org.apache.beam.sdk.io.gcp.pubsub.PubsubIO;\n+import org.apache.beam.sdk.options.PipelineOptionsFactory;\n+import org.apache.beam.sdk.transforms.Values;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * The {@link KafkaToPubsub} pipeline is a streaming pipeline which ingests data in JSON format from\n+ * Kafka, and outputs the resulting records to PubSub. Input topics, output topic, Bootstrap servers\n+ * are specified by the user as template parameters. <br>\n+ * Kafka may be configured with SASL/SCRAM security mechanism, in this case a Vault secret storage\n+ * with credentials should be provided. URL to credentials and Vault token are specified by the user\n+ * as template parameters.\n+ *\n+ * <p><b>Pipeline Requirements</b>\n+ *\n+ * <ul>\n+ *   <li>Kafka Bootstrap Server(s).\n+ *   <li>Kafka Topic(s) exists.\n+ *   <li>The PubSub output topic exists.\n+ *   <li>(Optional) An existing HashiCorp Vault secret storage\n+ * </ul>\n+ *\n+ * <p><b>Example Usage</b>\n+ *\n+ * <pre>\n+ * # Set the pipeline vars\n+ * PROJECT=id-of-my-project\n+ * BUCKET_NAME=my-bucket\n+ *\n+ * # Set containerization vars\n+ * IMAGE_NAME=my-image-name\n+ * TARGET_GCR_IMAGE=gcr.io/${PROJECT}/${IMAGE_NAME}\n+ * BASE_CONTAINER_IMAGE=my-base-container-image\n+ * TEMPLATE_PATH=\"gs://${BUCKET_NAME}/templates/kafka-pubsub.json\"\n+ *\n+ * # Create bucket in the cloud storage\n+ * gsutil mb gs://${BUCKET_NAME}\n+ *\n+ * # Go to the beam folder\n+ * cd /path/to/beam\n+ *\n+ * <b>FLEX TEMPLATE</b>\n+ * # Assemble uber-jar\n+ * ./gradlew -p templates/kafka-to-pubsub clean shadowJar\n+ *\n+ * # Go to the template folder\n+ * cd /path/to/beam/templates/kafka-to-pubsub\n+ *\n+ * # Build the flex template\n+ * gcloud dataflow flex-template build ${TEMPLATE_PATH} \\\n+ *       --image-gcr-path \"${TARGET_GCR_IMAGE}\" \\\n+ *       --sdk-language \"JAVA\" \\\n+ *       --flex-template-base-image ${BASE_CONTAINER_IMAGE} \\\n+ *       --metadata-file \"src/main/resources/kafka_to_pubsub_metadata.json\" \\\n+ *       --jar \"build/libs/beam-templates-kafka-to-pubsub-<version>-all.jar\" \\\n+ *       --env FLEX_TEMPLATE_JAVA_MAIN_CLASS=\"org.apache.beam.templates.KafkaToPubsub\"\n+ *\n+ * # Execute template:\n+ *    API_ROOT_URL=\"https://dataflow.googleapis.com\"\n+ *    TEMPLATES_LAUNCH_API=\"${API_ROOT_URL}/v1b3/projects/${PROJECT}/locations/${REGION}/flexTemplates:launch\"\n+ *    JOB_NAME=\"kafka-to-pubsub-`date +%Y%m%d-%H%M%S-%N`\"\n+ *\n+ *    time curl -X POST -H \"Content-Type: application/json\" \\\n+ *            -H \"Authorization: Bearer $(gcloud auth print-access-token)\" \\\n+ *            -d '\n+ *             {\n+ *                 \"launch_parameter\": {\n+ *                     \"jobName\": \"'$JOB_NAME'\",\n+ *                     \"containerSpecGcsPath\": \"'$TEMPLATE_PATH'\",\n+ *                     \"parameters\": {\n+ *                         \"bootstrapServers\": \"broker_1:9091, broker_2:9092\",\n+ *                         \"inputTopics\": \"topic1, topic2\",\n+ *                         \"outputTopic\": \"projects/'$PROJECT'/topics/your-topic-name\",\n+ *                         \"secretStoreUrl\": \"http(s)://host:port/path/to/credentials\",\n+ *                         \"vaultToken\": \"your-token\"\n+ *                     }\n+ *                 }\n+ *             }\n+ *            '\n+ *            \"${TEMPLATES_LAUNCH_API}\"\n+ * </pre>\n+ *\n+ * <p><b>Example Avro usage</b>\n+ *\n+ * <pre>\n+ * This template contains an example Class to deserialize AVRO from Kafka and serialize it to AVRO in Pub/Sub.\n+ *\n+ * To use this example in the specific case, follow the few steps:\n+ * <ul>\n+ * <li> Create your own class to describe AVRO schema. As an example use {@link AvroDataClass}. Just define necessary fields.\n+ * <li> Create your own Avro Deserializer class. As an example use {@link AvroDataClassKafkaAvroDeserializer}. Just rename it, and put your own Schema class as the necessary types.\n+ * <li> Modify the {@link FormatTransform}. Put your Schema class and Deserializer to the related parameter.\n+ * <li> Modify write step in the {@link KafkaToPubsub} by put your Schema class to \"writeAvrosToPubSub\" step.\n+ * </ul>\n+ * </pre>\n+ */\n+public class KafkaToPubsub {\n+\n+  /* Logger for class.*/\n+  private static final Logger LOG = LoggerFactory.getLogger(KafkaToPubsub.class);\n+\n+  /**\n+   * Main entry point for pipeline execution.\n+   *\n+   * @param args Command line arguments to the pipeline.\n+   */\n+  public static void main(String[] args) {\n+    KafkaToPubsubOptions options =\n+        PipelineOptionsFactory.fromArgs(args).withValidation().as(KafkaToPubsubOptions.class);\n+\n+    run(options);\n+  }\n+\n+  /**\n+   * Runs a pipeline which reads message from Kafka and writes it to GCS.\n+   *\n+   * @param options arguments to the pipeline\n+   */\n+  public static PipelineResult run(KafkaToPubsubOptions options) {\n+    // Configure Kafka consumer properties\n+    Map<String, Object> kafkaConfig = new HashMap<>();\n+    Map<String, String> sslConfig = new HashMap<>();\n+    if (options.getSecretStoreUrl() != null && options.getVaultToken() != null) {\n+      Map<String, Map<String, String>> credentials =\n+          getKafkaCredentialsFromVault(options.getSecretStoreUrl(), options.getVaultToken());\n+      kafkaConfig = configureKafka(credentials.get(KafkaPubsubConstants.KAFKA_CREDENTIALS));\n+    } else {\n+      LOG.warn(\n+          \"No information to retrieve Kafka credentials was provided. \"\n+              + \"Trying to initiate an unauthorized connection.\");\n+    }\n+\n+    if (isSslSpecified(options)) {\n+      sslConfig.putAll(configureSsl(options));\n+    } else {\n+      LOG.info(\n+          \"No information to retrieve SSL certificate was provided by parameters.\"\n+              + \"Trying to initiate a plain text connection.\");\n+    }\n+\n+    List<String> topicsList = new ArrayList<>(Arrays.asList(options.getInputTopics().split(\",\")));\n+\n+    checkArgument(\n+        topicsList.size() > 0 && topicsList.get(0).length() > 0,\n+        \"inputTopics cannot be an empty string.\");\n+\n+    List<String> bootstrapServersList =\n+        new ArrayList<>(Arrays.asList(options.getBootstrapServers().split(\",\")));\n+\n+    checkArgument(\n+        bootstrapServersList.size() > 0 && topicsList.get(0).length() > 0,\n+        \"bootstrapServers cannot be an empty string.\");\n+\n+    // Create the pipeline\n+    Pipeline pipeline = Pipeline.create(options);\n+    LOG.info(\n+        \"Starting Kafka-To-PubSub pipeline with parameters bootstrap servers:\"\n+            + options.getBootstrapServers()\n+            + \" input topics: \"\n+            + options.getInputTopics()\n+            + \" output pubsub topic: \"\n+            + options.getOutputTopic());\n+\n+    /*\n+     * Steps:\n+     *  1) Read messages in from Kafka\n+     *  2) Extract values only\n+     *  3) Write successful records to PubSub\n+     */\n+\n+    if (options.getOutputFormat() == FormatTransform.FORMAT.AVRO) {\n+      pipeline\n+          .apply(\n+              \"readAvrosFromKafka\",\n+              FormatTransform.readAvrosFromKafka(\n+                  options.getBootstrapServers(), topicsList, kafkaConfig, sslConfig))\n+          .apply(\"createValues\", Values.create())\n+          .apply(\"writeAvrosToPubSub\", PubsubIO.writeAvros(AvroDataClass.class));\n+\n+    } else {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDUzODY2NzI2NQ=="}, "originalCommit": {"oid": "4372d6ff56bdef40e106f358adb1881cd3032e6f"}, "originalPosition": 217}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2978, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}