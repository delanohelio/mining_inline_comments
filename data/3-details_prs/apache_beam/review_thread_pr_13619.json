{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NTQ2MDA3NzYz", "number": 13619, "reviewThreads": {"totalCount": 14, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzoxODo0MFrOFJhoKA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzo1NzoyMVrOFJiIxg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NTMyNDU2OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzoxODo0MFrOIL5Xag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzoxODo0MFrOIL5Xag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0NTEzMA==", "bodyText": "Can you please remove this method and replace its uses with setConfiguration(makeHadoopConfiguration(...))", "url": "https://github.com/apache/beam/pull/13619#discussion_r549345130", "createdAt": "2020-12-28T13:18:40Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -311,6 +313,12 @@ public static ReadFiles readFiles(Schema schema) {\n \n       abstract Builder setAvroDataModel(GenericData model);\n \n+      abstract Builder setConfiguration(SerializableConfiguration configuration);\n+\n+      Builder setHadoopConfigurationFlags(Map<String, String> flags) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 15}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NTMyOTk0OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzoyMToxMlrOIL5aYA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzoyMToxMlrOIL5aYA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0NTg4OA==", "bodyText": "Please remove all definitions of this method and replace its uses with setConfiguration(makeHadoopConfiguration(...)) in all classes where it appears", "url": "https://github.com/apache/beam/pull/13619#discussion_r549345888", "createdAt": "2020-12-28T13:21:12Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -388,6 +402,12 @@ public void populateDisplayData(DisplayData.Builder builder) {\n \n       abstract Builder<T> setParseFn(SerializableFunction<GenericRecord, T> parseFn);\n \n+      abstract Builder<T> setConfiguration(SerializableConfiguration configuration);\n+\n+      Builder<T> setHadoopConfigurationFlags(Map<String, String> flags) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 48}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NTMzNjI2OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzoyNDowN1rOIL5dng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzoyNDowN1rOIL5dng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0NjcxOA==", "bodyText": "rename to configuration", "url": "https://github.com/apache/beam/pull/13619#discussion_r549346718", "createdAt": "2020-12-28T13:24:07Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -564,14 +623,20 @@ public ReadFiles withSplit() {\n       // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n       private static final long SPLIT_LIMIT = 64000000;\n \n+      private final SerializableConfiguration hadoopBaseConfig;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 177}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NTMzNjU0OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzoyNDoxOFrOIL5dyg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzoyNDoxOFrOIL5dyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0Njc2Mg==", "bodyText": "rename to configuration", "url": "https://github.com/apache/beam/pull/13619#discussion_r549346762", "createdAt": "2020-12-28T13:24:18Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -564,14 +623,20 @@ public ReadFiles withSplit() {\n       // Default initial splitting the file into blocks of 64MB. Unit of SPLIT_LIMIT is byte.\n       private static final long SPLIT_LIMIT = 64000000;\n \n+      private final SerializableConfiguration hadoopBaseConfig;\n+\n       private final SerializableFunction<GenericRecord, T> parseFn;\n \n       SplitReadFn(\n-          GenericData model, Schema requestSchema, SerializableFunction<GenericRecord, T> parseFn) {\n+          GenericData model,\n+          Schema requestSchema,\n+          SerializableFunction<GenericRecord, T> parseFn,\n+          SerializableConfiguration hadoopBaseConfig) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 186}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NTMzODYwOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzoyNToxM1rOIL5e6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzoyNToxM1rOIL5e6w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0NzA1MQ==", "bodyText": "rename to configuration", "url": "https://github.com/apache/beam/pull/13619#discussion_r549347051", "createdAt": "2020-12-28T13:25:13Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -819,9 +884,15 @@ public Progress getProgress() {\n \n       private final SerializableFunction<GenericRecord, T> parseFn;\n \n-      ReadFn(GenericData model, SerializableFunction<GenericRecord, T> parseFn) {\n+      private final SerializableConfiguration hadoopBaseConfig;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 209}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NTM0MTgwOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzoyNjozM1rOIL5gsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzoyNjozM1rOIL5gsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0NzUwNQ==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/apache/beam/pull/13619#discussion_r549347505", "createdAt": "2020-12-28T13:26:33Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -920,13 +996,7 @@ public Sink withCompressionCodec(CompressionCodecName compressionCodecName) {\n \n     /** Specifies configuration to be passed into the sink's writer. */\n     public Sink withConfiguration(Map<String, String> configuration) {\n-      Configuration hadoopConfiguration = new Configuration();\n-      for (Map.Entry<String, String> entry : configuration.entrySet()) {\n-        hadoopConfiguration.set(entry.getKey(), entry.getValue());\n-      }\n-      return toBuilder()\n-          .setConfiguration(new SerializableConfiguration(hadoopConfiguration))\n-          .build();\n+      return toBuilder().setConfiguration(makeHadoopConfigurationUsingFlags(configuration)).build();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 253}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NTM1NjIyOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzozMzoxOFrOIL5ohQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwNDoxNTozNVrOIMGl6Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0OTUwOQ==", "bodyText": "Can we move this method into the SerializableConfiguration class and make it public static SerializableConfiguration fromMap(Map<String, string> entries) {", "url": "https://github.com/apache/beam/pull/13619#discussion_r549349509", "createdAt": "2020-12-28T13:33:18Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -1058,6 +1128,18 @@ public GenericRecord apply(GenericRecord input) {\n     private GenericRecordPassthroughFn() {}\n   }\n \n+  /** Returns a new Hadoop {@link Configuration} instance with provided flags. */\n+  private static SerializableConfiguration makeHadoopConfigurationUsingFlags(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 262}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU2MTgzMw==", "bodyText": "Done, added a single test for the new method.\nInitially I was contemplating this alternative but had discarded to reduce touching more files.", "url": "https://github.com/apache/beam/pull/13619#discussion_r549561833", "createdAt": "2020-12-29T04:15:35Z", "author": {"login": "anantdamle"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -1058,6 +1128,18 @@ public GenericRecord apply(GenericRecord input) {\n     private GenericRecordPassthroughFn() {}\n   }\n \n+  /** Returns a new Hadoop {@link Configuration} instance with provided flags. */\n+  private static SerializableConfiguration makeHadoopConfigurationUsingFlags(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0OTUwOQ=="}, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 262}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NTM1NzM4OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzozMzo0OFrOIL5pFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwNDoxNzo1OVrOIMGnew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0OTY1NQ==", "bodyText": "test or remove", "url": "https://github.com/apache/beam/pull/13619#discussion_r549349655", "createdAt": "2020-12-28T13:33:48Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java", "diffHunk": "@@ -416,6 +416,9 @@ public void testWriteAndReadwithSplitUsingReflectDataSchemaWithDataModel() {\n     readPipeline.run().waitUntilFinish();\n   }\n \n+  @Test\n+  public void testConfigurationReadFile() {}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 14}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU2MjIzNQ==", "bodyText": "Thanks, Removed.", "url": "https://github.com/apache/beam/pull/13619#discussion_r549562235", "createdAt": "2020-12-29T04:17:59Z", "author": {"login": "anantdamle"}, "path": "sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java", "diffHunk": "@@ -416,6 +416,9 @@ public void testWriteAndReadwithSplitUsingReflectDataSchemaWithDataModel() {\n     readPipeline.run().waitUntilFinish();\n   }\n \n+  @Test\n+  public void testConfigurationReadFile() {}", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM0OTY1NQ=="}, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 14}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NTM2MDg3OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzozNToyM1rOIL5rAg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzozNToyM1rOIL5rAg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1MDE0Ng==", "bodyText": "can you please name the argument of the withConfiguration methods consistently everywhere as configuration instead of flags or hadoopConfigFlags", "url": "https://github.com/apache/beam/pull/13619#discussion_r549350146", "createdAt": "2020-12-28T13:35:23Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -332,6 +340,10 @@ public Read withProjection(Schema projectionSchema, Schema encoderSchema) {\n           .build();\n     }\n \n+    public Read withConfiguration(Map<String, String> flags) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 26}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NTM2ODEzOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzozODozOVrOIL5u-A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzozODozOVrOIL5u-A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1MTE2MA==", "bodyText": "Rename to withConfiguration to be consistent with the other methods + s/configurationFlags/configuration", "url": "https://github.com/apache/beam/pull/13619#discussion_r549351160", "createdAt": "2020-12-28T13:38:39Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -532,6 +581,12 @@ public ReadFiles withProjection(Schema projectionSchema, Schema encoderSchema) {\n           .setSplittable(true)\n           .build();\n     }\n+\n+    /** Specify Hadoop configuration for ParquetReader. */\n+    public ReadFiles withHadoopConfiguration(Map<String, String> configurationFlags) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 148}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NTM3ODI0OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzo0Mzo0NlrOIL505w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOVQwNDoxNDoxMlrOIMGlPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1MjY3OQ==", "bodyText": "We should probably define a default value inside of the builders (read, readFiles, parseGenericRecords, parseFilesGenericRecords)  .setConfiguration(...) and since we define a default value we won't need this if", "url": "https://github.com/apache/beam/pull/13619#discussion_r549352679", "createdAt": "2020-12-28T13:43:46Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -835,13 +906,18 @@ public void processElement(ProcessContext processContext) throws Exception {\n \n         SeekableByteChannel seekableByteChannel = file.openSeekable();\n \n-        AvroParquetReader.Builder builder =\n-            AvroParquetReader.<GenericRecord>builder(new BeamParquetInputFile(seekableByteChannel));\n+        AvroParquetReader.Builder<GenericRecord> builder =\n+            AvroParquetReader.builder(new BeamParquetInputFile(seekableByteChannel));\n         if (modelClass != null) {\n           // all GenericData implementations have a static get method\n           builder = builder.withDataModel((GenericData) modelClass.getMethod(\"get\").invoke(null));\n         }\n \n+        if (hadoopBaseConfig != null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 234}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU2MTY2MA==", "bodyText": "looking at the SerializableConfiguration#91 , It seems null is the expected value for building a default configuration.\nI've also replaced all configuration != null checks with SerializableConfiguration.newConfiguration(configuration) for consistency and to avoid NPE. What do you think?", "url": "https://github.com/apache/beam/pull/13619#discussion_r549561660", "createdAt": "2020-12-29T04:14:12Z", "author": {"login": "anantdamle"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -835,13 +906,18 @@ public void processElement(ProcessContext processContext) throws Exception {\n \n         SeekableByteChannel seekableByteChannel = file.openSeekable();\n \n-        AvroParquetReader.Builder builder =\n-            AvroParquetReader.<GenericRecord>builder(new BeamParquetInputFile(seekableByteChannel));\n+        AvroParquetReader.Builder<GenericRecord> builder =\n+            AvroParquetReader.builder(new BeamParquetInputFile(seekableByteChannel));\n         if (modelClass != null) {\n           // all GenericData implementations have a static get method\n           builder = builder.withDataModel((GenericData) modelClass.getMethod(\"get\").invoke(null));\n         }\n \n+        if (hadoopBaseConfig != null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1MjY3OQ=="}, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 234}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NTM3OTg2OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzo0NDozM1rOIL51zA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzo0NDozM1rOIL51zA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1MjkwOA==", "bodyText": "s/Hadoop {@link Configuration}/{@link SerializableConfiguration}", "url": "https://github.com/apache/beam/pull/13619#discussion_r549352908", "createdAt": "2020-12-28T13:44:33Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -1058,6 +1128,18 @@ public GenericRecord apply(GenericRecord input) {\n     private GenericRecordPassthroughFn() {}\n   }\n \n+  /** Returns a new Hadoop {@link Configuration} instance with provided flags. */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 261}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NTQwMTk0OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzo1NDo0NFrOIL6CAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0zMVQxMzo1ODoyMlrOIM_KgA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1NjAzMw==", "bodyText": "Test with new Configuration(), this should not be nullable", "url": "https://github.com/apache/beam/pull/13619#discussion_r549356033", "createdAt": "2020-12-28T13:54:44Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java", "diffHunk": "@@ -147,7 +147,7 @@ public void testBlockTracker() throws Exception {\n   public void testSplitBlockWithLimit() {\n     ParquetIO.ReadFiles.SplitReadFn<GenericRecord> testFn =\n         new ParquetIO.ReadFiles.SplitReadFn<>(\n-            null, null, ParquetIO.GenericRecordPassthroughFn.create());\n+            null, null, ParquetIO.GenericRecordPassthroughFn.create(), null);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTU2MjIwOQ==", "bodyText": "As I'm using SerializableConfiguration#newConfiguration it can be null.\nDo you want to me to add a test where a non-null configuration is tested?", "url": "https://github.com/apache/beam/pull/13619#discussion_r549562209", "createdAt": "2020-12-29T04:17:48Z", "author": {"login": "anantdamle"}, "path": "sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java", "diffHunk": "@@ -147,7 +147,7 @@ public void testBlockTracker() throws Exception {\n   public void testSplitBlockWithLimit() {\n     ParquetIO.ReadFiles.SplitReadFn<GenericRecord> testFn =\n         new ParquetIO.ReadFiles.SplitReadFn<>(\n-            null, null, ParquetIO.GenericRecordPassthroughFn.create());\n+            null, null, ParquetIO.GenericRecordPassthroughFn.create(), null);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1NjAzMw=="}, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU1MDQ4ODcwNA==", "bodyText": "I would prefer it to not be Nullable but since this is internal I suppose we can adjust this later, on the other hand if someday Parquet finally gets rid of its Hadoop dependencies probably the null value would align better.", "url": "https://github.com/apache/beam/pull/13619#discussion_r550488704", "createdAt": "2020-12-31T13:58:22Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/test/java/org/apache/beam/sdk/io/parquet/ParquetIOTest.java", "diffHunk": "@@ -147,7 +147,7 @@ public void testBlockTracker() throws Exception {\n   public void testSplitBlockWithLimit() {\n     ParquetIO.ReadFiles.SplitReadFn<GenericRecord> testFn =\n         new ParquetIO.ReadFiles.SplitReadFn<>(\n-            null, null, ParquetIO.GenericRecordPassthroughFn.create());\n+            null, null, ParquetIO.GenericRecordPassthroughFn.create(), null);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1NjAzMw=="}, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 5}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzQ1NTQwODA2OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzo1NzoyMVrOIL6Fag==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0xMi0yOFQxMzo1NzoyMVrOIL6Fag==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDU0OTM1NjkwNg==", "bodyText": "\ud83d\udc4d", "url": "https://github.com/apache/beam/pull/13619#discussion_r549356906", "createdAt": "2020-12-28T13:57:21Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -682,7 +747,7 @@ public void processElement(\n       }\n \n       public Configuration getConfWithModelClass() throws Exception {\n-        Configuration conf = new Configuration();\n+        Configuration conf = SerializableConfiguration.newConfiguration(hadoopBaseConfig);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a7301b751791308965715df247657a8755e75b13"}, "originalPosition": 200}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 2459, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}