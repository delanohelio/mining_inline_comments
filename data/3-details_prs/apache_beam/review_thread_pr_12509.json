{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDY1MzI0NTI1", "number": 12509, "reviewThreads": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMjo0NToxMVrOEZSjfQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMjo0NToxMVrOEZSjfQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjk0OTUzODUzOnYy", "diffSide": "RIGHT", "path": "sdks/python/apache_beam/io/external/xlang_snowflakeio_it_test.py", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMjo0NToxMVrOHCBWEw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yNlQwNzoxMDozNVrOHG-xBA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg4MTIzNQ==", "bodyText": "Should we verify the output written here ?", "url": "https://github.com/apache/beam/pull/12509#discussion_r471881235", "createdAt": "2020-08-18T02:45:11Z", "author": {"login": "chamikaramj"}, "path": "sdks/python/apache_beam/io/external/xlang_snowflakeio_it_test.py", "diffHunk": "@@ -0,0 +1,269 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+Integration test for cross-language snowflake io operations.\n+\n+Example run:\n+\n+python setup.py nosetests --tests=apache_beam.io.external.snowflake_test \\\n+--test-pipeline-options=\"\n+  --server_name=<SNOWFLAKE_SERVER_NAME>\n+  --username=<SNOWFLAKE_USERNAME>\n+  --password=<SNOWFLAKE_PASSWORD>\n+  --private_key_path=<PATH_TO_PRIVATE_KEY>\n+  --private_key_passphrase=<PASSWORD_TO_PRIVATE_KEY>\n+  --o_auth_token=<TOKEN>\n+  --staging_bucket_name=<GCP_BUCKET_PATH>\n+  --storage_integration_name=<SNOWFLAKE_STORAGE_INTEGRATION_NAME>\n+  --database=<DATABASE>\n+  --schema=<SCHEMA>\n+  --role=<ROLE>\n+  --warehouse=<WAREHOUSE>\n+  --table=<TABLE_NAME>\n+  --runner=FlinkRunner\"\n+\"\"\"\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import argparse\n+import binascii\n+import logging\n+import unittest\n+from typing import ByteString\n+from typing import NamedTuple\n+\n+import apache_beam as beam\n+from apache_beam import coders\n+from apache_beam.io.snowflake import CreateDisposition\n+from apache_beam.io.snowflake import ReadFromSnowflake\n+from apache_beam.io.snowflake import WriteDisposition\n+from apache_beam.io.snowflake import WriteToSnowflake\n+from apache_beam.options.pipeline_options import PipelineOptions\n+from apache_beam.testing.test_pipeline import TestPipeline\n+from apache_beam.testing.util import assert_that\n+from apache_beam.testing.util import equal_to\n+\n+# pylint: disable=wrong-import-order, wrong-import-position, ungrouped-imports\n+try:\n+  from apache_beam.io.gcp.gcsfilesystem import GCSFileSystem\n+except ImportError:\n+  GCSFileSystem = None\n+# pylint: enable=wrong-import-order, wrong-import-position, ungrouped-imports\n+\n+SCHEMA_STRING = \"\"\"\n+{\"schema\":[\n+    {\"dataType\":{\"type\":\"integer\",\"precision\":38,\"scale\":0},\"name\":\"number_column\",\"nullable\":false},\n+    {\"dataType\":{\"type\":\"boolean\"},\"name\":\"boolean_column\",\"nullable\":false},\n+    {\"dataType\":{\"type\":\"binary\",\"size\":100},\"name\":\"bytes_column\",\"nullable\":true}\n+]}\n+\"\"\"\n+\n+TestRow = NamedTuple(\n+    'TestRow',\n+    [\n+        ('number_column', int),\n+        ('boolean_column', bool),\n+        ('bytes_column', ByteString),\n+    ])\n+\n+coders.registry.register_coder(TestRow, coders.RowCoder)\n+\n+NUM_RECORDS = 100\n+\n+\n+@unittest.skipIf(GCSFileSystem is None, 'GCP dependencies are not installed')\n+@unittest.skipIf(\n+    TestPipeline().get_option('server_name') is None,\n+    'Snowflake IT test requires external configuration to be run.')\n+class SnowflakeTest(unittest.TestCase):\n+  def test_snowflake_write_read(self):\n+    self.run_write()\n+    self.run_read()\n+\n+  def run_write(self):\n+    def user_data_mapper(test_row):\n+      return [\n+          str(test_row.number_column).encode('utf-8'),\n+          str(test_row.boolean_column).encode('utf-8'),\n+          binascii.hexlify(test_row.bytes_column),\n+      ]\n+\n+    with TestPipeline(options=PipelineOptions(self.pipeline_args)) as p:\n+      p.not_use_test_runner_api = True\n+      _ = (\n+          p\n+          | 'Impulse' >> beam.Impulse()\n+          | 'Generate' >> beam.FlatMap(lambda x: range(NUM_RECORDS))  # pylint: disable=range-builtin-not-iterating\n+          | 'Map to TestRow' >> beam.Map(\n+              lambda num: TestRow(\n+                  num, num % 2 == 0, b\"test\" + str(num).encode()))\n+          | WriteToSnowflake(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "2cd2dd8217425e1ee9e11bc69702fc4960d69a97"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTkzMjA3MQ==", "bodyText": "I'm not sure what kind of verification could be added here. run_read function verfies the values written here. write_disposition=TRUNCATE means that the table has been recreated before writing to it. But I can be missing something.", "url": "https://github.com/apache/beam/pull/12509#discussion_r471932071", "createdAt": "2020-08-18T05:57:57Z", "author": {"login": "piotr-szuberski"}, "path": "sdks/python/apache_beam/io/external/xlang_snowflakeio_it_test.py", "diffHunk": "@@ -0,0 +1,269 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+Integration test for cross-language snowflake io operations.\n+\n+Example run:\n+\n+python setup.py nosetests --tests=apache_beam.io.external.snowflake_test \\\n+--test-pipeline-options=\"\n+  --server_name=<SNOWFLAKE_SERVER_NAME>\n+  --username=<SNOWFLAKE_USERNAME>\n+  --password=<SNOWFLAKE_PASSWORD>\n+  --private_key_path=<PATH_TO_PRIVATE_KEY>\n+  --private_key_passphrase=<PASSWORD_TO_PRIVATE_KEY>\n+  --o_auth_token=<TOKEN>\n+  --staging_bucket_name=<GCP_BUCKET_PATH>\n+  --storage_integration_name=<SNOWFLAKE_STORAGE_INTEGRATION_NAME>\n+  --database=<DATABASE>\n+  --schema=<SCHEMA>\n+  --role=<ROLE>\n+  --warehouse=<WAREHOUSE>\n+  --table=<TABLE_NAME>\n+  --runner=FlinkRunner\"\n+\"\"\"\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import argparse\n+import binascii\n+import logging\n+import unittest\n+from typing import ByteString\n+from typing import NamedTuple\n+\n+import apache_beam as beam\n+from apache_beam import coders\n+from apache_beam.io.snowflake import CreateDisposition\n+from apache_beam.io.snowflake import ReadFromSnowflake\n+from apache_beam.io.snowflake import WriteDisposition\n+from apache_beam.io.snowflake import WriteToSnowflake\n+from apache_beam.options.pipeline_options import PipelineOptions\n+from apache_beam.testing.test_pipeline import TestPipeline\n+from apache_beam.testing.util import assert_that\n+from apache_beam.testing.util import equal_to\n+\n+# pylint: disable=wrong-import-order, wrong-import-position, ungrouped-imports\n+try:\n+  from apache_beam.io.gcp.gcsfilesystem import GCSFileSystem\n+except ImportError:\n+  GCSFileSystem = None\n+# pylint: enable=wrong-import-order, wrong-import-position, ungrouped-imports\n+\n+SCHEMA_STRING = \"\"\"\n+{\"schema\":[\n+    {\"dataType\":{\"type\":\"integer\",\"precision\":38,\"scale\":0},\"name\":\"number_column\",\"nullable\":false},\n+    {\"dataType\":{\"type\":\"boolean\"},\"name\":\"boolean_column\",\"nullable\":false},\n+    {\"dataType\":{\"type\":\"binary\",\"size\":100},\"name\":\"bytes_column\",\"nullable\":true}\n+]}\n+\"\"\"\n+\n+TestRow = NamedTuple(\n+    'TestRow',\n+    [\n+        ('number_column', int),\n+        ('boolean_column', bool),\n+        ('bytes_column', ByteString),\n+    ])\n+\n+coders.registry.register_coder(TestRow, coders.RowCoder)\n+\n+NUM_RECORDS = 100\n+\n+\n+@unittest.skipIf(GCSFileSystem is None, 'GCP dependencies are not installed')\n+@unittest.skipIf(\n+    TestPipeline().get_option('server_name') is None,\n+    'Snowflake IT test requires external configuration to be run.')\n+class SnowflakeTest(unittest.TestCase):\n+  def test_snowflake_write_read(self):\n+    self.run_write()\n+    self.run_read()\n+\n+  def run_write(self):\n+    def user_data_mapper(test_row):\n+      return [\n+          str(test_row.number_column).encode('utf-8'),\n+          str(test_row.boolean_column).encode('utf-8'),\n+          binascii.hexlify(test_row.bytes_column),\n+      ]\n+\n+    with TestPipeline(options=PipelineOptions(self.pipeline_args)) as p:\n+      p.not_use_test_runner_api = True\n+      _ = (\n+          p\n+          | 'Impulse' >> beam.Impulse()\n+          | 'Generate' >> beam.FlatMap(lambda x: range(NUM_RECORDS))  # pylint: disable=range-builtin-not-iterating\n+          | 'Map to TestRow' >> beam.Map(\n+              lambda num: TestRow(\n+                  num, num % 2 == 0, b\"test\" + str(num).encode()))\n+          | WriteToSnowflake(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg4MTIzNQ=="}, "originalCommit": {"oid": "2cd2dd8217425e1ee9e11bc69702fc4960d69a97"}, "originalPosition": 117}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NzA4MTg2MA==", "bodyText": "@chamikaramj What are your thoughts?  Would you mind responding to @piotr-szuberski comment? Thanks!", "url": "https://github.com/apache/beam/pull/12509#discussion_r477081860", "createdAt": "2020-08-26T07:10:35Z", "author": {"login": "slawomir-andrian"}, "path": "sdks/python/apache_beam/io/external/xlang_snowflakeio_it_test.py", "diffHunk": "@@ -0,0 +1,269 @@\n+#\n+# Licensed to the Apache Software Foundation (ASF) under one or more\n+# contributor license agreements.  See the NOTICE file distributed with\n+# this work for additional information regarding copyright ownership.\n+# The ASF licenses this file to You under the Apache License, Version 2.0\n+# (the \"License\"); you may not use this file except in compliance with\n+# the License.  You may obtain a copy of the License at\n+#\n+#    http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+#\n+\n+\"\"\"\n+Integration test for cross-language snowflake io operations.\n+\n+Example run:\n+\n+python setup.py nosetests --tests=apache_beam.io.external.snowflake_test \\\n+--test-pipeline-options=\"\n+  --server_name=<SNOWFLAKE_SERVER_NAME>\n+  --username=<SNOWFLAKE_USERNAME>\n+  --password=<SNOWFLAKE_PASSWORD>\n+  --private_key_path=<PATH_TO_PRIVATE_KEY>\n+  --private_key_passphrase=<PASSWORD_TO_PRIVATE_KEY>\n+  --o_auth_token=<TOKEN>\n+  --staging_bucket_name=<GCP_BUCKET_PATH>\n+  --storage_integration_name=<SNOWFLAKE_STORAGE_INTEGRATION_NAME>\n+  --database=<DATABASE>\n+  --schema=<SCHEMA>\n+  --role=<ROLE>\n+  --warehouse=<WAREHOUSE>\n+  --table=<TABLE_NAME>\n+  --runner=FlinkRunner\"\n+\"\"\"\n+\n+# pytype: skip-file\n+\n+from __future__ import absolute_import\n+\n+import argparse\n+import binascii\n+import logging\n+import unittest\n+from typing import ByteString\n+from typing import NamedTuple\n+\n+import apache_beam as beam\n+from apache_beam import coders\n+from apache_beam.io.snowflake import CreateDisposition\n+from apache_beam.io.snowflake import ReadFromSnowflake\n+from apache_beam.io.snowflake import WriteDisposition\n+from apache_beam.io.snowflake import WriteToSnowflake\n+from apache_beam.options.pipeline_options import PipelineOptions\n+from apache_beam.testing.test_pipeline import TestPipeline\n+from apache_beam.testing.util import assert_that\n+from apache_beam.testing.util import equal_to\n+\n+# pylint: disable=wrong-import-order, wrong-import-position, ungrouped-imports\n+try:\n+  from apache_beam.io.gcp.gcsfilesystem import GCSFileSystem\n+except ImportError:\n+  GCSFileSystem = None\n+# pylint: enable=wrong-import-order, wrong-import-position, ungrouped-imports\n+\n+SCHEMA_STRING = \"\"\"\n+{\"schema\":[\n+    {\"dataType\":{\"type\":\"integer\",\"precision\":38,\"scale\":0},\"name\":\"number_column\",\"nullable\":false},\n+    {\"dataType\":{\"type\":\"boolean\"},\"name\":\"boolean_column\",\"nullable\":false},\n+    {\"dataType\":{\"type\":\"binary\",\"size\":100},\"name\":\"bytes_column\",\"nullable\":true}\n+]}\n+\"\"\"\n+\n+TestRow = NamedTuple(\n+    'TestRow',\n+    [\n+        ('number_column', int),\n+        ('boolean_column', bool),\n+        ('bytes_column', ByteString),\n+    ])\n+\n+coders.registry.register_coder(TestRow, coders.RowCoder)\n+\n+NUM_RECORDS = 100\n+\n+\n+@unittest.skipIf(GCSFileSystem is None, 'GCP dependencies are not installed')\n+@unittest.skipIf(\n+    TestPipeline().get_option('server_name') is None,\n+    'Snowflake IT test requires external configuration to be run.')\n+class SnowflakeTest(unittest.TestCase):\n+  def test_snowflake_write_read(self):\n+    self.run_write()\n+    self.run_read()\n+\n+  def run_write(self):\n+    def user_data_mapper(test_row):\n+      return [\n+          str(test_row.number_column).encode('utf-8'),\n+          str(test_row.boolean_column).encode('utf-8'),\n+          binascii.hexlify(test_row.bytes_column),\n+      ]\n+\n+    with TestPipeline(options=PipelineOptions(self.pipeline_args)) as p:\n+      p.not_use_test_runner_api = True\n+      _ = (\n+          p\n+          | 'Impulse' >> beam.Impulse()\n+          | 'Generate' >> beam.FlatMap(lambda x: range(NUM_RECORDS))  # pylint: disable=range-builtin-not-iterating\n+          | 'Map to TestRow' >> beam.Map(\n+              lambda num: TestRow(\n+                  num, num % 2 == 0, b\"test\" + str(num).encode()))\n+          | WriteToSnowflake(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTg4MTIzNQ=="}, "originalCommit": {"oid": "2cd2dd8217425e1ee9e11bc69702fc4960d69a97"}, "originalPosition": 117}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 609, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}