{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDE5ODM4MTgx", "number": 11749, "reviewThreads": {"totalCount": 106, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNzoxMDozMFrOEWSoBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwODoyOTo1OVrOEWs2Mw==", "hasNextPage": false, "hasPreviousPage": true}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxODA5Mjg3OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNzoxMDozMFrOG9hgOA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNzoxMDozMFrOG9hgOA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE2NTI0MA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Sets \"isolation_level\" to \"read_committed\" in Kafka consumer configuration. This is ensures\n          \n          \n            \n                 * that the consumer does not read uncommitted messages. Kafka version 0.11 introduced\n          \n          \n            \n                 * transactional writes. Applications requiring end-to-end exactly-once semantics should only\n          \n          \n            \n                 * read committed messages. See JavaDoc for {@link KafkaConsumer} for more description.\n          \n          \n            \n                 * Sets \"isolation_level\" to \"read_committed\" in Kafka consumer configuration. This ensures\n          \n          \n            \n                 * that the consumer does not read uncommitted messages. Kafka version 0.11 introduced\n          \n          \n            \n                 * transactional writes. Applications requiring end-to-end exactly-once semantics should only\n          \n          \n            \n                 * read committed messages. See JavaDoc for {@link KafkaConsumer} for more description.", "url": "https://github.com/apache/beam/pull/11749#discussion_r467165240", "createdAt": "2020-08-07T17:10:30Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1306,112 +1315,206 @@ public void populateDisplayData(DisplayData.Builder builder) {\n       return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} to interpret key bytes read from Kafka.\n+     *\n+     * <p>In addition, Beam also needs a {@link Coder} to serialize and deserialize key objects at\n+     * runtime. KafkaIO tries to infer a coder for the key based on the {@link Deserializer} class,\n+     * however in case that fails, you can use {@link #withKeyDeserializerAndCoder(Class, Coder)} to\n+     * provide the key coder explicitly.\n+     */\n     public ReadSourceDescriptors<K, V> withKeyDeserializer(\n         Class<? extends Deserializer<K>> keyDeserializer) {\n       return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} for interpreting key bytes read from Kafka along with a\n+     * {@link Coder} for helping the Beam runner materialize key objects at runtime if necessary.\n+     *\n+     * <p>Use this method only if your pipeline doesn't work with plain {@link\n+     * #withKeyDeserializer(Class)}.\n+     */\n     public ReadSourceDescriptors<K, V> withValueDeserializer(\n         Class<? extends Deserializer<V>> valueDeserializer) {\n       return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} for interpreting key bytes read from Kafka along with a\n+     * {@link Coder} for helping the Beam runner materialize key objects at runtime if necessary.\n+     *\n+     * <p>Use this method only if your pipeline doesn't work with plain {@link\n+     * #withKeyDeserializer(Class)}.\n+     */\n     public ReadSourceDescriptors<K, V> withKeyDeserializerAndCoder(\n         Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n       return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} for interpreting value bytes read from Kafka along with a\n+     * {@link Coder} for helping the Beam runner materialize value objects at runtime if necessary.\n+     *\n+     * <p>Use this method only if your pipeline doesn't work with plain {@link\n+     * #withValueDeserializer(Class)}.\n+     */\n     public ReadSourceDescriptors<K, V> withValueDeserializerAndCoder(\n         Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n       return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n     }\n \n+    /**\n+     * A factory to create Kafka {@link Consumer} from consumer configuration. This is useful for\n+     * supporting another version of Kafka consumer. Default is {@link KafkaConsumer}.\n+     */\n     public ReadSourceDescriptors<K, V> withConsumerFactoryFn(\n         SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n       return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n     }\n \n+    /**\n+     * Update configuration for the backend main consumer. Note that the default consumer properties\n+     * will not be completely overridden. This method only updates the value which has the same key.\n+     *\n+     * <p>In {@link ReadFromKafkaDoFn}, there're two consumers running in the backend actually:<br>\n+     * 1. the main consumer, which reads data from kafka;<br>\n+     * 2. the secondary offset consumer, which is used to estimate backlog, by fetching latest\n+     * offset;<br>\n+     *\n+     * <p>By default, main consumer uses the configuration from {@link\n+     * KafkaIOUtils#DEFAULT_CONSUMER_PROPERTIES}.\n+     */\n     public ReadSourceDescriptors<K, V> withConsumerConfigUpdates(\n         Map<String, Object> configUpdates) {\n       Map<String, Object> config =\n           KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n       return toBuilder().setConsumerConfig(config).build();\n     }\n \n+    /**\n+     * A function to calculate output timestamp for a given {@link KafkaRecord}. The default value\n+     * is {@link #withProcessingTime()}.\n+     */\n     public ReadSourceDescriptors<K, V> withExtractOutputTimestampFn(\n         SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n       return toBuilder().setExtractOutputTimestampFn(fn).build();\n     }\n \n+    /**\n+     * A function to create a {@link WatermarkEstimator}. The default value is {@link\n+     * MonotonicallyIncreasing}.\n+     */\n     public ReadSourceDescriptors<K, V> withCreatWatermarkEstimatorFn(\n         SerializableFunction<Instant, WatermarkEstimator<Instant>> fn) {\n       return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n     }\n \n+    /** Use the log append time as the output timestamp. */\n     public ReadSourceDescriptors<K, V> withLogAppendTime() {\n       return withExtractOutputTimestampFn(\n           ReadSourceDescriptors.ExtractOutputTimestampFns.useLogAppendTime());\n     }\n \n+    /** Use the processing time as the output timestamp. */\n     public ReadSourceDescriptors<K, V> withProcessingTime() {\n       return withExtractOutputTimestampFn(\n           ReadSourceDescriptors.ExtractOutputTimestampFns.useProcessingTime());\n     }\n \n+    /** Use the creation time of {@link KafkaRecord} as the output timestamp. */\n     public ReadSourceDescriptors<K, V> withCreateTime() {\n       return withExtractOutputTimestampFn(\n           ReadSourceDescriptors.ExtractOutputTimestampFns.useCreateTime());\n     }\n \n+    /** Use the {@link WallTime} as the watermark estimator. */\n     public ReadSourceDescriptors<K, V> withWallTimeWatermarkEstimator() {\n       return withCreatWatermarkEstimatorFn(\n           state -> {\n             return new WallTime(state);\n           });\n     }\n \n+    /** Use the {@link MonotonicallyIncreasing} as the watermark estimator. */\n     public ReadSourceDescriptors<K, V> withMonotonicallyIncreasingWatermarkEstimator() {\n       return withCreatWatermarkEstimatorFn(\n           state -> {\n             return new MonotonicallyIncreasing(state);\n           });\n     }\n \n+    /** Use the {@link Manual} as the watermark estimator. */\n     public ReadSourceDescriptors<K, V> withManualWatermarkEstimator() {\n       return withCreatWatermarkEstimatorFn(\n           state -> {\n             return new Manual(state);\n           });\n     }\n \n-    // If a transactional producer is used and it's desired to only read records from committed\n-    // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the\n-    // default value.\n+    /**\n+     * Sets \"isolation_level\" to \"read_committed\" in Kafka consumer configuration. This is ensures\n+     * that the consumer does not read uncommitted messages. Kafka version 0.11 introduced\n+     * transactional writes. Applications requiring end-to-end exactly-once semantics should only\n+     * read committed messages. See JavaDoc for {@link KafkaConsumer} for more description.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24042b8ea789ae16849369848e3269e230e35f39"}, "originalPosition": 194}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxODEwOTAxOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNzoxNToyOVrOG9hptw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNzoxNToyOVrOG9hptw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE2NzY3MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Set additional configuration for the backend offset consumer. It may be required for a\n          \n          \n            \n                 * secured Kafka cluster, especially when you see similar WARN log message 'exception while\n          \n          \n            \n                 * fetching latest offset for partition {}. will be retried'.\n          \n          \n            \n                 *\n          \n          \n            \n                 * <p>In {@link ReadFromKafkaDoFn}, there're two consumers running in the backend actually:<br>\n          \n          \n            \n                 * 1. the main consumer, which reads data from kafka;<br>\n          \n          \n            \n                 * 2. the secondary offset consumer, which is used to estimate backlog, by fetching latest\n          \n          \n            \n                 * offset;<br>\n          \n          \n            \n                 *\n          \n          \n            \n                 * <p>By default, offset consumer inherits the configuration from main consumer, with an\n          \n          \n            \n                 * auto-generated {@link ConsumerConfig#GROUP_ID_CONFIG}. This may not work in a secured Kafka\n          \n          \n            \n                 * which requires more configurations.\n          \n          \n            \n                 * Set additional configuration for the offset consumer. It may be required for a\n          \n          \n            \n                 * secured Kafka cluster, especially when you see similar WARN log message {@code exception while\n          \n          \n            \n                 * fetching latest offset for partition {}. will be retried}.\n          \n          \n            \n                 *\n          \n          \n            \n                 * <p>In {@link ReadFromKafkaDoFn}, there are two consumers running in the backend:\n          \n          \n            \n                 * <ol>\n          \n          \n            \n                 *   <li>the main consumer which reads data from kafka.\n          \n          \n            \n                 *   <li>the secondary offset consumer which is used to estimate the backlog by fetching the latest offset.\n          \n          \n            \n                 * </ol>\n          \n          \n            \n                 *\n          \n          \n            \n                 * <p>By default, offset consumer inherits the configuration from main consumer, with an\n          \n          \n            \n                 * auto-generated {@link ConsumerConfig#GROUP_ID_CONFIG}. This may not work in a secured Kafka\n          \n          \n            \n                 * which requires additional configuration.\n          \n          \n            \n                 *\n          \n          \n            \n                 * <p>See {@link #withConsumerConfigUpdates} for configuring the main consumer.", "url": "https://github.com/apache/beam/pull/11749#discussion_r467167671", "createdAt": "2020-08-07T17:15:29Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1306,112 +1315,206 @@ public void populateDisplayData(DisplayData.Builder builder) {\n       return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} to interpret key bytes read from Kafka.\n+     *\n+     * <p>In addition, Beam also needs a {@link Coder} to serialize and deserialize key objects at\n+     * runtime. KafkaIO tries to infer a coder for the key based on the {@link Deserializer} class,\n+     * however in case that fails, you can use {@link #withKeyDeserializerAndCoder(Class, Coder)} to\n+     * provide the key coder explicitly.\n+     */\n     public ReadSourceDescriptors<K, V> withKeyDeserializer(\n         Class<? extends Deserializer<K>> keyDeserializer) {\n       return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} for interpreting key bytes read from Kafka along with a\n+     * {@link Coder} for helping the Beam runner materialize key objects at runtime if necessary.\n+     *\n+     * <p>Use this method only if your pipeline doesn't work with plain {@link\n+     * #withKeyDeserializer(Class)}.\n+     */\n     public ReadSourceDescriptors<K, V> withValueDeserializer(\n         Class<? extends Deserializer<V>> valueDeserializer) {\n       return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} for interpreting key bytes read from Kafka along with a\n+     * {@link Coder} for helping the Beam runner materialize key objects at runtime if necessary.\n+     *\n+     * <p>Use this method only if your pipeline doesn't work with plain {@link\n+     * #withKeyDeserializer(Class)}.\n+     */\n     public ReadSourceDescriptors<K, V> withKeyDeserializerAndCoder(\n         Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n       return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} for interpreting value bytes read from Kafka along with a\n+     * {@link Coder} for helping the Beam runner materialize value objects at runtime if necessary.\n+     *\n+     * <p>Use this method only if your pipeline doesn't work with plain {@link\n+     * #withValueDeserializer(Class)}.\n+     */\n     public ReadSourceDescriptors<K, V> withValueDeserializerAndCoder(\n         Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n       return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n     }\n \n+    /**\n+     * A factory to create Kafka {@link Consumer} from consumer configuration. This is useful for\n+     * supporting another version of Kafka consumer. Default is {@link KafkaConsumer}.\n+     */\n     public ReadSourceDescriptors<K, V> withConsumerFactoryFn(\n         SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n       return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n     }\n \n+    /**\n+     * Update configuration for the backend main consumer. Note that the default consumer properties\n+     * will not be completely overridden. This method only updates the value which has the same key.\n+     *\n+     * <p>In {@link ReadFromKafkaDoFn}, there're two consumers running in the backend actually:<br>\n+     * 1. the main consumer, which reads data from kafka;<br>\n+     * 2. the secondary offset consumer, which is used to estimate backlog, by fetching latest\n+     * offset;<br>\n+     *\n+     * <p>By default, main consumer uses the configuration from {@link\n+     * KafkaIOUtils#DEFAULT_CONSUMER_PROPERTIES}.\n+     */\n     public ReadSourceDescriptors<K, V> withConsumerConfigUpdates(\n         Map<String, Object> configUpdates) {\n       Map<String, Object> config =\n           KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n       return toBuilder().setConsumerConfig(config).build();\n     }\n \n+    /**\n+     * A function to calculate output timestamp for a given {@link KafkaRecord}. The default value\n+     * is {@link #withProcessingTime()}.\n+     */\n     public ReadSourceDescriptors<K, V> withExtractOutputTimestampFn(\n         SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n       return toBuilder().setExtractOutputTimestampFn(fn).build();\n     }\n \n+    /**\n+     * A function to create a {@link WatermarkEstimator}. The default value is {@link\n+     * MonotonicallyIncreasing}.\n+     */\n     public ReadSourceDescriptors<K, V> withCreatWatermarkEstimatorFn(\n         SerializableFunction<Instant, WatermarkEstimator<Instant>> fn) {\n       return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n     }\n \n+    /** Use the log append time as the output timestamp. */\n     public ReadSourceDescriptors<K, V> withLogAppendTime() {\n       return withExtractOutputTimestampFn(\n           ReadSourceDescriptors.ExtractOutputTimestampFns.useLogAppendTime());\n     }\n \n+    /** Use the processing time as the output timestamp. */\n     public ReadSourceDescriptors<K, V> withProcessingTime() {\n       return withExtractOutputTimestampFn(\n           ReadSourceDescriptors.ExtractOutputTimestampFns.useProcessingTime());\n     }\n \n+    /** Use the creation time of {@link KafkaRecord} as the output timestamp. */\n     public ReadSourceDescriptors<K, V> withCreateTime() {\n       return withExtractOutputTimestampFn(\n           ReadSourceDescriptors.ExtractOutputTimestampFns.useCreateTime());\n     }\n \n+    /** Use the {@link WallTime} as the watermark estimator. */\n     public ReadSourceDescriptors<K, V> withWallTimeWatermarkEstimator() {\n       return withCreatWatermarkEstimatorFn(\n           state -> {\n             return new WallTime(state);\n           });\n     }\n \n+    /** Use the {@link MonotonicallyIncreasing} as the watermark estimator. */\n     public ReadSourceDescriptors<K, V> withMonotonicallyIncreasingWatermarkEstimator() {\n       return withCreatWatermarkEstimatorFn(\n           state -> {\n             return new MonotonicallyIncreasing(state);\n           });\n     }\n \n+    /** Use the {@link Manual} as the watermark estimator. */\n     public ReadSourceDescriptors<K, V> withManualWatermarkEstimator() {\n       return withCreatWatermarkEstimatorFn(\n           state -> {\n             return new Manual(state);\n           });\n     }\n \n-    // If a transactional producer is used and it's desired to only read records from committed\n-    // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the\n-    // default value.\n+    /**\n+     * Sets \"isolation_level\" to \"read_committed\" in Kafka consumer configuration. This is ensures\n+     * that the consumer does not read uncommitted messages. Kafka version 0.11 introduced\n+     * transactional writes. Applications requiring end-to-end exactly-once semantics should only\n+     * read committed messages. See JavaDoc for {@link KafkaConsumer} for more description.\n+     */\n     public ReadSourceDescriptors<K, V> withReadCommitted() {\n       return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n     }\n \n+    /**\n+     * Enable committing record offset. If {@link #withReadCommitted()} or {@link\n+     * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set together with {@link #commitOffsets()},\n+     * {@link #commitOffsets()} will be ignored.\n+     */\n     public ReadSourceDescriptors<K, V> commitOffsets() {\n       return toBuilder().setCommitOffsetEnabled(true).build();\n     }\n \n+    /**\n+     * Set additional configuration for the backend offset consumer. It may be required for a\n+     * secured Kafka cluster, especially when you see similar WARN log message 'exception while\n+     * fetching latest offset for partition {}. will be retried'.\n+     *\n+     * <p>In {@link ReadFromKafkaDoFn}, there're two consumers running in the backend actually:<br>\n+     * 1. the main consumer, which reads data from kafka;<br>\n+     * 2. the secondary offset consumer, which is used to estimate backlog, by fetching latest\n+     * offset;<br>\n+     *\n+     * <p>By default, offset consumer inherits the configuration from main consumer, with an\n+     * auto-generated {@link ConsumerConfig#GROUP_ID_CONFIG}. This may not work in a secured Kafka\n+     * which requires more configurations.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24042b8ea789ae16849369848e3269e230e35f39"}, "originalPosition": 221}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxODExNTI0OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNzoxNzozOFrOG9htwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNzoxNzozOFrOG9htwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE2ODcwNg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Update configuration for the backend main consumer. Note that the default consumer properties\n          \n          \n            \n                 * will not be completely overridden. This method only updates the value which has the same key.\n          \n          \n            \n                 *\n          \n          \n            \n                 * <p>In {@link ReadFromKafkaDoFn}, there're two consumers running in the backend actually:<br>\n          \n          \n            \n                 * 1. the main consumer, which reads data from kafka;<br>\n          \n          \n            \n                 * 2. the secondary offset consumer, which is used to estimate backlog, by fetching latest\n          \n          \n            \n                 * offset;<br>\n          \n          \n            \n                 *\n          \n          \n            \n                 * <p>By default, main consumer uses the configuration from {@link\n          \n          \n            \n                 * KafkaIOUtils#DEFAULT_CONSUMER_PROPERTIES}.\n          \n          \n            \n                 * Replaces the configuration for the main consumer.\n          \n          \n            \n                 *\n          \n          \n            \n                 * <p>In {@link ReadFromKafkaDoFn}, there are two consumers running in the backend:\n          \n          \n            \n                 * <ol>\n          \n          \n            \n                 *   <li>the main consumer which reads data from kafka.\n          \n          \n            \n                 *   <li>the secondary offset consumer which is used to estimate the backlog by fetching the latest offset.\n          \n          \n            \n                 * </ol>\n          \n          \n            \n                 *\n          \n          \n            \n                 * <p>By default, main consumer uses the configuration from {@link\n          \n          \n            \n                 * KafkaIOUtils#DEFAULT_CONSUMER_PROPERTIES}.\n          \n          \n            \n                 *\n          \n          \n            \n                 * See {@link #withConsumerConfigUpdates} for updating the configuration instead of overriding it.", "url": "https://github.com/apache/beam/pull/11749#discussion_r467168706", "createdAt": "2020-08-07T17:17:38Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1306,112 +1315,206 @@ public void populateDisplayData(DisplayData.Builder builder) {\n       return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} to interpret key bytes read from Kafka.\n+     *\n+     * <p>In addition, Beam also needs a {@link Coder} to serialize and deserialize key objects at\n+     * runtime. KafkaIO tries to infer a coder for the key based on the {@link Deserializer} class,\n+     * however in case that fails, you can use {@link #withKeyDeserializerAndCoder(Class, Coder)} to\n+     * provide the key coder explicitly.\n+     */\n     public ReadSourceDescriptors<K, V> withKeyDeserializer(\n         Class<? extends Deserializer<K>> keyDeserializer) {\n       return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} for interpreting key bytes read from Kafka along with a\n+     * {@link Coder} for helping the Beam runner materialize key objects at runtime if necessary.\n+     *\n+     * <p>Use this method only if your pipeline doesn't work with plain {@link\n+     * #withKeyDeserializer(Class)}.\n+     */\n     public ReadSourceDescriptors<K, V> withValueDeserializer(\n         Class<? extends Deserializer<V>> valueDeserializer) {\n       return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} for interpreting key bytes read from Kafka along with a\n+     * {@link Coder} for helping the Beam runner materialize key objects at runtime if necessary.\n+     *\n+     * <p>Use this method only if your pipeline doesn't work with plain {@link\n+     * #withKeyDeserializer(Class)}.\n+     */\n     public ReadSourceDescriptors<K, V> withKeyDeserializerAndCoder(\n         Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n       return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} for interpreting value bytes read from Kafka along with a\n+     * {@link Coder} for helping the Beam runner materialize value objects at runtime if necessary.\n+     *\n+     * <p>Use this method only if your pipeline doesn't work with plain {@link\n+     * #withValueDeserializer(Class)}.\n+     */\n     public ReadSourceDescriptors<K, V> withValueDeserializerAndCoder(\n         Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n       return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n     }\n \n+    /**\n+     * A factory to create Kafka {@link Consumer} from consumer configuration. This is useful for\n+     * supporting another version of Kafka consumer. Default is {@link KafkaConsumer}.\n+     */\n     public ReadSourceDescriptors<K, V> withConsumerFactoryFn(\n         SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n       return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n     }\n \n+    /**\n+     * Update configuration for the backend main consumer. Note that the default consumer properties\n+     * will not be completely overridden. This method only updates the value which has the same key.\n+     *\n+     * <p>In {@link ReadFromKafkaDoFn}, there're two consumers running in the backend actually:<br>\n+     * 1. the main consumer, which reads data from kafka;<br>\n+     * 2. the secondary offset consumer, which is used to estimate backlog, by fetching latest\n+     * offset;<br>\n+     *\n+     * <p>By default, main consumer uses the configuration from {@link\n+     * KafkaIOUtils#DEFAULT_CONSUMER_PROPERTIES}.\n+     */\n     public ReadSourceDescriptors<K, V> withConsumerConfigUpdates(\n         Map<String, Object> configUpdates) {\n       Map<String, Object> config =\n           KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n       return toBuilder().setConsumerConfig(config).build();\n     }\n \n+    /**\n+     * A function to calculate output timestamp for a given {@link KafkaRecord}. The default value\n+     * is {@link #withProcessingTime()}.\n+     */\n     public ReadSourceDescriptors<K, V> withExtractOutputTimestampFn(\n         SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n       return toBuilder().setExtractOutputTimestampFn(fn).build();\n     }\n \n+    /**\n+     * A function to create a {@link WatermarkEstimator}. The default value is {@link\n+     * MonotonicallyIncreasing}.\n+     */\n     public ReadSourceDescriptors<K, V> withCreatWatermarkEstimatorFn(\n         SerializableFunction<Instant, WatermarkEstimator<Instant>> fn) {\n       return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n     }\n \n+    /** Use the log append time as the output timestamp. */\n     public ReadSourceDescriptors<K, V> withLogAppendTime() {\n       return withExtractOutputTimestampFn(\n           ReadSourceDescriptors.ExtractOutputTimestampFns.useLogAppendTime());\n     }\n \n+    /** Use the processing time as the output timestamp. */\n     public ReadSourceDescriptors<K, V> withProcessingTime() {\n       return withExtractOutputTimestampFn(\n           ReadSourceDescriptors.ExtractOutputTimestampFns.useProcessingTime());\n     }\n \n+    /** Use the creation time of {@link KafkaRecord} as the output timestamp. */\n     public ReadSourceDescriptors<K, V> withCreateTime() {\n       return withExtractOutputTimestampFn(\n           ReadSourceDescriptors.ExtractOutputTimestampFns.useCreateTime());\n     }\n \n+    /** Use the {@link WallTime} as the watermark estimator. */\n     public ReadSourceDescriptors<K, V> withWallTimeWatermarkEstimator() {\n       return withCreatWatermarkEstimatorFn(\n           state -> {\n             return new WallTime(state);\n           });\n     }\n \n+    /** Use the {@link MonotonicallyIncreasing} as the watermark estimator. */\n     public ReadSourceDescriptors<K, V> withMonotonicallyIncreasingWatermarkEstimator() {\n       return withCreatWatermarkEstimatorFn(\n           state -> {\n             return new MonotonicallyIncreasing(state);\n           });\n     }\n \n+    /** Use the {@link Manual} as the watermark estimator. */\n     public ReadSourceDescriptors<K, V> withManualWatermarkEstimator() {\n       return withCreatWatermarkEstimatorFn(\n           state -> {\n             return new Manual(state);\n           });\n     }\n \n-    // If a transactional producer is used and it's desired to only read records from committed\n-    // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the\n-    // default value.\n+    /**\n+     * Sets \"isolation_level\" to \"read_committed\" in Kafka consumer configuration. This is ensures\n+     * that the consumer does not read uncommitted messages. Kafka version 0.11 introduced\n+     * transactional writes. Applications requiring end-to-end exactly-once semantics should only\n+     * read committed messages. See JavaDoc for {@link KafkaConsumer} for more description.\n+     */\n     public ReadSourceDescriptors<K, V> withReadCommitted() {\n       return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n     }\n \n+    /**\n+     * Enable committing record offset. If {@link #withReadCommitted()} or {@link\n+     * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set together with {@link #commitOffsets()},\n+     * {@link #commitOffsets()} will be ignored.\n+     */\n     public ReadSourceDescriptors<K, V> commitOffsets() {\n       return toBuilder().setCommitOffsetEnabled(true).build();\n     }\n \n+    /**\n+     * Set additional configuration for the backend offset consumer. It may be required for a\n+     * secured Kafka cluster, especially when you see similar WARN log message 'exception while\n+     * fetching latest offset for partition {}. will be retried'.\n+     *\n+     * <p>In {@link ReadFromKafkaDoFn}, there're two consumers running in the backend actually:<br>\n+     * 1. the main consumer, which reads data from kafka;<br>\n+     * 2. the secondary offset consumer, which is used to estimate backlog, by fetching latest\n+     * offset;<br>\n+     *\n+     * <p>By default, offset consumer inherits the configuration from main consumer, with an\n+     * auto-generated {@link ConsumerConfig#GROUP_ID_CONFIG}. This may not work in a secured Kafka\n+     * which requires more configurations.\n+     */\n     public ReadSourceDescriptors<K, V> withOffsetConsumerConfigOverrides(\n         Map<String, Object> offsetConsumerConfig) {\n       return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n     }\n \n+    /**\n+     * Update configuration for the backend main consumer. Note that the default consumer properties\n+     * will not be completely overridden. This method only updates the value which has the same key.\n+     *\n+     * <p>In {@link ReadFromKafkaDoFn}, there're two consumers running in the backend actually:<br>\n+     * 1. the main consumer, which reads data from kafka;<br>\n+     * 2. the secondary offset consumer, which is used to estimate backlog, by fetching latest\n+     * offset;<br>\n+     *\n+     * <p>By default, main consumer uses the configuration from {@link\n+     * KafkaIOUtils#DEFAULT_CONSUMER_PROPERTIES}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24042b8ea789ae16849369848e3269e230e35f39"}, "originalPosition": 238}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxODEyNjU3OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNzoyMTozMlrOG9h07w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNzoyMTozMlrOG9h07w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE3MDU0Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Set the {@link TimestampPolicyFactory}. If the {@link TimestampPolicyFactory} is given, the\n          \n          \n            \n                 * output timestamp will be computed by the {@link\n          \n          \n            \n                 * TimestampPolicyFactory#createTimestampPolicy(TopicPartition, Optional)} and the {@link\n          \n          \n            \n                 * Manual} is used as the watermark estimator.\n          \n          \n            \n                 * Set the {@link TimestampPolicyFactory}. If the {@link TimestampPolicyFactory} is given, the\n          \n          \n            \n                 * output timestamp will be computed by the {@link\n          \n          \n            \n                 * TimestampPolicyFactory#createTimestampPolicy(TopicPartition, Optional)} and {@link\n          \n          \n            \n                 * Manual} is used as the watermark estimator.", "url": "https://github.com/apache/beam/pull/11749#discussion_r467170543", "createdAt": "2020-08-07T17:21:32Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1439,9 +1542,18 @@ public void processElement(\n       }\n     }\n \n+    /**\n+     * Set the {@link TimestampPolicyFactory}. If the {@link TimestampPolicyFactory} is given, the\n+     * output timestamp will be computed by the {@link\n+     * TimestampPolicyFactory#createTimestampPolicy(TopicPartition, Optional)} and the {@link\n+     * Manual} is used as the watermark estimator.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24042b8ea789ae16849369848e3269e230e35f39"}, "originalPosition": 267}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxODEyOTgxOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNzoyMjozM1rOG9h20A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNzoyMjozM1rOG9h20A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE3MTAyNA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * A transform that is used in cross-language case. The input Row should be encoded with an\n          \n          \n            \n                 * equivalent schema as {@link KafkaSourceDescriptor}.\n          \n          \n            \n                 * A transform that is used in cross-language case. The input {@link Row} should be encoded with an\n          \n          \n            \n                 * equivalent schema as {@link KafkaSourceDescriptor}.", "url": "https://github.com/apache/beam/pull/11749#discussion_r467171024", "createdAt": "2020-08-07T17:22:33Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1306,112 +1315,206 @@ public void populateDisplayData(DisplayData.Builder builder) {\n       return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} to interpret key bytes read from Kafka.\n+     *\n+     * <p>In addition, Beam also needs a {@link Coder} to serialize and deserialize key objects at\n+     * runtime. KafkaIO tries to infer a coder for the key based on the {@link Deserializer} class,\n+     * however in case that fails, you can use {@link #withKeyDeserializerAndCoder(Class, Coder)} to\n+     * provide the key coder explicitly.\n+     */\n     public ReadSourceDescriptors<K, V> withKeyDeserializer(\n         Class<? extends Deserializer<K>> keyDeserializer) {\n       return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} for interpreting key bytes read from Kafka along with a\n+     * {@link Coder} for helping the Beam runner materialize key objects at runtime if necessary.\n+     *\n+     * <p>Use this method only if your pipeline doesn't work with plain {@link\n+     * #withKeyDeserializer(Class)}.\n+     */\n     public ReadSourceDescriptors<K, V> withValueDeserializer(\n         Class<? extends Deserializer<V>> valueDeserializer) {\n       return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} for interpreting key bytes read from Kafka along with a\n+     * {@link Coder} for helping the Beam runner materialize key objects at runtime if necessary.\n+     *\n+     * <p>Use this method only if your pipeline doesn't work with plain {@link\n+     * #withKeyDeserializer(Class)}.\n+     */\n     public ReadSourceDescriptors<K, V> withKeyDeserializerAndCoder(\n         Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n       return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} for interpreting value bytes read from Kafka along with a\n+     * {@link Coder} for helping the Beam runner materialize value objects at runtime if necessary.\n+     *\n+     * <p>Use this method only if your pipeline doesn't work with plain {@link\n+     * #withValueDeserializer(Class)}.\n+     */\n     public ReadSourceDescriptors<K, V> withValueDeserializerAndCoder(\n         Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n       return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n     }\n \n+    /**\n+     * A factory to create Kafka {@link Consumer} from consumer configuration. This is useful for\n+     * supporting another version of Kafka consumer. Default is {@link KafkaConsumer}.\n+     */\n     public ReadSourceDescriptors<K, V> withConsumerFactoryFn(\n         SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n       return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n     }\n \n+    /**\n+     * Update configuration for the backend main consumer. Note that the default consumer properties\n+     * will not be completely overridden. This method only updates the value which has the same key.\n+     *\n+     * <p>In {@link ReadFromKafkaDoFn}, there're two consumers running in the backend actually:<br>\n+     * 1. the main consumer, which reads data from kafka;<br>\n+     * 2. the secondary offset consumer, which is used to estimate backlog, by fetching latest\n+     * offset;<br>\n+     *\n+     * <p>By default, main consumer uses the configuration from {@link\n+     * KafkaIOUtils#DEFAULT_CONSUMER_PROPERTIES}.\n+     */\n     public ReadSourceDescriptors<K, V> withConsumerConfigUpdates(\n         Map<String, Object> configUpdates) {\n       Map<String, Object> config =\n           KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n       return toBuilder().setConsumerConfig(config).build();\n     }\n \n+    /**\n+     * A function to calculate output timestamp for a given {@link KafkaRecord}. The default value\n+     * is {@link #withProcessingTime()}.\n+     */\n     public ReadSourceDescriptors<K, V> withExtractOutputTimestampFn(\n         SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n       return toBuilder().setExtractOutputTimestampFn(fn).build();\n     }\n \n+    /**\n+     * A function to create a {@link WatermarkEstimator}. The default value is {@link\n+     * MonotonicallyIncreasing}.\n+     */\n     public ReadSourceDescriptors<K, V> withCreatWatermarkEstimatorFn(\n         SerializableFunction<Instant, WatermarkEstimator<Instant>> fn) {\n       return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n     }\n \n+    /** Use the log append time as the output timestamp. */\n     public ReadSourceDescriptors<K, V> withLogAppendTime() {\n       return withExtractOutputTimestampFn(\n           ReadSourceDescriptors.ExtractOutputTimestampFns.useLogAppendTime());\n     }\n \n+    /** Use the processing time as the output timestamp. */\n     public ReadSourceDescriptors<K, V> withProcessingTime() {\n       return withExtractOutputTimestampFn(\n           ReadSourceDescriptors.ExtractOutputTimestampFns.useProcessingTime());\n     }\n \n+    /** Use the creation time of {@link KafkaRecord} as the output timestamp. */\n     public ReadSourceDescriptors<K, V> withCreateTime() {\n       return withExtractOutputTimestampFn(\n           ReadSourceDescriptors.ExtractOutputTimestampFns.useCreateTime());\n     }\n \n+    /** Use the {@link WallTime} as the watermark estimator. */\n     public ReadSourceDescriptors<K, V> withWallTimeWatermarkEstimator() {\n       return withCreatWatermarkEstimatorFn(\n           state -> {\n             return new WallTime(state);\n           });\n     }\n \n+    /** Use the {@link MonotonicallyIncreasing} as the watermark estimator. */\n     public ReadSourceDescriptors<K, V> withMonotonicallyIncreasingWatermarkEstimator() {\n       return withCreatWatermarkEstimatorFn(\n           state -> {\n             return new MonotonicallyIncreasing(state);\n           });\n     }\n \n+    /** Use the {@link Manual} as the watermark estimator. */\n     public ReadSourceDescriptors<K, V> withManualWatermarkEstimator() {\n       return withCreatWatermarkEstimatorFn(\n           state -> {\n             return new Manual(state);\n           });\n     }\n \n-    // If a transactional producer is used and it's desired to only read records from committed\n-    // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the\n-    // default value.\n+    /**\n+     * Sets \"isolation_level\" to \"read_committed\" in Kafka consumer configuration. This is ensures\n+     * that the consumer does not read uncommitted messages. Kafka version 0.11 introduced\n+     * transactional writes. Applications requiring end-to-end exactly-once semantics should only\n+     * read committed messages. See JavaDoc for {@link KafkaConsumer} for more description.\n+     */\n     public ReadSourceDescriptors<K, V> withReadCommitted() {\n       return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n     }\n \n+    /**\n+     * Enable committing record offset. If {@link #withReadCommitted()} or {@link\n+     * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set together with {@link #commitOffsets()},\n+     * {@link #commitOffsets()} will be ignored.\n+     */\n     public ReadSourceDescriptors<K, V> commitOffsets() {\n       return toBuilder().setCommitOffsetEnabled(true).build();\n     }\n \n+    /**\n+     * Set additional configuration for the backend offset consumer. It may be required for a\n+     * secured Kafka cluster, especially when you see similar WARN log message 'exception while\n+     * fetching latest offset for partition {}. will be retried'.\n+     *\n+     * <p>In {@link ReadFromKafkaDoFn}, there're two consumers running in the backend actually:<br>\n+     * 1. the main consumer, which reads data from kafka;<br>\n+     * 2. the secondary offset consumer, which is used to estimate backlog, by fetching latest\n+     * offset;<br>\n+     *\n+     * <p>By default, offset consumer inherits the configuration from main consumer, with an\n+     * auto-generated {@link ConsumerConfig#GROUP_ID_CONFIG}. This may not work in a secured Kafka\n+     * which requires more configurations.\n+     */\n     public ReadSourceDescriptors<K, V> withOffsetConsumerConfigOverrides(\n         Map<String, Object> offsetConsumerConfig) {\n       return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n     }\n \n+    /**\n+     * Update configuration for the backend main consumer. Note that the default consumer properties\n+     * will not be completely overridden. This method only updates the value which has the same key.\n+     *\n+     * <p>In {@link ReadFromKafkaDoFn}, there're two consumers running in the backend actually:<br>\n+     * 1. the main consumer, which reads data from kafka;<br>\n+     * 2. the secondary offset consumer, which is used to estimate backlog, by fetching latest\n+     * offset;<br>\n+     *\n+     * <p>By default, main consumer uses the configuration from {@link\n+     * KafkaIOUtils#DEFAULT_CONSUMER_PROPERTIES}.\n+     */\n     public ReadSourceDescriptors<K, V> withConsumerConfigOverrides(\n         Map<String, Object> consumerConfig) {\n       return toBuilder().setConsumerConfig(consumerConfig).build();\n     }\n \n-    // TODO(BEAM-10320): Create external build transform for ReadSourceDescriptors().\n     ReadAllFromRow forExternalBuild() {\n       return new ReadAllFromRow(this);\n     }\n \n-    // This transform is used in cross-language case. The input Row should be encoded with an\n-    // equivalent schema as KafkaSourceDescriptor.\n+    /**\n+     * A transform that is used in cross-language case. The input Row should be encoded with an\n+     * equivalent schema as {@link KafkaSourceDescriptor}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24042b8ea789ae16849369848e3269e230e35f39"}, "originalPosition": 254}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkyMjM4ODk5OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwODoyOTo1OVrOG-FnDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xMFQwODoyOTo1OVrOG-FnDA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2Nzc1NjgxMg==", "bodyText": "MaxNumRecords is set 2 lines below, this line should be removed.", "url": "https://github.com/apache/beam/pull/11749#discussion_r467756812", "createdAt": "2020-08-10T08:29:59Z", "author": {"login": "piotr-szuberski"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -451,7 +574,9 @@\n \n         // Set required defaults\n         setTopicPartitions(Collections.emptyList());\n-        setConsumerFactoryFn(Read.KAFKA_CONSUMER_FACTORY_FN);\n+        setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN);\n+        setMaxNumRecords(Long.MAX_VALUE);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "18d64939b5a84053cfe8af4728a70371392ba2a0"}, "originalPosition": 223}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNzU1MDQxOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaViaSDF.java", "isResolved": false, "comments": {"totalCount": 13, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNTo0NDowM1rOGeh8zA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xMlQxNzo1OToyN1rOGjM_yQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY2NjcwMA==", "bodyText": "This seems strangely close to something we lived in the SDF version of HBaseIO. In the first version we did an artificial object called HBaseQuery that contained the minimum information we needed to be able to query the Data store in a SDF way, but then other requirements came in and we started to add extra parameters to end up with something that was almost close to the exact 'complete' specification of the Read class so we decided to switch to use a PCollection<Read> as input otherwise we will be duplicating code, so we ended up with \n  \n    \n      beam/sdks/java/io/hbase/src/main/java/org/apache/beam/sdk/io/hbase/HBaseIO.java\n    \n    \n         Line 353\n      in\n      f6ef903\n    \n    \n    \n    \n\n        \n          \n           public static class ReadAll extends PTransform<PCollection<Read>, PCollection<Result>> { \n        \n    \n  \n\n\nHere you can have PCollection<Read> as an input and get rid of KafkaSourceDescription if you move the missing parameters like TopicPartition into normal Read and this will have a more consistent user experience for final users. Notice that this ReadAll like pattern is also now used in SolrIO and there is an ongoing PR to introduce it for CassandraIO so maybe it is a good idea we follow it for consistency.\nNotice that in the SolrIO case the change looks even closer to this one because we ended up introducing ReplicaInfo (the spiritual equivalent of TopicPartition) into normal Read and we guarantee in expansion that this field gets filled if the users don't do it, but if they do well we asume they know what they are doing and we go with it.\nAnother advantage of having the full specification is that you will be able to read not only from multiple topics but also from different clusters because of the power of having the full Read specification,", "url": "https://github.com/apache/beam/pull/11749#discussion_r434666700", "createdAt": "2020-06-03T15:44:03Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaViaSDF.java", "diffHunk": "@@ -0,0 +1,697 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.Element;\n+import org.apache.beam.sdk.transforms.DoFn.GetRestrictionCoder;\n+import org.apache.beam.sdk.transforms.DoFn.OutputReceiver;\n+import org.apache.beam.sdk.transforms.DoFn.ProcessElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the beam blog post: https://beam.apache.org/blog/splittable-do-fn/ and design\n+ * doc:https://s.apache.org/beam-fn-api. The major difference from {@link KafkaIO.Read} is, {@link\n+ * ReadFromKafkaViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadFromKafkaViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyCoder()} is the same as {@link\n+ *       KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueCoder()} is the same as {@link\n+ *       KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadFromKafkaViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadFromKafkaViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#commitOffsets()} enables committing offset after processing the\n+ * record. Note that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadFromKafkaViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a\n+ * function which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is\n+ * used to produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadFromKafkaViaSDF#withProcessingTime()}, {@link ReadFromKafkaViaSDF#withCreateTime()} and\n+ * {@link ReadFromKafkaViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadFromKafkaViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@link KafkaSourceDescription#getStartOffset()} is set, use this offset as start.\n+ *   <li>If {@link KafkaSourceDescription#getStartReadTime()} is set, seek the start offset based on\n+ *       this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer}\n+ * is used in the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link KafkaSourceDescription}.\n+ */\n+@AutoValue\n+public abstract class ReadFromKafkaViaSDF<K, V>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcyOTA5Ng==", "bodyText": "I would shorter the name of this class to ReadWithSDF or ReadViaSDF since this is clear that it's used to read from Kafka.", "url": "https://github.com/apache/beam/pull/11749#discussion_r434729096", "createdAt": "2020-06-03T17:17:12Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaViaSDF.java", "diffHunk": "@@ -0,0 +1,697 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.Element;\n+import org.apache.beam.sdk.transforms.DoFn.GetRestrictionCoder;\n+import org.apache.beam.sdk.transforms.DoFn.OutputReceiver;\n+import org.apache.beam.sdk.transforms.DoFn.ProcessElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the beam blog post: https://beam.apache.org/blog/splittable-do-fn/ and design\n+ * doc:https://s.apache.org/beam-fn-api. The major difference from {@link KafkaIO.Read} is, {@link\n+ * ReadFromKafkaViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadFromKafkaViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyCoder()} is the same as {@link\n+ *       KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueCoder()} is the same as {@link\n+ *       KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadFromKafkaViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadFromKafkaViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#commitOffsets()} enables committing offset after processing the\n+ * record. Note that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadFromKafkaViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a\n+ * function which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is\n+ * used to produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadFromKafkaViaSDF#withProcessingTime()}, {@link ReadFromKafkaViaSDF#withCreateTime()} and\n+ * {@link ReadFromKafkaViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadFromKafkaViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@link KafkaSourceDescription#getStartOffset()} is set, use this offset as start.\n+ *   <li>If {@link KafkaSourceDescription#getStartReadTime()} is set, seek the start offset based on\n+ *       this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer}\n+ * is used in the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link KafkaSourceDescription}.\n+ */\n+@AutoValue\n+public abstract class ReadFromKafkaViaSDF<K, V>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY2NjcwMA=="}, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDczOTY1OQ==", "bodyText": "Hi Isma\u00ebl, thanks for your review and comments!\nYes I thought about using KafkaIO.Read as element and there are some pushbacks from my side:\n\nKafkaIO.Read is kind of heavy. For me, KafkaIO.Read is more like a configuration + element. For me, the element should be something you may only know about it during the pipeline execution time. So I want to isolate element-like into KafkaSourceDescription.\nFor the case you mentioned that reading from different clusters, I thought about it and not sure whether it would be a common case for reading from Kafka. So I sent out an email titled with [Discuss] Build Kafka read transform on top of SplittableDoFn to our dev mailing list to figure what could be an element in common. So far I didn't hear back from the community for the need of reading from different clusters.\nWe also consider x-lang usage for ReadFromKafkaViaSDF, which requires we can encode and decode the element over the wire. So I want to make the element as low weight as possible.\n\nFor the concern of increasing needs of element, we want to have KafkaSourceDescription easy to be extended, as well as the coder.", "url": "https://github.com/apache/beam/pull/11749#discussion_r434739659", "createdAt": "2020-06-03T17:35:09Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaViaSDF.java", "diffHunk": "@@ -0,0 +1,697 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.Element;\n+import org.apache.beam.sdk.transforms.DoFn.GetRestrictionCoder;\n+import org.apache.beam.sdk.transforms.DoFn.OutputReceiver;\n+import org.apache.beam.sdk.transforms.DoFn.ProcessElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the beam blog post: https://beam.apache.org/blog/splittable-do-fn/ and design\n+ * doc:https://s.apache.org/beam-fn-api. The major difference from {@link KafkaIO.Read} is, {@link\n+ * ReadFromKafkaViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadFromKafkaViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyCoder()} is the same as {@link\n+ *       KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueCoder()} is the same as {@link\n+ *       KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadFromKafkaViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadFromKafkaViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#commitOffsets()} enables committing offset after processing the\n+ * record. Note that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadFromKafkaViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a\n+ * function which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is\n+ * used to produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadFromKafkaViaSDF#withProcessingTime()}, {@link ReadFromKafkaViaSDF#withCreateTime()} and\n+ * {@link ReadFromKafkaViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadFromKafkaViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@link KafkaSourceDescription#getStartOffset()} is set, use this offset as start.\n+ *   <li>If {@link KafkaSourceDescription#getStartReadTime()} is set, seek the start offset based on\n+ *       this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer}\n+ * is used in the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link KafkaSourceDescription}.\n+ */\n+@AutoValue\n+public abstract class ReadFromKafkaViaSDF<K, V>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY2NjcwMA=="}, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTM1NDc1MA==", "bodyText": "I'm agree with @iemejia in the way that HbaseIO is a good example when we can end up with reimplementing new Read in the dedicated initially more light class, like KafkaSourceDescription, in the end.\n\nYes, it's quite heavy but we don't expect too many elements like this in the PCollection, right? So it should not be a problem.\nOf course, reading from several clusters is not a main feature but, afair, we had several requests on that from users.\nAgree, this is a good point. Do we have any principal objections to use Read for that?", "url": "https://github.com/apache/beam/pull/11749#discussion_r435354750", "createdAt": "2020-06-04T15:36:58Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaViaSDF.java", "diffHunk": "@@ -0,0 +1,697 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.Element;\n+import org.apache.beam.sdk.transforms.DoFn.GetRestrictionCoder;\n+import org.apache.beam.sdk.transforms.DoFn.OutputReceiver;\n+import org.apache.beam.sdk.transforms.DoFn.ProcessElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the beam blog post: https://beam.apache.org/blog/splittable-do-fn/ and design\n+ * doc:https://s.apache.org/beam-fn-api. The major difference from {@link KafkaIO.Read} is, {@link\n+ * ReadFromKafkaViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadFromKafkaViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyCoder()} is the same as {@link\n+ *       KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueCoder()} is the same as {@link\n+ *       KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadFromKafkaViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadFromKafkaViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#commitOffsets()} enables committing offset after processing the\n+ * record. Note that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadFromKafkaViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a\n+ * function which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is\n+ * used to produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadFromKafkaViaSDF#withProcessingTime()}, {@link ReadFromKafkaViaSDF#withCreateTime()} and\n+ * {@link ReadFromKafkaViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadFromKafkaViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@link KafkaSourceDescription#getStartOffset()} is set, use this offset as start.\n+ *   <li>If {@link KafkaSourceDescription#getStartReadTime()} is set, seek the start offset based on\n+ *       this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer}\n+ * is used in the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link KafkaSourceDescription}.\n+ */\n+@AutoValue\n+public abstract class ReadFromKafkaViaSDF<K, V>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY2NjcwMA=="}, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjEyMzMzMA==", "bodyText": "Sorry for the late.\n\n\nAgree, this is a good point. Do we have any principal objections to use Read for that?\n\n\nYes, there are some concerns around that. As I mentioned in point1, KafkaIO.Read contains lots of configurations which are supported to determined during runtime. For these part, they shouldn't be the element logically. Also KafkaIO.Read is also used by the user who doesn't migrate to fnapi yet, which may last for a long time. It will not be easy for us to maintain or develop new things based on KafkaIO.Read when considering compatibility.\nI think we should figure out the major needs from KafkaIO beam user, especially what they want to be dynamic during pipeline runtime. Alexey, do you know what can be a good place to collect such idea?", "url": "https://github.com/apache/beam/pull/11749#discussion_r436123330", "createdAt": "2020-06-05T19:32:49Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaViaSDF.java", "diffHunk": "@@ -0,0 +1,697 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.Element;\n+import org.apache.beam.sdk.transforms.DoFn.GetRestrictionCoder;\n+import org.apache.beam.sdk.transforms.DoFn.OutputReceiver;\n+import org.apache.beam.sdk.transforms.DoFn.ProcessElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the beam blog post: https://beam.apache.org/blog/splittable-do-fn/ and design\n+ * doc:https://s.apache.org/beam-fn-api. The major difference from {@link KafkaIO.Read} is, {@link\n+ * ReadFromKafkaViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadFromKafkaViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyCoder()} is the same as {@link\n+ *       KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueCoder()} is the same as {@link\n+ *       KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadFromKafkaViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadFromKafkaViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#commitOffsets()} enables committing offset after processing the\n+ * record. Note that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadFromKafkaViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a\n+ * function which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is\n+ * used to produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadFromKafkaViaSDF#withProcessingTime()}, {@link ReadFromKafkaViaSDF#withCreateTime()} and\n+ * {@link ReadFromKafkaViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadFromKafkaViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@link KafkaSourceDescription#getStartOffset()} is set, use this offset as start.\n+ *   <li>If {@link KafkaSourceDescription#getStartReadTime()} is set, seek the start offset based on\n+ *       this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer}\n+ * is used in the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link KafkaSourceDescription}.\n+ */\n+@AutoValue\n+public abstract class ReadFromKafkaViaSDF<K, V>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY2NjcwMA=="}, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjg3NTY3Mw==", "bodyText": "I'm not worried about the size of the source descriptor for Kafka but more about the pattern in which people would build and use these objects and about the backwards/forwards compatibility in a pipeline update scenario. I think it would be best if we used a dedicated object not extending the PTransform because:\n\nit simplifies the migration scenario to a 100% SDF world. Read becomes a wrapper around Create(source descriptor) -> ReadAll when using SDF otherwise we use the existing UnboundedSource implementation and it pulls parameters off of this source descriptor\nwe don't couple source descriptor with classes/evolution of pipeline construction APIs\nseems awkward from a user perspective to have a PCollection -> PTransform -> PCollection", "url": "https://github.com/apache/beam/pull/11749#discussion_r436875673", "createdAt": "2020-06-08T17:30:46Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaViaSDF.java", "diffHunk": "@@ -0,0 +1,697 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.Element;\n+import org.apache.beam.sdk.transforms.DoFn.GetRestrictionCoder;\n+import org.apache.beam.sdk.transforms.DoFn.OutputReceiver;\n+import org.apache.beam.sdk.transforms.DoFn.ProcessElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the beam blog post: https://beam.apache.org/blog/splittable-do-fn/ and design\n+ * doc:https://s.apache.org/beam-fn-api. The major difference from {@link KafkaIO.Read} is, {@link\n+ * ReadFromKafkaViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadFromKafkaViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyCoder()} is the same as {@link\n+ *       KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueCoder()} is the same as {@link\n+ *       KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadFromKafkaViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadFromKafkaViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#commitOffsets()} enables committing offset after processing the\n+ * record. Note that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadFromKafkaViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a\n+ * function which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is\n+ * used to produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadFromKafkaViaSDF#withProcessingTime()}, {@link ReadFromKafkaViaSDF#withCreateTime()} and\n+ * {@link ReadFromKafkaViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadFromKafkaViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@link KafkaSourceDescription#getStartOffset()} is set, use this offset as start.\n+ *   <li>If {@link KafkaSourceDescription#getStartReadTime()} is set, seek the start offset based on\n+ *       this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer}\n+ * is used in the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link KafkaSourceDescription}.\n+ */\n+@AutoValue\n+public abstract class ReadFromKafkaViaSDF<K, V>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY2NjcwMA=="}, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzODk0OTk5OQ==", "bodyText": "I'm agree with @iemejia in the way that HbaseIO is a good example when we can end up with reimplementing new Read in the dedicated initially more light class, like KafkaSourceDescription, in the end.\n\nYes, it's quite heavy but we don't expect too many elements like this in the PCollection, right? So it should not be a problem.\nOf course, reading from several clusters is not a main feature but, afair, we had several requests on that from users.\nAgree, this is a good point. Do we have any principal objections to use Read for that?\n\n\nHi Alexey, I want to elaborate  more details on having bootstrapServer dynamically. I'm wondering what should we do if the user sets bootstrapServer in the consumer config but also emits different bootstrapServer dynamically. In this case, should we override the consumer config with the dynamic one or we just update the consumer config with the new one?", "url": "https://github.com/apache/beam/pull/11749#discussion_r438949999", "createdAt": "2020-06-11T17:26:57Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaViaSDF.java", "diffHunk": "@@ -0,0 +1,697 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.Element;\n+import org.apache.beam.sdk.transforms.DoFn.GetRestrictionCoder;\n+import org.apache.beam.sdk.transforms.DoFn.OutputReceiver;\n+import org.apache.beam.sdk.transforms.DoFn.ProcessElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the beam blog post: https://beam.apache.org/blog/splittable-do-fn/ and design\n+ * doc:https://s.apache.org/beam-fn-api. The major difference from {@link KafkaIO.Read} is, {@link\n+ * ReadFromKafkaViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadFromKafkaViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyCoder()} is the same as {@link\n+ *       KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueCoder()} is the same as {@link\n+ *       KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadFromKafkaViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadFromKafkaViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#commitOffsets()} enables committing offset after processing the\n+ * record. Note that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadFromKafkaViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a\n+ * function which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is\n+ * used to produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadFromKafkaViaSDF#withProcessingTime()}, {@link ReadFromKafkaViaSDF#withCreateTime()} and\n+ * {@link ReadFromKafkaViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadFromKafkaViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@link KafkaSourceDescription#getStartOffset()} is set, use this offset as start.\n+ *   <li>If {@link KafkaSourceDescription#getStartReadTime()} is set, seek the start offset based on\n+ *       this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer}\n+ * is used in the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link KafkaSourceDescription}.\n+ */\n+@AutoValue\n+public abstract class ReadFromKafkaViaSDF<K, V>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY2NjcwMA=="}, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUwOTkxMw==", "bodyText": "@boyuanzz It's quite tricky question. Well, initially we use the static list of bootstrap servers for message and offset consumers. So we expect them equal and I think it should be the same with SDF.\nMessage consumer is also used to fetch the topic(s) partitions for initial split. With SDF we don't need that.\nWhat is not clear for me, is by \"user emits different bootstrapServer dynamically\" you mean that it will be set in a runtime? Is it going to be changed during a runtime or set only once?", "url": "https://github.com/apache/beam/pull/11749#discussion_r439509913", "createdAt": "2020-06-12T16:08:20Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaViaSDF.java", "diffHunk": "@@ -0,0 +1,697 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.Element;\n+import org.apache.beam.sdk.transforms.DoFn.GetRestrictionCoder;\n+import org.apache.beam.sdk.transforms.DoFn.OutputReceiver;\n+import org.apache.beam.sdk.transforms.DoFn.ProcessElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the beam blog post: https://beam.apache.org/blog/splittable-do-fn/ and design\n+ * doc:https://s.apache.org/beam-fn-api. The major difference from {@link KafkaIO.Read} is, {@link\n+ * ReadFromKafkaViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadFromKafkaViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyCoder()} is the same as {@link\n+ *       KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueCoder()} is the same as {@link\n+ *       KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadFromKafkaViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadFromKafkaViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#commitOffsets()} enables committing offset after processing the\n+ * record. Note that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadFromKafkaViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a\n+ * function which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is\n+ * used to produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadFromKafkaViaSDF#withProcessingTime()}, {@link ReadFromKafkaViaSDF#withCreateTime()} and\n+ * {@link ReadFromKafkaViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadFromKafkaViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@link KafkaSourceDescription#getStartOffset()} is set, use this offset as start.\n+ *   <li>If {@link KafkaSourceDescription#getStartReadTime()} is set, seek the start offset based on\n+ *       this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer}\n+ * is used in the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link KafkaSourceDescription}.\n+ */\n+@AutoValue\n+public abstract class ReadFromKafkaViaSDF<K, V>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY2NjcwMA=="}, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTUzMTI1NA==", "bodyText": "The question here is, do we want to only have bootstrap servers in element during runtime, or we want to allow the user to set the bootstrap server when constructing the pipeline, and mean while also allow the user to provide new bootstrap server during runtime. For example, if the customer gives the static list of [bootstrap1] during construction time, and also give new bootstrap2 in the runtime, what should be the list we use to connect:\n\njust use [bootstrap1]\nor just use [bootstrap2]\nor [bootstrap1, bootstrap2]", "url": "https://github.com/apache/beam/pull/11749#discussion_r439531254", "createdAt": "2020-06-12T16:46:42Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaViaSDF.java", "diffHunk": "@@ -0,0 +1,697 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.Element;\n+import org.apache.beam.sdk.transforms.DoFn.GetRestrictionCoder;\n+import org.apache.beam.sdk.transforms.DoFn.OutputReceiver;\n+import org.apache.beam.sdk.transforms.DoFn.ProcessElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the beam blog post: https://beam.apache.org/blog/splittable-do-fn/ and design\n+ * doc:https://s.apache.org/beam-fn-api. The major difference from {@link KafkaIO.Read} is, {@link\n+ * ReadFromKafkaViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadFromKafkaViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyCoder()} is the same as {@link\n+ *       KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueCoder()} is the same as {@link\n+ *       KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadFromKafkaViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadFromKafkaViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#commitOffsets()} enables committing offset after processing the\n+ * record. Note that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadFromKafkaViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a\n+ * function which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is\n+ * used to produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadFromKafkaViaSDF#withProcessingTime()}, {@link ReadFromKafkaViaSDF#withCreateTime()} and\n+ * {@link ReadFromKafkaViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadFromKafkaViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@link KafkaSourceDescription#getStartOffset()} is set, use this offset as start.\n+ *   <li>If {@link KafkaSourceDescription#getStartReadTime()} is set, seek the start offset based on\n+ *       this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer}\n+ * is used in the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link KafkaSourceDescription}.\n+ */\n+@AutoValue\n+public abstract class ReadFromKafkaViaSDF<K, V>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY2NjcwMA=="}, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU0NzQyNw==", "bodyText": "I'd suggest to stick with 1 (at least for now). 2 and 3 seems to me could be an error prone since I expect it will require much more work in case if the new servers will be related to different cluster, for example. Wdyt?", "url": "https://github.com/apache/beam/pull/11749#discussion_r439547427", "createdAt": "2020-06-12T17:19:53Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaViaSDF.java", "diffHunk": "@@ -0,0 +1,697 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.Element;\n+import org.apache.beam.sdk.transforms.DoFn.GetRestrictionCoder;\n+import org.apache.beam.sdk.transforms.DoFn.OutputReceiver;\n+import org.apache.beam.sdk.transforms.DoFn.ProcessElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the beam blog post: https://beam.apache.org/blog/splittable-do-fn/ and design\n+ * doc:https://s.apache.org/beam-fn-api. The major difference from {@link KafkaIO.Read} is, {@link\n+ * ReadFromKafkaViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadFromKafkaViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyCoder()} is the same as {@link\n+ *       KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueCoder()} is the same as {@link\n+ *       KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadFromKafkaViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadFromKafkaViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#commitOffsets()} enables committing offset after processing the\n+ * record. Note that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadFromKafkaViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a\n+ * function which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is\n+ * used to produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadFromKafkaViaSDF#withProcessingTime()}, {@link ReadFromKafkaViaSDF#withCreateTime()} and\n+ * {@link ReadFromKafkaViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadFromKafkaViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@link KafkaSourceDescription#getStartOffset()} is set, use this offset as start.\n+ *   <li>If {@link KafkaSourceDescription#getStartReadTime()} is set, seek the start offset based on\n+ *       this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer}\n+ * is used in the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link KafkaSourceDescription}.\n+ */\n+@AutoValue\n+public abstract class ReadFromKafkaViaSDF<K, V>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY2NjcwMA=="}, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU2Mjk1MQ==", "bodyText": "Since we want to empower the user to have bootstrapServer on the fly, I think we we have 2 options:\n\nWe remove the bootstrapServers from construction time configuration and KafkaSourceDescription will be the source of truth.\nWe still allow setting default bootstrapServers at construction time, and treat KafkaSourceDescription.bootstrapServers as override.\n\nI'm leaning to #2 as long as it's easy for the user to understand the meaning of override.", "url": "https://github.com/apache/beam/pull/11749#discussion_r439562951", "createdAt": "2020-06-12T17:52:02Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaViaSDF.java", "diffHunk": "@@ -0,0 +1,697 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.Element;\n+import org.apache.beam.sdk.transforms.DoFn.GetRestrictionCoder;\n+import org.apache.beam.sdk.transforms.DoFn.OutputReceiver;\n+import org.apache.beam.sdk.transforms.DoFn.ProcessElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the beam blog post: https://beam.apache.org/blog/splittable-do-fn/ and design\n+ * doc:https://s.apache.org/beam-fn-api. The major difference from {@link KafkaIO.Read} is, {@link\n+ * ReadFromKafkaViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadFromKafkaViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyCoder()} is the same as {@link\n+ *       KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueCoder()} is the same as {@link\n+ *       KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadFromKafkaViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadFromKafkaViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#commitOffsets()} enables committing offset after processing the\n+ * record. Note that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadFromKafkaViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a\n+ * function which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is\n+ * used to produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadFromKafkaViaSDF#withProcessingTime()}, {@link ReadFromKafkaViaSDF#withCreateTime()} and\n+ * {@link ReadFromKafkaViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadFromKafkaViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@link KafkaSourceDescription#getStartOffset()} is set, use this offset as start.\n+ *   <li>If {@link KafkaSourceDescription#getStartReadTime()} is set, seek the start offset based on\n+ *       this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer}\n+ * is used in the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link KafkaSourceDescription}.\n+ */\n+@AutoValue\n+public abstract class ReadFromKafkaViaSDF<K, V>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY2NjcwMA=="}, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU2NTM5MQ==", "bodyText": "In this sense, I agree with option #2 as well.", "url": "https://github.com/apache/beam/pull/11749#discussion_r439565391", "createdAt": "2020-06-12T17:57:23Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaViaSDF.java", "diffHunk": "@@ -0,0 +1,697 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.Element;\n+import org.apache.beam.sdk.transforms.DoFn.GetRestrictionCoder;\n+import org.apache.beam.sdk.transforms.DoFn.OutputReceiver;\n+import org.apache.beam.sdk.transforms.DoFn.ProcessElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the beam blog post: https://beam.apache.org/blog/splittable-do-fn/ and design\n+ * doc:https://s.apache.org/beam-fn-api. The major difference from {@link KafkaIO.Read} is, {@link\n+ * ReadFromKafkaViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadFromKafkaViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyCoder()} is the same as {@link\n+ *       KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueCoder()} is the same as {@link\n+ *       KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadFromKafkaViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadFromKafkaViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#commitOffsets()} enables committing offset after processing the\n+ * record. Note that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadFromKafkaViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a\n+ * function which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is\n+ * used to produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadFromKafkaViaSDF#withProcessingTime()}, {@link ReadFromKafkaViaSDF#withCreateTime()} and\n+ * {@link ReadFromKafkaViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadFromKafkaViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@link KafkaSourceDescription#getStartOffset()} is set, use this offset as start.\n+ *   <li>If {@link KafkaSourceDescription#getStartReadTime()} is set, seek the start offset based on\n+ *       this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer}\n+ * is used in the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link KafkaSourceDescription}.\n+ */\n+@AutoValue\n+public abstract class ReadFromKafkaViaSDF<K, V>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY2NjcwMA=="}, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzOTU2NjI4MQ==", "bodyText": "Thanks! I'll ping this thread once the PR is ready for another round of review.", "url": "https://github.com/apache/beam/pull/11749#discussion_r439566281", "createdAt": "2020-06-12T17:59:27Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaViaSDF.java", "diffHunk": "@@ -0,0 +1,697 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.Element;\n+import org.apache.beam.sdk.transforms.DoFn.GetRestrictionCoder;\n+import org.apache.beam.sdk.transforms.DoFn.OutputReceiver;\n+import org.apache.beam.sdk.transforms.DoFn.ProcessElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the beam blog post: https://beam.apache.org/blog/splittable-do-fn/ and design\n+ * doc:https://s.apache.org/beam-fn-api. The major difference from {@link KafkaIO.Read} is, {@link\n+ * ReadFromKafkaViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadFromKafkaViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyCoder()} is the same as {@link\n+ *       KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueCoder()} is the same as {@link\n+ *       KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadFromKafkaViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadFromKafkaViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#commitOffsets()} enables committing offset after processing the\n+ * record. Note that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadFromKafkaViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a\n+ * function which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is\n+ * used to produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadFromKafkaViaSDF#withProcessingTime()}, {@link ReadFromKafkaViaSDF#withCreateTime()} and\n+ * {@link ReadFromKafkaViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadFromKafkaViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@link KafkaSourceDescription#getStartOffset()} is set, use this offset as start.\n+ *   <li>If {@link KafkaSourceDescription#getStartReadTime()} is set, seek the start offset based on\n+ *       this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer}\n+ * is used in the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link KafkaSourceDescription}.\n+ */\n+@AutoValue\n+public abstract class ReadFromKafkaViaSDF<K, V>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDY2NjcwMA=="}, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 186}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNzg2MjQyOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNjo1NjoxNFrOGelBWw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QyMjoyMzowOVrOGevoUw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcxNzAxOQ==", "bodyText": "Maybe call it just useSDF()? Because it's already known that it's a PTransform used in Read", "url": "https://github.com/apache/beam/pull/11749#discussion_r434717019", "createdAt": "2020-06-03T16:56:14Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -817,6 +847,24 @@ public void setValueDeserializer(String valueDeserializer) {\n       return toBuilder().setCommitOffsetsInFinalizeEnabled(true).build();\n     }\n \n+    /**\n+     * The {@link Read} transform will be expanded with {@link ReadFromKafkaViaSDF} transform. While\n+     * working with {@link #useSDFTransformInRead()} and {@link\n+     * #withTimestampPolicyFactory(TimestampPolicyFactory)} together, only {@link\n+     * TimestampPolicyFactory#withCreateTime(Duration)}, {@link\n+     * TimestampPolicyFactory#withLogAppendTime()} and {@link\n+     * TimestampPolicyFactory#withProcessingTime()} will be populated correctly. For other custom\n+     * {@link TimestampPolicy}, the transform will use {@link\n+     * TimestampPolicyFactory#withProcessingTime()} by default. It's recommended to use {@link\n+     * ReadFromKafkaViaSDF} directly in that case.\n+     *\n+     * <p>Note that the expansion only happens when tbe pipeline has \"beam_fn_api\" experiment and\n+     * meanwhile \"beam_fn_api_use_deprecated_read\" is not set.\n+     */\n+    public Read<K, V> useSDFTransformInRead() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 182}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDg5MDgzNQ==", "bodyText": "Discussed with Luke offline. We think it would be better to make KafkaIO.Read() expand with SDF transform bu default when beam_fn_api is enabled( before introducing this SDF transform, we expand the Read with SDFUnboundedWrapper with beam_fn_api).", "url": "https://github.com/apache/beam/pull/11749#discussion_r434890835", "createdAt": "2020-06-03T22:23:09Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -817,6 +847,24 @@ public void setValueDeserializer(String valueDeserializer) {\n       return toBuilder().setCommitOffsetsInFinalizeEnabled(true).build();\n     }\n \n+    /**\n+     * The {@link Read} transform will be expanded with {@link ReadFromKafkaViaSDF} transform. While\n+     * working with {@link #useSDFTransformInRead()} and {@link\n+     * #withTimestampPolicyFactory(TimestampPolicyFactory)} together, only {@link\n+     * TimestampPolicyFactory#withCreateTime(Duration)}, {@link\n+     * TimestampPolicyFactory#withLogAppendTime()} and {@link\n+     * TimestampPolicyFactory#withProcessingTime()} will be populated correctly. For other custom\n+     * {@link TimestampPolicy}, the transform will use {@link\n+     * TimestampPolicyFactory#withProcessingTime()} by default. It's recommended to use {@link\n+     * ReadFromKafkaViaSDF} directly in that case.\n+     *\n+     * <p>Note that the expansion only happens when tbe pipeline has \"beam_fn_api\" experiment and\n+     * meanwhile \"beam_fn_api_use_deprecated_read\" is not set.\n+     */\n+    public Read<K, V> useSDFTransformInRead() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcxNzAxOQ=="}, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 182}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNzg3NzgxOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzowMDoxOVrOGelLUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNFQxNTo0MToxOVrOGfMNGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcxOTU2OA==", "bodyText": "It looks that we depend on some specific pipeline business logic here. I'd prefer to avoid this if possible.", "url": "https://github.com/apache/beam/pull/11749#discussion_r434719568", "createdAt": "2020-06-03T17:00:19Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -906,19 +955,110 @@ public void setValueDeserializer(String valueDeserializer) {\n       Coder<K> keyCoder = getKeyCoder(coderRegistry);\n       Coder<V> valueCoder = getValueCoder(coderRegistry);\n \n-      // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n-      Unbounded<KafkaRecord<K, V>> unbounded =\n-          org.apache.beam.sdk.io.Read.from(\n-              toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+      if (!isUseSDFTransform()\n+          || !ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc0NTcxMA==", "bodyText": "SDF is only supported over beam_fn_api. We shouldn't expand the KafkaIO.Read with SDF when the beam_fn_api is not enbaled, or beam_fn_api_use_deprecated_read  is enabled.", "url": "https://github.com/apache/beam/pull/11749#discussion_r434745710", "createdAt": "2020-06-03T17:45:47Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -906,19 +955,110 @@ public void setValueDeserializer(String valueDeserializer) {\n       Coder<K> keyCoder = getKeyCoder(coderRegistry);\n       Coder<V> valueCoder = getValueCoder(coderRegistry);\n \n-      // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n-      Unbounded<KafkaRecord<K, V>> unbounded =\n-          org.apache.beam.sdk.io.Read.from(\n-              toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+      if (!isUseSDFTransform()\n+          || !ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcxOTU2OA=="}, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 213}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNTM1OTAwMQ==", "bodyText": "In this case, please, add a comment about that.", "url": "https://github.com/apache/beam/pull/11749#discussion_r435359001", "createdAt": "2020-06-04T15:41:19Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -906,19 +955,110 @@ public void setValueDeserializer(String valueDeserializer) {\n       Coder<K> keyCoder = getKeyCoder(coderRegistry);\n       Coder<V> valueCoder = getValueCoder(coderRegistry);\n \n-      // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n-      Unbounded<KafkaRecord<K, V>> unbounded =\n-          org.apache.beam.sdk.io.Read.from(\n-              toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+      if (!isUseSDFTransform()\n+          || !ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcxOTU2OA=="}, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 213}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNzg5MDMyOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzowMzo1MFrOGelTVg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzowMzo1MFrOGelTVg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcyMTYyMg==", "bodyText": "The same point as above.", "url": "https://github.com/apache/beam/pull/11749#discussion_r434721622", "createdAt": "2020-06-03T17:03:50Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -906,19 +955,110 @@ public void setValueDeserializer(String valueDeserializer) {\n       Coder<K> keyCoder = getKeyCoder(coderRegistry);\n       Coder<V> valueCoder = getValueCoder(coderRegistry);\n \n-      // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n-      Unbounded<KafkaRecord<K, V>> unbounded =\n-          org.apache.beam.sdk.io.Read.from(\n-              toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+      if (!isUseSDFTransform()\n+          || !ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\")\n+          || ExperimentalOptions.hasExperiment(\n+              input.getPipeline().getOptions(), \"beam_fn_api_use_deprecated_read\")) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 215}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNzkxMDc1OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIOUtils.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzoxMDowMlrOGelg4A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzo0Njo0OFrOGemznQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcyNTA4OA==", "bodyText": "I expect that all these constants and methods below were moved here without any changes and just for the sake of code refactoring. If not, please, add some comments on these.", "url": "https://github.com/apache/beam/pull/11749#discussion_r434725088", "createdAt": "2020-06-03T17:10:02Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIOUtils.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Random;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer;\n+\n+/**\n+ * Common utility functions and default configurations for {@link KafkaIO.Read} and {@link\n+ * ReadFromKafkaViaSDF}.\n+ */\n+final class KafkaIOUtils {\n+  // A set of config defaults.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc0NjI2OQ==", "bodyText": "Yes, the util is just moved from KafkaIO.java and for code-reuse purpose only.", "url": "https://github.com/apache/beam/pull/11749#discussion_r434746269", "createdAt": "2020-06-03T17:46:48Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIOUtils.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Random;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer;\n+\n+/**\n+ * Common utility functions and default configurations for {@link KafkaIO.Read} and {@link\n+ * ReadFromKafkaViaSDF}.\n+ */\n+final class KafkaIOUtils {\n+  // A set of config defaults.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDcyNTA4OA=="}, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 37}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNzk5MTE1OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaViaSDF.java", "isResolved": true, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzozMzowNFrOGemVTw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMTo1ODoxMFrOGmBlCw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDczODUxMQ==", "bodyText": "Do we need to expose it to user? Could it be just read() to be consistent with KafkaIO.Read?", "url": "https://github.com/apache/beam/pull/11749#discussion_r434738511", "createdAt": "2020-06-03T17:33:04Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaViaSDF.java", "diffHunk": "@@ -0,0 +1,697 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.Element;\n+import org.apache.beam.sdk.transforms.DoFn.GetRestrictionCoder;\n+import org.apache.beam.sdk.transforms.DoFn.OutputReceiver;\n+import org.apache.beam.sdk.transforms.DoFn.ProcessElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the beam blog post: https://beam.apache.org/blog/splittable-do-fn/ and design\n+ * doc:https://s.apache.org/beam-fn-api. The major difference from {@link KafkaIO.Read} is, {@link\n+ * ReadFromKafkaViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadFromKafkaViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyCoder()} is the same as {@link\n+ *       KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueCoder()} is the same as {@link\n+ *       KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadFromKafkaViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadFromKafkaViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#commitOffsets()} enables committing offset after processing the\n+ * record. Note that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadFromKafkaViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a\n+ * function which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is\n+ * used to produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadFromKafkaViaSDF#withProcessingTime()}, {@link ReadFromKafkaViaSDF#withCreateTime()} and\n+ * {@link ReadFromKafkaViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadFromKafkaViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@link KafkaSourceDescription#getStartOffset()} is set, use this offset as start.\n+ *   <li>If {@link KafkaSourceDescription#getStartReadTime()} is set, seek the start offset based on\n+ *       this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer}\n+ * is used in the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link KafkaSourceDescription}.\n+ */\n+@AutoValue\n+public abstract class ReadFromKafkaViaSDF<K, V>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadFromKafkaViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract ReadFromKafkaViaSDF<K, V> build();\n+  }\n+\n+  public static <K, V> ReadFromKafkaViaSDF<K, V> create() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 242}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE4NzI0NQ==", "bodyText": "Yes, it's exposed to the user since the user may want to use this transform separate from KafkaIO.Read(), for example, the user can get TopicPartition from BigQuery during runtime.\nI'll rename to read. Thanks for the naming suggestion!", "url": "https://github.com/apache/beam/pull/11749#discussion_r436187245", "createdAt": "2020-06-05T22:11:22Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaViaSDF.java", "diffHunk": "@@ -0,0 +1,697 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.Element;\n+import org.apache.beam.sdk.transforms.DoFn.GetRestrictionCoder;\n+import org.apache.beam.sdk.transforms.DoFn.OutputReceiver;\n+import org.apache.beam.sdk.transforms.DoFn.ProcessElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the beam blog post: https://beam.apache.org/blog/splittable-do-fn/ and design\n+ * doc:https://s.apache.org/beam-fn-api. The major difference from {@link KafkaIO.Read} is, {@link\n+ * ReadFromKafkaViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadFromKafkaViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyCoder()} is the same as {@link\n+ *       KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueCoder()} is the same as {@link\n+ *       KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadFromKafkaViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadFromKafkaViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#commitOffsets()} enables committing offset after processing the\n+ * record. Note that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadFromKafkaViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a\n+ * function which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is\n+ * used to produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadFromKafkaViaSDF#withProcessingTime()}, {@link ReadFromKafkaViaSDF#withCreateTime()} and\n+ * {@link ReadFromKafkaViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadFromKafkaViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@link KafkaSourceDescription#getStartOffset()} is set, use this offset as start.\n+ *   <li>If {@link KafkaSourceDescription#getStartReadTime()} is set, seek the start offset based on\n+ *       this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer}\n+ * is used in the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link KafkaSourceDescription}.\n+ */\n+@AutoValue\n+public abstract class ReadFromKafkaViaSDF<K, V>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadFromKafkaViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract ReadFromKafkaViaSDF<K, V> build();\n+  }\n+\n+  public static <K, V> ReadFromKafkaViaSDF<K, V> create() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDczODUxMQ=="}, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 242}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjg4MDEwNw==", "bodyText": "I don't think we should expose ReadFromKafkaViaSDF and instead rely on the experiments to toggle which of the two backing implementations the user gets for the existing KafkaIO.Read transform. This transform would be like\nexpand() {\nif (beam_fn_api) {\n    Create(source descriptor) -> ReadAll()\n  } else {\n    Read(UnboundedKafkaSource)\n  }\n}\n\nWe could add a separate ReadAll implementation which takes a PCollection of source descriptors that exercises the SDF.", "url": "https://github.com/apache/beam/pull/11749#discussion_r436880107", "createdAt": "2020-06-08T17:38:01Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaViaSDF.java", "diffHunk": "@@ -0,0 +1,697 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.Element;\n+import org.apache.beam.sdk.transforms.DoFn.GetRestrictionCoder;\n+import org.apache.beam.sdk.transforms.DoFn.OutputReceiver;\n+import org.apache.beam.sdk.transforms.DoFn.ProcessElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the beam blog post: https://beam.apache.org/blog/splittable-do-fn/ and design\n+ * doc:https://s.apache.org/beam-fn-api. The major difference from {@link KafkaIO.Read} is, {@link\n+ * ReadFromKafkaViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadFromKafkaViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyCoder()} is the same as {@link\n+ *       KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueCoder()} is the same as {@link\n+ *       KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadFromKafkaViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadFromKafkaViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#commitOffsets()} enables committing offset after processing the\n+ * record. Note that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadFromKafkaViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a\n+ * function which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is\n+ * used to produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadFromKafkaViaSDF#withProcessingTime()}, {@link ReadFromKafkaViaSDF#withCreateTime()} and\n+ * {@link ReadFromKafkaViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadFromKafkaViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@link KafkaSourceDescription#getStartOffset()} is set, use this offset as start.\n+ *   <li>If {@link KafkaSourceDescription#getStartReadTime()} is set, seek the start offset based on\n+ *       this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer}\n+ * is used in the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link KafkaSourceDescription}.\n+ */\n+@AutoValue\n+public abstract class ReadFromKafkaViaSDF<K, V>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadFromKafkaViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract ReadFromKafkaViaSDF<K, V> build();\n+  }\n+\n+  public static <K, V> ReadFromKafkaViaSDF<K, V> create() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDczODUxMQ=="}, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 242}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjkyMjg1MA==", "bodyText": "The current implementation does do the the same thing as you mentioned, except the naming(read() vs ReadAll()).", "url": "https://github.com/apache/beam/pull/11749#discussion_r436922850", "createdAt": "2020-06-08T18:54:33Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaViaSDF.java", "diffHunk": "@@ -0,0 +1,697 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.Element;\n+import org.apache.beam.sdk.transforms.DoFn.GetRestrictionCoder;\n+import org.apache.beam.sdk.transforms.DoFn.OutputReceiver;\n+import org.apache.beam.sdk.transforms.DoFn.ProcessElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the beam blog post: https://beam.apache.org/blog/splittable-do-fn/ and design\n+ * doc:https://s.apache.org/beam-fn-api. The major difference from {@link KafkaIO.Read} is, {@link\n+ * ReadFromKafkaViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadFromKafkaViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyCoder()} is the same as {@link\n+ *       KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueCoder()} is the same as {@link\n+ *       KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadFromKafkaViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadFromKafkaViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#commitOffsets()} enables committing offset after processing the\n+ * record. Note that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadFromKafkaViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a\n+ * function which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is\n+ * used to produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadFromKafkaViaSDF#withProcessingTime()}, {@link ReadFromKafkaViaSDF#withCreateTime()} and\n+ * {@link ReadFromKafkaViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadFromKafkaViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@link KafkaSourceDescription#getStartOffset()} is set, use this offset as start.\n+ *   <li>If {@link KafkaSourceDescription#getStartReadTime()} is set, seek the start offset based on\n+ *       this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer}\n+ * is used in the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link KafkaSourceDescription}.\n+ */\n+@AutoValue\n+public abstract class ReadFromKafkaViaSDF<K, V>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadFromKafkaViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract ReadFromKafkaViaSDF<K, V> build();\n+  }\n+\n+  public static <K, V> ReadFromKafkaViaSDF<K, V> create() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDczODUxMQ=="}, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 242}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUyNDkzOQ==", "bodyText": "I would suggest making ReadViaSDF package private and add a ReadAll PTransform to KafkaIO.", "url": "https://github.com/apache/beam/pull/11749#discussion_r442524939", "createdAt": "2020-06-18T21:58:10Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaViaSDF.java", "diffHunk": "@@ -0,0 +1,697 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.Element;\n+import org.apache.beam.sdk.transforms.DoFn.GetRestrictionCoder;\n+import org.apache.beam.sdk.transforms.DoFn.OutputReceiver;\n+import org.apache.beam.sdk.transforms.DoFn.ProcessElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the beam blog post: https://beam.apache.org/blog/splittable-do-fn/ and design\n+ * doc:https://s.apache.org/beam-fn-api. The major difference from {@link KafkaIO.Read} is, {@link\n+ * ReadFromKafkaViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadFromKafkaViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyCoder()} is the same as {@link\n+ *       KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueCoder()} is the same as {@link\n+ *       KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadFromKafkaViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadFromKafkaViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#commitOffsets()} enables committing offset after processing the\n+ * record. Note that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadFromKafkaViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a\n+ * function which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is\n+ * used to produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadFromKafkaViaSDF#withProcessingTime()}, {@link ReadFromKafkaViaSDF#withCreateTime()} and\n+ * {@link ReadFromKafkaViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadFromKafkaViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@link KafkaSourceDescription#getStartOffset()} is set, use this offset as start.\n+ *   <li>If {@link KafkaSourceDescription#getStartReadTime()} is set, seek the start offset based on\n+ *       this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer}\n+ * is used in the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link KafkaSourceDescription}.\n+ */\n+@AutoValue\n+public abstract class ReadFromKafkaViaSDF<K, V>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadFromKafkaViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract ReadFromKafkaViaSDF<K, V> build();\n+  }\n+\n+  public static <K, V> ReadFromKafkaViaSDF<K, V> create() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDczODUxMQ=="}, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 242}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcwNzk5ODA0OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaViaSDF.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxNzozNToxMFrOGemZ2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wM1QxODoyMToyOVrOGeoB2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDczOTY3Mw==", "bodyText": "Do all these configuration methods repeat KafkaIO.Read methods? Can we avoid a code duplication with new ReadFromKafkaViaSDF transform?", "url": "https://github.com/apache/beam/pull/11749#discussion_r434739673", "createdAt": "2020-06-03T17:35:10Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaViaSDF.java", "diffHunk": "@@ -0,0 +1,697 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.Element;\n+import org.apache.beam.sdk.transforms.DoFn.GetRestrictionCoder;\n+import org.apache.beam.sdk.transforms.DoFn.OutputReceiver;\n+import org.apache.beam.sdk.transforms.DoFn.ProcessElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the beam blog post: https://beam.apache.org/blog/splittable-do-fn/ and design\n+ * doc:https://s.apache.org/beam-fn-api. The major difference from {@link KafkaIO.Read} is, {@link\n+ * ReadFromKafkaViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadFromKafkaViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyCoder()} is the same as {@link\n+ *       KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueCoder()} is the same as {@link\n+ *       KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadFromKafkaViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadFromKafkaViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#commitOffsets()} enables committing offset after processing the\n+ * record. Note that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadFromKafkaViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a\n+ * function which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is\n+ * used to produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadFromKafkaViaSDF#withProcessingTime()}, {@link ReadFromKafkaViaSDF#withCreateTime()} and\n+ * {@link ReadFromKafkaViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadFromKafkaViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@link KafkaSourceDescription#getStartOffset()} is set, use this offset as start.\n+ *   <li>If {@link KafkaSourceDescription#getStartReadTime()} is set, seek the start offset based on\n+ *       this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer}\n+ * is used in the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link KafkaSourceDescription}.\n+ */\n+@AutoValue\n+public abstract class ReadFromKafkaViaSDF<K, V>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadFromKafkaViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract ReadFromKafkaViaSDF<K, V> build();\n+  }\n+\n+  public static <K, V> ReadFromKafkaViaSDF<K, V> create() {\n+    return new AutoValue_ReadFromKafkaViaSDF.Builder<K, V>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime())\n+        .setCommitOffsetEnabled(false)\n+        .build();\n+  }\n+", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 250}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDc2NjI5Nw==", "bodyText": "As mentioned in the javadoc of ReadFromKafkaViaSDF, all consumer-related configurations are the same to KafkaIO.Read. ReadFromKafkaViaSDF has 2 special configurations:\n\nsetExtractOutputTimestampFn to set a function to extract output timestamp from KafkaRecord\nsetCommitOffsetEnabled to enable committing offset manually.\n\nThe duplications are all for building the transform with common configurations. I was thinking about refactoring KafkaIO into a factory like pattern, which can build a KafkaIO.Read or ReadFromKafkaViaSDF. This refactor is not backward compatible for the API user, which means the user need to rewrite the pipeline if they want to work with the new SDK. My purpose here is to make the code changes for the use as less as possible. But if that's a minor issue, refactor is the way to go.", "url": "https://github.com/apache/beam/pull/11749#discussion_r434766297", "createdAt": "2020-06-03T18:21:29Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaViaSDF.java", "diffHunk": "@@ -0,0 +1,697 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.Element;\n+import org.apache.beam.sdk.transforms.DoFn.GetRestrictionCoder;\n+import org.apache.beam.sdk.transforms.DoFn.OutputReceiver;\n+import org.apache.beam.sdk.transforms.DoFn.ProcessElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the beam blog post: https://beam.apache.org/blog/splittable-do-fn/ and design\n+ * doc:https://s.apache.org/beam-fn-api. The major difference from {@link KafkaIO.Read} is, {@link\n+ * ReadFromKafkaViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadFromKafkaViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyCoder()} is the same as {@link\n+ *       KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueCoder()} is the same as {@link\n+ *       KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadFromKafkaViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadFromKafkaViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadFromKafkaViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#commitOffsets()} enables committing offset after processing the\n+ * record. Note that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadFromKafkaViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadFromKafkaViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a\n+ * function which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is\n+ * used to produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadFromKafkaViaSDF#withProcessingTime()}, {@link ReadFromKafkaViaSDF#withCreateTime()} and\n+ * {@link ReadFromKafkaViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadFromKafkaViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@link KafkaSourceDescription#getStartOffset()} is set, use this offset as start.\n+ *   <li>If {@link KafkaSourceDescription#getStartReadTime()} is set, seek the start offset based on\n+ *       this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer}\n+ * is used in the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link KafkaSourceDescription}.\n+ */\n+@AutoValue\n+public abstract class ReadFromKafkaViaSDF<K, V>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadFromKafkaViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract ReadFromKafkaViaSDF<K, V> build();\n+  }\n+\n+  public static <K, V> ReadFromKafkaViaSDF<K, V> create() {\n+    return new AutoValue_ReadFromKafkaViaSDF.Builder<K, V>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime())\n+        .setCommitOffsetEnabled(false)\n+        .build();\n+  }\n+", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNDczOTY3Mw=="}, "originalCommit": {"oid": "23ae88e6e442ca55637a4b1c57560fb4698c513a"}, "originalPosition": 250}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxNjkxOTc2OnYy", "diffSide": "LEFT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQyMTo0MzoyNVrOGf-Jow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQyMTo0MzoyNVrOGf-Jow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE3NzMxNQ==", "bodyText": "This common part is moved to the KafkaIOUtil.java.", "url": "https://github.com/apache/beam/pull/11749#discussion_r436177315", "createdAt": "2020-06-05T21:43:25Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -949,45 +1077,6 @@ public void setValueDeserializer(String valueDeserializer) {\n             final SerializableFunction<KV<KeyT, ValueT>, OutT> fn) {\n       return record -> fn.apply(record.getKV());\n     }\n-    ///////////////////////////////////////////////////////////////////////////////////////", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "deaae9aa7782ef87a2a9c477d375c01896e959d7"}, "originalPosition": 340}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjcxNjkzMjI0OnYy", "diffSide": "LEFT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQyMTo0OTozN1rOGf-RnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0wNVQyMTo0OTozN1rOGf-RnA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQzNjE3OTM1Ng==", "bodyText": "This common part is moved to the KafkaIOUtil.java", "url": "https://github.com/apache/beam/pull/11749#discussion_r436179356", "createdAt": "2020-06-05T21:49:37Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1055,29 +1144,6 @@ public void populateDisplayData(DisplayData.Builder builder) {\n \n   private static final Logger LOG = LoggerFactory.getLogger(KafkaIO.class);\n \n-  /**", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "deaae9aa7782ef87a2a9c477d375c01896e959d7"}, "originalPosition": 395}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1Njc3OTYxOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIOUtils.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMDo0NTozMFrOGl_lxw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMDo0NTozMFrOGl_lxw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ5MjM1OQ==", "bodyText": "nit:\nIGNORED_CONSUMER_PROPERTIES -> DISALLOWED_CONSUMER_PROPERTIES", "url": "https://github.com/apache/beam/pull/11749#discussion_r442492359", "createdAt": "2020-06-18T20:45:30Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIOUtils.java", "diffHunk": "@@ -0,0 +1,146 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Random;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.KafkaConsumer;\n+import org.apache.kafka.common.serialization.ByteArrayDeserializer;\n+\n+/**\n+ * Common utility functions and default configurations for {@link KafkaIO.Read} and {@link\n+ * ReadViaSDF}.\n+ */\n+final class KafkaIOUtils {\n+  // A set of config defaults.\n+  static final Map<String, Object> DEFAULT_CONSUMER_PROPERTIES =\n+      ImmutableMap.of(\n+          ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG,\n+          ByteArrayDeserializer.class.getName(),\n+          ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG,\n+          ByteArrayDeserializer.class.getName(),\n+\n+          // Use large receive buffer. Once KAFKA-3135 is fixed, this _may_ not be required.\n+          // with default value of of 32K, It takes multiple seconds between successful polls.\n+          // All the consumer work is done inside poll(), with smaller send buffer size, it\n+          // takes many polls before a 1MB chunk from the server is fully read. In my testing\n+          // about half of the time select() inside kafka consumer waited for 20-30ms, though\n+          // the server had lots of data in tcp send buffers on its side. Compared to default,\n+          // this setting increased throughput by many fold (3-4x).\n+          ConsumerConfig.RECEIVE_BUFFER_CONFIG,\n+          512 * 1024,\n+\n+          // default to latest offset when we are not resuming.\n+          ConsumerConfig.AUTO_OFFSET_RESET_CONFIG,\n+          \"latest\",\n+          // disable auto commit of offsets. we don't require group_id. could be enabled by user.\n+          ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG,\n+          false);\n+\n+  // A set of properties that are not required or don't make sense for our consumer.\n+  static final Map<String, String> IGNORED_CONSUMER_PROPERTIES =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 63}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1Njc5Mjk5OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMDo1MDowMFrOGl_ucQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxNzowMDoxMFrOGmbmIw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ5NDU3Nw==", "bodyText": "It looks like we overwrite the setExtractOutputTimestampFn regardless of whether the experiment is enabled or not which doesn't align with what the comment is telling us.", "url": "https://github.com/apache/beam/pull/11749#discussion_r442494577", "createdAt": "2020-06-18T20:50:00Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -681,11 +696,13 @@ public void setValueDeserializer(String valueDeserializer) {\n     }\n \n     /**\n-     * Sets {@link TimestampPolicy} to {@link TimestampPolicyFactory.LogAppendTimePolicy}. The\n-     * policy assigns Kafka's log append time (server side ingestion time) to each record. The\n-     * watermark for each Kafka partition is the timestamp of the last record read. If a partition\n-     * is idle, the watermark advances to couple of seconds behind wall time. Every record consumed\n-     * from Kafka is expected to have its timestamp type set to 'LOG_APPEND_TIME'.\n+     * Sets {@link TimestampPolicy} to {@link TimestampPolicyFactory.LogAppendTimePolicy} which is\n+     * used when beam_fn_api is disabled, and sets {@code extractOutputTimestampFn} as {@link\n+     * ReadViaSDF.ExtractOutputTimestampFns#withLogAppendTime()}, which is used when beam_fn_api is\n+     * enabled. The policy assigns Kafka's log append time (server side ingestion time) to each\n+     * record. The watermark for each Kafka partition is the timestamp of the last record read. If a\n+     * partition is idle, the watermark advances to couple of seconds behind wall time. Every record\n+     * consumed from Kafka is expected to have its timestamp type set to 'LOG_APPEND_TIME'.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 130}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk1MTIwMw==", "bodyText": "The  which is used when beam_fn_api is disabled means TimestampPolicy is used when beam_fn_api is disabled. I'll say it explicitly.", "url": "https://github.com/apache/beam/pull/11749#discussion_r442951203", "createdAt": "2020-06-19T17:00:10Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -681,11 +696,13 @@ public void setValueDeserializer(String valueDeserializer) {\n     }\n \n     /**\n-     * Sets {@link TimestampPolicy} to {@link TimestampPolicyFactory.LogAppendTimePolicy}. The\n-     * policy assigns Kafka's log append time (server side ingestion time) to each record. The\n-     * watermark for each Kafka partition is the timestamp of the last record read. If a partition\n-     * is idle, the watermark advances to couple of seconds behind wall time. Every record consumed\n-     * from Kafka is expected to have its timestamp type set to 'LOG_APPEND_TIME'.\n+     * Sets {@link TimestampPolicy} to {@link TimestampPolicyFactory.LogAppendTimePolicy} which is\n+     * used when beam_fn_api is disabled, and sets {@code extractOutputTimestampFn} as {@link\n+     * ReadViaSDF.ExtractOutputTimestampFns#withLogAppendTime()}, which is used when beam_fn_api is\n+     * enabled. The policy assigns Kafka's log append time (server side ingestion time) to each\n+     * record. The watermark for each Kafka partition is the timestamp of the last record read. If a\n+     * partition is idle, the watermark advances to couple of seconds behind wall time. Every record\n+     * consumed from Kafka is expected to have its timestamp type set to 'LOG_APPEND_TIME'.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ5NDU3Nw=="}, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 130}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjgyMDk3OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMDo1OTozNVrOGmAA5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwMDoxNToxOFrOGmEHTg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ5OTMwMw==", "bodyText": "How is this different from withTimestampFn2?\nWe could fix the \"translation\" logic by storing each property the user sets and clear other properties that would be affected. Then in the expand step we can \"translate\" timestampFn2 to a WatermarkPolicy for the UnboundedSource version and use it directly in the SDF version.\nSetting the top level properties allow us to say that this property is supported when used as an SDF.\nAlso, what prevents us from supporting TimestampPolicy? We should be able to call it and give it the three pieces of information it requests (message backlog / backlog check time / current kafka record).", "url": "https://github.com/apache/beam/pull/11749#discussion_r442499303", "createdAt": "2020-06-18T20:59:35Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -795,6 +828,12 @@ public void setValueDeserializer(String valueDeserializer) {\n       return withWatermarkFn2(unwrapKafkaAndThen(watermarkFn));\n     }\n \n+    /** A function to the compute output timestamp from a {@link KafkaRecord}. */\n+    public Read<K, V> withExtractOutputTimestampFn(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 208}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU2NjQ3OA==", "bodyText": "How is this different from withTimestampFn2?\n\nwithTimestampFn2 has been deprecated  in KafkaIO.Read. The major concern of reusing withTimestampFn2 is, it will means differently under SDF and UnboundedSource, which causes confusion.\n\nSetting the top level properties allow us to say that this property is supported when used as an SDF.\n\nThis is what I want to do by having withExtractOutputTimestampFn \n\nAlso, what prevents us from supporting TimestampPolicy? We should be able to call it and give it the three pieces of information it requests (message backlog / backlog check time / current kafka record).\n\nThe difficulty is the message backlog / backlog check time is not memorized per (element, restriction). With SDF framework, the backlog is retrieved by called RestricitonTracker.getProgress(), we cannot call it per element in order to extract timestamp.", "url": "https://github.com/apache/beam/pull/11749#discussion_r442566478", "createdAt": "2020-06-19T00:15:18Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -795,6 +828,12 @@ public void setValueDeserializer(String valueDeserializer) {\n       return withWatermarkFn2(unwrapKafkaAndThen(watermarkFn));\n     }\n \n+    /** A function to the compute output timestamp from a {@link KafkaRecord}. */\n+    public Read<K, V> withExtractOutputTimestampFn(", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjQ5OTMwMw=="}, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 208}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1Njg1MzAwOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMToxMDozNlrOGmAVVQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMToxMDozNlrOGmAVVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUwNDUzMw==", "bodyText": "You should make the class static to prevent pulling in the parent transform during serialization. This would require passing forward arguments from the Read transform to the constructor. This will prevent accidentally capturing things we don't want captured.", "url": "https://github.com/apache/beam/pull/11749#discussion_r442504533", "createdAt": "2020-06-18T21:10:36Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -906,19 +946,123 @@ public void setValueDeserializer(String valueDeserializer) {\n       Coder<K> keyCoder = getKeyCoder(coderRegistry);\n       Coder<V> valueCoder = getValueCoder(coderRegistry);\n \n-      // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n-      Unbounded<KafkaRecord<K, V>> unbounded =\n-          org.apache.beam.sdk.io.Read.from(\n-              toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+      // The Read will be expanded into SDF transform when \"beam_fn_api\" is enabled and\n+      // \"beam_fn_api_use_deprecated_read\" is not enabled.\n+      if (!ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\")\n+          || ExperimentalOptions.hasExperiment(\n+              input.getPipeline().getOptions(), \"beam_fn_api_use_deprecated_read\")) {\n+        // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n+        Unbounded<KafkaRecord<K, V>> unbounded =\n+            org.apache.beam.sdk.io.Read.from(\n+                toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+\n+        PTransform<PBegin, PCollection<KafkaRecord<K, V>>> transform = unbounded;\n+\n+        if (getMaxNumRecords() < Long.MAX_VALUE || getMaxReadTime() != null) {\n+          transform =\n+              unbounded.withMaxReadTime(getMaxReadTime()).withMaxNumRecords(getMaxNumRecords());\n+        }\n+\n+        return input.getPipeline().apply(transform);\n+      } else {\n+        // If extractOutputTimestampFn is not set, use processing time by default.\n+        SerializableFunction<KafkaRecord<K, V>, Instant> timestampFn;\n+        if (getExtractOutputTimestampFn() != null) {\n+          timestampFn = getExtractOutputTimestampFn();\n+        } else {\n+          timestampFn = ReadViaSDF.ExtractOutputTimestampFns.useProcessingTime();\n+        }\n+        ReadViaSDF<K, V> readTransform =\n+            ReadViaSDF.<K, V>read()\n+                .withConsumerConfigOverrides(getConsumerConfig())\n+                .withOffsetConsumerConfigOverrides(getOffsetConsumerConfig())\n+                .withConsumerFactoryFn(getConsumerFactoryFn())\n+                .withKeyDeserializerProvider(getKeyDeserializerProvider())\n+                .withValueDeserializerProvider(getValueDeserializerProvider())\n+                .withExtractOutputTimestampFn(timestampFn);\n+        if (isCommitOffsetsInFinalizeEnabled()) {\n+          readTransform = readTransform.commitOffsets();\n+        }\n+\n+        return input\n+            .getPipeline()\n+            .apply(Impulse.create())\n+            .apply(\n+                ParDo.of(\n+                    new GenerateKafkaSourceDescription(\n+                        readTransform.getKafkaSourceDescriptionSchema())))\n+            .setCoder(RowCoder.of(readTransform.getKafkaSourceDescriptionSchema()))\n+            .apply(readTransform)\n+            .setCoder(KafkaRecordCoder.of(keyCoder, valueCoder));\n+      }\n+    }\n+\n+    /**\n+     * A DoFn which generates {@link Row} with {@link KafkaSourceDescriptionSchemas#getSchema()}\n+     * based on the configuration of {@link Read}.\n+     */\n+    @VisibleForTesting\n+    class GenerateKafkaSourceDescription extends DoFn<byte[], Row> {\n+      GenerateKafkaSourceDescription(Schema schema) {\n+        this.kafkaSourceDescriptionSchema = schema;\n+      }\n+\n+      private final Schema kafkaSourceDescriptionSchema;\n+\n+      private final Map<String, Object> consumerConfig = Read.this.getConsumerConfig();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 302}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1Njg1NTUzOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMToxMToyM1rOGmAW2A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMToxMToyM1rOGmAW2A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUwNDkyMA==", "bodyText": "instead of adding methods, make the member variable @VisibleForTesting", "url": "https://github.com/apache/beam/pull/11749#discussion_r442504920", "createdAt": "2020-06-18T21:11:23Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -906,19 +946,123 @@ public void setValueDeserializer(String valueDeserializer) {\n       Coder<K> keyCoder = getKeyCoder(coderRegistry);\n       Coder<V> valueCoder = getValueCoder(coderRegistry);\n \n-      // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n-      Unbounded<KafkaRecord<K, V>> unbounded =\n-          org.apache.beam.sdk.io.Read.from(\n-              toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+      // The Read will be expanded into SDF transform when \"beam_fn_api\" is enabled and\n+      // \"beam_fn_api_use_deprecated_read\" is not enabled.\n+      if (!ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\")\n+          || ExperimentalOptions.hasExperiment(\n+              input.getPipeline().getOptions(), \"beam_fn_api_use_deprecated_read\")) {\n+        // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n+        Unbounded<KafkaRecord<K, V>> unbounded =\n+            org.apache.beam.sdk.io.Read.from(\n+                toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+\n+        PTransform<PBegin, PCollection<KafkaRecord<K, V>>> transform = unbounded;\n+\n+        if (getMaxNumRecords() < Long.MAX_VALUE || getMaxReadTime() != null) {\n+          transform =\n+              unbounded.withMaxReadTime(getMaxReadTime()).withMaxNumRecords(getMaxNumRecords());\n+        }\n+\n+        return input.getPipeline().apply(transform);\n+      } else {\n+        // If extractOutputTimestampFn is not set, use processing time by default.\n+        SerializableFunction<KafkaRecord<K, V>, Instant> timestampFn;\n+        if (getExtractOutputTimestampFn() != null) {\n+          timestampFn = getExtractOutputTimestampFn();\n+        } else {\n+          timestampFn = ReadViaSDF.ExtractOutputTimestampFns.useProcessingTime();\n+        }\n+        ReadViaSDF<K, V> readTransform =\n+            ReadViaSDF.<K, V>read()\n+                .withConsumerConfigOverrides(getConsumerConfig())\n+                .withOffsetConsumerConfigOverrides(getOffsetConsumerConfig())\n+                .withConsumerFactoryFn(getConsumerFactoryFn())\n+                .withKeyDeserializerProvider(getKeyDeserializerProvider())\n+                .withValueDeserializerProvider(getValueDeserializerProvider())\n+                .withExtractOutputTimestampFn(timestampFn);\n+        if (isCommitOffsetsInFinalizeEnabled()) {\n+          readTransform = readTransform.commitOffsets();\n+        }\n+\n+        return input\n+            .getPipeline()\n+            .apply(Impulse.create())\n+            .apply(\n+                ParDo.of(\n+                    new GenerateKafkaSourceDescription(\n+                        readTransform.getKafkaSourceDescriptionSchema())))\n+            .setCoder(RowCoder.of(readTransform.getKafkaSourceDescriptionSchema()))\n+            .apply(readTransform)\n+            .setCoder(KafkaRecordCoder.of(keyCoder, valueCoder));\n+      }\n+    }\n+\n+    /**\n+     * A DoFn which generates {@link Row} with {@link KafkaSourceDescriptionSchemas#getSchema()}\n+     * based on the configuration of {@link Read}.\n+     */\n+    @VisibleForTesting\n+    class GenerateKafkaSourceDescription extends DoFn<byte[], Row> {\n+      GenerateKafkaSourceDescription(Schema schema) {\n+        this.kafkaSourceDescriptionSchema = schema;\n+      }\n+\n+      private final Schema kafkaSourceDescriptionSchema;\n+\n+      private final Map<String, Object> consumerConfig = Read.this.getConsumerConfig();\n \n-      PTransform<PBegin, PCollection<KafkaRecord<K, V>>> transform = unbounded;\n+      private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+          consumerFactoryFn = Read.this.getConsumerFactoryFn();\n \n-      if (getMaxNumRecords() < Long.MAX_VALUE || getMaxReadTime() != null) {\n-        transform =\n-            unbounded.withMaxReadTime(getMaxReadTime()).withMaxNumRecords(getMaxNumRecords());\n+      private final List<String> topics = Read.this.getTopics();\n+\n+      private final List<TopicPartition> topicPartitions = Read.this.getTopicPartitions();\n+\n+      private final Instant startReadTime = Read.this.getStartReadTime();\n+\n+      @VisibleForTesting\n+      Map<String, Object> getConsumerConfig() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 318}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1Njg1ODY4OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMToxMjoyNlrOGmAYww==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMToxMjoyNlrOGmAYww==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUwNTQxMQ==", "bodyText": "nit: this is much harder to read then a for loop. Also the performance of stream() is poor relative to the for loop as well.", "url": "https://github.com/apache/beam/pull/11749#discussion_r442505411", "createdAt": "2020-06-18T21:12:26Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -906,19 +946,123 @@ public void setValueDeserializer(String valueDeserializer) {\n       Coder<K> keyCoder = getKeyCoder(coderRegistry);\n       Coder<V> valueCoder = getValueCoder(coderRegistry);\n \n-      // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n-      Unbounded<KafkaRecord<K, V>> unbounded =\n-          org.apache.beam.sdk.io.Read.from(\n-              toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+      // The Read will be expanded into SDF transform when \"beam_fn_api\" is enabled and\n+      // \"beam_fn_api_use_deprecated_read\" is not enabled.\n+      if (!ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\")\n+          || ExperimentalOptions.hasExperiment(\n+              input.getPipeline().getOptions(), \"beam_fn_api_use_deprecated_read\")) {\n+        // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n+        Unbounded<KafkaRecord<K, V>> unbounded =\n+            org.apache.beam.sdk.io.Read.from(\n+                toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+\n+        PTransform<PBegin, PCollection<KafkaRecord<K, V>>> transform = unbounded;\n+\n+        if (getMaxNumRecords() < Long.MAX_VALUE || getMaxReadTime() != null) {\n+          transform =\n+              unbounded.withMaxReadTime(getMaxReadTime()).withMaxNumRecords(getMaxNumRecords());\n+        }\n+\n+        return input.getPipeline().apply(transform);\n+      } else {\n+        // If extractOutputTimestampFn is not set, use processing time by default.\n+        SerializableFunction<KafkaRecord<K, V>, Instant> timestampFn;\n+        if (getExtractOutputTimestampFn() != null) {\n+          timestampFn = getExtractOutputTimestampFn();\n+        } else {\n+          timestampFn = ReadViaSDF.ExtractOutputTimestampFns.useProcessingTime();\n+        }\n+        ReadViaSDF<K, V> readTransform =\n+            ReadViaSDF.<K, V>read()\n+                .withConsumerConfigOverrides(getConsumerConfig())\n+                .withOffsetConsumerConfigOverrides(getOffsetConsumerConfig())\n+                .withConsumerFactoryFn(getConsumerFactoryFn())\n+                .withKeyDeserializerProvider(getKeyDeserializerProvider())\n+                .withValueDeserializerProvider(getValueDeserializerProvider())\n+                .withExtractOutputTimestampFn(timestampFn);\n+        if (isCommitOffsetsInFinalizeEnabled()) {\n+          readTransform = readTransform.commitOffsets();\n+        }\n+\n+        return input\n+            .getPipeline()\n+            .apply(Impulse.create())\n+            .apply(\n+                ParDo.of(\n+                    new GenerateKafkaSourceDescription(\n+                        readTransform.getKafkaSourceDescriptionSchema())))\n+            .setCoder(RowCoder.of(readTransform.getKafkaSourceDescriptionSchema()))\n+            .apply(readTransform)\n+            .setCoder(KafkaRecordCoder.of(keyCoder, valueCoder));\n+      }\n+    }\n+\n+    /**\n+     * A DoFn which generates {@link Row} with {@link KafkaSourceDescriptionSchemas#getSchema()}\n+     * based on the configuration of {@link Read}.\n+     */\n+    @VisibleForTesting\n+    class GenerateKafkaSourceDescription extends DoFn<byte[], Row> {\n+      GenerateKafkaSourceDescription(Schema schema) {\n+        this.kafkaSourceDescriptionSchema = schema;\n+      }\n+\n+      private final Schema kafkaSourceDescriptionSchema;\n+\n+      private final Map<String, Object> consumerConfig = Read.this.getConsumerConfig();\n \n-      PTransform<PBegin, PCollection<KafkaRecord<K, V>>> transform = unbounded;\n+      private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+          consumerFactoryFn = Read.this.getConsumerFactoryFn();\n \n-      if (getMaxNumRecords() < Long.MAX_VALUE || getMaxReadTime() != null) {\n-        transform =\n-            unbounded.withMaxReadTime(getMaxReadTime()).withMaxNumRecords(getMaxNumRecords());\n+      private final List<String> topics = Read.this.getTopics();\n+\n+      private final List<TopicPartition> topicPartitions = Read.this.getTopicPartitions();\n+\n+      private final Instant startReadTime = Read.this.getStartReadTime();\n+\n+      @VisibleForTesting\n+      Map<String, Object> getConsumerConfig() {\n+        return consumerConfig;\n       }\n \n-      return input.getPipeline().apply(transform);\n+      @VisibleForTesting\n+      List<String> getTopics() {\n+        return topics;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(OutputReceiver<Row> receiver) {\n+        List<TopicPartition> partitions = new ArrayList<>(topicPartitions);\n+        if (partitions.isEmpty()) {\n+          try (Consumer<?, ?> consumer = consumerFactoryFn.apply(consumerConfig)) {\n+            for (String topic : topics) {\n+              for (PartitionInfo p : consumer.partitionsFor(topic)) {\n+                partitions.add(new TopicPartition(p.topic(), p.partition()));\n+              }\n+            }\n+          }\n+        }\n+        partitions.stream()", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 340}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1Njg2MTQxOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMToxMzoyNVrOGmAalA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMToxMzoyNVrOGmAalA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUwNTg3Ng==", "bodyText": "descriptiorBuilder -> descriptorBuilder", "url": "https://github.com/apache/beam/pull/11749#discussion_r442505876", "createdAt": "2020-06-18T21:13:25Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -906,19 +946,123 @@ public void setValueDeserializer(String valueDeserializer) {\n       Coder<K> keyCoder = getKeyCoder(coderRegistry);\n       Coder<V> valueCoder = getValueCoder(coderRegistry);\n \n-      // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n-      Unbounded<KafkaRecord<K, V>> unbounded =\n-          org.apache.beam.sdk.io.Read.from(\n-              toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+      // The Read will be expanded into SDF transform when \"beam_fn_api\" is enabled and\n+      // \"beam_fn_api_use_deprecated_read\" is not enabled.\n+      if (!ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\")\n+          || ExperimentalOptions.hasExperiment(\n+              input.getPipeline().getOptions(), \"beam_fn_api_use_deprecated_read\")) {\n+        // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n+        Unbounded<KafkaRecord<K, V>> unbounded =\n+            org.apache.beam.sdk.io.Read.from(\n+                toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+\n+        PTransform<PBegin, PCollection<KafkaRecord<K, V>>> transform = unbounded;\n+\n+        if (getMaxNumRecords() < Long.MAX_VALUE || getMaxReadTime() != null) {\n+          transform =\n+              unbounded.withMaxReadTime(getMaxReadTime()).withMaxNumRecords(getMaxNumRecords());\n+        }\n+\n+        return input.getPipeline().apply(transform);\n+      } else {\n+        // If extractOutputTimestampFn is not set, use processing time by default.\n+        SerializableFunction<KafkaRecord<K, V>, Instant> timestampFn;\n+        if (getExtractOutputTimestampFn() != null) {\n+          timestampFn = getExtractOutputTimestampFn();\n+        } else {\n+          timestampFn = ReadViaSDF.ExtractOutputTimestampFns.useProcessingTime();\n+        }\n+        ReadViaSDF<K, V> readTransform =\n+            ReadViaSDF.<K, V>read()\n+                .withConsumerConfigOverrides(getConsumerConfig())\n+                .withOffsetConsumerConfigOverrides(getOffsetConsumerConfig())\n+                .withConsumerFactoryFn(getConsumerFactoryFn())\n+                .withKeyDeserializerProvider(getKeyDeserializerProvider())\n+                .withValueDeserializerProvider(getValueDeserializerProvider())\n+                .withExtractOutputTimestampFn(timestampFn);\n+        if (isCommitOffsetsInFinalizeEnabled()) {\n+          readTransform = readTransform.commitOffsets();\n+        }\n+\n+        return input\n+            .getPipeline()\n+            .apply(Impulse.create())\n+            .apply(\n+                ParDo.of(\n+                    new GenerateKafkaSourceDescription(\n+                        readTransform.getKafkaSourceDescriptionSchema())))\n+            .setCoder(RowCoder.of(readTransform.getKafkaSourceDescriptionSchema()))\n+            .apply(readTransform)\n+            .setCoder(KafkaRecordCoder.of(keyCoder, valueCoder));\n+      }\n+    }\n+\n+    /**\n+     * A DoFn which generates {@link Row} with {@link KafkaSourceDescriptionSchemas#getSchema()}\n+     * based on the configuration of {@link Read}.\n+     */\n+    @VisibleForTesting\n+    class GenerateKafkaSourceDescription extends DoFn<byte[], Row> {\n+      GenerateKafkaSourceDescription(Schema schema) {\n+        this.kafkaSourceDescriptionSchema = schema;\n+      }\n+\n+      private final Schema kafkaSourceDescriptionSchema;\n+\n+      private final Map<String, Object> consumerConfig = Read.this.getConsumerConfig();\n \n-      PTransform<PBegin, PCollection<KafkaRecord<K, V>>> transform = unbounded;\n+      private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+          consumerFactoryFn = Read.this.getConsumerFactoryFn();\n \n-      if (getMaxNumRecords() < Long.MAX_VALUE || getMaxReadTime() != null) {\n-        transform =\n-            unbounded.withMaxReadTime(getMaxReadTime()).withMaxNumRecords(getMaxNumRecords());\n+      private final List<String> topics = Read.this.getTopics();\n+\n+      private final List<TopicPartition> topicPartitions = Read.this.getTopicPartitions();\n+\n+      private final Instant startReadTime = Read.this.getStartReadTime();\n+\n+      @VisibleForTesting\n+      Map<String, Object> getConsumerConfig() {\n+        return consumerConfig;\n       }\n \n-      return input.getPipeline().apply(transform);\n+      @VisibleForTesting\n+      List<String> getTopics() {\n+        return topics;\n+      }\n+\n+      @ProcessElement\n+      public void processElement(OutputReceiver<Row> receiver) {\n+        List<TopicPartition> partitions = new ArrayList<>(topicPartitions);\n+        if (partitions.isEmpty()) {\n+          try (Consumer<?, ?> consumer = consumerFactoryFn.apply(consumerConfig)) {\n+            for (String topic : topics) {\n+              for (PartitionInfo p : consumer.partitionsFor(topic)) {\n+                partitions.add(new TopicPartition(p.topic(), p.partition()));\n+              }\n+            }\n+          }\n+        }\n+        partitions.stream()\n+            .forEach(\n+                topicPartition -> {\n+                  FieldValueBuilder descriptiorBuilder =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 343}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1Njg2NjA4OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMToxNTowMVrOGmAdhg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxNzoxNToxNVrOGmb-5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUwNjYzMA==", "bodyText": "I would not make this class public and instead add future variants such as readAll to KafkaIO keeping this as an implementation detail.", "url": "https://github.com/apache/beam/pull/11749#discussion_r442506630", "createdAt": "2020-06-18T21:15:01Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n+ * The major difference from {@link KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source\n+ * descriptions(e.g., {@link KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()},\n+ * {@link KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead,\n+ * the pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from Kafka source description in {@link Row}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * Row}, and the restriction is an {@link OffsetRange} which represents record offset. A {@link\n+ * GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with {@code\n+ * Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(Row)} creates an initial range for a input element\n+ * {@link Row}. The end of range will be initialized as {@code Long.MAX_VALUE}. For the start of the\n+ * range:\n+ *\n+ * <ul>\n+ *   <li>If {@code start_read_offset} in {@link Row} is set, use this offset as start.\n+ *   <li>If {@code start_read_time} in {@link Row} is set, seek the start offset based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link Row} and move to process the next element. These deferred elements\n+ * will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link Row}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in the\n+ * {@link GrowableOffsetRangeTracker} as the {@link GrowableOffsetRangeTracker.RangeEndEstimator} to\n+ * poll the latest offset. Please refer to {@link ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for\n+ * details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(Row, OffsetRange).} A {@link\n+ * KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link Row}.\n+ */\n+@AutoValue\n+public abstract class ReadViaSDF<K, V>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 184}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU3Mjk5Mw==", "bodyText": "I was under impression that IO means it will be the root transform. If that's not the case, readAll sounds good.", "url": "https://github.com/apache/beam/pull/11749#discussion_r442572993", "createdAt": "2020-06-19T00:42:32Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n+ * The major difference from {@link KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source\n+ * descriptions(e.g., {@link KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()},\n+ * {@link KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead,\n+ * the pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from Kafka source description in {@link Row}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * Row}, and the restriction is an {@link OffsetRange} which represents record offset. A {@link\n+ * GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with {@code\n+ * Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(Row)} creates an initial range for a input element\n+ * {@link Row}. The end of range will be initialized as {@code Long.MAX_VALUE}. For the start of the\n+ * range:\n+ *\n+ * <ul>\n+ *   <li>If {@code start_read_offset} in {@link Row} is set, use this offset as start.\n+ *   <li>If {@code start_read_time} in {@link Row} is set, seek the start offset based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link Row} and move to process the next element. These deferred elements\n+ * will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link Row}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in the\n+ * {@link GrowableOffsetRangeTracker} as the {@link GrowableOffsetRangeTracker.RangeEndEstimator} to\n+ * poll the latest offset. Please refer to {@link ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for\n+ * details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(Row, OffsetRange).} A {@link\n+ * KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link Row}.\n+ */\n+@AutoValue\n+public abstract class ReadViaSDF<K, V>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUwNjYzMA=="}, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 184}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk1NzU0Mw==", "bodyText": "If you take a look at FileIO, you can see we have PTransforms like ReadMatches, MatchAll, ...\nI think we can do the same with KafkaIO where we add ReadTopics<PCollection, PCollection> and these are wrappers over the ReadViaSDF implementation with a transform that converts String -> Row in this example.", "url": "https://github.com/apache/beam/pull/11749#discussion_r442957543", "createdAt": "2020-06-19T17:15:15Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n+ * The major difference from {@link KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source\n+ * descriptions(e.g., {@link KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()},\n+ * {@link KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead,\n+ * the pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from Kafka source description in {@link Row}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * Row}, and the restriction is an {@link OffsetRange} which represents record offset. A {@link\n+ * GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with {@code\n+ * Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(Row)} creates an initial range for a input element\n+ * {@link Row}. The end of range will be initialized as {@code Long.MAX_VALUE}. For the start of the\n+ * range:\n+ *\n+ * <ul>\n+ *   <li>If {@code start_read_offset} in {@link Row} is set, use this offset as start.\n+ *   <li>If {@code start_read_time} in {@link Row} is set, seek the start offset based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link Row} and move to process the next element. These deferred elements\n+ * will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link Row}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in the\n+ * {@link GrowableOffsetRangeTracker} as the {@link GrowableOffsetRangeTracker.RangeEndEstimator} to\n+ * poll the latest offset. Please refer to {@link ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for\n+ * details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(Row, OffsetRange).} A {@link\n+ * KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link Row}.\n+ */\n+@AutoValue\n+public abstract class ReadViaSDF<K, V>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUwNjYzMA=="}, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 184}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1Njg3NDk5OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMToxODowOFrOGmAjIg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwMDo0Mjo1MFrOGmEhFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUwODA2Ng==", "bodyText": "Shouldn't this be a construction time error instead of a runtime warning?", "url": "https://github.com/apache/beam/pull/11749#discussion_r442508066", "createdAt": "2020-06-18T21:18:08Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n+ * The major difference from {@link KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source\n+ * descriptions(e.g., {@link KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()},\n+ * {@link KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead,\n+ * the pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from Kafka source description in {@link Row}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * Row}, and the restriction is an {@link OffsetRange} which represents record offset. A {@link\n+ * GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with {@code\n+ * Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(Row)} creates an initial range for a input element\n+ * {@link Row}. The end of range will be initialized as {@code Long.MAX_VALUE}. For the start of the\n+ * range:\n+ *\n+ * <ul>\n+ *   <li>If {@code start_read_offset} in {@link Row} is set, use this offset as start.\n+ *   <li>If {@code start_read_time} in {@link Row} is set, seek the start offset based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link Row} and move to process the next element. These deferred elements\n+ * will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link Row}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in the\n+ * {@link GrowableOffsetRangeTracker} as the {@link GrowableOffsetRangeTracker.RangeEndEstimator} to\n+ * poll the latest offset. Please refer to {@link ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for\n+ * details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(Row, OffsetRange).} A {@link\n+ * KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link Row}.\n+ */\n+@AutoValue\n+public abstract class ReadViaSDF<K, V>\n+    extends PTransform<PCollection<Row>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Schema getKafkaSourceDescriptionSchema();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V> setKafkaSourceDescriptionSchema(Schema schema);\n+\n+    abstract ReadViaSDF<K, V> build();\n+  }\n+\n+  static class KafkaSourceDescriptionSchemas {\n+    static final String TOPIC = \"topic\";\n+    static final String PARTITION = \"partition\";\n+    static final String START_READ_OFFSET = \"start_read_offset\";\n+    static final String START_READ_TIME = \"start_read_time\";\n+    static final String BOOTSTRAP_SERVERS = \"bootstrap_servers\";\n+\n+    static Schema getSchema() {\n+      return Schema.builder()\n+          .addStringField(TOPIC)\n+          .addInt32Field(PARTITION)\n+          .addNullableField(START_READ_OFFSET, FieldType.INT32)\n+          .addNullableField(START_READ_TIME, FieldType.INT64)\n+          .addNullableField(BOOTSTRAP_SERVERS, FieldType.array(FieldType.STRING))\n+          .build();\n+    }\n+  }\n+\n+  public static <K, V> ReadViaSDF<K, V> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime())\n+        .setCommitOffsetEnabled(false)\n+        .setKafkaSourceDescriptionSchema(KafkaSourceDescriptionSchemas.getSchema())\n+        .build();\n+  }\n+\n+  public ReadViaSDF<K, V> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializer(Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigUpdates(Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(\n+            getConsumerConfig(), KafkaIOUtils.IGNORED_CONSUMER_PROPERTIES, configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigOverrides(Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<Row> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input.apply(ParDo.of(new ReadFromKafkaDoFn())).setCoder(outputCoder);\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+      LOG.warn(\"Offset committed is not supported yet. Ignore the value.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 394}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU3MzA3OQ==", "bodyText": "Yes, it should be an error.", "url": "https://github.com/apache/beam/pull/11749#discussion_r442573079", "createdAt": "2020-06-19T00:42:50Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n+ * The major difference from {@link KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source\n+ * descriptions(e.g., {@link KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()},\n+ * {@link KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead,\n+ * the pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from Kafka source description in {@link Row}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * Row}, and the restriction is an {@link OffsetRange} which represents record offset. A {@link\n+ * GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with {@code\n+ * Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(Row)} creates an initial range for a input element\n+ * {@link Row}. The end of range will be initialized as {@code Long.MAX_VALUE}. For the start of the\n+ * range:\n+ *\n+ * <ul>\n+ *   <li>If {@code start_read_offset} in {@link Row} is set, use this offset as start.\n+ *   <li>If {@code start_read_time} in {@link Row} is set, seek the start offset based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link Row} and move to process the next element. These deferred elements\n+ * will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link Row}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in the\n+ * {@link GrowableOffsetRangeTracker} as the {@link GrowableOffsetRangeTracker.RangeEndEstimator} to\n+ * poll the latest offset. Please refer to {@link ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for\n+ * details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(Row, OffsetRange).} A {@link\n+ * KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link Row}.\n+ */\n+@AutoValue\n+public abstract class ReadViaSDF<K, V>\n+    extends PTransform<PCollection<Row>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Schema getKafkaSourceDescriptionSchema();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V> setKafkaSourceDescriptionSchema(Schema schema);\n+\n+    abstract ReadViaSDF<K, V> build();\n+  }\n+\n+  static class KafkaSourceDescriptionSchemas {\n+    static final String TOPIC = \"topic\";\n+    static final String PARTITION = \"partition\";\n+    static final String START_READ_OFFSET = \"start_read_offset\";\n+    static final String START_READ_TIME = \"start_read_time\";\n+    static final String BOOTSTRAP_SERVERS = \"bootstrap_servers\";\n+\n+    static Schema getSchema() {\n+      return Schema.builder()\n+          .addStringField(TOPIC)\n+          .addInt32Field(PARTITION)\n+          .addNullableField(START_READ_OFFSET, FieldType.INT32)\n+          .addNullableField(START_READ_TIME, FieldType.INT64)\n+          .addNullableField(BOOTSTRAP_SERVERS, FieldType.array(FieldType.STRING))\n+          .build();\n+    }\n+  }\n+\n+  public static <K, V> ReadViaSDF<K, V> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime())\n+        .setCommitOffsetEnabled(false)\n+        .setKafkaSourceDescriptionSchema(KafkaSourceDescriptionSchemas.getSchema())\n+        .build();\n+  }\n+\n+  public ReadViaSDF<K, V> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializer(Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigUpdates(Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(\n+            getConsumerConfig(), KafkaIOUtils.IGNORED_CONSUMER_PROPERTIES, configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigOverrides(Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<Row> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input.apply(ParDo.of(new ReadFromKafkaDoFn())).setCoder(outputCoder);\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+      LOG.warn(\"Offset committed is not supported yet. Ignore the value.\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUwODA2Ng=="}, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 394}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1Njg4NDQ4OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMToyMToyMlrOGmAo0A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQwMDo0ODoyMlrOGmEmPQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUwOTUyMA==", "bodyText": "Why store these as member variables if we plan to pull them from the parent enclosed class?", "url": "https://github.com/apache/beam/pull/11749#discussion_r442509520", "createdAt": "2020-06-18T21:21:22Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n+ * The major difference from {@link KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source\n+ * descriptions(e.g., {@link KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()},\n+ * {@link KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead,\n+ * the pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from Kafka source description in {@link Row}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * Row}, and the restriction is an {@link OffsetRange} which represents record offset. A {@link\n+ * GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with {@code\n+ * Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(Row)} creates an initial range for a input element\n+ * {@link Row}. The end of range will be initialized as {@code Long.MAX_VALUE}. For the start of the\n+ * range:\n+ *\n+ * <ul>\n+ *   <li>If {@code start_read_offset} in {@link Row} is set, use this offset as start.\n+ *   <li>If {@code start_read_time} in {@link Row} is set, seek the start offset based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link Row} and move to process the next element. These deferred elements\n+ * will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link Row}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in the\n+ * {@link GrowableOffsetRangeTracker} as the {@link GrowableOffsetRangeTracker.RangeEndEstimator} to\n+ * poll the latest offset. Please refer to {@link ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for\n+ * details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(Row, OffsetRange).} A {@link\n+ * KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link Row}.\n+ */\n+@AutoValue\n+public abstract class ReadViaSDF<K, V>\n+    extends PTransform<PCollection<Row>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Schema getKafkaSourceDescriptionSchema();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V> setKafkaSourceDescriptionSchema(Schema schema);\n+\n+    abstract ReadViaSDF<K, V> build();\n+  }\n+\n+  static class KafkaSourceDescriptionSchemas {\n+    static final String TOPIC = \"topic\";\n+    static final String PARTITION = \"partition\";\n+    static final String START_READ_OFFSET = \"start_read_offset\";\n+    static final String START_READ_TIME = \"start_read_time\";\n+    static final String BOOTSTRAP_SERVERS = \"bootstrap_servers\";\n+\n+    static Schema getSchema() {\n+      return Schema.builder()\n+          .addStringField(TOPIC)\n+          .addInt32Field(PARTITION)\n+          .addNullableField(START_READ_OFFSET, FieldType.INT32)\n+          .addNullableField(START_READ_TIME, FieldType.INT64)\n+          .addNullableField(BOOTSTRAP_SERVERS, FieldType.array(FieldType.STRING))\n+          .build();\n+    }\n+  }\n+\n+  public static <K, V> ReadViaSDF<K, V> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime())\n+        .setCommitOffsetEnabled(false)\n+        .setKafkaSourceDescriptionSchema(KafkaSourceDescriptionSchemas.getSchema())\n+        .build();\n+  }\n+\n+  public ReadViaSDF<K, V> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializer(Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigUpdates(Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(\n+            getConsumerConfig(), KafkaIOUtils.IGNORED_CONSUMER_PROPERTIES, configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigOverrides(Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<Row> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input.apply(ParDo.of(new ReadFromKafkaDoFn())).setCoder(outputCoder);\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+      LOG.warn(\"Offset committed is not supported yet. Ignore the value.\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link Row} in {@link KafkaSourceDescriptionSchemas} which\n+   * represents a Kafka source description and outputs {@link KafkaRecord}. By default, a {@link\n+   * MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+   */\n+  @VisibleForTesting\n+  class ReadFromKafkaDoFn extends DoFn<Row, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn() {}\n+\n+    private final Map<String, Object> consumerConfig = ReadViaSDF.this.getConsumerConfig();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 460}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU3NDM5Nw==", "bodyText": "I misunderstood how nested and inner class gets serialized. I think I should mark ReadFromKafkaDoFn as static to avoid serializing outer class. Thanks for pointing it out!", "url": "https://github.com/apache/beam/pull/11749#discussion_r442574397", "createdAt": "2020-06-19T00:48:22Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n+ * The major difference from {@link KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source\n+ * descriptions(e.g., {@link KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()},\n+ * {@link KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead,\n+ * the pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from Kafka source description in {@link Row}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * Row}, and the restriction is an {@link OffsetRange} which represents record offset. A {@link\n+ * GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with {@code\n+ * Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(Row)} creates an initial range for a input element\n+ * {@link Row}. The end of range will be initialized as {@code Long.MAX_VALUE}. For the start of the\n+ * range:\n+ *\n+ * <ul>\n+ *   <li>If {@code start_read_offset} in {@link Row} is set, use this offset as start.\n+ *   <li>If {@code start_read_time} in {@link Row} is set, seek the start offset based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link Row} and move to process the next element. These deferred elements\n+ * will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link Row}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in the\n+ * {@link GrowableOffsetRangeTracker} as the {@link GrowableOffsetRangeTracker.RangeEndEstimator} to\n+ * poll the latest offset. Please refer to {@link ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for\n+ * details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(Row, OffsetRange).} A {@link\n+ * KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link Row}.\n+ */\n+@AutoValue\n+public abstract class ReadViaSDF<K, V>\n+    extends PTransform<PCollection<Row>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Schema getKafkaSourceDescriptionSchema();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V> setKafkaSourceDescriptionSchema(Schema schema);\n+\n+    abstract ReadViaSDF<K, V> build();\n+  }\n+\n+  static class KafkaSourceDescriptionSchemas {\n+    static final String TOPIC = \"topic\";\n+    static final String PARTITION = \"partition\";\n+    static final String START_READ_OFFSET = \"start_read_offset\";\n+    static final String START_READ_TIME = \"start_read_time\";\n+    static final String BOOTSTRAP_SERVERS = \"bootstrap_servers\";\n+\n+    static Schema getSchema() {\n+      return Schema.builder()\n+          .addStringField(TOPIC)\n+          .addInt32Field(PARTITION)\n+          .addNullableField(START_READ_OFFSET, FieldType.INT32)\n+          .addNullableField(START_READ_TIME, FieldType.INT64)\n+          .addNullableField(BOOTSTRAP_SERVERS, FieldType.array(FieldType.STRING))\n+          .build();\n+    }\n+  }\n+\n+  public static <K, V> ReadViaSDF<K, V> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime())\n+        .setCommitOffsetEnabled(false)\n+        .setKafkaSourceDescriptionSchema(KafkaSourceDescriptionSchemas.getSchema())\n+        .build();\n+  }\n+\n+  public ReadViaSDF<K, V> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializer(Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigUpdates(Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(\n+            getConsumerConfig(), KafkaIOUtils.IGNORED_CONSUMER_PROPERTIES, configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigOverrides(Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<Row> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input.apply(ParDo.of(new ReadFromKafkaDoFn())).setCoder(outputCoder);\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+      LOG.warn(\"Offset committed is not supported yet. Ignore the value.\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link Row} in {@link KafkaSourceDescriptionSchemas} which\n+   * represents a Kafka source description and outputs {@link KafkaRecord}. By default, a {@link\n+   * MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+   */\n+  @VisibleForTesting\n+  class ReadFromKafkaDoFn extends DoFn<Row, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn() {}\n+\n+    private final Map<String, Object> consumerConfig = ReadViaSDF.this.getConsumerConfig();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUwOTUyMA=="}, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 460}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1Njg4NzUyOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMToyMjoyM1rOGmAqxQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxNzoyNTozM1rOGmcQ5Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUxMDAyMQ==", "bodyText": "Why would we close this when the object goes out of scope?\nWould it make sense to have support for a KafkaConsumer pool that expires automatically after X amount of time of not being used?", "url": "https://github.com/apache/beam/pull/11749#discussion_r442510021", "createdAt": "2020-06-18T21:22:23Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n+ * The major difference from {@link KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source\n+ * descriptions(e.g., {@link KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()},\n+ * {@link KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead,\n+ * the pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from Kafka source description in {@link Row}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * Row}, and the restriction is an {@link OffsetRange} which represents record offset. A {@link\n+ * GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with {@code\n+ * Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(Row)} creates an initial range for a input element\n+ * {@link Row}. The end of range will be initialized as {@code Long.MAX_VALUE}. For the start of the\n+ * range:\n+ *\n+ * <ul>\n+ *   <li>If {@code start_read_offset} in {@link Row} is set, use this offset as start.\n+ *   <li>If {@code start_read_time} in {@link Row} is set, seek the start offset based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link Row} and move to process the next element. These deferred elements\n+ * will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link Row}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in the\n+ * {@link GrowableOffsetRangeTracker} as the {@link GrowableOffsetRangeTracker.RangeEndEstimator} to\n+ * poll the latest offset. Please refer to {@link ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for\n+ * details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(Row, OffsetRange).} A {@link\n+ * KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link Row}.\n+ */\n+@AutoValue\n+public abstract class ReadViaSDF<K, V>\n+    extends PTransform<PCollection<Row>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Schema getKafkaSourceDescriptionSchema();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V> setKafkaSourceDescriptionSchema(Schema schema);\n+\n+    abstract ReadViaSDF<K, V> build();\n+  }\n+\n+  static class KafkaSourceDescriptionSchemas {\n+    static final String TOPIC = \"topic\";\n+    static final String PARTITION = \"partition\";\n+    static final String START_READ_OFFSET = \"start_read_offset\";\n+    static final String START_READ_TIME = \"start_read_time\";\n+    static final String BOOTSTRAP_SERVERS = \"bootstrap_servers\";\n+\n+    static Schema getSchema() {\n+      return Schema.builder()\n+          .addStringField(TOPIC)\n+          .addInt32Field(PARTITION)\n+          .addNullableField(START_READ_OFFSET, FieldType.INT32)\n+          .addNullableField(START_READ_TIME, FieldType.INT64)\n+          .addNullableField(BOOTSTRAP_SERVERS, FieldType.array(FieldType.STRING))\n+          .build();\n+    }\n+  }\n+\n+  public static <K, V> ReadViaSDF<K, V> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime())\n+        .setCommitOffsetEnabled(false)\n+        .setKafkaSourceDescriptionSchema(KafkaSourceDescriptionSchemas.getSchema())\n+        .build();\n+  }\n+\n+  public ReadViaSDF<K, V> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializer(Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigUpdates(Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(\n+            getConsumerConfig(), KafkaIOUtils.IGNORED_CONSUMER_PROPERTIES, configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigOverrides(Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<Row> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input.apply(ParDo.of(new ReadFromKafkaDoFn())).setCoder(outputCoder);\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+      LOG.warn(\"Offset committed is not supported yet. Ignore the value.\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link Row} in {@link KafkaSourceDescriptionSchemas} which\n+   * represents a Kafka source description and outputs {@link KafkaRecord}. By default, a {@link\n+   * MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+   */\n+  @VisibleForTesting\n+  class ReadFromKafkaDoFn extends DoFn<Row, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn() {}\n+\n+    private final Map<String, Object> consumerConfig = ReadViaSDF.this.getConsumerConfig();\n+\n+    private final Map<String, Object> offsetConsumerConfig =\n+        ReadViaSDF.this.getOffsetConsumerConfig();\n+\n+    private final DeserializerProvider keyDeserializerProvider =\n+        ReadViaSDF.this.getKeyDeserializerProvider();\n+    private final DeserializerProvider valueDeserializerProvider =\n+        ReadViaSDF.this.getValueDeserializerProvider();\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn = ReadViaSDF.this.getConsumerFactoryFn();\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn =\n+        ReadViaSDF.this.getExtractOutputTimestampFn();\n+\n+    private final Duration kafkaPollTimeout = Duration.millis(1000);\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient KafkaIOUtils.MovingAvg avgRecordSize = null;\n+    private transient KafkaIOUtils.MovingAvg avgOffsetGap = null;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 507}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU3OTE0MA==", "bodyText": "Why would we close this when the object goes out of scope?\n\nI need to close the connection when there is no more usage. There are 2 option:\n\nThe connection is only created when it's needed, like when querying the backlog.\nThe connection is created when the tracker is created and keep until the track is out of scope.\nI go with 2 to avoid creating the same connection multiple time.\n\n\nWould it make sense to have support for a KafkaConsumer pool that expires automatically after X amount of time of not being used?\n\nFor Kafka, it's important to close consumer effectively to benefit from Kafka dynamic load balancing mechanism. But we are using manual assignment now. So as long as the connection can be created cheaply, we may not need the pool.", "url": "https://github.com/apache/beam/pull/11749#discussion_r442579140", "createdAt": "2020-06-19T01:08:51Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n+ * The major difference from {@link KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source\n+ * descriptions(e.g., {@link KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()},\n+ * {@link KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead,\n+ * the pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from Kafka source description in {@link Row}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * Row}, and the restriction is an {@link OffsetRange} which represents record offset. A {@link\n+ * GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with {@code\n+ * Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(Row)} creates an initial range for a input element\n+ * {@link Row}. The end of range will be initialized as {@code Long.MAX_VALUE}. For the start of the\n+ * range:\n+ *\n+ * <ul>\n+ *   <li>If {@code start_read_offset} in {@link Row} is set, use this offset as start.\n+ *   <li>If {@code start_read_time} in {@link Row} is set, seek the start offset based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link Row} and move to process the next element. These deferred elements\n+ * will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link Row}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in the\n+ * {@link GrowableOffsetRangeTracker} as the {@link GrowableOffsetRangeTracker.RangeEndEstimator} to\n+ * poll the latest offset. Please refer to {@link ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for\n+ * details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(Row, OffsetRange).} A {@link\n+ * KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link Row}.\n+ */\n+@AutoValue\n+public abstract class ReadViaSDF<K, V>\n+    extends PTransform<PCollection<Row>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Schema getKafkaSourceDescriptionSchema();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V> setKafkaSourceDescriptionSchema(Schema schema);\n+\n+    abstract ReadViaSDF<K, V> build();\n+  }\n+\n+  static class KafkaSourceDescriptionSchemas {\n+    static final String TOPIC = \"topic\";\n+    static final String PARTITION = \"partition\";\n+    static final String START_READ_OFFSET = \"start_read_offset\";\n+    static final String START_READ_TIME = \"start_read_time\";\n+    static final String BOOTSTRAP_SERVERS = \"bootstrap_servers\";\n+\n+    static Schema getSchema() {\n+      return Schema.builder()\n+          .addStringField(TOPIC)\n+          .addInt32Field(PARTITION)\n+          .addNullableField(START_READ_OFFSET, FieldType.INT32)\n+          .addNullableField(START_READ_TIME, FieldType.INT64)\n+          .addNullableField(BOOTSTRAP_SERVERS, FieldType.array(FieldType.STRING))\n+          .build();\n+    }\n+  }\n+\n+  public static <K, V> ReadViaSDF<K, V> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime())\n+        .setCommitOffsetEnabled(false)\n+        .setKafkaSourceDescriptionSchema(KafkaSourceDescriptionSchemas.getSchema())\n+        .build();\n+  }\n+\n+  public ReadViaSDF<K, V> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializer(Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigUpdates(Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(\n+            getConsumerConfig(), KafkaIOUtils.IGNORED_CONSUMER_PROPERTIES, configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigOverrides(Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<Row> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input.apply(ParDo.of(new ReadFromKafkaDoFn())).setCoder(outputCoder);\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+      LOG.warn(\"Offset committed is not supported yet. Ignore the value.\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link Row} in {@link KafkaSourceDescriptionSchemas} which\n+   * represents a Kafka source description and outputs {@link KafkaRecord}. By default, a {@link\n+   * MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+   */\n+  @VisibleForTesting\n+  class ReadFromKafkaDoFn extends DoFn<Row, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn() {}\n+\n+    private final Map<String, Object> consumerConfig = ReadViaSDF.this.getConsumerConfig();\n+\n+    private final Map<String, Object> offsetConsumerConfig =\n+        ReadViaSDF.this.getOffsetConsumerConfig();\n+\n+    private final DeserializerProvider keyDeserializerProvider =\n+        ReadViaSDF.this.getKeyDeserializerProvider();\n+    private final DeserializerProvider valueDeserializerProvider =\n+        ReadViaSDF.this.getValueDeserializerProvider();\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn = ReadViaSDF.this.getConsumerFactoryFn();\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn =\n+        ReadViaSDF.this.getExtractOutputTimestampFn();\n+\n+    private final Duration kafkaPollTimeout = Duration.millis(1000);\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient KafkaIOUtils.MovingAvg avgRecordSize = null;\n+    private transient KafkaIOUtils.MovingAvg avgOffsetGap = null;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUxMDAyMQ=="}, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 507}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk2MjE0OQ==", "bodyText": "Relying on finalize leads to having the JVM perform closing during garbage collection which means you can't control the timing of it and it could be a long time.\nLets stick with this for now and maybe there will be a better suggestion later if we think it will be a problem.", "url": "https://github.com/apache/beam/pull/11749#discussion_r442962149", "createdAt": "2020-06-19T17:25:33Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n+ * The major difference from {@link KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source\n+ * descriptions(e.g., {@link KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()},\n+ * {@link KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead,\n+ * the pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from Kafka source description in {@link Row}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * Row}, and the restriction is an {@link OffsetRange} which represents record offset. A {@link\n+ * GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with {@code\n+ * Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(Row)} creates an initial range for a input element\n+ * {@link Row}. The end of range will be initialized as {@code Long.MAX_VALUE}. For the start of the\n+ * range:\n+ *\n+ * <ul>\n+ *   <li>If {@code start_read_offset} in {@link Row} is set, use this offset as start.\n+ *   <li>If {@code start_read_time} in {@link Row} is set, seek the start offset based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link Row} and move to process the next element. These deferred elements\n+ * will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link Row}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in the\n+ * {@link GrowableOffsetRangeTracker} as the {@link GrowableOffsetRangeTracker.RangeEndEstimator} to\n+ * poll the latest offset. Please refer to {@link ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for\n+ * details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(Row, OffsetRange).} A {@link\n+ * KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link Row}.\n+ */\n+@AutoValue\n+public abstract class ReadViaSDF<K, V>\n+    extends PTransform<PCollection<Row>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Schema getKafkaSourceDescriptionSchema();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V> setKafkaSourceDescriptionSchema(Schema schema);\n+\n+    abstract ReadViaSDF<K, V> build();\n+  }\n+\n+  static class KafkaSourceDescriptionSchemas {\n+    static final String TOPIC = \"topic\";\n+    static final String PARTITION = \"partition\";\n+    static final String START_READ_OFFSET = \"start_read_offset\";\n+    static final String START_READ_TIME = \"start_read_time\";\n+    static final String BOOTSTRAP_SERVERS = \"bootstrap_servers\";\n+\n+    static Schema getSchema() {\n+      return Schema.builder()\n+          .addStringField(TOPIC)\n+          .addInt32Field(PARTITION)\n+          .addNullableField(START_READ_OFFSET, FieldType.INT32)\n+          .addNullableField(START_READ_TIME, FieldType.INT64)\n+          .addNullableField(BOOTSTRAP_SERVERS, FieldType.array(FieldType.STRING))\n+          .build();\n+    }\n+  }\n+\n+  public static <K, V> ReadViaSDF<K, V> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime())\n+        .setCommitOffsetEnabled(false)\n+        .setKafkaSourceDescriptionSchema(KafkaSourceDescriptionSchemas.getSchema())\n+        .build();\n+  }\n+\n+  public ReadViaSDF<K, V> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializer(Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigUpdates(Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(\n+            getConsumerConfig(), KafkaIOUtils.IGNORED_CONSUMER_PROPERTIES, configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigOverrides(Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<Row> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input.apply(ParDo.of(new ReadFromKafkaDoFn())).setCoder(outputCoder);\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+      LOG.warn(\"Offset committed is not supported yet. Ignore the value.\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link Row} in {@link KafkaSourceDescriptionSchemas} which\n+   * represents a Kafka source description and outputs {@link KafkaRecord}. By default, a {@link\n+   * MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+   */\n+  @VisibleForTesting\n+  class ReadFromKafkaDoFn extends DoFn<Row, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn() {}\n+\n+    private final Map<String, Object> consumerConfig = ReadViaSDF.this.getConsumerConfig();\n+\n+    private final Map<String, Object> offsetConsumerConfig =\n+        ReadViaSDF.this.getOffsetConsumerConfig();\n+\n+    private final DeserializerProvider keyDeserializerProvider =\n+        ReadViaSDF.this.getKeyDeserializerProvider();\n+    private final DeserializerProvider valueDeserializerProvider =\n+        ReadViaSDF.this.getValueDeserializerProvider();\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn = ReadViaSDF.this.getConsumerFactoryFn();\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn =\n+        ReadViaSDF.this.getExtractOutputTimestampFn();\n+\n+    private final Duration kafkaPollTimeout = Duration.millis(1000);\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient KafkaIOUtils.MovingAvg avgRecordSize = null;\n+    private transient KafkaIOUtils.MovingAvg avgOffsetGap = null;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUxMDAyMQ=="}, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 507}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjkwODgxOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMTozMDowMlrOGmA4Yg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxNzoyNjowMlrOGmcRyg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUxMzUwNg==", "bodyText": "Why not invoke initialRestriction and pass in the necessary arguments instead of duplicating the setup work.", "url": "https://github.com/apache/beam/pull/11749#discussion_r442513506", "createdAt": "2020-06-18T21:30:02Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n+ * The major difference from {@link KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source\n+ * descriptions(e.g., {@link KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()},\n+ * {@link KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead,\n+ * the pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from Kafka source description in {@link Row}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * Row}, and the restriction is an {@link OffsetRange} which represents record offset. A {@link\n+ * GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with {@code\n+ * Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(Row)} creates an initial range for a input element\n+ * {@link Row}. The end of range will be initialized as {@code Long.MAX_VALUE}. For the start of the\n+ * range:\n+ *\n+ * <ul>\n+ *   <li>If {@code start_read_offset} in {@link Row} is set, use this offset as start.\n+ *   <li>If {@code start_read_time} in {@link Row} is set, seek the start offset based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link Row} and move to process the next element. These deferred elements\n+ * will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link Row}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in the\n+ * {@link GrowableOffsetRangeTracker} as the {@link GrowableOffsetRangeTracker.RangeEndEstimator} to\n+ * poll the latest offset. Please refer to {@link ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for\n+ * details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(Row, OffsetRange).} A {@link\n+ * KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link Row}.\n+ */\n+@AutoValue\n+public abstract class ReadViaSDF<K, V>\n+    extends PTransform<PCollection<Row>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Schema getKafkaSourceDescriptionSchema();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V> setKafkaSourceDescriptionSchema(Schema schema);\n+\n+    abstract ReadViaSDF<K, V> build();\n+  }\n+\n+  static class KafkaSourceDescriptionSchemas {\n+    static final String TOPIC = \"topic\";\n+    static final String PARTITION = \"partition\";\n+    static final String START_READ_OFFSET = \"start_read_offset\";\n+    static final String START_READ_TIME = \"start_read_time\";\n+    static final String BOOTSTRAP_SERVERS = \"bootstrap_servers\";\n+\n+    static Schema getSchema() {\n+      return Schema.builder()\n+          .addStringField(TOPIC)\n+          .addInt32Field(PARTITION)\n+          .addNullableField(START_READ_OFFSET, FieldType.INT32)\n+          .addNullableField(START_READ_TIME, FieldType.INT64)\n+          .addNullableField(BOOTSTRAP_SERVERS, FieldType.array(FieldType.STRING))\n+          .build();\n+    }\n+  }\n+\n+  public static <K, V> ReadViaSDF<K, V> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime())\n+        .setCommitOffsetEnabled(false)\n+        .setKafkaSourceDescriptionSchema(KafkaSourceDescriptionSchemas.getSchema())\n+        .build();\n+  }\n+\n+  public ReadViaSDF<K, V> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializer(Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigUpdates(Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(\n+            getConsumerConfig(), KafkaIOUtils.IGNORED_CONSUMER_PROPERTIES, configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigOverrides(Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<Row> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input.apply(ParDo.of(new ReadFromKafkaDoFn())).setCoder(outputCoder);\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+      LOG.warn(\"Offset committed is not supported yet. Ignore the value.\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link Row} in {@link KafkaSourceDescriptionSchemas} which\n+   * represents a Kafka source description and outputs {@link KafkaRecord}. By default, a {@link\n+   * MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+   */\n+  @VisibleForTesting\n+  class ReadFromKafkaDoFn extends DoFn<Row, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn() {}\n+\n+    private final Map<String, Object> consumerConfig = ReadViaSDF.this.getConsumerConfig();\n+\n+    private final Map<String, Object> offsetConsumerConfig =\n+        ReadViaSDF.this.getOffsetConsumerConfig();\n+\n+    private final DeserializerProvider keyDeserializerProvider =\n+        ReadViaSDF.this.getKeyDeserializerProvider();\n+    private final DeserializerProvider valueDeserializerProvider =\n+        ReadViaSDF.this.getValueDeserializerProvider();\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn = ReadViaSDF.this.getConsumerFactoryFn();\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn =\n+        ReadViaSDF.this.getExtractOutputTimestampFn();\n+\n+    private final Duration kafkaPollTimeout = Duration.millis(1000);\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient KafkaIOUtils.MovingAvg avgRecordSize = null;\n+    private transient KafkaIOUtils.MovingAvg avgOffsetGap = null;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+        return offsetConsumer.position(topicPartition);\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element Row kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        consumerSpEL.evaluateAssign(offsetConsumer, ImmutableList.of(topicPartition));\n+        long startOffset;\n+        if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET)\n+            != null) {\n+          startOffset =\n+              kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET);\n+        } else if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_TIME)\n+            != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  topicPartition,\n+                  Instant.ofEpochMilli(kafkaSourceDescription.getInt64(\"start_read_time\")));\n+        } else {\n+          startOffset = offsetConsumer.position(topicPartition);\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public MonotonicallyIncreasing newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return new MonotonicallyIncreasing(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(@Element Row kafkaSourceDescription, @Restriction OffsetRange offsetRange)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 564}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU3NTYyMw==", "bodyText": "initialRestriction still gives the infinite range. I should invoke restrictionTracker().getProgress()", "url": "https://github.com/apache/beam/pull/11749#discussion_r442575623", "createdAt": "2020-06-19T00:53:22Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n+ * The major difference from {@link KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source\n+ * descriptions(e.g., {@link KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()},\n+ * {@link KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead,\n+ * the pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from Kafka source description in {@link Row}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * Row}, and the restriction is an {@link OffsetRange} which represents record offset. A {@link\n+ * GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with {@code\n+ * Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(Row)} creates an initial range for a input element\n+ * {@link Row}. The end of range will be initialized as {@code Long.MAX_VALUE}. For the start of the\n+ * range:\n+ *\n+ * <ul>\n+ *   <li>If {@code start_read_offset} in {@link Row} is set, use this offset as start.\n+ *   <li>If {@code start_read_time} in {@link Row} is set, seek the start offset based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link Row} and move to process the next element. These deferred elements\n+ * will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link Row}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in the\n+ * {@link GrowableOffsetRangeTracker} as the {@link GrowableOffsetRangeTracker.RangeEndEstimator} to\n+ * poll the latest offset. Please refer to {@link ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for\n+ * details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(Row, OffsetRange).} A {@link\n+ * KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link Row}.\n+ */\n+@AutoValue\n+public abstract class ReadViaSDF<K, V>\n+    extends PTransform<PCollection<Row>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Schema getKafkaSourceDescriptionSchema();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V> setKafkaSourceDescriptionSchema(Schema schema);\n+\n+    abstract ReadViaSDF<K, V> build();\n+  }\n+\n+  static class KafkaSourceDescriptionSchemas {\n+    static final String TOPIC = \"topic\";\n+    static final String PARTITION = \"partition\";\n+    static final String START_READ_OFFSET = \"start_read_offset\";\n+    static final String START_READ_TIME = \"start_read_time\";\n+    static final String BOOTSTRAP_SERVERS = \"bootstrap_servers\";\n+\n+    static Schema getSchema() {\n+      return Schema.builder()\n+          .addStringField(TOPIC)\n+          .addInt32Field(PARTITION)\n+          .addNullableField(START_READ_OFFSET, FieldType.INT32)\n+          .addNullableField(START_READ_TIME, FieldType.INT64)\n+          .addNullableField(BOOTSTRAP_SERVERS, FieldType.array(FieldType.STRING))\n+          .build();\n+    }\n+  }\n+\n+  public static <K, V> ReadViaSDF<K, V> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime())\n+        .setCommitOffsetEnabled(false)\n+        .setKafkaSourceDescriptionSchema(KafkaSourceDescriptionSchemas.getSchema())\n+        .build();\n+  }\n+\n+  public ReadViaSDF<K, V> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializer(Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigUpdates(Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(\n+            getConsumerConfig(), KafkaIOUtils.IGNORED_CONSUMER_PROPERTIES, configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigOverrides(Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<Row> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input.apply(ParDo.of(new ReadFromKafkaDoFn())).setCoder(outputCoder);\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+      LOG.warn(\"Offset committed is not supported yet. Ignore the value.\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link Row} in {@link KafkaSourceDescriptionSchemas} which\n+   * represents a Kafka source description and outputs {@link KafkaRecord}. By default, a {@link\n+   * MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+   */\n+  @VisibleForTesting\n+  class ReadFromKafkaDoFn extends DoFn<Row, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn() {}\n+\n+    private final Map<String, Object> consumerConfig = ReadViaSDF.this.getConsumerConfig();\n+\n+    private final Map<String, Object> offsetConsumerConfig =\n+        ReadViaSDF.this.getOffsetConsumerConfig();\n+\n+    private final DeserializerProvider keyDeserializerProvider =\n+        ReadViaSDF.this.getKeyDeserializerProvider();\n+    private final DeserializerProvider valueDeserializerProvider =\n+        ReadViaSDF.this.getValueDeserializerProvider();\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn = ReadViaSDF.this.getConsumerFactoryFn();\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn =\n+        ReadViaSDF.this.getExtractOutputTimestampFn();\n+\n+    private final Duration kafkaPollTimeout = Duration.millis(1000);\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient KafkaIOUtils.MovingAvg avgRecordSize = null;\n+    private transient KafkaIOUtils.MovingAvg avgOffsetGap = null;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+        return offsetConsumer.position(topicPartition);\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element Row kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        consumerSpEL.evaluateAssign(offsetConsumer, ImmutableList.of(topicPartition));\n+        long startOffset;\n+        if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET)\n+            != null) {\n+          startOffset =\n+              kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET);\n+        } else if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_TIME)\n+            != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  topicPartition,\n+                  Instant.ofEpochMilli(kafkaSourceDescription.getInt64(\"start_read_time\")));\n+        } else {\n+          startOffset = offsetConsumer.position(topicPartition);\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public MonotonicallyIncreasing newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return new MonotonicallyIncreasing(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(@Element Row kafkaSourceDescription, @Restriction OffsetRange offsetRange)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUxMzUwNg=="}, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 564}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk2MjM3OA==", "bodyText": "My bad, totally meant to say restrictionTracker()", "url": "https://github.com/apache/beam/pull/11749#discussion_r442962378", "createdAt": "2020-06-19T17:26:02Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n+ * The major difference from {@link KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source\n+ * descriptions(e.g., {@link KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()},\n+ * {@link KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead,\n+ * the pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from Kafka source description in {@link Row}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * Row}, and the restriction is an {@link OffsetRange} which represents record offset. A {@link\n+ * GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with {@code\n+ * Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(Row)} creates an initial range for a input element\n+ * {@link Row}. The end of range will be initialized as {@code Long.MAX_VALUE}. For the start of the\n+ * range:\n+ *\n+ * <ul>\n+ *   <li>If {@code start_read_offset} in {@link Row} is set, use this offset as start.\n+ *   <li>If {@code start_read_time} in {@link Row} is set, seek the start offset based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link Row} and move to process the next element. These deferred elements\n+ * will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link Row}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in the\n+ * {@link GrowableOffsetRangeTracker} as the {@link GrowableOffsetRangeTracker.RangeEndEstimator} to\n+ * poll the latest offset. Please refer to {@link ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for\n+ * details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(Row, OffsetRange).} A {@link\n+ * KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link Row}.\n+ */\n+@AutoValue\n+public abstract class ReadViaSDF<K, V>\n+    extends PTransform<PCollection<Row>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Schema getKafkaSourceDescriptionSchema();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V> setKafkaSourceDescriptionSchema(Schema schema);\n+\n+    abstract ReadViaSDF<K, V> build();\n+  }\n+\n+  static class KafkaSourceDescriptionSchemas {\n+    static final String TOPIC = \"topic\";\n+    static final String PARTITION = \"partition\";\n+    static final String START_READ_OFFSET = \"start_read_offset\";\n+    static final String START_READ_TIME = \"start_read_time\";\n+    static final String BOOTSTRAP_SERVERS = \"bootstrap_servers\";\n+\n+    static Schema getSchema() {\n+      return Schema.builder()\n+          .addStringField(TOPIC)\n+          .addInt32Field(PARTITION)\n+          .addNullableField(START_READ_OFFSET, FieldType.INT32)\n+          .addNullableField(START_READ_TIME, FieldType.INT64)\n+          .addNullableField(BOOTSTRAP_SERVERS, FieldType.array(FieldType.STRING))\n+          .build();\n+    }\n+  }\n+\n+  public static <K, V> ReadViaSDF<K, V> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime())\n+        .setCommitOffsetEnabled(false)\n+        .setKafkaSourceDescriptionSchema(KafkaSourceDescriptionSchemas.getSchema())\n+        .build();\n+  }\n+\n+  public ReadViaSDF<K, V> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializer(Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigUpdates(Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(\n+            getConsumerConfig(), KafkaIOUtils.IGNORED_CONSUMER_PROPERTIES, configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigOverrides(Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<Row> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input.apply(ParDo.of(new ReadFromKafkaDoFn())).setCoder(outputCoder);\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+      LOG.warn(\"Offset committed is not supported yet. Ignore the value.\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link Row} in {@link KafkaSourceDescriptionSchemas} which\n+   * represents a Kafka source description and outputs {@link KafkaRecord}. By default, a {@link\n+   * MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+   */\n+  @VisibleForTesting\n+  class ReadFromKafkaDoFn extends DoFn<Row, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn() {}\n+\n+    private final Map<String, Object> consumerConfig = ReadViaSDF.this.getConsumerConfig();\n+\n+    private final Map<String, Object> offsetConsumerConfig =\n+        ReadViaSDF.this.getOffsetConsumerConfig();\n+\n+    private final DeserializerProvider keyDeserializerProvider =\n+        ReadViaSDF.this.getKeyDeserializerProvider();\n+    private final DeserializerProvider valueDeserializerProvider =\n+        ReadViaSDF.this.getValueDeserializerProvider();\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn = ReadViaSDF.this.getConsumerFactoryFn();\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn =\n+        ReadViaSDF.this.getExtractOutputTimestampFn();\n+\n+    private final Duration kafkaPollTimeout = Duration.millis(1000);\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient KafkaIOUtils.MovingAvg avgRecordSize = null;\n+    private transient KafkaIOUtils.MovingAvg avgOffsetGap = null;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+        return offsetConsumer.position(topicPartition);\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element Row kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        consumerSpEL.evaluateAssign(offsetConsumer, ImmutableList.of(topicPartition));\n+        long startOffset;\n+        if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET)\n+            != null) {\n+          startOffset =\n+              kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET);\n+        } else if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_TIME)\n+            != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  topicPartition,\n+                  Instant.ofEpochMilli(kafkaSourceDescription.getInt64(\"start_read_time\")));\n+        } else {\n+          startOffset = offsetConsumer.position(topicPartition);\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public MonotonicallyIncreasing newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return new MonotonicallyIncreasing(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(@Element Row kafkaSourceDescription, @Restriction OffsetRange offsetRange)", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUxMzUwNg=="}, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 564}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjkyMjcxOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMTozNToxM1rOGmBBFA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQyMjowMTo1NlrOGmiM_Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUxNTczMg==", "bodyText": "I believe this is the default implementation as well but I don't see it in the documentation so not sure if this is needed or not.", "url": "https://github.com/apache/beam/pull/11749#discussion_r442515732", "createdAt": "2020-06-18T21:35:13Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n+ * The major difference from {@link KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source\n+ * descriptions(e.g., {@link KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()},\n+ * {@link KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead,\n+ * the pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from Kafka source description in {@link Row}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * Row}, and the restriction is an {@link OffsetRange} which represents record offset. A {@link\n+ * GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with {@code\n+ * Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(Row)} creates an initial range for a input element\n+ * {@link Row}. The end of range will be initialized as {@code Long.MAX_VALUE}. For the start of the\n+ * range:\n+ *\n+ * <ul>\n+ *   <li>If {@code start_read_offset} in {@link Row} is set, use this offset as start.\n+ *   <li>If {@code start_read_time} in {@link Row} is set, seek the start offset based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link Row} and move to process the next element. These deferred elements\n+ * will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link Row}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in the\n+ * {@link GrowableOffsetRangeTracker} as the {@link GrowableOffsetRangeTracker.RangeEndEstimator} to\n+ * poll the latest offset. Please refer to {@link ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for\n+ * details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(Row, OffsetRange).} A {@link\n+ * KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link Row}.\n+ */\n+@AutoValue\n+public abstract class ReadViaSDF<K, V>\n+    extends PTransform<PCollection<Row>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Schema getKafkaSourceDescriptionSchema();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V> setKafkaSourceDescriptionSchema(Schema schema);\n+\n+    abstract ReadViaSDF<K, V> build();\n+  }\n+\n+  static class KafkaSourceDescriptionSchemas {\n+    static final String TOPIC = \"topic\";\n+    static final String PARTITION = \"partition\";\n+    static final String START_READ_OFFSET = \"start_read_offset\";\n+    static final String START_READ_TIME = \"start_read_time\";\n+    static final String BOOTSTRAP_SERVERS = \"bootstrap_servers\";\n+\n+    static Schema getSchema() {\n+      return Schema.builder()\n+          .addStringField(TOPIC)\n+          .addInt32Field(PARTITION)\n+          .addNullableField(START_READ_OFFSET, FieldType.INT32)\n+          .addNullableField(START_READ_TIME, FieldType.INT64)\n+          .addNullableField(BOOTSTRAP_SERVERS, FieldType.array(FieldType.STRING))\n+          .build();\n+    }\n+  }\n+\n+  public static <K, V> ReadViaSDF<K, V> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime())\n+        .setCommitOffsetEnabled(false)\n+        .setKafkaSourceDescriptionSchema(KafkaSourceDescriptionSchemas.getSchema())\n+        .build();\n+  }\n+\n+  public ReadViaSDF<K, V> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializer(Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigUpdates(Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(\n+            getConsumerConfig(), KafkaIOUtils.IGNORED_CONSUMER_PROPERTIES, configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigOverrides(Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<Row> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input.apply(ParDo.of(new ReadFromKafkaDoFn())).setCoder(outputCoder);\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+      LOG.warn(\"Offset committed is not supported yet. Ignore the value.\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link Row} in {@link KafkaSourceDescriptionSchemas} which\n+   * represents a Kafka source description and outputs {@link KafkaRecord}. By default, a {@link\n+   * MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+   */\n+  @VisibleForTesting\n+  class ReadFromKafkaDoFn extends DoFn<Row, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn() {}\n+\n+    private final Map<String, Object> consumerConfig = ReadViaSDF.this.getConsumerConfig();\n+\n+    private final Map<String, Object> offsetConsumerConfig =\n+        ReadViaSDF.this.getOffsetConsumerConfig();\n+\n+    private final DeserializerProvider keyDeserializerProvider =\n+        ReadViaSDF.this.getKeyDeserializerProvider();\n+    private final DeserializerProvider valueDeserializerProvider =\n+        ReadViaSDF.this.getValueDeserializerProvider();\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn = ReadViaSDF.this.getConsumerFactoryFn();\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn =\n+        ReadViaSDF.this.getExtractOutputTimestampFn();\n+\n+    private final Duration kafkaPollTimeout = Duration.millis(1000);\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient KafkaIOUtils.MovingAvg avgRecordSize = null;\n+    private transient KafkaIOUtils.MovingAvg avgOffsetGap = null;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+        return offsetConsumer.position(topicPartition);\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element Row kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        consumerSpEL.evaluateAssign(offsetConsumer, ImmutableList.of(topicPartition));\n+        long startOffset;\n+        if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET)\n+            != null) {\n+          startOffset =\n+              kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET);\n+        } else if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_TIME)\n+            != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  topicPartition,\n+                  Instant.ofEpochMilli(kafkaSourceDescription.getInt64(\"start_read_time\")));\n+        } else {\n+          startOffset = offsetConsumer.position(topicPartition);\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public MonotonicallyIncreasing newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return new MonotonicallyIncreasing(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(@Element Row kafkaSourceDescription, @Restriction OffsetRange offsetRange)\n+        throws Exception {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      double numOfRecords = 0.0;\n+      if (offsetRange.getTo() != Long.MAX_VALUE) {\n+        numOfRecords = (new OffsetRangeTracker(offsetRange)).getProgress().getWorkRemaining();\n+      } else {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        KafkaLatestOffsetEstimator offsetEstimator =\n+            new KafkaLatestOffsetEstimator(\n+                consumerFactoryFn.apply(\n+                    KafkaIOUtils.getOffsetConsumerConfig(\n+                        \"size-\" + topicPartition.toString(),\n+                        offsetConsumerConfig,\n+                        updatedConsumerConfig)),\n+                topicPartition);\n+        numOfRecords =\n+            (new GrowableOffsetRangeTracker(offsetRange.getFrom(), offsetEstimator))\n+                .getProgress()\n+                .getWorkRemaining();\n+      }\n+\n+      // Before processing elements, we don't have a good estimated size of records and offset gap.\n+      if (avgOffsetGap != null) {\n+        numOfRecords = numOfRecords / (1 + avgOffsetGap.get());\n+      }\n+      return (avgRecordSize == null ? 1 : avgRecordSize.get()) * numOfRecords;\n+    }\n+\n+    @SplitRestriction\n+    public void splitRestriction(\n+        @Element Row kafkaSourceDescription,\n+        @Restriction OffsetRange offsetRange,\n+        OutputReceiver<OffsetRange> receiver)\n+        throws Exception {\n+      receiver.output(offsetRange);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 603}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA0MDY5NA==", "bodyText": "Yep the default implementation is doing nothing: \n  \n    \n      beam/sdks/java/core/src/main/java/org/apache/beam/sdk/transforms/reflect/ByteBuddyDoFnInvokerFactory.java\n    \n    \n        Lines 300 to 307\n      in\n      591de34\n    \n    \n    \n    \n\n        \n          \n           /** Default implementation of {@link DoFn.SplitRestriction}, for delegation by bytebuddy. */ \n        \n\n        \n          \n           public static class DefaultSplitRestriction { \n        \n\n        \n          \n             /** Doesn't split the restriction. */ \n        \n\n        \n          \n             @SuppressWarnings(\"unused\") \n        \n\n        \n          \n             public static void invokeSplitRestriction(DoFnInvoker.ArgumentProvider argumentProvider) { \n        \n\n        \n          \n               argumentProvider.outputReceiver(null).output(argumentProvider.restriction()); \n        \n\n        \n          \n             } \n        \n\n        \n          \n           } \n        \n    \n  \n\n\nBut I still want to keep the implementation there to make it explicitly.", "url": "https://github.com/apache/beam/pull/11749#discussion_r443040694", "createdAt": "2020-06-19T20:57:02Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n+ * The major difference from {@link KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source\n+ * descriptions(e.g., {@link KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()},\n+ * {@link KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead,\n+ * the pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from Kafka source description in {@link Row}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * Row}, and the restriction is an {@link OffsetRange} which represents record offset. A {@link\n+ * GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with {@code\n+ * Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(Row)} creates an initial range for a input element\n+ * {@link Row}. The end of range will be initialized as {@code Long.MAX_VALUE}. For the start of the\n+ * range:\n+ *\n+ * <ul>\n+ *   <li>If {@code start_read_offset} in {@link Row} is set, use this offset as start.\n+ *   <li>If {@code start_read_time} in {@link Row} is set, seek the start offset based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link Row} and move to process the next element. These deferred elements\n+ * will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link Row}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in the\n+ * {@link GrowableOffsetRangeTracker} as the {@link GrowableOffsetRangeTracker.RangeEndEstimator} to\n+ * poll the latest offset. Please refer to {@link ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for\n+ * details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(Row, OffsetRange).} A {@link\n+ * KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link Row}.\n+ */\n+@AutoValue\n+public abstract class ReadViaSDF<K, V>\n+    extends PTransform<PCollection<Row>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Schema getKafkaSourceDescriptionSchema();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V> setKafkaSourceDescriptionSchema(Schema schema);\n+\n+    abstract ReadViaSDF<K, V> build();\n+  }\n+\n+  static class KafkaSourceDescriptionSchemas {\n+    static final String TOPIC = \"topic\";\n+    static final String PARTITION = \"partition\";\n+    static final String START_READ_OFFSET = \"start_read_offset\";\n+    static final String START_READ_TIME = \"start_read_time\";\n+    static final String BOOTSTRAP_SERVERS = \"bootstrap_servers\";\n+\n+    static Schema getSchema() {\n+      return Schema.builder()\n+          .addStringField(TOPIC)\n+          .addInt32Field(PARTITION)\n+          .addNullableField(START_READ_OFFSET, FieldType.INT32)\n+          .addNullableField(START_READ_TIME, FieldType.INT64)\n+          .addNullableField(BOOTSTRAP_SERVERS, FieldType.array(FieldType.STRING))\n+          .build();\n+    }\n+  }\n+\n+  public static <K, V> ReadViaSDF<K, V> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime())\n+        .setCommitOffsetEnabled(false)\n+        .setKafkaSourceDescriptionSchema(KafkaSourceDescriptionSchemas.getSchema())\n+        .build();\n+  }\n+\n+  public ReadViaSDF<K, V> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializer(Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigUpdates(Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(\n+            getConsumerConfig(), KafkaIOUtils.IGNORED_CONSUMER_PROPERTIES, configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigOverrides(Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<Row> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input.apply(ParDo.of(new ReadFromKafkaDoFn())).setCoder(outputCoder);\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+      LOG.warn(\"Offset committed is not supported yet. Ignore the value.\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link Row} in {@link KafkaSourceDescriptionSchemas} which\n+   * represents a Kafka source description and outputs {@link KafkaRecord}. By default, a {@link\n+   * MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+   */\n+  @VisibleForTesting\n+  class ReadFromKafkaDoFn extends DoFn<Row, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn() {}\n+\n+    private final Map<String, Object> consumerConfig = ReadViaSDF.this.getConsumerConfig();\n+\n+    private final Map<String, Object> offsetConsumerConfig =\n+        ReadViaSDF.this.getOffsetConsumerConfig();\n+\n+    private final DeserializerProvider keyDeserializerProvider =\n+        ReadViaSDF.this.getKeyDeserializerProvider();\n+    private final DeserializerProvider valueDeserializerProvider =\n+        ReadViaSDF.this.getValueDeserializerProvider();\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn = ReadViaSDF.this.getConsumerFactoryFn();\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn =\n+        ReadViaSDF.this.getExtractOutputTimestampFn();\n+\n+    private final Duration kafkaPollTimeout = Duration.millis(1000);\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient KafkaIOUtils.MovingAvg avgRecordSize = null;\n+    private transient KafkaIOUtils.MovingAvg avgOffsetGap = null;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+        return offsetConsumer.position(topicPartition);\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element Row kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        consumerSpEL.evaluateAssign(offsetConsumer, ImmutableList.of(topicPartition));\n+        long startOffset;\n+        if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET)\n+            != null) {\n+          startOffset =\n+              kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET);\n+        } else if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_TIME)\n+            != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  topicPartition,\n+                  Instant.ofEpochMilli(kafkaSourceDescription.getInt64(\"start_read_time\")));\n+        } else {\n+          startOffset = offsetConsumer.position(topicPartition);\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public MonotonicallyIncreasing newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return new MonotonicallyIncreasing(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(@Element Row kafkaSourceDescription, @Restriction OffsetRange offsetRange)\n+        throws Exception {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      double numOfRecords = 0.0;\n+      if (offsetRange.getTo() != Long.MAX_VALUE) {\n+        numOfRecords = (new OffsetRangeTracker(offsetRange)).getProgress().getWorkRemaining();\n+      } else {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        KafkaLatestOffsetEstimator offsetEstimator =\n+            new KafkaLatestOffsetEstimator(\n+                consumerFactoryFn.apply(\n+                    KafkaIOUtils.getOffsetConsumerConfig(\n+                        \"size-\" + topicPartition.toString(),\n+                        offsetConsumerConfig,\n+                        updatedConsumerConfig)),\n+                topicPartition);\n+        numOfRecords =\n+            (new GrowableOffsetRangeTracker(offsetRange.getFrom(), offsetEstimator))\n+                .getProgress()\n+                .getWorkRemaining();\n+      }\n+\n+      // Before processing elements, we don't have a good estimated size of records and offset gap.\n+      if (avgOffsetGap != null) {\n+        numOfRecords = numOfRecords / (1 + avgOffsetGap.get());\n+      }\n+      return (avgRecordSize == null ? 1 : avgRecordSize.get()) * numOfRecords;\n+    }\n+\n+    @SplitRestriction\n+    public void splitRestriction(\n+        @Element Row kafkaSourceDescription,\n+        @Restriction OffsetRange offsetRange,\n+        OutputReceiver<OffsetRange> receiver)\n+        throws Exception {\n+      receiver.output(offsetRange);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUxNTczMg=="}, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 603}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA1OTQ1Mw==", "bodyText": "I think we should remove it and update the DoFn documentation stating the default clearly.", "url": "https://github.com/apache/beam/pull/11749#discussion_r443059453", "createdAt": "2020-06-19T22:01:56Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n+ * The major difference from {@link KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source\n+ * descriptions(e.g., {@link KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()},\n+ * {@link KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead,\n+ * the pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from Kafka source description in {@link Row}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * Row}, and the restriction is an {@link OffsetRange} which represents record offset. A {@link\n+ * GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with {@code\n+ * Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(Row)} creates an initial range for a input element\n+ * {@link Row}. The end of range will be initialized as {@code Long.MAX_VALUE}. For the start of the\n+ * range:\n+ *\n+ * <ul>\n+ *   <li>If {@code start_read_offset} in {@link Row} is set, use this offset as start.\n+ *   <li>If {@code start_read_time} in {@link Row} is set, seek the start offset based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link Row} and move to process the next element. These deferred elements\n+ * will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link Row}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in the\n+ * {@link GrowableOffsetRangeTracker} as the {@link GrowableOffsetRangeTracker.RangeEndEstimator} to\n+ * poll the latest offset. Please refer to {@link ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for\n+ * details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(Row, OffsetRange).} A {@link\n+ * KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link Row}.\n+ */\n+@AutoValue\n+public abstract class ReadViaSDF<K, V>\n+    extends PTransform<PCollection<Row>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Schema getKafkaSourceDescriptionSchema();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V> setKafkaSourceDescriptionSchema(Schema schema);\n+\n+    abstract ReadViaSDF<K, V> build();\n+  }\n+\n+  static class KafkaSourceDescriptionSchemas {\n+    static final String TOPIC = \"topic\";\n+    static final String PARTITION = \"partition\";\n+    static final String START_READ_OFFSET = \"start_read_offset\";\n+    static final String START_READ_TIME = \"start_read_time\";\n+    static final String BOOTSTRAP_SERVERS = \"bootstrap_servers\";\n+\n+    static Schema getSchema() {\n+      return Schema.builder()\n+          .addStringField(TOPIC)\n+          .addInt32Field(PARTITION)\n+          .addNullableField(START_READ_OFFSET, FieldType.INT32)\n+          .addNullableField(START_READ_TIME, FieldType.INT64)\n+          .addNullableField(BOOTSTRAP_SERVERS, FieldType.array(FieldType.STRING))\n+          .build();\n+    }\n+  }\n+\n+  public static <K, V> ReadViaSDF<K, V> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime())\n+        .setCommitOffsetEnabled(false)\n+        .setKafkaSourceDescriptionSchema(KafkaSourceDescriptionSchemas.getSchema())\n+        .build();\n+  }\n+\n+  public ReadViaSDF<K, V> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializer(Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigUpdates(Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(\n+            getConsumerConfig(), KafkaIOUtils.IGNORED_CONSUMER_PROPERTIES, configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigOverrides(Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<Row> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input.apply(ParDo.of(new ReadFromKafkaDoFn())).setCoder(outputCoder);\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+      LOG.warn(\"Offset committed is not supported yet. Ignore the value.\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link Row} in {@link KafkaSourceDescriptionSchemas} which\n+   * represents a Kafka source description and outputs {@link KafkaRecord}. By default, a {@link\n+   * MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+   */\n+  @VisibleForTesting\n+  class ReadFromKafkaDoFn extends DoFn<Row, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn() {}\n+\n+    private final Map<String, Object> consumerConfig = ReadViaSDF.this.getConsumerConfig();\n+\n+    private final Map<String, Object> offsetConsumerConfig =\n+        ReadViaSDF.this.getOffsetConsumerConfig();\n+\n+    private final DeserializerProvider keyDeserializerProvider =\n+        ReadViaSDF.this.getKeyDeserializerProvider();\n+    private final DeserializerProvider valueDeserializerProvider =\n+        ReadViaSDF.this.getValueDeserializerProvider();\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn = ReadViaSDF.this.getConsumerFactoryFn();\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn =\n+        ReadViaSDF.this.getExtractOutputTimestampFn();\n+\n+    private final Duration kafkaPollTimeout = Duration.millis(1000);\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient KafkaIOUtils.MovingAvg avgRecordSize = null;\n+    private transient KafkaIOUtils.MovingAvg avgOffsetGap = null;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+        return offsetConsumer.position(topicPartition);\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element Row kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        consumerSpEL.evaluateAssign(offsetConsumer, ImmutableList.of(topicPartition));\n+        long startOffset;\n+        if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET)\n+            != null) {\n+          startOffset =\n+              kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET);\n+        } else if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_TIME)\n+            != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  topicPartition,\n+                  Instant.ofEpochMilli(kafkaSourceDescription.getInt64(\"start_read_time\")));\n+        } else {\n+          startOffset = offsetConsumer.position(topicPartition);\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public MonotonicallyIncreasing newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return new MonotonicallyIncreasing(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(@Element Row kafkaSourceDescription, @Restriction OffsetRange offsetRange)\n+        throws Exception {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      double numOfRecords = 0.0;\n+      if (offsetRange.getTo() != Long.MAX_VALUE) {\n+        numOfRecords = (new OffsetRangeTracker(offsetRange)).getProgress().getWorkRemaining();\n+      } else {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        KafkaLatestOffsetEstimator offsetEstimator =\n+            new KafkaLatestOffsetEstimator(\n+                consumerFactoryFn.apply(\n+                    KafkaIOUtils.getOffsetConsumerConfig(\n+                        \"size-\" + topicPartition.toString(),\n+                        offsetConsumerConfig,\n+                        updatedConsumerConfig)),\n+                topicPartition);\n+        numOfRecords =\n+            (new GrowableOffsetRangeTracker(offsetRange.getFrom(), offsetEstimator))\n+                .getProgress()\n+                .getWorkRemaining();\n+      }\n+\n+      // Before processing elements, we don't have a good estimated size of records and offset gap.\n+      if (avgOffsetGap != null) {\n+        numOfRecords = numOfRecords / (1 + avgOffsetGap.get());\n+      }\n+      return (avgRecordSize == null ? 1 : avgRecordSize.get()) * numOfRecords;\n+    }\n+\n+    @SplitRestriction\n+    public void splitRestriction(\n+        @Element Row kafkaSourceDescription,\n+        @Restriction OffsetRange offsetRange,\n+        OutputReceiver<OffsetRange> receiver)\n+        throws Exception {\n+      receiver.output(offsetRange);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUxNTczMg=="}, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 603}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1NjkzOTkwOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMTo0MjowMlrOGmBMDA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQyMjowMTozMlrOGmiMmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUxODU0MA==", "bodyText": "Will avgRecordSize / avgOffsetGap be consistent across multiple topics / partitions.\nIf we need to track them at the topic/partition level then they will need to part of the restriction.", "url": "https://github.com/apache/beam/pull/11749#discussion_r442518540", "createdAt": "2020-06-18T21:42:02Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n+ * The major difference from {@link KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source\n+ * descriptions(e.g., {@link KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()},\n+ * {@link KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead,\n+ * the pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from Kafka source description in {@link Row}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * Row}, and the restriction is an {@link OffsetRange} which represents record offset. A {@link\n+ * GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with {@code\n+ * Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(Row)} creates an initial range for a input element\n+ * {@link Row}. The end of range will be initialized as {@code Long.MAX_VALUE}. For the start of the\n+ * range:\n+ *\n+ * <ul>\n+ *   <li>If {@code start_read_offset} in {@link Row} is set, use this offset as start.\n+ *   <li>If {@code start_read_time} in {@link Row} is set, seek the start offset based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link Row} and move to process the next element. These deferred elements\n+ * will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link Row}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in the\n+ * {@link GrowableOffsetRangeTracker} as the {@link GrowableOffsetRangeTracker.RangeEndEstimator} to\n+ * poll the latest offset. Please refer to {@link ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for\n+ * details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(Row, OffsetRange).} A {@link\n+ * KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link Row}.\n+ */\n+@AutoValue\n+public abstract class ReadViaSDF<K, V>\n+    extends PTransform<PCollection<Row>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Schema getKafkaSourceDescriptionSchema();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V> setKafkaSourceDescriptionSchema(Schema schema);\n+\n+    abstract ReadViaSDF<K, V> build();\n+  }\n+\n+  static class KafkaSourceDescriptionSchemas {\n+    static final String TOPIC = \"topic\";\n+    static final String PARTITION = \"partition\";\n+    static final String START_READ_OFFSET = \"start_read_offset\";\n+    static final String START_READ_TIME = \"start_read_time\";\n+    static final String BOOTSTRAP_SERVERS = \"bootstrap_servers\";\n+\n+    static Schema getSchema() {\n+      return Schema.builder()\n+          .addStringField(TOPIC)\n+          .addInt32Field(PARTITION)\n+          .addNullableField(START_READ_OFFSET, FieldType.INT32)\n+          .addNullableField(START_READ_TIME, FieldType.INT64)\n+          .addNullableField(BOOTSTRAP_SERVERS, FieldType.array(FieldType.STRING))\n+          .build();\n+    }\n+  }\n+\n+  public static <K, V> ReadViaSDF<K, V> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime())\n+        .setCommitOffsetEnabled(false)\n+        .setKafkaSourceDescriptionSchema(KafkaSourceDescriptionSchemas.getSchema())\n+        .build();\n+  }\n+\n+  public ReadViaSDF<K, V> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializer(Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigUpdates(Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(\n+            getConsumerConfig(), KafkaIOUtils.IGNORED_CONSUMER_PROPERTIES, configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigOverrides(Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<Row> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input.apply(ParDo.of(new ReadFromKafkaDoFn())).setCoder(outputCoder);\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+      LOG.warn(\"Offset committed is not supported yet. Ignore the value.\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link Row} in {@link KafkaSourceDescriptionSchemas} which\n+   * represents a Kafka source description and outputs {@link KafkaRecord}. By default, a {@link\n+   * MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+   */\n+  @VisibleForTesting\n+  class ReadFromKafkaDoFn extends DoFn<Row, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn() {}\n+\n+    private final Map<String, Object> consumerConfig = ReadViaSDF.this.getConsumerConfig();\n+\n+    private final Map<String, Object> offsetConsumerConfig =\n+        ReadViaSDF.this.getOffsetConsumerConfig();\n+\n+    private final DeserializerProvider keyDeserializerProvider =\n+        ReadViaSDF.this.getKeyDeserializerProvider();\n+    private final DeserializerProvider valueDeserializerProvider =\n+        ReadViaSDF.this.getValueDeserializerProvider();\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn = ReadViaSDF.this.getConsumerFactoryFn();\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn =\n+        ReadViaSDF.this.getExtractOutputTimestampFn();\n+\n+    private final Duration kafkaPollTimeout = Duration.millis(1000);\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient KafkaIOUtils.MovingAvg avgRecordSize = null;\n+    private transient KafkaIOUtils.MovingAvg avgOffsetGap = null;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+        return offsetConsumer.position(topicPartition);\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element Row kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        consumerSpEL.evaluateAssign(offsetConsumer, ImmutableList.of(topicPartition));\n+        long startOffset;\n+        if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET)\n+            != null) {\n+          startOffset =\n+              kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET);\n+        } else if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_TIME)\n+            != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  topicPartition,\n+                  Instant.ofEpochMilli(kafkaSourceDescription.getInt64(\"start_read_time\")));\n+        } else {\n+          startOffset = offsetConsumer.position(topicPartition);\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public MonotonicallyIncreasing newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return new MonotonicallyIncreasing(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(@Element Row kafkaSourceDescription, @Restriction OffsetRange offsetRange)\n+        throws Exception {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      double numOfRecords = 0.0;\n+      if (offsetRange.getTo() != Long.MAX_VALUE) {\n+        numOfRecords = (new OffsetRangeTracker(offsetRange)).getProgress().getWorkRemaining();\n+      } else {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        KafkaLatestOffsetEstimator offsetEstimator =\n+            new KafkaLatestOffsetEstimator(\n+                consumerFactoryFn.apply(\n+                    KafkaIOUtils.getOffsetConsumerConfig(\n+                        \"size-\" + topicPartition.toString(),\n+                        offsetConsumerConfig,\n+                        updatedConsumerConfig)),\n+                topicPartition);\n+        numOfRecords =\n+            (new GrowableOffsetRangeTracker(offsetRange.getFrom(), offsetEstimator))\n+                .getProgress()\n+                .getWorkRemaining();\n+      }\n+\n+      // Before processing elements, we don't have a good estimated size of records and offset gap.\n+      if (avgOffsetGap != null) {\n+        numOfRecords = numOfRecords / (1 + avgOffsetGap.get());\n+      }\n+      return (avgRecordSize == null ? 1 : avgRecordSize.get()) * numOfRecords;\n+    }\n+\n+    @SplitRestriction\n+    public void splitRestriction(\n+        @Element Row kafkaSourceDescription,\n+        @Restriction OffsetRange offsetRange,\n+        OutputReceiver<OffsetRange> receiver)\n+        throws Exception {\n+      receiver.output(offsetRange);\n+    }\n+\n+    @NewTracker\n+    public RestrictionTracker<OffsetRange, Long> restrictionTracker(\n+        @Element Row kafkaSourceDescription, @Restriction OffsetRange restriction) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      if (restriction.getTo() == Long.MAX_VALUE) {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        KafkaLatestOffsetEstimator offsetPoller =\n+            new KafkaLatestOffsetEstimator(\n+                consumerFactoryFn.apply(\n+                    KafkaIOUtils.getOffsetConsumerConfig(\n+                        \"tracker-\" + topicPartition, offsetConsumerConfig, updatedConsumerConfig)),\n+                topicPartition);\n+        return new GrowableOffsetRangeTracker(restriction.getFrom(), offsetPoller);\n+      }\n+      return new OffsetRangeTracker(restriction);\n+    }\n+\n+    @ProcessElement\n+    public ProcessContinuation processElement(\n+        @Element Row kafkaSourceDescription,\n+        RestrictionTracker<OffsetRange, Long> tracker,\n+        WatermarkEstimator watermarkEstimator,\n+        OutputReceiver<KafkaRecord<K, V>> receiver) {\n+      TopicPartition topicPartition =\n+          new TopicPartition(\n+              kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+              kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> consumer = consumerFactoryFn.apply(updatedConsumerConfig)) {\n+        consumerSpEL.evaluateAssign(consumer, ImmutableList.of(topicPartition));\n+        long startOffset = tracker.currentRestriction().getFrom();\n+        long expectedOffset = startOffset;\n+        consumer.seek(topicPartition, startOffset);\n+        ConsumerRecords<byte[], byte[]> rawRecords = ConsumerRecords.empty();\n+\n+        try {\n+          while (true) {\n+            rawRecords = consumer.poll(kafkaPollTimeout.getMillis());\n+            // When there is no records from the current TopicPartition temporarily, self-checkpoint\n+            // and move to process the next element.\n+            if (rawRecords.isEmpty()) {\n+              return ProcessContinuation.resume();\n+            }\n+            for (ConsumerRecord<byte[], byte[]> rawRecord : rawRecords) {\n+              if (!tracker.tryClaim(rawRecord.offset())) {\n+                return ProcessContinuation.stop();\n+              }\n+              KafkaRecord<K, V> kafkaRecord =\n+                  new KafkaRecord<>(\n+                      rawRecord.topic(),\n+                      rawRecord.partition(),\n+                      rawRecord.offset(),\n+                      consumerSpEL.getRecordTimestamp(rawRecord),\n+                      consumerSpEL.getRecordTimestampType(rawRecord),\n+                      ConsumerSpEL.hasHeaders() ? rawRecord.headers() : null,\n+                      keyDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.key()),\n+                      valueDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.value()));\n+              Instant outputTimestamp = extractOutputTimestampFn.apply(kafkaRecord);\n+              int recordSize =\n+                  (rawRecord.key() == null ? 0 : rawRecord.key().length)\n+                      + (rawRecord.value() == null ? 0 : rawRecord.value().length);\n+              avgRecordSize.update(recordSize);\n+              avgOffsetGap.update(expectedOffset - rawRecord.offset());\n+              expectedOffset = rawRecord.offset() + 1;\n+              receiver.outputWithTimestamp(kafkaRecord, outputTimestamp);\n+            }\n+          }\n+        } catch (Exception anyException) {\n+          LOG.error(\"{}: Exception while reading from Kafka\", this, anyException);\n+          throw anyException;\n+        }\n+      }\n+    }\n+\n+    @GetRestrictionCoder\n+    public Coder<OffsetRange> restrictionCoder() {\n+      return new OffsetRange.Coder();\n+    }\n+\n+    @Setup\n+    public void setup() throws Exception {\n+      // Start to track record size and offset gap per bundle.\n+      avgRecordSize = new KafkaIOUtils.MovingAvg();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 693}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjU3MDM2NA==", "bodyText": "Will avgRecordSize / avgOffsetGap be consistent across multiple topics / partitions.\n\nI don't think there is a guarantee . I even don't think there is a guarantee per topic. The avgRecordSize and avgOffsetGap are used to calculate size in GetSize, which should be an estimated one.", "url": "https://github.com/apache/beam/pull/11749#discussion_r442570364", "createdAt": "2020-06-19T00:30:42Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n+ * The major difference from {@link KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source\n+ * descriptions(e.g., {@link KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()},\n+ * {@link KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead,\n+ * the pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from Kafka source description in {@link Row}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * Row}, and the restriction is an {@link OffsetRange} which represents record offset. A {@link\n+ * GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with {@code\n+ * Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(Row)} creates an initial range for a input element\n+ * {@link Row}. The end of range will be initialized as {@code Long.MAX_VALUE}. For the start of the\n+ * range:\n+ *\n+ * <ul>\n+ *   <li>If {@code start_read_offset} in {@link Row} is set, use this offset as start.\n+ *   <li>If {@code start_read_time} in {@link Row} is set, seek the start offset based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link Row} and move to process the next element. These deferred elements\n+ * will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link Row}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in the\n+ * {@link GrowableOffsetRangeTracker} as the {@link GrowableOffsetRangeTracker.RangeEndEstimator} to\n+ * poll the latest offset. Please refer to {@link ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for\n+ * details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(Row, OffsetRange).} A {@link\n+ * KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link Row}.\n+ */\n+@AutoValue\n+public abstract class ReadViaSDF<K, V>\n+    extends PTransform<PCollection<Row>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Schema getKafkaSourceDescriptionSchema();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V> setKafkaSourceDescriptionSchema(Schema schema);\n+\n+    abstract ReadViaSDF<K, V> build();\n+  }\n+\n+  static class KafkaSourceDescriptionSchemas {\n+    static final String TOPIC = \"topic\";\n+    static final String PARTITION = \"partition\";\n+    static final String START_READ_OFFSET = \"start_read_offset\";\n+    static final String START_READ_TIME = \"start_read_time\";\n+    static final String BOOTSTRAP_SERVERS = \"bootstrap_servers\";\n+\n+    static Schema getSchema() {\n+      return Schema.builder()\n+          .addStringField(TOPIC)\n+          .addInt32Field(PARTITION)\n+          .addNullableField(START_READ_OFFSET, FieldType.INT32)\n+          .addNullableField(START_READ_TIME, FieldType.INT64)\n+          .addNullableField(BOOTSTRAP_SERVERS, FieldType.array(FieldType.STRING))\n+          .build();\n+    }\n+  }\n+\n+  public static <K, V> ReadViaSDF<K, V> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime())\n+        .setCommitOffsetEnabled(false)\n+        .setKafkaSourceDescriptionSchema(KafkaSourceDescriptionSchemas.getSchema())\n+        .build();\n+  }\n+\n+  public ReadViaSDF<K, V> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializer(Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigUpdates(Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(\n+            getConsumerConfig(), KafkaIOUtils.IGNORED_CONSUMER_PROPERTIES, configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigOverrides(Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<Row> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input.apply(ParDo.of(new ReadFromKafkaDoFn())).setCoder(outputCoder);\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+      LOG.warn(\"Offset committed is not supported yet. Ignore the value.\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link Row} in {@link KafkaSourceDescriptionSchemas} which\n+   * represents a Kafka source description and outputs {@link KafkaRecord}. By default, a {@link\n+   * MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+   */\n+  @VisibleForTesting\n+  class ReadFromKafkaDoFn extends DoFn<Row, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn() {}\n+\n+    private final Map<String, Object> consumerConfig = ReadViaSDF.this.getConsumerConfig();\n+\n+    private final Map<String, Object> offsetConsumerConfig =\n+        ReadViaSDF.this.getOffsetConsumerConfig();\n+\n+    private final DeserializerProvider keyDeserializerProvider =\n+        ReadViaSDF.this.getKeyDeserializerProvider();\n+    private final DeserializerProvider valueDeserializerProvider =\n+        ReadViaSDF.this.getValueDeserializerProvider();\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn = ReadViaSDF.this.getConsumerFactoryFn();\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn =\n+        ReadViaSDF.this.getExtractOutputTimestampFn();\n+\n+    private final Duration kafkaPollTimeout = Duration.millis(1000);\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient KafkaIOUtils.MovingAvg avgRecordSize = null;\n+    private transient KafkaIOUtils.MovingAvg avgOffsetGap = null;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+        return offsetConsumer.position(topicPartition);\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element Row kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        consumerSpEL.evaluateAssign(offsetConsumer, ImmutableList.of(topicPartition));\n+        long startOffset;\n+        if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET)\n+            != null) {\n+          startOffset =\n+              kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET);\n+        } else if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_TIME)\n+            != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  topicPartition,\n+                  Instant.ofEpochMilli(kafkaSourceDescription.getInt64(\"start_read_time\")));\n+        } else {\n+          startOffset = offsetConsumer.position(topicPartition);\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public MonotonicallyIncreasing newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return new MonotonicallyIncreasing(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(@Element Row kafkaSourceDescription, @Restriction OffsetRange offsetRange)\n+        throws Exception {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      double numOfRecords = 0.0;\n+      if (offsetRange.getTo() != Long.MAX_VALUE) {\n+        numOfRecords = (new OffsetRangeTracker(offsetRange)).getProgress().getWorkRemaining();\n+      } else {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        KafkaLatestOffsetEstimator offsetEstimator =\n+            new KafkaLatestOffsetEstimator(\n+                consumerFactoryFn.apply(\n+                    KafkaIOUtils.getOffsetConsumerConfig(\n+                        \"size-\" + topicPartition.toString(),\n+                        offsetConsumerConfig,\n+                        updatedConsumerConfig)),\n+                topicPartition);\n+        numOfRecords =\n+            (new GrowableOffsetRangeTracker(offsetRange.getFrom(), offsetEstimator))\n+                .getProgress()\n+                .getWorkRemaining();\n+      }\n+\n+      // Before processing elements, we don't have a good estimated size of records and offset gap.\n+      if (avgOffsetGap != null) {\n+        numOfRecords = numOfRecords / (1 + avgOffsetGap.get());\n+      }\n+      return (avgRecordSize == null ? 1 : avgRecordSize.get()) * numOfRecords;\n+    }\n+\n+    @SplitRestriction\n+    public void splitRestriction(\n+        @Element Row kafkaSourceDescription,\n+        @Restriction OffsetRange offsetRange,\n+        OutputReceiver<OffsetRange> receiver)\n+        throws Exception {\n+      receiver.output(offsetRange);\n+    }\n+\n+    @NewTracker\n+    public RestrictionTracker<OffsetRange, Long> restrictionTracker(\n+        @Element Row kafkaSourceDescription, @Restriction OffsetRange restriction) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      if (restriction.getTo() == Long.MAX_VALUE) {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        KafkaLatestOffsetEstimator offsetPoller =\n+            new KafkaLatestOffsetEstimator(\n+                consumerFactoryFn.apply(\n+                    KafkaIOUtils.getOffsetConsumerConfig(\n+                        \"tracker-\" + topicPartition, offsetConsumerConfig, updatedConsumerConfig)),\n+                topicPartition);\n+        return new GrowableOffsetRangeTracker(restriction.getFrom(), offsetPoller);\n+      }\n+      return new OffsetRangeTracker(restriction);\n+    }\n+\n+    @ProcessElement\n+    public ProcessContinuation processElement(\n+        @Element Row kafkaSourceDescription,\n+        RestrictionTracker<OffsetRange, Long> tracker,\n+        WatermarkEstimator watermarkEstimator,\n+        OutputReceiver<KafkaRecord<K, V>> receiver) {\n+      TopicPartition topicPartition =\n+          new TopicPartition(\n+              kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+              kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> consumer = consumerFactoryFn.apply(updatedConsumerConfig)) {\n+        consumerSpEL.evaluateAssign(consumer, ImmutableList.of(topicPartition));\n+        long startOffset = tracker.currentRestriction().getFrom();\n+        long expectedOffset = startOffset;\n+        consumer.seek(topicPartition, startOffset);\n+        ConsumerRecords<byte[], byte[]> rawRecords = ConsumerRecords.empty();\n+\n+        try {\n+          while (true) {\n+            rawRecords = consumer.poll(kafkaPollTimeout.getMillis());\n+            // When there is no records from the current TopicPartition temporarily, self-checkpoint\n+            // and move to process the next element.\n+            if (rawRecords.isEmpty()) {\n+              return ProcessContinuation.resume();\n+            }\n+            for (ConsumerRecord<byte[], byte[]> rawRecord : rawRecords) {\n+              if (!tracker.tryClaim(rawRecord.offset())) {\n+                return ProcessContinuation.stop();\n+              }\n+              KafkaRecord<K, V> kafkaRecord =\n+                  new KafkaRecord<>(\n+                      rawRecord.topic(),\n+                      rawRecord.partition(),\n+                      rawRecord.offset(),\n+                      consumerSpEL.getRecordTimestamp(rawRecord),\n+                      consumerSpEL.getRecordTimestampType(rawRecord),\n+                      ConsumerSpEL.hasHeaders() ? rawRecord.headers() : null,\n+                      keyDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.key()),\n+                      valueDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.value()));\n+              Instant outputTimestamp = extractOutputTimestampFn.apply(kafkaRecord);\n+              int recordSize =\n+                  (rawRecord.key() == null ? 0 : rawRecord.key().length)\n+                      + (rawRecord.value() == null ? 0 : rawRecord.value().length);\n+              avgRecordSize.update(recordSize);\n+              avgOffsetGap.update(expectedOffset - rawRecord.offset());\n+              expectedOffset = rawRecord.offset() + 1;\n+              receiver.outputWithTimestamp(kafkaRecord, outputTimestamp);\n+            }\n+          }\n+        } catch (Exception anyException) {\n+          LOG.error(\"{}: Exception while reading from Kafka\", this, anyException);\n+          throw anyException;\n+        }\n+      }\n+    }\n+\n+    @GetRestrictionCoder\n+    public Coder<OffsetRange> restrictionCoder() {\n+      return new OffsetRange.Coder();\n+    }\n+\n+    @Setup\n+    public void setup() throws Exception {\n+      // Start to track record size and offset gap per bundle.\n+      avgRecordSize = new KafkaIOUtils.MovingAvg();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUxODU0MA=="}, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 693}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk2Mzk4NQ==", "bodyText": "The UnboundedSource implementation tracked it for a specific TopicPartition and it didn't apply to multiple topics. When we are processing multiple restrictions we'll conflate this information from all of them.", "url": "https://github.com/apache/beam/pull/11749#discussion_r442963985", "createdAt": "2020-06-19T17:30:05Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n+ * The major difference from {@link KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source\n+ * descriptions(e.g., {@link KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()},\n+ * {@link KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead,\n+ * the pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from Kafka source description in {@link Row}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * Row}, and the restriction is an {@link OffsetRange} which represents record offset. A {@link\n+ * GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with {@code\n+ * Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(Row)} creates an initial range for a input element\n+ * {@link Row}. The end of range will be initialized as {@code Long.MAX_VALUE}. For the start of the\n+ * range:\n+ *\n+ * <ul>\n+ *   <li>If {@code start_read_offset} in {@link Row} is set, use this offset as start.\n+ *   <li>If {@code start_read_time} in {@link Row} is set, seek the start offset based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link Row} and move to process the next element. These deferred elements\n+ * will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link Row}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in the\n+ * {@link GrowableOffsetRangeTracker} as the {@link GrowableOffsetRangeTracker.RangeEndEstimator} to\n+ * poll the latest offset. Please refer to {@link ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for\n+ * details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(Row, OffsetRange).} A {@link\n+ * KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link Row}.\n+ */\n+@AutoValue\n+public abstract class ReadViaSDF<K, V>\n+    extends PTransform<PCollection<Row>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Schema getKafkaSourceDescriptionSchema();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V> setKafkaSourceDescriptionSchema(Schema schema);\n+\n+    abstract ReadViaSDF<K, V> build();\n+  }\n+\n+  static class KafkaSourceDescriptionSchemas {\n+    static final String TOPIC = \"topic\";\n+    static final String PARTITION = \"partition\";\n+    static final String START_READ_OFFSET = \"start_read_offset\";\n+    static final String START_READ_TIME = \"start_read_time\";\n+    static final String BOOTSTRAP_SERVERS = \"bootstrap_servers\";\n+\n+    static Schema getSchema() {\n+      return Schema.builder()\n+          .addStringField(TOPIC)\n+          .addInt32Field(PARTITION)\n+          .addNullableField(START_READ_OFFSET, FieldType.INT32)\n+          .addNullableField(START_READ_TIME, FieldType.INT64)\n+          .addNullableField(BOOTSTRAP_SERVERS, FieldType.array(FieldType.STRING))\n+          .build();\n+    }\n+  }\n+\n+  public static <K, V> ReadViaSDF<K, V> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime())\n+        .setCommitOffsetEnabled(false)\n+        .setKafkaSourceDescriptionSchema(KafkaSourceDescriptionSchemas.getSchema())\n+        .build();\n+  }\n+\n+  public ReadViaSDF<K, V> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializer(Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigUpdates(Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(\n+            getConsumerConfig(), KafkaIOUtils.IGNORED_CONSUMER_PROPERTIES, configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigOverrides(Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<Row> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input.apply(ParDo.of(new ReadFromKafkaDoFn())).setCoder(outputCoder);\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+      LOG.warn(\"Offset committed is not supported yet. Ignore the value.\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link Row} in {@link KafkaSourceDescriptionSchemas} which\n+   * represents a Kafka source description and outputs {@link KafkaRecord}. By default, a {@link\n+   * MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+   */\n+  @VisibleForTesting\n+  class ReadFromKafkaDoFn extends DoFn<Row, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn() {}\n+\n+    private final Map<String, Object> consumerConfig = ReadViaSDF.this.getConsumerConfig();\n+\n+    private final Map<String, Object> offsetConsumerConfig =\n+        ReadViaSDF.this.getOffsetConsumerConfig();\n+\n+    private final DeserializerProvider keyDeserializerProvider =\n+        ReadViaSDF.this.getKeyDeserializerProvider();\n+    private final DeserializerProvider valueDeserializerProvider =\n+        ReadViaSDF.this.getValueDeserializerProvider();\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn = ReadViaSDF.this.getConsumerFactoryFn();\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn =\n+        ReadViaSDF.this.getExtractOutputTimestampFn();\n+\n+    private final Duration kafkaPollTimeout = Duration.millis(1000);\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient KafkaIOUtils.MovingAvg avgRecordSize = null;\n+    private transient KafkaIOUtils.MovingAvg avgOffsetGap = null;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+        return offsetConsumer.position(topicPartition);\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element Row kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        consumerSpEL.evaluateAssign(offsetConsumer, ImmutableList.of(topicPartition));\n+        long startOffset;\n+        if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET)\n+            != null) {\n+          startOffset =\n+              kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET);\n+        } else if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_TIME)\n+            != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  topicPartition,\n+                  Instant.ofEpochMilli(kafkaSourceDescription.getInt64(\"start_read_time\")));\n+        } else {\n+          startOffset = offsetConsumer.position(topicPartition);\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public MonotonicallyIncreasing newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return new MonotonicallyIncreasing(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(@Element Row kafkaSourceDescription, @Restriction OffsetRange offsetRange)\n+        throws Exception {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      double numOfRecords = 0.0;\n+      if (offsetRange.getTo() != Long.MAX_VALUE) {\n+        numOfRecords = (new OffsetRangeTracker(offsetRange)).getProgress().getWorkRemaining();\n+      } else {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        KafkaLatestOffsetEstimator offsetEstimator =\n+            new KafkaLatestOffsetEstimator(\n+                consumerFactoryFn.apply(\n+                    KafkaIOUtils.getOffsetConsumerConfig(\n+                        \"size-\" + topicPartition.toString(),\n+                        offsetConsumerConfig,\n+                        updatedConsumerConfig)),\n+                topicPartition);\n+        numOfRecords =\n+            (new GrowableOffsetRangeTracker(offsetRange.getFrom(), offsetEstimator))\n+                .getProgress()\n+                .getWorkRemaining();\n+      }\n+\n+      // Before processing elements, we don't have a good estimated size of records and offset gap.\n+      if (avgOffsetGap != null) {\n+        numOfRecords = numOfRecords / (1 + avgOffsetGap.get());\n+      }\n+      return (avgRecordSize == null ? 1 : avgRecordSize.get()) * numOfRecords;\n+    }\n+\n+    @SplitRestriction\n+    public void splitRestriction(\n+        @Element Row kafkaSourceDescription,\n+        @Restriction OffsetRange offsetRange,\n+        OutputReceiver<OffsetRange> receiver)\n+        throws Exception {\n+      receiver.output(offsetRange);\n+    }\n+\n+    @NewTracker\n+    public RestrictionTracker<OffsetRange, Long> restrictionTracker(\n+        @Element Row kafkaSourceDescription, @Restriction OffsetRange restriction) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      if (restriction.getTo() == Long.MAX_VALUE) {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        KafkaLatestOffsetEstimator offsetPoller =\n+            new KafkaLatestOffsetEstimator(\n+                consumerFactoryFn.apply(\n+                    KafkaIOUtils.getOffsetConsumerConfig(\n+                        \"tracker-\" + topicPartition, offsetConsumerConfig, updatedConsumerConfig)),\n+                topicPartition);\n+        return new GrowableOffsetRangeTracker(restriction.getFrom(), offsetPoller);\n+      }\n+      return new OffsetRangeTracker(restriction);\n+    }\n+\n+    @ProcessElement\n+    public ProcessContinuation processElement(\n+        @Element Row kafkaSourceDescription,\n+        RestrictionTracker<OffsetRange, Long> tracker,\n+        WatermarkEstimator watermarkEstimator,\n+        OutputReceiver<KafkaRecord<K, V>> receiver) {\n+      TopicPartition topicPartition =\n+          new TopicPartition(\n+              kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+              kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> consumer = consumerFactoryFn.apply(updatedConsumerConfig)) {\n+        consumerSpEL.evaluateAssign(consumer, ImmutableList.of(topicPartition));\n+        long startOffset = tracker.currentRestriction().getFrom();\n+        long expectedOffset = startOffset;\n+        consumer.seek(topicPartition, startOffset);\n+        ConsumerRecords<byte[], byte[]> rawRecords = ConsumerRecords.empty();\n+\n+        try {\n+          while (true) {\n+            rawRecords = consumer.poll(kafkaPollTimeout.getMillis());\n+            // When there is no records from the current TopicPartition temporarily, self-checkpoint\n+            // and move to process the next element.\n+            if (rawRecords.isEmpty()) {\n+              return ProcessContinuation.resume();\n+            }\n+            for (ConsumerRecord<byte[], byte[]> rawRecord : rawRecords) {\n+              if (!tracker.tryClaim(rawRecord.offset())) {\n+                return ProcessContinuation.stop();\n+              }\n+              KafkaRecord<K, V> kafkaRecord =\n+                  new KafkaRecord<>(\n+                      rawRecord.topic(),\n+                      rawRecord.partition(),\n+                      rawRecord.offset(),\n+                      consumerSpEL.getRecordTimestamp(rawRecord),\n+                      consumerSpEL.getRecordTimestampType(rawRecord),\n+                      ConsumerSpEL.hasHeaders() ? rawRecord.headers() : null,\n+                      keyDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.key()),\n+                      valueDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.value()));\n+              Instant outputTimestamp = extractOutputTimestampFn.apply(kafkaRecord);\n+              int recordSize =\n+                  (rawRecord.key() == null ? 0 : rawRecord.key().length)\n+                      + (rawRecord.value() == null ? 0 : rawRecord.value().length);\n+              avgRecordSize.update(recordSize);\n+              avgOffsetGap.update(expectedOffset - rawRecord.offset());\n+              expectedOffset = rawRecord.offset() + 1;\n+              receiver.outputWithTimestamp(kafkaRecord, outputTimestamp);\n+            }\n+          }\n+        } catch (Exception anyException) {\n+          LOG.error(\"{}: Exception while reading from Kafka\", this, anyException);\n+          throw anyException;\n+        }\n+      }\n+    }\n+\n+    @GetRestrictionCoder\n+    public Coder<OffsetRange> restrictionCoder() {\n+      return new OffsetRange.Coder();\n+    }\n+\n+    @Setup\n+    public void setup() throws Exception {\n+      // Start to track record size and offset gap per bundle.\n+      avgRecordSize = new KafkaIOUtils.MovingAvg();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUxODU0MA=="}, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 693}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA1MjQ1Mw==", "bodyText": "Yes, currently KafkaUnboundedReader tracks avgRecordSize /avgOffsetGap per TopicPartition, but I don't think these values are serialized when checkpoint happens. We can bound avgRecordSize /avgOffsetGap to a TopicPartition by always creating a new MovingAvg when processElement is started.", "url": "https://github.com/apache/beam/pull/11749#discussion_r443052453", "createdAt": "2020-06-19T21:35:00Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n+ * The major difference from {@link KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source\n+ * descriptions(e.g., {@link KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()},\n+ * {@link KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead,\n+ * the pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from Kafka source description in {@link Row}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * Row}, and the restriction is an {@link OffsetRange} which represents record offset. A {@link\n+ * GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with {@code\n+ * Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(Row)} creates an initial range for a input element\n+ * {@link Row}. The end of range will be initialized as {@code Long.MAX_VALUE}. For the start of the\n+ * range:\n+ *\n+ * <ul>\n+ *   <li>If {@code start_read_offset} in {@link Row} is set, use this offset as start.\n+ *   <li>If {@code start_read_time} in {@link Row} is set, seek the start offset based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link Row} and move to process the next element. These deferred elements\n+ * will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link Row}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in the\n+ * {@link GrowableOffsetRangeTracker} as the {@link GrowableOffsetRangeTracker.RangeEndEstimator} to\n+ * poll the latest offset. Please refer to {@link ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for\n+ * details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(Row, OffsetRange).} A {@link\n+ * KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link Row}.\n+ */\n+@AutoValue\n+public abstract class ReadViaSDF<K, V>\n+    extends PTransform<PCollection<Row>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Schema getKafkaSourceDescriptionSchema();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V> setKafkaSourceDescriptionSchema(Schema schema);\n+\n+    abstract ReadViaSDF<K, V> build();\n+  }\n+\n+  static class KafkaSourceDescriptionSchemas {\n+    static final String TOPIC = \"topic\";\n+    static final String PARTITION = \"partition\";\n+    static final String START_READ_OFFSET = \"start_read_offset\";\n+    static final String START_READ_TIME = \"start_read_time\";\n+    static final String BOOTSTRAP_SERVERS = \"bootstrap_servers\";\n+\n+    static Schema getSchema() {\n+      return Schema.builder()\n+          .addStringField(TOPIC)\n+          .addInt32Field(PARTITION)\n+          .addNullableField(START_READ_OFFSET, FieldType.INT32)\n+          .addNullableField(START_READ_TIME, FieldType.INT64)\n+          .addNullableField(BOOTSTRAP_SERVERS, FieldType.array(FieldType.STRING))\n+          .build();\n+    }\n+  }\n+\n+  public static <K, V> ReadViaSDF<K, V> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime())\n+        .setCommitOffsetEnabled(false)\n+        .setKafkaSourceDescriptionSchema(KafkaSourceDescriptionSchemas.getSchema())\n+        .build();\n+  }\n+\n+  public ReadViaSDF<K, V> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializer(Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigUpdates(Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(\n+            getConsumerConfig(), KafkaIOUtils.IGNORED_CONSUMER_PROPERTIES, configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigOverrides(Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<Row> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input.apply(ParDo.of(new ReadFromKafkaDoFn())).setCoder(outputCoder);\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+      LOG.warn(\"Offset committed is not supported yet. Ignore the value.\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link Row} in {@link KafkaSourceDescriptionSchemas} which\n+   * represents a Kafka source description and outputs {@link KafkaRecord}. By default, a {@link\n+   * MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+   */\n+  @VisibleForTesting\n+  class ReadFromKafkaDoFn extends DoFn<Row, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn() {}\n+\n+    private final Map<String, Object> consumerConfig = ReadViaSDF.this.getConsumerConfig();\n+\n+    private final Map<String, Object> offsetConsumerConfig =\n+        ReadViaSDF.this.getOffsetConsumerConfig();\n+\n+    private final DeserializerProvider keyDeserializerProvider =\n+        ReadViaSDF.this.getKeyDeserializerProvider();\n+    private final DeserializerProvider valueDeserializerProvider =\n+        ReadViaSDF.this.getValueDeserializerProvider();\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn = ReadViaSDF.this.getConsumerFactoryFn();\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn =\n+        ReadViaSDF.this.getExtractOutputTimestampFn();\n+\n+    private final Duration kafkaPollTimeout = Duration.millis(1000);\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient KafkaIOUtils.MovingAvg avgRecordSize = null;\n+    private transient KafkaIOUtils.MovingAvg avgOffsetGap = null;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+        return offsetConsumer.position(topicPartition);\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element Row kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        consumerSpEL.evaluateAssign(offsetConsumer, ImmutableList.of(topicPartition));\n+        long startOffset;\n+        if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET)\n+            != null) {\n+          startOffset =\n+              kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET);\n+        } else if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_TIME)\n+            != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  topicPartition,\n+                  Instant.ofEpochMilli(kafkaSourceDescription.getInt64(\"start_read_time\")));\n+        } else {\n+          startOffset = offsetConsumer.position(topicPartition);\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public MonotonicallyIncreasing newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return new MonotonicallyIncreasing(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(@Element Row kafkaSourceDescription, @Restriction OffsetRange offsetRange)\n+        throws Exception {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      double numOfRecords = 0.0;\n+      if (offsetRange.getTo() != Long.MAX_VALUE) {\n+        numOfRecords = (new OffsetRangeTracker(offsetRange)).getProgress().getWorkRemaining();\n+      } else {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        KafkaLatestOffsetEstimator offsetEstimator =\n+            new KafkaLatestOffsetEstimator(\n+                consumerFactoryFn.apply(\n+                    KafkaIOUtils.getOffsetConsumerConfig(\n+                        \"size-\" + topicPartition.toString(),\n+                        offsetConsumerConfig,\n+                        updatedConsumerConfig)),\n+                topicPartition);\n+        numOfRecords =\n+            (new GrowableOffsetRangeTracker(offsetRange.getFrom(), offsetEstimator))\n+                .getProgress()\n+                .getWorkRemaining();\n+      }\n+\n+      // Before processing elements, we don't have a good estimated size of records and offset gap.\n+      if (avgOffsetGap != null) {\n+        numOfRecords = numOfRecords / (1 + avgOffsetGap.get());\n+      }\n+      return (avgRecordSize == null ? 1 : avgRecordSize.get()) * numOfRecords;\n+    }\n+\n+    @SplitRestriction\n+    public void splitRestriction(\n+        @Element Row kafkaSourceDescription,\n+        @Restriction OffsetRange offsetRange,\n+        OutputReceiver<OffsetRange> receiver)\n+        throws Exception {\n+      receiver.output(offsetRange);\n+    }\n+\n+    @NewTracker\n+    public RestrictionTracker<OffsetRange, Long> restrictionTracker(\n+        @Element Row kafkaSourceDescription, @Restriction OffsetRange restriction) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      if (restriction.getTo() == Long.MAX_VALUE) {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        KafkaLatestOffsetEstimator offsetPoller =\n+            new KafkaLatestOffsetEstimator(\n+                consumerFactoryFn.apply(\n+                    KafkaIOUtils.getOffsetConsumerConfig(\n+                        \"tracker-\" + topicPartition, offsetConsumerConfig, updatedConsumerConfig)),\n+                topicPartition);\n+        return new GrowableOffsetRangeTracker(restriction.getFrom(), offsetPoller);\n+      }\n+      return new OffsetRangeTracker(restriction);\n+    }\n+\n+    @ProcessElement\n+    public ProcessContinuation processElement(\n+        @Element Row kafkaSourceDescription,\n+        RestrictionTracker<OffsetRange, Long> tracker,\n+        WatermarkEstimator watermarkEstimator,\n+        OutputReceiver<KafkaRecord<K, V>> receiver) {\n+      TopicPartition topicPartition =\n+          new TopicPartition(\n+              kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+              kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> consumer = consumerFactoryFn.apply(updatedConsumerConfig)) {\n+        consumerSpEL.evaluateAssign(consumer, ImmutableList.of(topicPartition));\n+        long startOffset = tracker.currentRestriction().getFrom();\n+        long expectedOffset = startOffset;\n+        consumer.seek(topicPartition, startOffset);\n+        ConsumerRecords<byte[], byte[]> rawRecords = ConsumerRecords.empty();\n+\n+        try {\n+          while (true) {\n+            rawRecords = consumer.poll(kafkaPollTimeout.getMillis());\n+            // When there is no records from the current TopicPartition temporarily, self-checkpoint\n+            // and move to process the next element.\n+            if (rawRecords.isEmpty()) {\n+              return ProcessContinuation.resume();\n+            }\n+            for (ConsumerRecord<byte[], byte[]> rawRecord : rawRecords) {\n+              if (!tracker.tryClaim(rawRecord.offset())) {\n+                return ProcessContinuation.stop();\n+              }\n+              KafkaRecord<K, V> kafkaRecord =\n+                  new KafkaRecord<>(\n+                      rawRecord.topic(),\n+                      rawRecord.partition(),\n+                      rawRecord.offset(),\n+                      consumerSpEL.getRecordTimestamp(rawRecord),\n+                      consumerSpEL.getRecordTimestampType(rawRecord),\n+                      ConsumerSpEL.hasHeaders() ? rawRecord.headers() : null,\n+                      keyDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.key()),\n+                      valueDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.value()));\n+              Instant outputTimestamp = extractOutputTimestampFn.apply(kafkaRecord);\n+              int recordSize =\n+                  (rawRecord.key() == null ? 0 : rawRecord.key().length)\n+                      + (rawRecord.value() == null ? 0 : rawRecord.value().length);\n+              avgRecordSize.update(recordSize);\n+              avgOffsetGap.update(expectedOffset - rawRecord.offset());\n+              expectedOffset = rawRecord.offset() + 1;\n+              receiver.outputWithTimestamp(kafkaRecord, outputTimestamp);\n+            }\n+          }\n+        } catch (Exception anyException) {\n+          LOG.error(\"{}: Exception while reading from Kafka\", this, anyException);\n+          throw anyException;\n+        }\n+      }\n+    }\n+\n+    @GetRestrictionCoder\n+    public Coder<OffsetRange> restrictionCoder() {\n+      return new OffsetRange.Coder();\n+    }\n+\n+    @Setup\n+    public void setup() throws Exception {\n+      // Start to track record size and offset gap per bundle.\n+      avgRecordSize = new KafkaIOUtils.MovingAvg();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUxODU0MA=="}, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 693}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzA1OTM1Mg==", "bodyText": "That makes more sense but relies on knowing the lifecycle of when things get called. What if GetSize is invoked before ProcessElement instead of after. This will lead to implementation assumptions which will prevent SDF evolution.", "url": "https://github.com/apache/beam/pull/11749#discussion_r443059352", "createdAt": "2020-06-19T22:01:32Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n+ * The major difference from {@link KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source\n+ * descriptions(e.g., {@link KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()},\n+ * {@link KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead,\n+ * the pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from Kafka source description in {@link Row}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * Row}, and the restriction is an {@link OffsetRange} which represents record offset. A {@link\n+ * GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with {@code\n+ * Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(Row)} creates an initial range for a input element\n+ * {@link Row}. The end of range will be initialized as {@code Long.MAX_VALUE}. For the start of the\n+ * range:\n+ *\n+ * <ul>\n+ *   <li>If {@code start_read_offset} in {@link Row} is set, use this offset as start.\n+ *   <li>If {@code start_read_time} in {@link Row} is set, seek the start offset based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link Row} and move to process the next element. These deferred elements\n+ * will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link Row}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in the\n+ * {@link GrowableOffsetRangeTracker} as the {@link GrowableOffsetRangeTracker.RangeEndEstimator} to\n+ * poll the latest offset. Please refer to {@link ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for\n+ * details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(Row, OffsetRange).} A {@link\n+ * KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link Row}.\n+ */\n+@AutoValue\n+public abstract class ReadViaSDF<K, V>\n+    extends PTransform<PCollection<Row>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Schema getKafkaSourceDescriptionSchema();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V> setKafkaSourceDescriptionSchema(Schema schema);\n+\n+    abstract ReadViaSDF<K, V> build();\n+  }\n+\n+  static class KafkaSourceDescriptionSchemas {\n+    static final String TOPIC = \"topic\";\n+    static final String PARTITION = \"partition\";\n+    static final String START_READ_OFFSET = \"start_read_offset\";\n+    static final String START_READ_TIME = \"start_read_time\";\n+    static final String BOOTSTRAP_SERVERS = \"bootstrap_servers\";\n+\n+    static Schema getSchema() {\n+      return Schema.builder()\n+          .addStringField(TOPIC)\n+          .addInt32Field(PARTITION)\n+          .addNullableField(START_READ_OFFSET, FieldType.INT32)\n+          .addNullableField(START_READ_TIME, FieldType.INT64)\n+          .addNullableField(BOOTSTRAP_SERVERS, FieldType.array(FieldType.STRING))\n+          .build();\n+    }\n+  }\n+\n+  public static <K, V> ReadViaSDF<K, V> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime())\n+        .setCommitOffsetEnabled(false)\n+        .setKafkaSourceDescriptionSchema(KafkaSourceDescriptionSchemas.getSchema())\n+        .build();\n+  }\n+\n+  public ReadViaSDF<K, V> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializer(Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigUpdates(Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(\n+            getConsumerConfig(), KafkaIOUtils.IGNORED_CONSUMER_PROPERTIES, configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigOverrides(Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<Row> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input.apply(ParDo.of(new ReadFromKafkaDoFn())).setCoder(outputCoder);\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+      LOG.warn(\"Offset committed is not supported yet. Ignore the value.\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link Row} in {@link KafkaSourceDescriptionSchemas} which\n+   * represents a Kafka source description and outputs {@link KafkaRecord}. By default, a {@link\n+   * MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+   */\n+  @VisibleForTesting\n+  class ReadFromKafkaDoFn extends DoFn<Row, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn() {}\n+\n+    private final Map<String, Object> consumerConfig = ReadViaSDF.this.getConsumerConfig();\n+\n+    private final Map<String, Object> offsetConsumerConfig =\n+        ReadViaSDF.this.getOffsetConsumerConfig();\n+\n+    private final DeserializerProvider keyDeserializerProvider =\n+        ReadViaSDF.this.getKeyDeserializerProvider();\n+    private final DeserializerProvider valueDeserializerProvider =\n+        ReadViaSDF.this.getValueDeserializerProvider();\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn = ReadViaSDF.this.getConsumerFactoryFn();\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn =\n+        ReadViaSDF.this.getExtractOutputTimestampFn();\n+\n+    private final Duration kafkaPollTimeout = Duration.millis(1000);\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient KafkaIOUtils.MovingAvg avgRecordSize = null;\n+    private transient KafkaIOUtils.MovingAvg avgOffsetGap = null;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+        return offsetConsumer.position(topicPartition);\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element Row kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        consumerSpEL.evaluateAssign(offsetConsumer, ImmutableList.of(topicPartition));\n+        long startOffset;\n+        if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET)\n+            != null) {\n+          startOffset =\n+              kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET);\n+        } else if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_TIME)\n+            != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  topicPartition,\n+                  Instant.ofEpochMilli(kafkaSourceDescription.getInt64(\"start_read_time\")));\n+        } else {\n+          startOffset = offsetConsumer.position(topicPartition);\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public MonotonicallyIncreasing newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return new MonotonicallyIncreasing(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(@Element Row kafkaSourceDescription, @Restriction OffsetRange offsetRange)\n+        throws Exception {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      double numOfRecords = 0.0;\n+      if (offsetRange.getTo() != Long.MAX_VALUE) {\n+        numOfRecords = (new OffsetRangeTracker(offsetRange)).getProgress().getWorkRemaining();\n+      } else {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        KafkaLatestOffsetEstimator offsetEstimator =\n+            new KafkaLatestOffsetEstimator(\n+                consumerFactoryFn.apply(\n+                    KafkaIOUtils.getOffsetConsumerConfig(\n+                        \"size-\" + topicPartition.toString(),\n+                        offsetConsumerConfig,\n+                        updatedConsumerConfig)),\n+                topicPartition);\n+        numOfRecords =\n+            (new GrowableOffsetRangeTracker(offsetRange.getFrom(), offsetEstimator))\n+                .getProgress()\n+                .getWorkRemaining();\n+      }\n+\n+      // Before processing elements, we don't have a good estimated size of records and offset gap.\n+      if (avgOffsetGap != null) {\n+        numOfRecords = numOfRecords / (1 + avgOffsetGap.get());\n+      }\n+      return (avgRecordSize == null ? 1 : avgRecordSize.get()) * numOfRecords;\n+    }\n+\n+    @SplitRestriction\n+    public void splitRestriction(\n+        @Element Row kafkaSourceDescription,\n+        @Restriction OffsetRange offsetRange,\n+        OutputReceiver<OffsetRange> receiver)\n+        throws Exception {\n+      receiver.output(offsetRange);\n+    }\n+\n+    @NewTracker\n+    public RestrictionTracker<OffsetRange, Long> restrictionTracker(\n+        @Element Row kafkaSourceDescription, @Restriction OffsetRange restriction) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      if (restriction.getTo() == Long.MAX_VALUE) {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        KafkaLatestOffsetEstimator offsetPoller =\n+            new KafkaLatestOffsetEstimator(\n+                consumerFactoryFn.apply(\n+                    KafkaIOUtils.getOffsetConsumerConfig(\n+                        \"tracker-\" + topicPartition, offsetConsumerConfig, updatedConsumerConfig)),\n+                topicPartition);\n+        return new GrowableOffsetRangeTracker(restriction.getFrom(), offsetPoller);\n+      }\n+      return new OffsetRangeTracker(restriction);\n+    }\n+\n+    @ProcessElement\n+    public ProcessContinuation processElement(\n+        @Element Row kafkaSourceDescription,\n+        RestrictionTracker<OffsetRange, Long> tracker,\n+        WatermarkEstimator watermarkEstimator,\n+        OutputReceiver<KafkaRecord<K, V>> receiver) {\n+      TopicPartition topicPartition =\n+          new TopicPartition(\n+              kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+              kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> consumer = consumerFactoryFn.apply(updatedConsumerConfig)) {\n+        consumerSpEL.evaluateAssign(consumer, ImmutableList.of(topicPartition));\n+        long startOffset = tracker.currentRestriction().getFrom();\n+        long expectedOffset = startOffset;\n+        consumer.seek(topicPartition, startOffset);\n+        ConsumerRecords<byte[], byte[]> rawRecords = ConsumerRecords.empty();\n+\n+        try {\n+          while (true) {\n+            rawRecords = consumer.poll(kafkaPollTimeout.getMillis());\n+            // When there is no records from the current TopicPartition temporarily, self-checkpoint\n+            // and move to process the next element.\n+            if (rawRecords.isEmpty()) {\n+              return ProcessContinuation.resume();\n+            }\n+            for (ConsumerRecord<byte[], byte[]> rawRecord : rawRecords) {\n+              if (!tracker.tryClaim(rawRecord.offset())) {\n+                return ProcessContinuation.stop();\n+              }\n+              KafkaRecord<K, V> kafkaRecord =\n+                  new KafkaRecord<>(\n+                      rawRecord.topic(),\n+                      rawRecord.partition(),\n+                      rawRecord.offset(),\n+                      consumerSpEL.getRecordTimestamp(rawRecord),\n+                      consumerSpEL.getRecordTimestampType(rawRecord),\n+                      ConsumerSpEL.hasHeaders() ? rawRecord.headers() : null,\n+                      keyDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.key()),\n+                      valueDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.value()));\n+              Instant outputTimestamp = extractOutputTimestampFn.apply(kafkaRecord);\n+              int recordSize =\n+                  (rawRecord.key() == null ? 0 : rawRecord.key().length)\n+                      + (rawRecord.value() == null ? 0 : rawRecord.value().length);\n+              avgRecordSize.update(recordSize);\n+              avgOffsetGap.update(expectedOffset - rawRecord.offset());\n+              expectedOffset = rawRecord.offset() + 1;\n+              receiver.outputWithTimestamp(kafkaRecord, outputTimestamp);\n+            }\n+          }\n+        } catch (Exception anyException) {\n+          LOG.error(\"{}: Exception while reading from Kafka\", this, anyException);\n+          throw anyException;\n+        }\n+      }\n+    }\n+\n+    @GetRestrictionCoder\n+    public Coder<OffsetRange> restrictionCoder() {\n+      return new OffsetRange.Coder();\n+    }\n+\n+    @Setup\n+    public void setup() throws Exception {\n+      // Start to track record size and offset gap per bundle.\n+      avgRecordSize = new KafkaIOUtils.MovingAvg();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUxODU0MA=="}, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 693}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1Njk0Mzc3OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMTo0MzozOVrOGmBOgw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMTo0MzozOVrOGmBOgw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUxOTE3MQ==", "bodyText": "these will go out of scope so no need to setup them to null in teardown.", "url": "https://github.com/apache/beam/pull/11749#discussion_r442519171", "createdAt": "2020-06-18T21:43:39Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n+ * The major difference from {@link KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source\n+ * descriptions(e.g., {@link KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()},\n+ * {@link KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead,\n+ * the pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from Kafka source description in {@link Row}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * Row}, and the restriction is an {@link OffsetRange} which represents record offset. A {@link\n+ * GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with {@code\n+ * Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(Row)} creates an initial range for a input element\n+ * {@link Row}. The end of range will be initialized as {@code Long.MAX_VALUE}. For the start of the\n+ * range:\n+ *\n+ * <ul>\n+ *   <li>If {@code start_read_offset} in {@link Row} is set, use this offset as start.\n+ *   <li>If {@code start_read_time} in {@link Row} is set, seek the start offset based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link Row} and move to process the next element. These deferred elements\n+ * will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link Row}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in the\n+ * {@link GrowableOffsetRangeTracker} as the {@link GrowableOffsetRangeTracker.RangeEndEstimator} to\n+ * poll the latest offset. Please refer to {@link ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for\n+ * details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(Row, OffsetRange).} A {@link\n+ * KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link Row}.\n+ */\n+@AutoValue\n+public abstract class ReadViaSDF<K, V>\n+    extends PTransform<PCollection<Row>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Schema getKafkaSourceDescriptionSchema();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V> setKafkaSourceDescriptionSchema(Schema schema);\n+\n+    abstract ReadViaSDF<K, V> build();\n+  }\n+\n+  static class KafkaSourceDescriptionSchemas {\n+    static final String TOPIC = \"topic\";\n+    static final String PARTITION = \"partition\";\n+    static final String START_READ_OFFSET = \"start_read_offset\";\n+    static final String START_READ_TIME = \"start_read_time\";\n+    static final String BOOTSTRAP_SERVERS = \"bootstrap_servers\";\n+\n+    static Schema getSchema() {\n+      return Schema.builder()\n+          .addStringField(TOPIC)\n+          .addInt32Field(PARTITION)\n+          .addNullableField(START_READ_OFFSET, FieldType.INT32)\n+          .addNullableField(START_READ_TIME, FieldType.INT64)\n+          .addNullableField(BOOTSTRAP_SERVERS, FieldType.array(FieldType.STRING))\n+          .build();\n+    }\n+  }\n+\n+  public static <K, V> ReadViaSDF<K, V> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime())\n+        .setCommitOffsetEnabled(false)\n+        .setKafkaSourceDescriptionSchema(KafkaSourceDescriptionSchemas.getSchema())\n+        .build();\n+  }\n+\n+  public ReadViaSDF<K, V> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializer(Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigUpdates(Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(\n+            getConsumerConfig(), KafkaIOUtils.IGNORED_CONSUMER_PROPERTIES, configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigOverrides(Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<Row> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input.apply(ParDo.of(new ReadFromKafkaDoFn())).setCoder(outputCoder);\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+      LOG.warn(\"Offset committed is not supported yet. Ignore the value.\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link Row} in {@link KafkaSourceDescriptionSchemas} which\n+   * represents a Kafka source description and outputs {@link KafkaRecord}. By default, a {@link\n+   * MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+   */\n+  @VisibleForTesting\n+  class ReadFromKafkaDoFn extends DoFn<Row, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn() {}\n+\n+    private final Map<String, Object> consumerConfig = ReadViaSDF.this.getConsumerConfig();\n+\n+    private final Map<String, Object> offsetConsumerConfig =\n+        ReadViaSDF.this.getOffsetConsumerConfig();\n+\n+    private final DeserializerProvider keyDeserializerProvider =\n+        ReadViaSDF.this.getKeyDeserializerProvider();\n+    private final DeserializerProvider valueDeserializerProvider =\n+        ReadViaSDF.this.getValueDeserializerProvider();\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn = ReadViaSDF.this.getConsumerFactoryFn();\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn =\n+        ReadViaSDF.this.getExtractOutputTimestampFn();\n+\n+    private final Duration kafkaPollTimeout = Duration.millis(1000);\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient KafkaIOUtils.MovingAvg avgRecordSize = null;\n+    private transient KafkaIOUtils.MovingAvg avgOffsetGap = null;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+        return offsetConsumer.position(topicPartition);\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element Row kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        consumerSpEL.evaluateAssign(offsetConsumer, ImmutableList.of(topicPartition));\n+        long startOffset;\n+        if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET)\n+            != null) {\n+          startOffset =\n+              kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET);\n+        } else if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_TIME)\n+            != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  topicPartition,\n+                  Instant.ofEpochMilli(kafkaSourceDescription.getInt64(\"start_read_time\")));\n+        } else {\n+          startOffset = offsetConsumer.position(topicPartition);\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public MonotonicallyIncreasing newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return new MonotonicallyIncreasing(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(@Element Row kafkaSourceDescription, @Restriction OffsetRange offsetRange)\n+        throws Exception {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      double numOfRecords = 0.0;\n+      if (offsetRange.getTo() != Long.MAX_VALUE) {\n+        numOfRecords = (new OffsetRangeTracker(offsetRange)).getProgress().getWorkRemaining();\n+      } else {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        KafkaLatestOffsetEstimator offsetEstimator =\n+            new KafkaLatestOffsetEstimator(\n+                consumerFactoryFn.apply(\n+                    KafkaIOUtils.getOffsetConsumerConfig(\n+                        \"size-\" + topicPartition.toString(),\n+                        offsetConsumerConfig,\n+                        updatedConsumerConfig)),\n+                topicPartition);\n+        numOfRecords =\n+            (new GrowableOffsetRangeTracker(offsetRange.getFrom(), offsetEstimator))\n+                .getProgress()\n+                .getWorkRemaining();\n+      }\n+\n+      // Before processing elements, we don't have a good estimated size of records and offset gap.\n+      if (avgOffsetGap != null) {\n+        numOfRecords = numOfRecords / (1 + avgOffsetGap.get());\n+      }\n+      return (avgRecordSize == null ? 1 : avgRecordSize.get()) * numOfRecords;\n+    }\n+\n+    @SplitRestriction\n+    public void splitRestriction(\n+        @Element Row kafkaSourceDescription,\n+        @Restriction OffsetRange offsetRange,\n+        OutputReceiver<OffsetRange> receiver)\n+        throws Exception {\n+      receiver.output(offsetRange);\n+    }\n+\n+    @NewTracker\n+    public RestrictionTracker<OffsetRange, Long> restrictionTracker(\n+        @Element Row kafkaSourceDescription, @Restriction OffsetRange restriction) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      if (restriction.getTo() == Long.MAX_VALUE) {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        KafkaLatestOffsetEstimator offsetPoller =\n+            new KafkaLatestOffsetEstimator(\n+                consumerFactoryFn.apply(\n+                    KafkaIOUtils.getOffsetConsumerConfig(\n+                        \"tracker-\" + topicPartition, offsetConsumerConfig, updatedConsumerConfig)),\n+                topicPartition);\n+        return new GrowableOffsetRangeTracker(restriction.getFrom(), offsetPoller);\n+      }\n+      return new OffsetRangeTracker(restriction);\n+    }\n+\n+    @ProcessElement\n+    public ProcessContinuation processElement(\n+        @Element Row kafkaSourceDescription,\n+        RestrictionTracker<OffsetRange, Long> tracker,\n+        WatermarkEstimator watermarkEstimator,\n+        OutputReceiver<KafkaRecord<K, V>> receiver) {\n+      TopicPartition topicPartition =\n+          new TopicPartition(\n+              kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+              kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> consumer = consumerFactoryFn.apply(updatedConsumerConfig)) {\n+        consumerSpEL.evaluateAssign(consumer, ImmutableList.of(topicPartition));\n+        long startOffset = tracker.currentRestriction().getFrom();\n+        long expectedOffset = startOffset;\n+        consumer.seek(topicPartition, startOffset);\n+        ConsumerRecords<byte[], byte[]> rawRecords = ConsumerRecords.empty();\n+\n+        try {\n+          while (true) {\n+            rawRecords = consumer.poll(kafkaPollTimeout.getMillis());\n+            // When there is no records from the current TopicPartition temporarily, self-checkpoint\n+            // and move to process the next element.\n+            if (rawRecords.isEmpty()) {\n+              return ProcessContinuation.resume();\n+            }\n+            for (ConsumerRecord<byte[], byte[]> rawRecord : rawRecords) {\n+              if (!tracker.tryClaim(rawRecord.offset())) {\n+                return ProcessContinuation.stop();\n+              }\n+              KafkaRecord<K, V> kafkaRecord =\n+                  new KafkaRecord<>(\n+                      rawRecord.topic(),\n+                      rawRecord.partition(),\n+                      rawRecord.offset(),\n+                      consumerSpEL.getRecordTimestamp(rawRecord),\n+                      consumerSpEL.getRecordTimestampType(rawRecord),\n+                      ConsumerSpEL.hasHeaders() ? rawRecord.headers() : null,\n+                      keyDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.key()),\n+                      valueDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.value()));\n+              Instant outputTimestamp = extractOutputTimestampFn.apply(kafkaRecord);\n+              int recordSize =\n+                  (rawRecord.key() == null ? 0 : rawRecord.key().length)\n+                      + (rawRecord.value() == null ? 0 : rawRecord.value().length);\n+              avgRecordSize.update(recordSize);\n+              avgOffsetGap.update(expectedOffset - rawRecord.offset());\n+              expectedOffset = rawRecord.offset() + 1;\n+              receiver.outputWithTimestamp(kafkaRecord, outputTimestamp);\n+            }\n+          }\n+        } catch (Exception anyException) {\n+          LOG.error(\"{}: Exception while reading from Kafka\", this, anyException);\n+          throw anyException;\n+        }\n+      }\n+    }\n+\n+    @GetRestrictionCoder\n+    public Coder<OffsetRange> restrictionCoder() {\n+      return new OffsetRange.Coder();\n+    }\n+\n+    @Setup\n+    public void setup() throws Exception {\n+      // Start to track record size and offset gap per bundle.\n+      avgRecordSize = new KafkaIOUtils.MovingAvg();\n+      avgOffsetGap = new KafkaIOUtils.MovingAvg();\n+      consumerSpEL = new ConsumerSpEL();\n+      keyDeserializerInstance = keyDeserializerProvider.getDeserializer(consumerConfig, true);\n+      valueDeserializerInstance = valueDeserializerProvider.getDeserializer(consumerConfig, false);\n+    }\n+\n+    @Teardown\n+    public void teardown() throws Exception {\n+      try {\n+        Closeables.close(keyDeserializerInstance, true);\n+        Closeables.close(valueDeserializerInstance, true);\n+        avgRecordSize = null;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 705}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1Njk1MTAyOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMTo0NjoyOFrOGmBS-w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMTo0NjoyOFrOGmBS-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUyMDMxNQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n          \n          \n            \n             * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n          \n          \n            \n             * For more details about the concept of {@code SplittableDoFn}, please refer to the <a href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a>\n          \n          \n            \n             *  and <a href=\"https://s.apache.org/beam-fn-api\">design doc</a>.", "url": "https://github.com/apache/beam/pull/11749#discussion_r442520315", "createdAt": "2020-06-18T21:46:28Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 65}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1Njk1NDM0OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMTo0NzozOFrOGmBU7w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOFQyMTo0NzozOFrOGmBU7w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MjUyMDgxNQ==", "bodyText": "ReadFromKafkaViaSDF?", "url": "https://github.com/apache/beam/pull/11749#discussion_r442520815", "createdAt": "2020-06-18T21:47:38Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n+ * The major difference from {@link KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source\n+ * descriptions(e.g., {@link KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()},\n+ * {@link KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead,\n+ * the pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d4dc7fe3cf1bff6902aeecda7751ae1c2ddcbdeb"}, "originalPosition": 101}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc1OTc1NTUwOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxNzozNTowNVrOGmcgHw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0xOVQxODowOTowOVrOGmdYAw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk2NjA0Nw==", "bodyText": "Do we really need to poll here, can we check if there is anything left and if there is nothing return a process continuation with a resume delay that represents the poll timeout?", "url": "https://github.com/apache/beam/pull/11749#discussion_r442966047", "createdAt": "2020-06-19T17:35:05Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n+ * The major difference from {@link KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source\n+ * descriptions(e.g., {@link KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()},\n+ * {@link KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead,\n+ * the pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from Kafka source description in {@link Row}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * Row}, and the restriction is an {@link OffsetRange} which represents record offset. A {@link\n+ * GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with {@code\n+ * Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(Row)} creates an initial range for a input element\n+ * {@link Row}. The end of range will be initialized as {@code Long.MAX_VALUE}. For the start of the\n+ * range:\n+ *\n+ * <ul>\n+ *   <li>If {@code start_read_offset} in {@link Row} is set, use this offset as start.\n+ *   <li>If {@code start_read_time} in {@link Row} is set, seek the start offset based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link Row} and move to process the next element. These deferred elements\n+ * will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link Row}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in the\n+ * {@link GrowableOffsetRangeTracker} as the {@link GrowableOffsetRangeTracker.RangeEndEstimator} to\n+ * poll the latest offset. Please refer to {@link ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for\n+ * details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(Row, OffsetRange).} A {@link\n+ * KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link Row}.\n+ */\n+@AutoValue\n+public abstract class ReadViaSDF<K, V>\n+    extends PTransform<PCollection<Row>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Schema getKafkaSourceDescriptionSchema();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V> setKafkaSourceDescriptionSchema(Schema schema);\n+\n+    abstract ReadViaSDF<K, V> build();\n+  }\n+\n+  static class KafkaSourceDescriptionSchemas {\n+    static final String TOPIC = \"topic\";\n+    static final String PARTITION = \"partition\";\n+    static final String START_READ_OFFSET = \"start_read_offset\";\n+    static final String START_READ_TIME = \"start_read_time\";\n+    static final String BOOTSTRAP_SERVERS = \"bootstrap_servers\";\n+\n+    static Schema getSchema() {\n+      return Schema.builder()\n+          .addStringField(TOPIC)\n+          .addInt32Field(PARTITION)\n+          .addNullableField(START_READ_OFFSET, FieldType.INT32)\n+          .addNullableField(START_READ_TIME, FieldType.INT64)\n+          .addNullableField(BOOTSTRAP_SERVERS, FieldType.array(FieldType.STRING))\n+          .build();\n+    }\n+  }\n+\n+  public static <K, V> ReadViaSDF<K, V> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime())\n+        .setCommitOffsetEnabled(false)\n+        .setKafkaSourceDescriptionSchema(KafkaSourceDescriptionSchemas.getSchema())\n+        .build();\n+  }\n+\n+  public ReadViaSDF<K, V> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializer(Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigUpdates(Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(\n+            getConsumerConfig(), KafkaIOUtils.IGNORED_CONSUMER_PROPERTIES, configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigOverrides(Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<Row> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input.apply(ParDo.of(new ReadFromKafkaDoFn())).setCoder(outputCoder);\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+      LOG.warn(\"Offset committed is not supported yet. Ignore the value.\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link Row} in {@link KafkaSourceDescriptionSchemas} which\n+   * represents a Kafka source description and outputs {@link KafkaRecord}. By default, a {@link\n+   * MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+   */\n+  @VisibleForTesting\n+  class ReadFromKafkaDoFn extends DoFn<Row, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn() {}\n+\n+    private final Map<String, Object> consumerConfig = ReadViaSDF.this.getConsumerConfig();\n+\n+    private final Map<String, Object> offsetConsumerConfig =\n+        ReadViaSDF.this.getOffsetConsumerConfig();\n+\n+    private final DeserializerProvider keyDeserializerProvider =\n+        ReadViaSDF.this.getKeyDeserializerProvider();\n+    private final DeserializerProvider valueDeserializerProvider =\n+        ReadViaSDF.this.getValueDeserializerProvider();\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn = ReadViaSDF.this.getConsumerFactoryFn();\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn =\n+        ReadViaSDF.this.getExtractOutputTimestampFn();\n+\n+    private final Duration kafkaPollTimeout = Duration.millis(1000);\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient KafkaIOUtils.MovingAvg avgRecordSize = null;\n+    private transient KafkaIOUtils.MovingAvg avgOffsetGap = null;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+        return offsetConsumer.position(topicPartition);\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element Row kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        consumerSpEL.evaluateAssign(offsetConsumer, ImmutableList.of(topicPartition));\n+        long startOffset;\n+        if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET)\n+            != null) {\n+          startOffset =\n+              kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET);\n+        } else if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_TIME)\n+            != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  topicPartition,\n+                  Instant.ofEpochMilli(kafkaSourceDescription.getInt64(\"start_read_time\")));\n+        } else {\n+          startOffset = offsetConsumer.position(topicPartition);\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public MonotonicallyIncreasing newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return new MonotonicallyIncreasing(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(@Element Row kafkaSourceDescription, @Restriction OffsetRange offsetRange)\n+        throws Exception {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      double numOfRecords = 0.0;\n+      if (offsetRange.getTo() != Long.MAX_VALUE) {\n+        numOfRecords = (new OffsetRangeTracker(offsetRange)).getProgress().getWorkRemaining();\n+      } else {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        KafkaLatestOffsetEstimator offsetEstimator =\n+            new KafkaLatestOffsetEstimator(\n+                consumerFactoryFn.apply(\n+                    KafkaIOUtils.getOffsetConsumerConfig(\n+                        \"size-\" + topicPartition.toString(),\n+                        offsetConsumerConfig,\n+                        updatedConsumerConfig)),\n+                topicPartition);\n+        numOfRecords =\n+            (new GrowableOffsetRangeTracker(offsetRange.getFrom(), offsetEstimator))\n+                .getProgress()\n+                .getWorkRemaining();\n+      }\n+\n+      // Before processing elements, we don't have a good estimated size of records and offset gap.\n+      if (avgOffsetGap != null) {\n+        numOfRecords = numOfRecords / (1 + avgOffsetGap.get());\n+      }\n+      return (avgRecordSize == null ? 1 : avgRecordSize.get()) * numOfRecords;\n+    }\n+\n+    @SplitRestriction\n+    public void splitRestriction(\n+        @Element Row kafkaSourceDescription,\n+        @Restriction OffsetRange offsetRange,\n+        OutputReceiver<OffsetRange> receiver)\n+        throws Exception {\n+      receiver.output(offsetRange);\n+    }\n+\n+    @NewTracker\n+    public RestrictionTracker<OffsetRange, Long> restrictionTracker(\n+        @Element Row kafkaSourceDescription, @Restriction OffsetRange restriction) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      if (restriction.getTo() == Long.MAX_VALUE) {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        KafkaLatestOffsetEstimator offsetPoller =\n+            new KafkaLatestOffsetEstimator(\n+                consumerFactoryFn.apply(\n+                    KafkaIOUtils.getOffsetConsumerConfig(\n+                        \"tracker-\" + topicPartition, offsetConsumerConfig, updatedConsumerConfig)),\n+                topicPartition);\n+        return new GrowableOffsetRangeTracker(restriction.getFrom(), offsetPoller);\n+      }\n+      return new OffsetRangeTracker(restriction);\n+    }\n+\n+    @ProcessElement\n+    public ProcessContinuation processElement(\n+        @Element Row kafkaSourceDescription,\n+        RestrictionTracker<OffsetRange, Long> tracker,\n+        WatermarkEstimator watermarkEstimator,\n+        OutputReceiver<KafkaRecord<K, V>> receiver) {\n+      TopicPartition topicPartition =\n+          new TopicPartition(\n+              kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+              kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> consumer = consumerFactoryFn.apply(updatedConsumerConfig)) {\n+        consumerSpEL.evaluateAssign(consumer, ImmutableList.of(topicPartition));\n+        long startOffset = tracker.currentRestriction().getFrom();\n+        long expectedOffset = startOffset;\n+        consumer.seek(topicPartition, startOffset);\n+        ConsumerRecords<byte[], byte[]> rawRecords = ConsumerRecords.empty();\n+\n+        try {\n+          while (true) {\n+            rawRecords = consumer.poll(kafkaPollTimeout.getMillis());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "07ddcb46c6245435aea78878b8de2589592d623c"}, "originalPosition": 648}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk4MDM1NQ==", "bodyText": "It seems like there is no difference between poll ->resume and resume -> poll? For example, a bad situation could be there is only one TopicPartition and there is always no available records. In this case, both poll ->resume and resume -> poll will poll the records with a time interval of api timeout + residual reschedule time.", "url": "https://github.com/apache/beam/pull/11749#discussion_r442980355", "createdAt": "2020-06-19T18:09:09Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,742 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.HashMap;\n+import java.util.Map;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link Row} IN {@link\n+ * KafkaSourceDescriptionSchemas} which represents Kafka source description as input and outputs a\n+ * PCollection of {@link KafkaRecord}. The core implementation is based on {@code SplittableDoFn}.\n+ * For more details about the concept of {@code SplittableDoFn}, please refer to the beam blog post:\n+ * https://beam.apache.org/blog/splittable-do-fn/ and design doc:https://s.apache.org/beam-fn-api.\n+ * The major difference from {@link KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source\n+ * descriptions(e.g., {@link KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()},\n+ * {@link KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead,\n+ * the pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ *  .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"my_topic\", 1))))\n+ * .apply(ReadFromKafkaViaSDF.create()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from Kafka source description in {@link Row}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * Row}, and the restriction is an {@link OffsetRange} which represents record offset. A {@link\n+ * GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with {@code\n+ * Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(Row)} creates an initial range for a input element\n+ * {@link Row}. The end of range will be initialized as {@code Long.MAX_VALUE}. For the start of the\n+ * range:\n+ *\n+ * <ul>\n+ *   <li>If {@code start_read_offset} in {@link Row} is set, use this offset as start.\n+ *   <li>If {@code start_read_time} in {@link Row} is set, seek the start offset based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link Row} and move to process the next element. These deferred elements\n+ * will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or {@link OffsetRangeTracker}\n+ * per {@link Row}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in the\n+ * {@link GrowableOffsetRangeTracker} as the {@link GrowableOffsetRangeTracker.RangeEndEstimator} to\n+ * poll the latest offset. Please refer to {@link ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for\n+ * details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(Row, OffsetRange).} A {@link\n+ * KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The estimated watermark is computed by {@link MonotonicallyIncreasing} based on output timestamps\n+ * per {@link Row}.\n+ */\n+@AutoValue\n+public abstract class ReadViaSDF<K, V>\n+    extends PTransform<PCollection<Row>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  abstract Schema getKafkaSourceDescriptionSchema();\n+\n+  abstract Builder<K, V> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V> {\n+    abstract Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V> setOffsetConsumerConfig(Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V> setKeyDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setValueDeserializerProvider(DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V> setKafkaSourceDescriptionSchema(Schema schema);\n+\n+    abstract ReadViaSDF<K, V> build();\n+  }\n+\n+  static class KafkaSourceDescriptionSchemas {\n+    static final String TOPIC = \"topic\";\n+    static final String PARTITION = \"partition\";\n+    static final String START_READ_OFFSET = \"start_read_offset\";\n+    static final String START_READ_TIME = \"start_read_time\";\n+    static final String BOOTSTRAP_SERVERS = \"bootstrap_servers\";\n+\n+    static Schema getSchema() {\n+      return Schema.builder()\n+          .addStringField(TOPIC)\n+          .addInt32Field(PARTITION)\n+          .addNullableField(START_READ_OFFSET, FieldType.INT32)\n+          .addNullableField(START_READ_TIME, FieldType.INT64)\n+          .addNullableField(BOOTSTRAP_SERVERS, FieldType.array(FieldType.STRING))\n+          .build();\n+    }\n+  }\n+\n+  public static <K, V> ReadViaSDF<K, V> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime())\n+        .setCommitOffsetEnabled(false)\n+        .setKafkaSourceDescriptionSchema(KafkaSourceDescriptionSchemas.getSchema())\n+        .build();\n+  }\n+\n+  public ReadViaSDF<K, V> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializer(Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigUpdates(Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(\n+            getConsumerConfig(), KafkaIOUtils.IGNORED_CONSUMER_PROPERTIES, configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V> withConsumerConfigOverrides(Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<Row> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input.apply(ParDo.of(new ReadFromKafkaDoFn())).setCoder(outputCoder);\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+      LOG.warn(\"Offset committed is not supported yet. Ignore the value.\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link Row} in {@link KafkaSourceDescriptionSchemas} which\n+   * represents a Kafka source description and outputs {@link KafkaRecord}. By default, a {@link\n+   * MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+   */\n+  @VisibleForTesting\n+  class ReadFromKafkaDoFn extends DoFn<Row, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn() {}\n+\n+    private final Map<String, Object> consumerConfig = ReadViaSDF.this.getConsumerConfig();\n+\n+    private final Map<String, Object> offsetConsumerConfig =\n+        ReadViaSDF.this.getOffsetConsumerConfig();\n+\n+    private final DeserializerProvider keyDeserializerProvider =\n+        ReadViaSDF.this.getKeyDeserializerProvider();\n+    private final DeserializerProvider valueDeserializerProvider =\n+        ReadViaSDF.this.getValueDeserializerProvider();\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn = ReadViaSDF.this.getConsumerFactoryFn();\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn =\n+        ReadViaSDF.this.getExtractOutputTimestampFn();\n+\n+    private final Duration kafkaPollTimeout = Duration.millis(1000);\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient KafkaIOUtils.MovingAvg avgRecordSize = null;\n+    private transient KafkaIOUtils.MovingAvg avgOffsetGap = null;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+        return offsetConsumer.position(topicPartition);\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element Row kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        consumerSpEL.evaluateAssign(offsetConsumer, ImmutableList.of(topicPartition));\n+        long startOffset;\n+        if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET)\n+            != null) {\n+          startOffset =\n+              kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_OFFSET);\n+        } else if (kafkaSourceDescription.getInt64(KafkaSourceDescriptionSchemas.START_READ_TIME)\n+            != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  topicPartition,\n+                  Instant.ofEpochMilli(kafkaSourceDescription.getInt64(\"start_read_time\")));\n+        } else {\n+          startOffset = offsetConsumer.position(topicPartition);\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public MonotonicallyIncreasing newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return new MonotonicallyIncreasing(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(@Element Row kafkaSourceDescription, @Restriction OffsetRange offsetRange)\n+        throws Exception {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      double numOfRecords = 0.0;\n+      if (offsetRange.getTo() != Long.MAX_VALUE) {\n+        numOfRecords = (new OffsetRangeTracker(offsetRange)).getProgress().getWorkRemaining();\n+      } else {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        KafkaLatestOffsetEstimator offsetEstimator =\n+            new KafkaLatestOffsetEstimator(\n+                consumerFactoryFn.apply(\n+                    KafkaIOUtils.getOffsetConsumerConfig(\n+                        \"size-\" + topicPartition.toString(),\n+                        offsetConsumerConfig,\n+                        updatedConsumerConfig)),\n+                topicPartition);\n+        numOfRecords =\n+            (new GrowableOffsetRangeTracker(offsetRange.getFrom(), offsetEstimator))\n+                .getProgress()\n+                .getWorkRemaining();\n+      }\n+\n+      // Before processing elements, we don't have a good estimated size of records and offset gap.\n+      if (avgOffsetGap != null) {\n+        numOfRecords = numOfRecords / (1 + avgOffsetGap.get());\n+      }\n+      return (avgRecordSize == null ? 1 : avgRecordSize.get()) * numOfRecords;\n+    }\n+\n+    @SplitRestriction\n+    public void splitRestriction(\n+        @Element Row kafkaSourceDescription,\n+        @Restriction OffsetRange offsetRange,\n+        OutputReceiver<OffsetRange> receiver)\n+        throws Exception {\n+      receiver.output(offsetRange);\n+    }\n+\n+    @NewTracker\n+    public RestrictionTracker<OffsetRange, Long> restrictionTracker(\n+        @Element Row kafkaSourceDescription, @Restriction OffsetRange restriction) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      if (restriction.getTo() == Long.MAX_VALUE) {\n+        TopicPartition topicPartition =\n+            new TopicPartition(\n+                kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+                kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+        KafkaLatestOffsetEstimator offsetPoller =\n+            new KafkaLatestOffsetEstimator(\n+                consumerFactoryFn.apply(\n+                    KafkaIOUtils.getOffsetConsumerConfig(\n+                        \"tracker-\" + topicPartition, offsetConsumerConfig, updatedConsumerConfig)),\n+                topicPartition);\n+        return new GrowableOffsetRangeTracker(restriction.getFrom(), offsetPoller);\n+      }\n+      return new OffsetRangeTracker(restriction);\n+    }\n+\n+    @ProcessElement\n+    public ProcessContinuation processElement(\n+        @Element Row kafkaSourceDescription,\n+        RestrictionTracker<OffsetRange, Long> tracker,\n+        WatermarkEstimator watermarkEstimator,\n+        OutputReceiver<KafkaRecord<K, V>> receiver) {\n+      TopicPartition topicPartition =\n+          new TopicPartition(\n+              kafkaSourceDescription.getString(KafkaSourceDescriptionSchemas.TOPIC),\n+              kafkaSourceDescription.getInt32(KafkaSourceDescriptionSchemas.PARTITION));\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> consumer = consumerFactoryFn.apply(updatedConsumerConfig)) {\n+        consumerSpEL.evaluateAssign(consumer, ImmutableList.of(topicPartition));\n+        long startOffset = tracker.currentRestriction().getFrom();\n+        long expectedOffset = startOffset;\n+        consumer.seek(topicPartition, startOffset);\n+        ConsumerRecords<byte[], byte[]> rawRecords = ConsumerRecords.empty();\n+\n+        try {\n+          while (true) {\n+            rawRecords = consumer.poll(kafkaPollTimeout.getMillis());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0Mjk2NjA0Nw=="}, "originalCommit": {"oid": "07ddcb46c6245435aea78878b8de2589592d623c"}, "originalPosition": 648}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2NjExMzcyOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QwMzoxMjozOVrOGnXxGA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzoxMzoyMFrOGny2Ng==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkzNzA0OA==", "bodyText": "I forgot the default progress interval. Is it 5s or 5 mins?", "url": "https://github.com/apache/beam/pull/11749#discussion_r443937048", "createdAt": "2020-06-23T03:12:39Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,861 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaSourceDescription.Schemas;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.Manual;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.WallTime;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the <a href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The {@link WatermarkEstimator} is created by {@link #getCreateWatermarkEstimatorFn()}. The\n+ * estimated watermark is computed by this {@link WatermarkEstimator} based on output timestamps\n+ * computed by {@link #getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link #withProcessingTime()} as {@code extractTimestampFn} and {@link\n+ * #withMonotonicallyIncreasingWatermarkEstimator()} as {@link WatermarkEstimator}.\n+ */\n+@Experimental(Kind.PORTABILITY)\n+@AutoValue\n+abstract class ReadViaSDF<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<Instant, WatermarkEstimatorT> getCreateWatermarkEstimatorFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  @Nullable\n+  abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+  abstract Builder<K, V, WatermarkEstimatorT> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>> {\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setOffsetConsumerConfig(\n+        Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCreateWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimatorT> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setTimestampPolicyFactory(\n+        TimestampPolicyFactory<K, V> policy);\n+\n+    abstract ReadViaSDF<K, V, WatermarkEstimatorT> build();\n+  }\n+\n+  public static <K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      ReadViaSDF<K, V, WatermarkEstimatorT> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V, WatermarkEstimatorT>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setCommitOffsetEnabled(false)\n+        .build()\n+        .withProcessingTime()\n+        .withMonotonicallyIncreasingWatermarkEstimator();\n+  }\n+\n+  // Note that if the bootstrapServers is set here but also populated with the element, the element\n+  // will override the bootstrapServers from the config.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializer(\n+      Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigUpdates(\n+      Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreatWatermarkEstimatorFn(\n+      SerializableFunction<Instant, WatermarkEstimatorT> fn) {\n+    return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withWallTimeWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new WallTime(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withMonotonicallyIncreasingWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new MonotonicallyIncreasing(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withManualWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new Manual(state);\n+        });\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigOverrides(\n+      Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  ReadViaSDFExternally forExternalBuild() {\n+    return new ReadViaSDFExternally(this);\n+  }\n+\n+  private static class ReadViaSDFExternally<\n+          K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends PTransform<PCollection<Row>, PCollection<KV<K, V>>> {\n+\n+    private final ReadViaSDF<K, V, WatermarkEstimatorT> readViaSDF;\n+\n+    ReadViaSDFExternally(ReadViaSDF read) {\n+      readViaSDF = read;\n+    }\n+\n+    @Override\n+    public PCollection<KV<K, V>> expand(PCollection<Row> input) {\n+      return input\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<Row, KafkaSourceDescription>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element Row row, OutputReceiver<KafkaSourceDescription> outputReceiver) {\n+                      TopicPartition topicPartition =\n+                          new TopicPartition(\n+                              row.getString(Schemas.TOPIC), row.getInt32(Schemas.PARTITION));\n+                      Instant startReadTime =\n+                          row.getInt64(Schemas.START_READ_TIME) != null\n+                              ? Instant.ofEpochMilli(row.getInt64(Schemas.START_READ_TIME))\n+                              : null;\n+                      outputReceiver.output(\n+                          KafkaSourceDescription.of(\n+                              topicPartition,\n+                              row.getInt64(Schemas.START_READ_OFFSET),\n+                              startReadTime,\n+                              new ArrayList<>(row.getArray(Schemas.BOOTSTRAP_SERVERS))));\n+                    }\n+                  }))\n+          .apply(readViaSDF)\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<KafkaRecord<K, V>, KV<K, V>>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element KafkaRecord element, OutputReceiver<KV<K, V>> outputReceiver) {\n+                      outputReceiver.output(element.getKV());\n+                    }\n+                  }))\n+          .setCoder(KvCoder.<K, V>of(readViaSDF.getKeyCoder(), readViaSDF.getValueCoder()));\n+    }\n+  }\n+\n+  ReadViaSDF<K, V, WatermarkEstimatorT> withTimestampPolicyFactory(\n+      TimestampPolicyFactory<K, V> timestampPolicyFactory) {\n+    return toBuilder().setTimestampPolicyFactory(timestampPolicyFactory).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<KafkaSourceDescription> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input\n+            .apply(ParDo.of(new ReadFromKafkaDoFn<K, V, WatermarkEstimatorT>(this)))\n+            .setCoder(outputCoder);\n+    // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      throw new IllegalStateException(\"Offset committed is not supported yet\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link KafkaSourceDescription} and outputs {@link\n+   * KafkaRecord}. By default, a {@link MonotonicallyIncreasing} watermark estimator is used to\n+   * track watermark.\n+   */\n+  @VisibleForTesting\n+  @UnboundedPerElement\n+  static class ReadFromKafkaDoFn<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends DoFn<KafkaSourceDescription, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn(ReadViaSDF transform) {\n+      this.consumerConfig = transform.getConsumerConfig();\n+      this.offsetConsumerConfig = transform.getOffsetConsumerConfig();\n+      this.keyDeserializerProvider = transform.getKeyDeserializerProvider();\n+      this.valueDeserializerProvider = transform.getValueDeserializerProvider();\n+      this.consumerFactoryFn = transform.getConsumerFactoryFn();\n+      this.extractOutputTimestampFn = transform.getExtractOutputTimestampFn();\n+      this.createWatermarkEstimatorFn = transform.getCreateWatermarkEstimatorFn();\n+      this.timestampPolicyFactory = transform.getTimestampPolicyFactory();\n+    }\n+\n+    private final Map<String, Object> offsetConsumerConfig;\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn;\n+    private final SerializableFunction<Instant, WatermarkEstimatorT> createWatermarkEstimatorFn;\n+    private final TimestampPolicyFactory<K, V> timestampPolicyFactory;\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgRecordSize;\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgOffsetGap;\n+\n+    private static final Duration KAFKA_POLL_TIMEOUT = Duration.millis(1000);\n+\n+    @VisibleForTesting final DeserializerProvider keyDeserializerProvider;\n+    @VisibleForTesting final DeserializerProvider valueDeserializerProvider;\n+    @VisibleForTesting final Map<String, Object> consumerConfig;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private static class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+      private final Supplier<Long> memorizedBacklog;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+        memorizedBacklog =\n+            Suppliers.memoizeWithExpiration(\n+                () -> {\n+                  consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+                  return offsetConsumer.position(topicPartition);\n+                },\n+                5,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "9a0848d854452f159599673cf74039cc64b8f0a9"}, "originalPosition": 641}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM1NDMxMQ==", "bodyText": "I believe the default is 30 seconds for Dataflow but I don't thing we should tune this to be Dataflow specific and more about making this relative to the average cost it takes to figure this out. 5 seconds doesn't sound too bad.", "url": "https://github.com/apache/beam/pull/11749#discussion_r444354311", "createdAt": "2020-06-23T16:29:20Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,861 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaSourceDescription.Schemas;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.Manual;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.WallTime;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the <a href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The {@link WatermarkEstimator} is created by {@link #getCreateWatermarkEstimatorFn()}. The\n+ * estimated watermark is computed by this {@link WatermarkEstimator} based on output timestamps\n+ * computed by {@link #getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link #withProcessingTime()} as {@code extractTimestampFn} and {@link\n+ * #withMonotonicallyIncreasingWatermarkEstimator()} as {@link WatermarkEstimator}.\n+ */\n+@Experimental(Kind.PORTABILITY)\n+@AutoValue\n+abstract class ReadViaSDF<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<Instant, WatermarkEstimatorT> getCreateWatermarkEstimatorFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  @Nullable\n+  abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+  abstract Builder<K, V, WatermarkEstimatorT> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>> {\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setOffsetConsumerConfig(\n+        Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCreateWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimatorT> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setTimestampPolicyFactory(\n+        TimestampPolicyFactory<K, V> policy);\n+\n+    abstract ReadViaSDF<K, V, WatermarkEstimatorT> build();\n+  }\n+\n+  public static <K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      ReadViaSDF<K, V, WatermarkEstimatorT> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V, WatermarkEstimatorT>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setCommitOffsetEnabled(false)\n+        .build()\n+        .withProcessingTime()\n+        .withMonotonicallyIncreasingWatermarkEstimator();\n+  }\n+\n+  // Note that if the bootstrapServers is set here but also populated with the element, the element\n+  // will override the bootstrapServers from the config.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializer(\n+      Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigUpdates(\n+      Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreatWatermarkEstimatorFn(\n+      SerializableFunction<Instant, WatermarkEstimatorT> fn) {\n+    return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withWallTimeWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new WallTime(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withMonotonicallyIncreasingWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new MonotonicallyIncreasing(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withManualWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new Manual(state);\n+        });\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigOverrides(\n+      Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  ReadViaSDFExternally forExternalBuild() {\n+    return new ReadViaSDFExternally(this);\n+  }\n+\n+  private static class ReadViaSDFExternally<\n+          K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends PTransform<PCollection<Row>, PCollection<KV<K, V>>> {\n+\n+    private final ReadViaSDF<K, V, WatermarkEstimatorT> readViaSDF;\n+\n+    ReadViaSDFExternally(ReadViaSDF read) {\n+      readViaSDF = read;\n+    }\n+\n+    @Override\n+    public PCollection<KV<K, V>> expand(PCollection<Row> input) {\n+      return input\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<Row, KafkaSourceDescription>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element Row row, OutputReceiver<KafkaSourceDescription> outputReceiver) {\n+                      TopicPartition topicPartition =\n+                          new TopicPartition(\n+                              row.getString(Schemas.TOPIC), row.getInt32(Schemas.PARTITION));\n+                      Instant startReadTime =\n+                          row.getInt64(Schemas.START_READ_TIME) != null\n+                              ? Instant.ofEpochMilli(row.getInt64(Schemas.START_READ_TIME))\n+                              : null;\n+                      outputReceiver.output(\n+                          KafkaSourceDescription.of(\n+                              topicPartition,\n+                              row.getInt64(Schemas.START_READ_OFFSET),\n+                              startReadTime,\n+                              new ArrayList<>(row.getArray(Schemas.BOOTSTRAP_SERVERS))));\n+                    }\n+                  }))\n+          .apply(readViaSDF)\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<KafkaRecord<K, V>, KV<K, V>>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element KafkaRecord element, OutputReceiver<KV<K, V>> outputReceiver) {\n+                      outputReceiver.output(element.getKV());\n+                    }\n+                  }))\n+          .setCoder(KvCoder.<K, V>of(readViaSDF.getKeyCoder(), readViaSDF.getValueCoder()));\n+    }\n+  }\n+\n+  ReadViaSDF<K, V, WatermarkEstimatorT> withTimestampPolicyFactory(\n+      TimestampPolicyFactory<K, V> timestampPolicyFactory) {\n+    return toBuilder().setTimestampPolicyFactory(timestampPolicyFactory).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<KafkaSourceDescription> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input\n+            .apply(ParDo.of(new ReadFromKafkaDoFn<K, V, WatermarkEstimatorT>(this)))\n+            .setCoder(outputCoder);\n+    // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      throw new IllegalStateException(\"Offset committed is not supported yet\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link KafkaSourceDescription} and outputs {@link\n+   * KafkaRecord}. By default, a {@link MonotonicallyIncreasing} watermark estimator is used to\n+   * track watermark.\n+   */\n+  @VisibleForTesting\n+  @UnboundedPerElement\n+  static class ReadFromKafkaDoFn<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends DoFn<KafkaSourceDescription, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn(ReadViaSDF transform) {\n+      this.consumerConfig = transform.getConsumerConfig();\n+      this.offsetConsumerConfig = transform.getOffsetConsumerConfig();\n+      this.keyDeserializerProvider = transform.getKeyDeserializerProvider();\n+      this.valueDeserializerProvider = transform.getValueDeserializerProvider();\n+      this.consumerFactoryFn = transform.getConsumerFactoryFn();\n+      this.extractOutputTimestampFn = transform.getExtractOutputTimestampFn();\n+      this.createWatermarkEstimatorFn = transform.getCreateWatermarkEstimatorFn();\n+      this.timestampPolicyFactory = transform.getTimestampPolicyFactory();\n+    }\n+\n+    private final Map<String, Object> offsetConsumerConfig;\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn;\n+    private final SerializableFunction<Instant, WatermarkEstimatorT> createWatermarkEstimatorFn;\n+    private final TimestampPolicyFactory<K, V> timestampPolicyFactory;\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgRecordSize;\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgOffsetGap;\n+\n+    private static final Duration KAFKA_POLL_TIMEOUT = Duration.millis(1000);\n+\n+    @VisibleForTesting final DeserializerProvider keyDeserializerProvider;\n+    @VisibleForTesting final DeserializerProvider valueDeserializerProvider;\n+    @VisibleForTesting final Map<String, Object> consumerConfig;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private static class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+      private final Supplier<Long> memorizedBacklog;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+        memorizedBacklog =\n+            Suppliers.memoizeWithExpiration(\n+                () -> {\n+                  consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+                  return offsetConsumer.position(topicPartition);\n+                },\n+                5,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkzNzA0OA=="}, "originalCommit": {"oid": "9a0848d854452f159599673cf74039cc64b8f0a9"}, "originalPosition": 641}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM4MDcyNg==", "bodyText": "I would be helpful to add a comment why this number was chosen.", "url": "https://github.com/apache/beam/pull/11749#discussion_r444380726", "createdAt": "2020-06-23T17:13:20Z", "author": {"login": "aromanenko-dev"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,861 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaSourceDescription.Schemas;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.Manual;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.WallTime;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the <a href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The {@link WatermarkEstimator} is created by {@link #getCreateWatermarkEstimatorFn()}. The\n+ * estimated watermark is computed by this {@link WatermarkEstimator} based on output timestamps\n+ * computed by {@link #getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link #withProcessingTime()} as {@code extractTimestampFn} and {@link\n+ * #withMonotonicallyIncreasingWatermarkEstimator()} as {@link WatermarkEstimator}.\n+ */\n+@Experimental(Kind.PORTABILITY)\n+@AutoValue\n+abstract class ReadViaSDF<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<Instant, WatermarkEstimatorT> getCreateWatermarkEstimatorFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  @Nullable\n+  abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+  abstract Builder<K, V, WatermarkEstimatorT> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>> {\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setOffsetConsumerConfig(\n+        Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCreateWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimatorT> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setTimestampPolicyFactory(\n+        TimestampPolicyFactory<K, V> policy);\n+\n+    abstract ReadViaSDF<K, V, WatermarkEstimatorT> build();\n+  }\n+\n+  public static <K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      ReadViaSDF<K, V, WatermarkEstimatorT> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V, WatermarkEstimatorT>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setCommitOffsetEnabled(false)\n+        .build()\n+        .withProcessingTime()\n+        .withMonotonicallyIncreasingWatermarkEstimator();\n+  }\n+\n+  // Note that if the bootstrapServers is set here but also populated with the element, the element\n+  // will override the bootstrapServers from the config.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializer(\n+      Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigUpdates(\n+      Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreatWatermarkEstimatorFn(\n+      SerializableFunction<Instant, WatermarkEstimatorT> fn) {\n+    return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withWallTimeWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new WallTime(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withMonotonicallyIncreasingWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new MonotonicallyIncreasing(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withManualWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new Manual(state);\n+        });\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigOverrides(\n+      Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  ReadViaSDFExternally forExternalBuild() {\n+    return new ReadViaSDFExternally(this);\n+  }\n+\n+  private static class ReadViaSDFExternally<\n+          K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends PTransform<PCollection<Row>, PCollection<KV<K, V>>> {\n+\n+    private final ReadViaSDF<K, V, WatermarkEstimatorT> readViaSDF;\n+\n+    ReadViaSDFExternally(ReadViaSDF read) {\n+      readViaSDF = read;\n+    }\n+\n+    @Override\n+    public PCollection<KV<K, V>> expand(PCollection<Row> input) {\n+      return input\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<Row, KafkaSourceDescription>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element Row row, OutputReceiver<KafkaSourceDescription> outputReceiver) {\n+                      TopicPartition topicPartition =\n+                          new TopicPartition(\n+                              row.getString(Schemas.TOPIC), row.getInt32(Schemas.PARTITION));\n+                      Instant startReadTime =\n+                          row.getInt64(Schemas.START_READ_TIME) != null\n+                              ? Instant.ofEpochMilli(row.getInt64(Schemas.START_READ_TIME))\n+                              : null;\n+                      outputReceiver.output(\n+                          KafkaSourceDescription.of(\n+                              topicPartition,\n+                              row.getInt64(Schemas.START_READ_OFFSET),\n+                              startReadTime,\n+                              new ArrayList<>(row.getArray(Schemas.BOOTSTRAP_SERVERS))));\n+                    }\n+                  }))\n+          .apply(readViaSDF)\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<KafkaRecord<K, V>, KV<K, V>>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element KafkaRecord element, OutputReceiver<KV<K, V>> outputReceiver) {\n+                      outputReceiver.output(element.getKV());\n+                    }\n+                  }))\n+          .setCoder(KvCoder.<K, V>of(readViaSDF.getKeyCoder(), readViaSDF.getValueCoder()));\n+    }\n+  }\n+\n+  ReadViaSDF<K, V, WatermarkEstimatorT> withTimestampPolicyFactory(\n+      TimestampPolicyFactory<K, V> timestampPolicyFactory) {\n+    return toBuilder().setTimestampPolicyFactory(timestampPolicyFactory).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<KafkaSourceDescription> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input\n+            .apply(ParDo.of(new ReadFromKafkaDoFn<K, V, WatermarkEstimatorT>(this)))\n+            .setCoder(outputCoder);\n+    // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      throw new IllegalStateException(\"Offset committed is not supported yet\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link KafkaSourceDescription} and outputs {@link\n+   * KafkaRecord}. By default, a {@link MonotonicallyIncreasing} watermark estimator is used to\n+   * track watermark.\n+   */\n+  @VisibleForTesting\n+  @UnboundedPerElement\n+  static class ReadFromKafkaDoFn<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends DoFn<KafkaSourceDescription, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn(ReadViaSDF transform) {\n+      this.consumerConfig = transform.getConsumerConfig();\n+      this.offsetConsumerConfig = transform.getOffsetConsumerConfig();\n+      this.keyDeserializerProvider = transform.getKeyDeserializerProvider();\n+      this.valueDeserializerProvider = transform.getValueDeserializerProvider();\n+      this.consumerFactoryFn = transform.getConsumerFactoryFn();\n+      this.extractOutputTimestampFn = transform.getExtractOutputTimestampFn();\n+      this.createWatermarkEstimatorFn = transform.getCreateWatermarkEstimatorFn();\n+      this.timestampPolicyFactory = transform.getTimestampPolicyFactory();\n+    }\n+\n+    private final Map<String, Object> offsetConsumerConfig;\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn;\n+    private final SerializableFunction<Instant, WatermarkEstimatorT> createWatermarkEstimatorFn;\n+    private final TimestampPolicyFactory<K, V> timestampPolicyFactory;\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgRecordSize;\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgOffsetGap;\n+\n+    private static final Duration KAFKA_POLL_TIMEOUT = Duration.millis(1000);\n+\n+    @VisibleForTesting final DeserializerProvider keyDeserializerProvider;\n+    @VisibleForTesting final DeserializerProvider valueDeserializerProvider;\n+    @VisibleForTesting final Map<String, Object> consumerConfig;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private static class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+      private final Supplier<Long> memorizedBacklog;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+        memorizedBacklog =\n+            Suppliers.memoizeWithExpiration(\n+                () -> {\n+                  consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+                  return offsetConsumer.position(topicPartition);\n+                },\n+                5,", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0MzkzNzA0OA=="}, "originalCommit": {"oid": "9a0848d854452f159599673cf74039cc64b8f0a9"}, "originalPosition": 641}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2ODcyOTEwOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNjozMjo1M1rOGnxXYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxODo0NzoxOFrOGn2Inw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM1NjQ1MA==", "bodyText": "I don't think it is important to the user to know which WatermarkEstimatorT that they are getting as the type variable doesn't provide any value to the user.", "url": "https://github.com/apache/beam/pull/11749#discussion_r444356450", "createdAt": "2020-06-23T16:32:53Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -295,21 +301,32 @@\n   /**\n    * Creates an uninitialized {@link Read} {@link PTransform}. Before use, basic Kafka configuration\n    * should set with {@link Read#withBootstrapServers(String)} and {@link Read#withTopics(List)}.\n-   * Other optional settings include key and value {@link Deserializer}s, custom timestamp and\n+   * Other optional settings include key and value {@link Deserializer}s, custom timestamp,\n    * watermark functions.\n    */\n   public static <K, V> Read<K, V> read() {\n     return new AutoValue_KafkaIO_Read.Builder<K, V>()\n         .setTopics(new ArrayList<>())\n         .setTopicPartitions(new ArrayList<>())\n-        .setConsumerFactoryFn(Read.KAFKA_CONSUMER_FACTORY_FN)\n-        .setConsumerConfig(Read.DEFAULT_CONSUMER_PROPERTIES)\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n         .setMaxNumRecords(Long.MAX_VALUE)\n         .setCommitOffsetsInFinalizeEnabled(false)\n         .setTimestampPolicyFactory(TimestampPolicyFactory.withProcessingTime())\n         .build();\n   }\n \n+  /**\n+   * Creates an uninitialized {@link ReadViaSDF} {@link PTransform}. Different from {@link Read},\n+   * setting up {@code topics} and {@code bootstrapServers} is not required during construction\n+   * time. But the {@code bootstrapServers} still can be configured {@link\n+   * ReadViaSDF#withBootstrapServers(String)}. Please refer to {@link ReadViaSDF} for more details.\n+   */\n+  public static <K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      ReadViaSDF<K, V, WatermarkEstimatorT> readAll() {\n+    return ReadViaSDF.<K, V, WatermarkEstimatorT>read();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 66}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDQzNDU5MQ==", "bodyText": "The WatermarkEstimatorT is used when defining createWatermarkEstimatorFn and when @NewWatermarkEstimator is called. I understand that we can always use WatermarkEstimator as the type, I thought it would be better to make the type explicitly,", "url": "https://github.com/apache/beam/pull/11749#discussion_r444434591", "createdAt": "2020-06-23T18:47:18Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -295,21 +301,32 @@\n   /**\n    * Creates an uninitialized {@link Read} {@link PTransform}. Before use, basic Kafka configuration\n    * should set with {@link Read#withBootstrapServers(String)} and {@link Read#withTopics(List)}.\n-   * Other optional settings include key and value {@link Deserializer}s, custom timestamp and\n+   * Other optional settings include key and value {@link Deserializer}s, custom timestamp,\n    * watermark functions.\n    */\n   public static <K, V> Read<K, V> read() {\n     return new AutoValue_KafkaIO_Read.Builder<K, V>()\n         .setTopics(new ArrayList<>())\n         .setTopicPartitions(new ArrayList<>())\n-        .setConsumerFactoryFn(Read.KAFKA_CONSUMER_FACTORY_FN)\n-        .setConsumerConfig(Read.DEFAULT_CONSUMER_PROPERTIES)\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n         .setMaxNumRecords(Long.MAX_VALUE)\n         .setCommitOffsetsInFinalizeEnabled(false)\n         .setTimestampPolicyFactory(TimestampPolicyFactory.withProcessingTime())\n         .build();\n   }\n \n+  /**\n+   * Creates an uninitialized {@link ReadViaSDF} {@link PTransform}. Different from {@link Read},\n+   * setting up {@code topics} and {@code bootstrapServers} is not required during construction\n+   * time. But the {@code bootstrapServers} still can be configured {@link\n+   * ReadViaSDF#withBootstrapServers(String)}. Please refer to {@link ReadViaSDF} for more details.\n+   */\n+  public static <K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      ReadViaSDF<K, V, WatermarkEstimatorT> readAll() {\n+    return ReadViaSDF.<K, V, WatermarkEstimatorT>read();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM1NjQ1MA=="}, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2ODc0NTkxOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNjozNzo0N1rOGnxiYQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNjozNzo0N1rOGnxiYQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM1OTI2NQ==", "bodyText": "nit: ignoredConsumerPropertiesKeys -> disallowedConsumerPropertiesKeys", "url": "https://github.com/apache/beam/pull/11749#discussion_r444359265", "createdAt": "2020-06-23T16:37:47Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1002,7 +1053,8 @@ public void populateDisplayData(DisplayData.Builder builder) {\n             DisplayData.item(\"topicPartitions\", Joiner.on(\",\").join(topicPartitions))\n                 .withLabel(\"Topic Partition/s\"));\n       }\n-      Set<String> ignoredConsumerPropertiesKeys = IGNORED_CONSUMER_PROPERTIES.keySet();\n+      Set<String> ignoredConsumerPropertiesKeys =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 290}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2ODc1MTg5OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaSourceDescription.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNjozOToyM1rOGnxmRw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNjozOToyM1rOGnxmRw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM2MDI2Mw==", "bodyText": "nit: Schemas -> Schema", "url": "https://github.com/apache/beam/pull/11749#discussion_r444360263", "createdAt": "2020-06-23T16:39:23Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaSourceDescription.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.util.List;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.kafka.common.TopicPartition;\n+import org.joda.time.Instant;\n+\n+/**\n+ * An AutoValue object which represents a Kafka source description. Note that this object should be\n+ * encoded/decoded with {@link Schemas#getSchema()} as a {@link Row} when crossing the wire.\n+ */\n+@AutoValue\n+public abstract class KafkaSourceDescription implements Serializable {\n+  abstract TopicPartition getTopicPartition();\n+\n+  @Nullable\n+  abstract Long getStartReadOffset();\n+\n+  @Nullable\n+  abstract Instant getStartReadTime();\n+\n+  @Nullable\n+  abstract List<String> getBootStrapServers();\n+\n+  public static KafkaSourceDescription of(\n+      TopicPartition topicPartition,\n+      Long startReadOffset,\n+      Instant startReadTime,\n+      List<String> bootstrapServers) {\n+    return new AutoValue_KafkaSourceDescription(\n+        topicPartition, startReadOffset, startReadTime, bootstrapServers);\n+  }\n+\n+  static class Schemas {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 56}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2ODc1NjI1OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaSourceDescription.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNjo0MDozNFrOGnxpHA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNjo0MDozNFrOGnxpHA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM2MDk4OA==", "bodyText": "Shouldn't we instead create a SchemaProviderRegistrar and the appropriate SchemaProvider instead of asking users to encode/decode with Schemas#getSchema?", "url": "https://github.com/apache/beam/pull/11749#discussion_r444360988", "createdAt": "2020-06-23T16:40:34Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaSourceDescription.java", "diffHunk": "@@ -0,0 +1,73 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.util.List;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.Schema.FieldType;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.kafka.common.TopicPartition;\n+import org.joda.time.Instant;\n+\n+/**\n+ * An AutoValue object which represents a Kafka source description. Note that this object should be\n+ * encoded/decoded with {@link Schemas#getSchema()} as a {@link Row} when crossing the wire.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 32}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2ODc3OTMzOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNjo0NjoyMlrOGnx3yg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNjo0NjoyMlrOGnx3yg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM2NDc0Ng==", "bodyText": "SchemaCoder?", "url": "https://github.com/apache/beam/pull/11749#discussion_r444364746", "createdAt": "2020-06-23T16:46:22Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -906,19 +926,89 @@ public void setValueDeserializer(String valueDeserializer) {\n       Coder<K> keyCoder = getKeyCoder(coderRegistry);\n       Coder<V> valueCoder = getValueCoder(coderRegistry);\n \n-      // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n-      Unbounded<KafkaRecord<K, V>> unbounded =\n-          org.apache.beam.sdk.io.Read.from(\n-              toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+      // The Read will be expanded into SDF transform when \"beam_fn_api\" is enabled and\n+      // \"beam_fn_api_use_deprecated_read\" is not enabled.\n+      if (!ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\")\n+          || ExperimentalOptions.hasExperiment(\n+              input.getPipeline().getOptions(), \"beam_fn_api_use_deprecated_read\")) {\n+        // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n+        Unbounded<KafkaRecord<K, V>> unbounded =\n+            org.apache.beam.sdk.io.Read.from(\n+                toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+\n+        PTransform<PBegin, PCollection<KafkaRecord<K, V>>> transform = unbounded;\n+\n+        if (getMaxNumRecords() < Long.MAX_VALUE || getMaxReadTime() != null) {\n+          transform =\n+              unbounded.withMaxReadTime(getMaxReadTime()).withMaxNumRecords(getMaxNumRecords());\n+        }\n \n-      PTransform<PBegin, PCollection<KafkaRecord<K, V>>> transform = unbounded;\n+        return input.getPipeline().apply(transform);\n+      } else {\n+        ReadViaSDF<K, V, Manual> readTransform =\n+            ReadViaSDF.<K, V, Manual>read()\n+                .withConsumerConfigOverrides(getConsumerConfig())\n+                .withOffsetConsumerConfigOverrides(getOffsetConsumerConfig())\n+                .withConsumerFactoryFn(getConsumerFactoryFn())\n+                .withKeyDeserializerProvider(getKeyDeserializerProvider())\n+                .withValueDeserializerProvider(getValueDeserializerProvider())\n+                .withManualWatermarkEstimator()\n+                .withTimestampPolicyFactory(getTimestampPolicyFactory());\n+        if (isCommitOffsetsInFinalizeEnabled()) {\n+          readTransform = readTransform.commitOffsets();\n+        }\n \n-      if (getMaxNumRecords() < Long.MAX_VALUE || getMaxReadTime() != null) {\n-        transform =\n-            unbounded.withMaxReadTime(getMaxReadTime()).withMaxNumRecords(getMaxNumRecords());\n+        return input\n+            .getPipeline()\n+            .apply(Impulse.create())\n+            .apply(ParDo.of(new GenerateKafkaSourceDescription(this)))\n+            .setCoder(SerializableCoder.of(KafkaSourceDescription.class))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 188}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2ODc4NDY4OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNjo0ODowM1rOGnx7mA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QyMzozMzowNFrOGn-COg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM2NTcyMA==", "bodyText": "ReadFromKafkaViaSDF?", "url": "https://github.com/apache/beam/pull/11749#discussion_r444365720", "createdAt": "2020-06-23T16:48:03Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,861 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaSourceDescription.Schemas;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.Manual;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.WallTime;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the <a href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The {@link WatermarkEstimator} is created by {@link #getCreateWatermarkEstimatorFn()}. The\n+ * estimated watermark is computed by this {@link WatermarkEstimator} based on output timestamps\n+ * computed by {@link #getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link #withProcessingTime()} as {@code extractTimestampFn} and {@link\n+ * #withMonotonicallyIncreasingWatermarkEstimator()} as {@link WatermarkEstimator}.\n+ */\n+@Experimental(Kind.PORTABILITY)\n+@AutoValue\n+abstract class ReadViaSDF<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<Instant, WatermarkEstimatorT> getCreateWatermarkEstimatorFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  @Nullable\n+  abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+  abstract Builder<K, V, WatermarkEstimatorT> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>> {\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setOffsetConsumerConfig(\n+        Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCreateWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimatorT> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setTimestampPolicyFactory(\n+        TimestampPolicyFactory<K, V> policy);\n+\n+    abstract ReadViaSDF<K, V, WatermarkEstimatorT> build();\n+  }\n+\n+  public static <K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      ReadViaSDF<K, V, WatermarkEstimatorT> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V, WatermarkEstimatorT>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setCommitOffsetEnabled(false)\n+        .build()\n+        .withProcessingTime()\n+        .withMonotonicallyIncreasingWatermarkEstimator();\n+  }\n+\n+  // Note that if the bootstrapServers is set here but also populated with the element, the element\n+  // will override the bootstrapServers from the config.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializer(\n+      Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigUpdates(\n+      Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreatWatermarkEstimatorFn(\n+      SerializableFunction<Instant, WatermarkEstimatorT> fn) {\n+    return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withWallTimeWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new WallTime(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withMonotonicallyIncreasingWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new MonotonicallyIncreasing(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withManualWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new Manual(state);\n+        });\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigOverrides(\n+      Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  ReadViaSDFExternally forExternalBuild() {\n+    return new ReadViaSDFExternally(this);\n+  }\n+\n+  private static class ReadViaSDFExternally<\n+          K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends PTransform<PCollection<Row>, PCollection<KV<K, V>>> {\n+\n+    private final ReadViaSDF<K, V, WatermarkEstimatorT> readViaSDF;\n+\n+    ReadViaSDFExternally(ReadViaSDF read) {\n+      readViaSDF = read;\n+    }\n+\n+    @Override\n+    public PCollection<KV<K, V>> expand(PCollection<Row> input) {\n+      return input\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<Row, KafkaSourceDescription>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element Row row, OutputReceiver<KafkaSourceDescription> outputReceiver) {\n+                      TopicPartition topicPartition =\n+                          new TopicPartition(\n+                              row.getString(Schemas.TOPIC), row.getInt32(Schemas.PARTITION));\n+                      Instant startReadTime =\n+                          row.getInt64(Schemas.START_READ_TIME) != null\n+                              ? Instant.ofEpochMilli(row.getInt64(Schemas.START_READ_TIME))\n+                              : null;\n+                      outputReceiver.output(\n+                          KafkaSourceDescription.of(\n+                              topicPartition,\n+                              row.getInt64(Schemas.START_READ_OFFSET),\n+                              startReadTime,\n+                              new ArrayList<>(row.getArray(Schemas.BOOTSTRAP_SERVERS))));\n+                    }\n+                  }))\n+          .apply(readViaSDF)\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<KafkaRecord<K, V>, KV<K, V>>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element KafkaRecord element, OutputReceiver<KV<K, V>> outputReceiver) {\n+                      outputReceiver.output(element.getKV());\n+                    }\n+                  }))\n+          .setCoder(KvCoder.<K, V>of(readViaSDF.getKeyCoder(), readViaSDF.getValueCoder()));\n+    }\n+  }\n+\n+  ReadViaSDF<K, V, WatermarkEstimatorT> withTimestampPolicyFactory(\n+      TimestampPolicyFactory<K, V> timestampPolicyFactory) {\n+    return toBuilder().setTimestampPolicyFactory(timestampPolicyFactory).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<KafkaSourceDescription> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 480}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDU2NDAyNg==", "bodyText": "It should be ReadAll.", "url": "https://github.com/apache/beam/pull/11749#discussion_r444564026", "createdAt": "2020-06-23T23:33:04Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,861 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaSourceDescription.Schemas;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.Manual;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.WallTime;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the <a href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The {@link WatermarkEstimator} is created by {@link #getCreateWatermarkEstimatorFn()}. The\n+ * estimated watermark is computed by this {@link WatermarkEstimator} based on output timestamps\n+ * computed by {@link #getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link #withProcessingTime()} as {@code extractTimestampFn} and {@link\n+ * #withMonotonicallyIncreasingWatermarkEstimator()} as {@link WatermarkEstimator}.\n+ */\n+@Experimental(Kind.PORTABILITY)\n+@AutoValue\n+abstract class ReadViaSDF<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<Instant, WatermarkEstimatorT> getCreateWatermarkEstimatorFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  @Nullable\n+  abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+  abstract Builder<K, V, WatermarkEstimatorT> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>> {\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setOffsetConsumerConfig(\n+        Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCreateWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimatorT> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setTimestampPolicyFactory(\n+        TimestampPolicyFactory<K, V> policy);\n+\n+    abstract ReadViaSDF<K, V, WatermarkEstimatorT> build();\n+  }\n+\n+  public static <K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      ReadViaSDF<K, V, WatermarkEstimatorT> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V, WatermarkEstimatorT>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setCommitOffsetEnabled(false)\n+        .build()\n+        .withProcessingTime()\n+        .withMonotonicallyIncreasingWatermarkEstimator();\n+  }\n+\n+  // Note that if the bootstrapServers is set here but also populated with the element, the element\n+  // will override the bootstrapServers from the config.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializer(\n+      Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigUpdates(\n+      Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreatWatermarkEstimatorFn(\n+      SerializableFunction<Instant, WatermarkEstimatorT> fn) {\n+    return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withWallTimeWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new WallTime(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withMonotonicallyIncreasingWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new MonotonicallyIncreasing(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withManualWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new Manual(state);\n+        });\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigOverrides(\n+      Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  ReadViaSDFExternally forExternalBuild() {\n+    return new ReadViaSDFExternally(this);\n+  }\n+\n+  private static class ReadViaSDFExternally<\n+          K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends PTransform<PCollection<Row>, PCollection<KV<K, V>>> {\n+\n+    private final ReadViaSDF<K, V, WatermarkEstimatorT> readViaSDF;\n+\n+    ReadViaSDFExternally(ReadViaSDF read) {\n+      readViaSDF = read;\n+    }\n+\n+    @Override\n+    public PCollection<KV<K, V>> expand(PCollection<Row> input) {\n+      return input\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<Row, KafkaSourceDescription>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element Row row, OutputReceiver<KafkaSourceDescription> outputReceiver) {\n+                      TopicPartition topicPartition =\n+                          new TopicPartition(\n+                              row.getString(Schemas.TOPIC), row.getInt32(Schemas.PARTITION));\n+                      Instant startReadTime =\n+                          row.getInt64(Schemas.START_READ_TIME) != null\n+                              ? Instant.ofEpochMilli(row.getInt64(Schemas.START_READ_TIME))\n+                              : null;\n+                      outputReceiver.output(\n+                          KafkaSourceDescription.of(\n+                              topicPartition,\n+                              row.getInt64(Schemas.START_READ_OFFSET),\n+                              startReadTime,\n+                              new ArrayList<>(row.getArray(Schemas.BOOTSTRAP_SERVERS))));\n+                    }\n+                  }))\n+          .apply(readViaSDF)\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<KafkaRecord<K, V>, KV<K, V>>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element KafkaRecord element, OutputReceiver<KV<K, V>> outputReceiver) {\n+                      outputReceiver.output(element.getKV());\n+                    }\n+                  }))\n+          .setCoder(KvCoder.<K, V>of(readViaSDF.getKeyCoder(), readViaSDF.getValueCoder()));\n+    }\n+  }\n+\n+  ReadViaSDF<K, V, WatermarkEstimatorT> withTimestampPolicyFactory(\n+      TimestampPolicyFactory<K, V> timestampPolicyFactory) {\n+    return toBuilder().setTimestampPolicyFactory(timestampPolicyFactory).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<KafkaSourceDescription> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM2NTcyMA=="}, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 480}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2ODc5MzQ1OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNjo1MDoyNVrOGnyBnA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wM1QyMToxNzoxOVrOG7JBhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM2NzI2MA==", "bodyText": "read transforms expand method already sets the output coder appropriately so this is not necessary", "url": "https://github.com/apache/beam/pull/11749#discussion_r444367260", "createdAt": "2020-06-23T16:50:25Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -906,19 +926,89 @@ public void setValueDeserializer(String valueDeserializer) {\n       Coder<K> keyCoder = getKeyCoder(coderRegistry);\n       Coder<V> valueCoder = getValueCoder(coderRegistry);\n \n-      // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n-      Unbounded<KafkaRecord<K, V>> unbounded =\n-          org.apache.beam.sdk.io.Read.from(\n-              toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+      // The Read will be expanded into SDF transform when \"beam_fn_api\" is enabled and\n+      // \"beam_fn_api_use_deprecated_read\" is not enabled.\n+      if (!ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\")\n+          || ExperimentalOptions.hasExperiment(\n+              input.getPipeline().getOptions(), \"beam_fn_api_use_deprecated_read\")) {\n+        // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n+        Unbounded<KafkaRecord<K, V>> unbounded =\n+            org.apache.beam.sdk.io.Read.from(\n+                toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+\n+        PTransform<PBegin, PCollection<KafkaRecord<K, V>>> transform = unbounded;\n+\n+        if (getMaxNumRecords() < Long.MAX_VALUE || getMaxReadTime() != null) {\n+          transform =\n+              unbounded.withMaxReadTime(getMaxReadTime()).withMaxNumRecords(getMaxNumRecords());\n+        }\n \n-      PTransform<PBegin, PCollection<KafkaRecord<K, V>>> transform = unbounded;\n+        return input.getPipeline().apply(transform);\n+      } else {\n+        ReadViaSDF<K, V, Manual> readTransform =\n+            ReadViaSDF.<K, V, Manual>read()\n+                .withConsumerConfigOverrides(getConsumerConfig())\n+                .withOffsetConsumerConfigOverrides(getOffsetConsumerConfig())\n+                .withConsumerFactoryFn(getConsumerFactoryFn())\n+                .withKeyDeserializerProvider(getKeyDeserializerProvider())\n+                .withValueDeserializerProvider(getValueDeserializerProvider())\n+                .withManualWatermarkEstimator()\n+                .withTimestampPolicyFactory(getTimestampPolicyFactory());\n+        if (isCommitOffsetsInFinalizeEnabled()) {\n+          readTransform = readTransform.commitOffsets();\n+        }\n \n-      if (getMaxNumRecords() < Long.MAX_VALUE || getMaxReadTime() != null) {\n-        transform =\n-            unbounded.withMaxReadTime(getMaxReadTime()).withMaxNumRecords(getMaxNumRecords());\n+        return input\n+            .getPipeline()\n+            .apply(Impulse.create())\n+            .apply(ParDo.of(new GenerateKafkaSourceDescription(this)))\n+            .setCoder(SerializableCoder.of(KafkaSourceDescription.class))\n+            .apply(readTransform)\n+            .setCoder(KafkaRecordCoder.of(keyCoder, valueCoder));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 190}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NDY2NzAxNQ==", "bodyText": "The coder needs to be set when in x-lang case. It seems like there is something not correct when x-lang expand the transform.", "url": "https://github.com/apache/beam/pull/11749#discussion_r464667015", "createdAt": "2020-08-03T21:17:19Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -906,19 +926,89 @@ public void setValueDeserializer(String valueDeserializer) {\n       Coder<K> keyCoder = getKeyCoder(coderRegistry);\n       Coder<V> valueCoder = getValueCoder(coderRegistry);\n \n-      // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n-      Unbounded<KafkaRecord<K, V>> unbounded =\n-          org.apache.beam.sdk.io.Read.from(\n-              toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+      // The Read will be expanded into SDF transform when \"beam_fn_api\" is enabled and\n+      // \"beam_fn_api_use_deprecated_read\" is not enabled.\n+      if (!ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\")\n+          || ExperimentalOptions.hasExperiment(\n+              input.getPipeline().getOptions(), \"beam_fn_api_use_deprecated_read\")) {\n+        // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n+        Unbounded<KafkaRecord<K, V>> unbounded =\n+            org.apache.beam.sdk.io.Read.from(\n+                toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+\n+        PTransform<PBegin, PCollection<KafkaRecord<K, V>>> transform = unbounded;\n+\n+        if (getMaxNumRecords() < Long.MAX_VALUE || getMaxReadTime() != null) {\n+          transform =\n+              unbounded.withMaxReadTime(getMaxReadTime()).withMaxNumRecords(getMaxNumRecords());\n+        }\n \n-      PTransform<PBegin, PCollection<KafkaRecord<K, V>>> transform = unbounded;\n+        return input.getPipeline().apply(transform);\n+      } else {\n+        ReadViaSDF<K, V, Manual> readTransform =\n+            ReadViaSDF.<K, V, Manual>read()\n+                .withConsumerConfigOverrides(getConsumerConfig())\n+                .withOffsetConsumerConfigOverrides(getOffsetConsumerConfig())\n+                .withConsumerFactoryFn(getConsumerFactoryFn())\n+                .withKeyDeserializerProvider(getKeyDeserializerProvider())\n+                .withValueDeserializerProvider(getValueDeserializerProvider())\n+                .withManualWatermarkEstimator()\n+                .withTimestampPolicyFactory(getTimestampPolicyFactory());\n+        if (isCommitOffsetsInFinalizeEnabled()) {\n+          readTransform = readTransform.commitOffsets();\n+        }\n \n-      if (getMaxNumRecords() < Long.MAX_VALUE || getMaxReadTime() != null) {\n-        transform =\n-            unbounded.withMaxReadTime(getMaxReadTime()).withMaxNumRecords(getMaxNumRecords());\n+        return input\n+            .getPipeline()\n+            .apply(Impulse.create())\n+            .apply(ParDo.of(new GenerateKafkaSourceDescription(this)))\n+            .setCoder(SerializableCoder.of(KafkaSourceDescription.class))\n+            .apply(readTransform)\n+            .setCoder(KafkaRecordCoder.of(keyCoder, valueCoder));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM2NzI2MA=="}, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 190}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2ODc5OTE1OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNjo1MjoxMFrOGnyFhg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNjo1MjoxMFrOGnyFhg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM2ODI2Mg==", "bodyText": "nit: We shouldn't need the additional indentation of the else part so we can structure this as a guard statement:\nif (x) {\n  // handle special case\n  return;\n}\n// do default\nreturn;", "url": "https://github.com/apache/beam/pull/11749#discussion_r444368262", "createdAt": "2020-06-23T16:52:10Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -906,19 +926,89 @@ public void setValueDeserializer(String valueDeserializer) {\n       Coder<K> keyCoder = getKeyCoder(coderRegistry);\n       Coder<V> valueCoder = getValueCoder(coderRegistry);\n \n-      // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n-      Unbounded<KafkaRecord<K, V>> unbounded =\n-          org.apache.beam.sdk.io.Read.from(\n-              toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+      // The Read will be expanded into SDF transform when \"beam_fn_api\" is enabled and\n+      // \"beam_fn_api_use_deprecated_read\" is not enabled.\n+      if (!ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\")\n+          || ExperimentalOptions.hasExperiment(\n+              input.getPipeline().getOptions(), \"beam_fn_api_use_deprecated_read\")) {\n+        // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n+        Unbounded<KafkaRecord<K, V>> unbounded =\n+            org.apache.beam.sdk.io.Read.from(\n+                toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+\n+        PTransform<PBegin, PCollection<KafkaRecord<K, V>>> transform = unbounded;\n+\n+        if (getMaxNumRecords() < Long.MAX_VALUE || getMaxReadTime() != null) {\n+          transform =\n+              unbounded.withMaxReadTime(getMaxReadTime()).withMaxNumRecords(getMaxNumRecords());\n+        }\n \n-      PTransform<PBegin, PCollection<KafkaRecord<K, V>>> transform = unbounded;\n+        return input.getPipeline().apply(transform);\n+      } else {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 167}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2ODgxMDUzOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNjo1NToyMlrOGnyNCA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNjo1NToyMlrOGnyNCA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM3MDE4NA==", "bodyText": "Might want to have a comment on the class specifically stating that we are using a row here since the xlang representation works by using records encoded in the raw row format KafkaSourceDescriptor#Schemas#getSchema\nnit: ReadViaSDFExternally -> ReadViaSDFFromRow", "url": "https://github.com/apache/beam/pull/11749#discussion_r444370184", "createdAt": "2020-06-23T16:55:22Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,861 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaSourceDescription.Schemas;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.Manual;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.WallTime;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the <a href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The {@link WatermarkEstimator} is created by {@link #getCreateWatermarkEstimatorFn()}. The\n+ * estimated watermark is computed by this {@link WatermarkEstimator} based on output timestamps\n+ * computed by {@link #getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link #withProcessingTime()} as {@code extractTimestampFn} and {@link\n+ * #withMonotonicallyIncreasingWatermarkEstimator()} as {@link WatermarkEstimator}.\n+ */\n+@Experimental(Kind.PORTABILITY)\n+@AutoValue\n+abstract class ReadViaSDF<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<Instant, WatermarkEstimatorT> getCreateWatermarkEstimatorFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  @Nullable\n+  abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+  abstract Builder<K, V, WatermarkEstimatorT> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>> {\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setOffsetConsumerConfig(\n+        Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCreateWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimatorT> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setTimestampPolicyFactory(\n+        TimestampPolicyFactory<K, V> policy);\n+\n+    abstract ReadViaSDF<K, V, WatermarkEstimatorT> build();\n+  }\n+\n+  public static <K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      ReadViaSDF<K, V, WatermarkEstimatorT> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V, WatermarkEstimatorT>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setCommitOffsetEnabled(false)\n+        .build()\n+        .withProcessingTime()\n+        .withMonotonicallyIncreasingWatermarkEstimator();\n+  }\n+\n+  // Note that if the bootstrapServers is set here but also populated with the element, the element\n+  // will override the bootstrapServers from the config.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializer(\n+      Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigUpdates(\n+      Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreatWatermarkEstimatorFn(\n+      SerializableFunction<Instant, WatermarkEstimatorT> fn) {\n+    return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withWallTimeWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new WallTime(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withMonotonicallyIncreasingWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new MonotonicallyIncreasing(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withManualWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new Manual(state);\n+        });\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigOverrides(\n+      Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  ReadViaSDFExternally forExternalBuild() {\n+    return new ReadViaSDFExternally(this);\n+  }\n+\n+  private static class ReadViaSDFExternally<", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 423}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2ODgxNTQzOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNjo1Njo0N1rOGnyQPg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNjo1Njo0N1rOGnyQPg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM3MTAwNg==", "bodyText": "We should be using the SchemaProvider fromRow function here.", "url": "https://github.com/apache/beam/pull/11749#discussion_r444371006", "createdAt": "2020-06-23T16:56:47Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,861 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaSourceDescription.Schemas;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.Manual;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.WallTime;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the <a href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The {@link WatermarkEstimator} is created by {@link #getCreateWatermarkEstimatorFn()}. The\n+ * estimated watermark is computed by this {@link WatermarkEstimator} based on output timestamps\n+ * computed by {@link #getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link #withProcessingTime()} as {@code extractTimestampFn} and {@link\n+ * #withMonotonicallyIncreasingWatermarkEstimator()} as {@link WatermarkEstimator}.\n+ */\n+@Experimental(Kind.PORTABILITY)\n+@AutoValue\n+abstract class ReadViaSDF<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<Instant, WatermarkEstimatorT> getCreateWatermarkEstimatorFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  @Nullable\n+  abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+  abstract Builder<K, V, WatermarkEstimatorT> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>> {\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setOffsetConsumerConfig(\n+        Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCreateWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimatorT> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setTimestampPolicyFactory(\n+        TimestampPolicyFactory<K, V> policy);\n+\n+    abstract ReadViaSDF<K, V, WatermarkEstimatorT> build();\n+  }\n+\n+  public static <K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      ReadViaSDF<K, V, WatermarkEstimatorT> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V, WatermarkEstimatorT>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setCommitOffsetEnabled(false)\n+        .build()\n+        .withProcessingTime()\n+        .withMonotonicallyIncreasingWatermarkEstimator();\n+  }\n+\n+  // Note that if the bootstrapServers is set here but also populated with the element, the element\n+  // will override the bootstrapServers from the config.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializer(\n+      Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigUpdates(\n+      Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreatWatermarkEstimatorFn(\n+      SerializableFunction<Instant, WatermarkEstimatorT> fn) {\n+    return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withWallTimeWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new WallTime(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withMonotonicallyIncreasingWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new MonotonicallyIncreasing(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withManualWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new Manual(state);\n+        });\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigOverrides(\n+      Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  ReadViaSDFExternally forExternalBuild() {\n+    return new ReadViaSDFExternally(this);\n+  }\n+\n+  private static class ReadViaSDFExternally<\n+          K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends PTransform<PCollection<Row>, PCollection<KV<K, V>>> {\n+\n+    private final ReadViaSDF<K, V, WatermarkEstimatorT> readViaSDF;\n+\n+    ReadViaSDFExternally(ReadViaSDF read) {\n+      readViaSDF = read;\n+    }\n+\n+    @Override\n+    public PCollection<KV<K, V>> expand(PCollection<Row> input) {\n+      return input\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<Row, KafkaSourceDescription>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element Row row, OutputReceiver<KafkaSourceDescription> outputReceiver) {\n+                      TopicPartition topicPartition =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 442}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2ODgyNTM0OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNjo1OTozM1rOGnyWwA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNjo1OTozM1rOGnyWwA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM3MjY3Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n          \n          \n            \n                // Valid between bundle start and bundle finish.", "url": "https://github.com/apache/beam/pull/11749#discussion_r444372672", "createdAt": "2020-06-23T16:59:33Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,861 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaSourceDescription.Schemas;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.Manual;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.WallTime;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the <a href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The {@link WatermarkEstimator} is created by {@link #getCreateWatermarkEstimatorFn()}. The\n+ * estimated watermark is computed by this {@link WatermarkEstimator} based on output timestamps\n+ * computed by {@link #getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link #withProcessingTime()} as {@code extractTimestampFn} and {@link\n+ * #withMonotonicallyIncreasingWatermarkEstimator()} as {@link WatermarkEstimator}.\n+ */\n+@Experimental(Kind.PORTABILITY)\n+@AutoValue\n+abstract class ReadViaSDF<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<Instant, WatermarkEstimatorT> getCreateWatermarkEstimatorFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  @Nullable\n+  abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+  abstract Builder<K, V, WatermarkEstimatorT> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>> {\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setOffsetConsumerConfig(\n+        Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCreateWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimatorT> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setTimestampPolicyFactory(\n+        TimestampPolicyFactory<K, V> policy);\n+\n+    abstract ReadViaSDF<K, V, WatermarkEstimatorT> build();\n+  }\n+\n+  public static <K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      ReadViaSDF<K, V, WatermarkEstimatorT> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V, WatermarkEstimatorT>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setCommitOffsetEnabled(false)\n+        .build()\n+        .withProcessingTime()\n+        .withMonotonicallyIncreasingWatermarkEstimator();\n+  }\n+\n+  // Note that if the bootstrapServers is set here but also populated with the element, the element\n+  // will override the bootstrapServers from the config.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializer(\n+      Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigUpdates(\n+      Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreatWatermarkEstimatorFn(\n+      SerializableFunction<Instant, WatermarkEstimatorT> fn) {\n+    return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withWallTimeWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new WallTime(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withMonotonicallyIncreasingWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new MonotonicallyIncreasing(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withManualWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new Manual(state);\n+        });\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigOverrides(\n+      Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  ReadViaSDFExternally forExternalBuild() {\n+    return new ReadViaSDFExternally(this);\n+  }\n+\n+  private static class ReadViaSDFExternally<\n+          K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends PTransform<PCollection<Row>, PCollection<KV<K, V>>> {\n+\n+    private final ReadViaSDF<K, V, WatermarkEstimatorT> readViaSDF;\n+\n+    ReadViaSDFExternally(ReadViaSDF read) {\n+      readViaSDF = read;\n+    }\n+\n+    @Override\n+    public PCollection<KV<K, V>> expand(PCollection<Row> input) {\n+      return input\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<Row, KafkaSourceDescription>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element Row row, OutputReceiver<KafkaSourceDescription> outputReceiver) {\n+                      TopicPartition topicPartition =\n+                          new TopicPartition(\n+                              row.getString(Schemas.TOPIC), row.getInt32(Schemas.PARTITION));\n+                      Instant startReadTime =\n+                          row.getInt64(Schemas.START_READ_TIME) != null\n+                              ? Instant.ofEpochMilli(row.getInt64(Schemas.START_READ_TIME))\n+                              : null;\n+                      outputReceiver.output(\n+                          KafkaSourceDescription.of(\n+                              topicPartition,\n+                              row.getInt64(Schemas.START_READ_OFFSET),\n+                              startReadTime,\n+                              new ArrayList<>(row.getArray(Schemas.BOOTSTRAP_SERVERS))));\n+                    }\n+                  }))\n+          .apply(readViaSDF)\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<KafkaRecord<K, V>, KV<K, V>>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element KafkaRecord element, OutputReceiver<KV<K, V>> outputReceiver) {\n+                      outputReceiver.output(element.getKV());\n+                    }\n+                  }))\n+          .setCoder(KvCoder.<K, V>of(readViaSDF.getKeyCoder(), readViaSDF.getValueCoder()));\n+    }\n+  }\n+\n+  ReadViaSDF<K, V, WatermarkEstimatorT> withTimestampPolicyFactory(\n+      TimestampPolicyFactory<K, V> timestampPolicyFactory) {\n+    return toBuilder().setTimestampPolicyFactory(timestampPolicyFactory).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<KafkaSourceDescription> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input\n+            .apply(ParDo.of(new ReadFromKafkaDoFn<K, V, WatermarkEstimatorT>(this)))\n+            .setCoder(outputCoder);\n+    // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      throw new IllegalStateException(\"Offset committed is not supported yet\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link KafkaSourceDescription} and outputs {@link\n+   * KafkaRecord}. By default, a {@link MonotonicallyIncreasing} watermark estimator is used to\n+   * track watermark.\n+   */\n+  @VisibleForTesting\n+  @UnboundedPerElement\n+  static class ReadFromKafkaDoFn<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends DoFn<KafkaSourceDescription, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn(ReadViaSDF transform) {\n+      this.consumerConfig = transform.getConsumerConfig();\n+      this.offsetConsumerConfig = transform.getOffsetConsumerConfig();\n+      this.keyDeserializerProvider = transform.getKeyDeserializerProvider();\n+      this.valueDeserializerProvider = transform.getValueDeserializerProvider();\n+      this.consumerFactoryFn = transform.getConsumerFactoryFn();\n+      this.extractOutputTimestampFn = transform.getExtractOutputTimestampFn();\n+      this.createWatermarkEstimatorFn = transform.getCreateWatermarkEstimatorFn();\n+      this.timestampPolicyFactory = transform.getTimestampPolicyFactory();\n+    }\n+\n+    private final Map<String, Object> offsetConsumerConfig;\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn;\n+    private final SerializableFunction<Instant, WatermarkEstimatorT> createWatermarkEstimatorFn;\n+    private final TimestampPolicyFactory<K, V> timestampPolicyFactory;\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 602}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2ODgzNzU3OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzowMzowM1rOGnyezQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzowMzowM1rOGnyezQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM3NDczMw==", "bodyText": "nit: memorizedBacklog -> memoizedBacklog", "url": "https://github.com/apache/beam/pull/11749#discussion_r444374733", "createdAt": "2020-06-23T17:03:03Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,861 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaSourceDescription.Schemas;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.Manual;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.WallTime;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the <a href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The {@link WatermarkEstimator} is created by {@link #getCreateWatermarkEstimatorFn()}. The\n+ * estimated watermark is computed by this {@link WatermarkEstimator} based on output timestamps\n+ * computed by {@link #getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link #withProcessingTime()} as {@code extractTimestampFn} and {@link\n+ * #withMonotonicallyIncreasingWatermarkEstimator()} as {@link WatermarkEstimator}.\n+ */\n+@Experimental(Kind.PORTABILITY)\n+@AutoValue\n+abstract class ReadViaSDF<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<Instant, WatermarkEstimatorT> getCreateWatermarkEstimatorFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  @Nullable\n+  abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+  abstract Builder<K, V, WatermarkEstimatorT> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>> {\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setOffsetConsumerConfig(\n+        Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCreateWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimatorT> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setTimestampPolicyFactory(\n+        TimestampPolicyFactory<K, V> policy);\n+\n+    abstract ReadViaSDF<K, V, WatermarkEstimatorT> build();\n+  }\n+\n+  public static <K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      ReadViaSDF<K, V, WatermarkEstimatorT> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V, WatermarkEstimatorT>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setCommitOffsetEnabled(false)\n+        .build()\n+        .withProcessingTime()\n+        .withMonotonicallyIncreasingWatermarkEstimator();\n+  }\n+\n+  // Note that if the bootstrapServers is set here but also populated with the element, the element\n+  // will override the bootstrapServers from the config.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializer(\n+      Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigUpdates(\n+      Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreatWatermarkEstimatorFn(\n+      SerializableFunction<Instant, WatermarkEstimatorT> fn) {\n+    return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withWallTimeWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new WallTime(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withMonotonicallyIncreasingWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new MonotonicallyIncreasing(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withManualWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new Manual(state);\n+        });\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigOverrides(\n+      Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  ReadViaSDFExternally forExternalBuild() {\n+    return new ReadViaSDFExternally(this);\n+  }\n+\n+  private static class ReadViaSDFExternally<\n+          K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends PTransform<PCollection<Row>, PCollection<KV<K, V>>> {\n+\n+    private final ReadViaSDF<K, V, WatermarkEstimatorT> readViaSDF;\n+\n+    ReadViaSDFExternally(ReadViaSDF read) {\n+      readViaSDF = read;\n+    }\n+\n+    @Override\n+    public PCollection<KV<K, V>> expand(PCollection<Row> input) {\n+      return input\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<Row, KafkaSourceDescription>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element Row row, OutputReceiver<KafkaSourceDescription> outputReceiver) {\n+                      TopicPartition topicPartition =\n+                          new TopicPartition(\n+                              row.getString(Schemas.TOPIC), row.getInt32(Schemas.PARTITION));\n+                      Instant startReadTime =\n+                          row.getInt64(Schemas.START_READ_TIME) != null\n+                              ? Instant.ofEpochMilli(row.getInt64(Schemas.START_READ_TIME))\n+                              : null;\n+                      outputReceiver.output(\n+                          KafkaSourceDescription.of(\n+                              topicPartition,\n+                              row.getInt64(Schemas.START_READ_OFFSET),\n+                              startReadTime,\n+                              new ArrayList<>(row.getArray(Schemas.BOOTSTRAP_SERVERS))));\n+                    }\n+                  }))\n+          .apply(readViaSDF)\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<KafkaRecord<K, V>, KV<K, V>>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element KafkaRecord element, OutputReceiver<KV<K, V>> outputReceiver) {\n+                      outputReceiver.output(element.getKV());\n+                    }\n+                  }))\n+          .setCoder(KvCoder.<K, V>of(readViaSDF.getKeyCoder(), readViaSDF.getValueCoder()));\n+    }\n+  }\n+\n+  ReadViaSDF<K, V, WatermarkEstimatorT> withTimestampPolicyFactory(\n+      TimestampPolicyFactory<K, V> timestampPolicyFactory) {\n+    return toBuilder().setTimestampPolicyFactory(timestampPolicyFactory).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<KafkaSourceDescription> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input\n+            .apply(ParDo.of(new ReadFromKafkaDoFn<K, V, WatermarkEstimatorT>(this)))\n+            .setCoder(outputCoder);\n+    // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      throw new IllegalStateException(\"Offset committed is not supported yet\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link KafkaSourceDescription} and outputs {@link\n+   * KafkaRecord}. By default, a {@link MonotonicallyIncreasing} watermark estimator is used to\n+   * track watermark.\n+   */\n+  @VisibleForTesting\n+  @UnboundedPerElement\n+  static class ReadFromKafkaDoFn<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends DoFn<KafkaSourceDescription, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn(ReadViaSDF transform) {\n+      this.consumerConfig = transform.getConsumerConfig();\n+      this.offsetConsumerConfig = transform.getOffsetConsumerConfig();\n+      this.keyDeserializerProvider = transform.getKeyDeserializerProvider();\n+      this.valueDeserializerProvider = transform.getValueDeserializerProvider();\n+      this.consumerFactoryFn = transform.getConsumerFactoryFn();\n+      this.extractOutputTimestampFn = transform.getExtractOutputTimestampFn();\n+      this.createWatermarkEstimatorFn = transform.getCreateWatermarkEstimatorFn();\n+      this.timestampPolicyFactory = transform.getTimestampPolicyFactory();\n+    }\n+\n+    private final Map<String, Object> offsetConsumerConfig;\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn;\n+    private final SerializableFunction<Instant, WatermarkEstimatorT> createWatermarkEstimatorFn;\n+    private final TimestampPolicyFactory<K, V> timestampPolicyFactory;\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgRecordSize;\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgOffsetGap;\n+\n+    private static final Duration KAFKA_POLL_TIMEOUT = Duration.millis(1000);\n+\n+    @VisibleForTesting final DeserializerProvider keyDeserializerProvider;\n+    @VisibleForTesting final DeserializerProvider valueDeserializerProvider;\n+    @VisibleForTesting final Map<String, Object> consumerConfig;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private static class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+      private final Supplier<Long> memorizedBacklog;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 626}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2ODg0NzkxOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzowNjowNFrOGnylwg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzowNjowNFrOGnylwg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM3NjUxNA==", "bodyText": "This method should be able to be GrowableOffsetRangeTracker. This will allow you to avoid the HasProgress cast in getSize", "url": "https://github.com/apache/beam/pull/11749#discussion_r444376514", "createdAt": "2020-06-23T17:06:04Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,861 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaSourceDescription.Schemas;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.Manual;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.WallTime;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the <a href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The {@link WatermarkEstimator} is created by {@link #getCreateWatermarkEstimatorFn()}. The\n+ * estimated watermark is computed by this {@link WatermarkEstimator} based on output timestamps\n+ * computed by {@link #getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link #withProcessingTime()} as {@code extractTimestampFn} and {@link\n+ * #withMonotonicallyIncreasingWatermarkEstimator()} as {@link WatermarkEstimator}.\n+ */\n+@Experimental(Kind.PORTABILITY)\n+@AutoValue\n+abstract class ReadViaSDF<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<Instant, WatermarkEstimatorT> getCreateWatermarkEstimatorFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  @Nullable\n+  abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+  abstract Builder<K, V, WatermarkEstimatorT> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>> {\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setOffsetConsumerConfig(\n+        Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCreateWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimatorT> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setTimestampPolicyFactory(\n+        TimestampPolicyFactory<K, V> policy);\n+\n+    abstract ReadViaSDF<K, V, WatermarkEstimatorT> build();\n+  }\n+\n+  public static <K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      ReadViaSDF<K, V, WatermarkEstimatorT> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V, WatermarkEstimatorT>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setCommitOffsetEnabled(false)\n+        .build()\n+        .withProcessingTime()\n+        .withMonotonicallyIncreasingWatermarkEstimator();\n+  }\n+\n+  // Note that if the bootstrapServers is set here but also populated with the element, the element\n+  // will override the bootstrapServers from the config.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializer(\n+      Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigUpdates(\n+      Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreatWatermarkEstimatorFn(\n+      SerializableFunction<Instant, WatermarkEstimatorT> fn) {\n+    return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withWallTimeWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new WallTime(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withMonotonicallyIncreasingWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new MonotonicallyIncreasing(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withManualWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new Manual(state);\n+        });\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigOverrides(\n+      Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  ReadViaSDFExternally forExternalBuild() {\n+    return new ReadViaSDFExternally(this);\n+  }\n+\n+  private static class ReadViaSDFExternally<\n+          K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends PTransform<PCollection<Row>, PCollection<KV<K, V>>> {\n+\n+    private final ReadViaSDF<K, V, WatermarkEstimatorT> readViaSDF;\n+\n+    ReadViaSDFExternally(ReadViaSDF read) {\n+      readViaSDF = read;\n+    }\n+\n+    @Override\n+    public PCollection<KV<K, V>> expand(PCollection<Row> input) {\n+      return input\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<Row, KafkaSourceDescription>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element Row row, OutputReceiver<KafkaSourceDescription> outputReceiver) {\n+                      TopicPartition topicPartition =\n+                          new TopicPartition(\n+                              row.getString(Schemas.TOPIC), row.getInt32(Schemas.PARTITION));\n+                      Instant startReadTime =\n+                          row.getInt64(Schemas.START_READ_TIME) != null\n+                              ? Instant.ofEpochMilli(row.getInt64(Schemas.START_READ_TIME))\n+                              : null;\n+                      outputReceiver.output(\n+                          KafkaSourceDescription.of(\n+                              topicPartition,\n+                              row.getInt64(Schemas.START_READ_OFFSET),\n+                              startReadTime,\n+                              new ArrayList<>(row.getArray(Schemas.BOOTSTRAP_SERVERS))));\n+                    }\n+                  }))\n+          .apply(readViaSDF)\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<KafkaRecord<K, V>, KV<K, V>>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element KafkaRecord element, OutputReceiver<KV<K, V>> outputReceiver) {\n+                      outputReceiver.output(element.getKV());\n+                    }\n+                  }))\n+          .setCoder(KvCoder.<K, V>of(readViaSDF.getKeyCoder(), readViaSDF.getValueCoder()));\n+    }\n+  }\n+\n+  ReadViaSDF<K, V, WatermarkEstimatorT> withTimestampPolicyFactory(\n+      TimestampPolicyFactory<K, V> timestampPolicyFactory) {\n+    return toBuilder().setTimestampPolicyFactory(timestampPolicyFactory).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<KafkaSourceDescription> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input\n+            .apply(ParDo.of(new ReadFromKafkaDoFn<K, V, WatermarkEstimatorT>(this)))\n+            .setCoder(outputCoder);\n+    // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      throw new IllegalStateException(\"Offset committed is not supported yet\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link KafkaSourceDescription} and outputs {@link\n+   * KafkaRecord}. By default, a {@link MonotonicallyIncreasing} watermark estimator is used to\n+   * track watermark.\n+   */\n+  @VisibleForTesting\n+  @UnboundedPerElement\n+  static class ReadFromKafkaDoFn<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends DoFn<KafkaSourceDescription, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn(ReadViaSDF transform) {\n+      this.consumerConfig = transform.getConsumerConfig();\n+      this.offsetConsumerConfig = transform.getOffsetConsumerConfig();\n+      this.keyDeserializerProvider = transform.getKeyDeserializerProvider();\n+      this.valueDeserializerProvider = transform.getValueDeserializerProvider();\n+      this.consumerFactoryFn = transform.getConsumerFactoryFn();\n+      this.extractOutputTimestampFn = transform.getExtractOutputTimestampFn();\n+      this.createWatermarkEstimatorFn = transform.getCreateWatermarkEstimatorFn();\n+      this.timestampPolicyFactory = transform.getTimestampPolicyFactory();\n+    }\n+\n+    private final Map<String, Object> offsetConsumerConfig;\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn;\n+    private final SerializableFunction<Instant, WatermarkEstimatorT> createWatermarkEstimatorFn;\n+    private final TimestampPolicyFactory<K, V> timestampPolicyFactory;\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgRecordSize;\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgOffsetGap;\n+\n+    private static final Duration KAFKA_POLL_TIMEOUT = Duration.millis(1000);\n+\n+    @VisibleForTesting final DeserializerProvider keyDeserializerProvider;\n+    @VisibleForTesting final DeserializerProvider valueDeserializerProvider;\n+    @VisibleForTesting final Map<String, Object> consumerConfig;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private static class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+      private final Supplier<Long> memorizedBacklog;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+        memorizedBacklog =\n+            Suppliers.memoizeWithExpiration(\n+                () -> {\n+                  consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+                  return offsetConsumer.position(topicPartition);\n+                },\n+                5,\n+                TimeUnit.SECONDS);\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        return memorizedBacklog.get();\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element KafkaSourceDescription kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        consumerSpEL.evaluateAssign(\n+            offsetConsumer, ImmutableList.of(kafkaSourceDescription.getTopicPartition()));\n+        long startOffset;\n+        if (kafkaSourceDescription.getStartReadOffset() != null) {\n+          startOffset = kafkaSourceDescription.getStartReadOffset();\n+        } else if (kafkaSourceDescription.getStartReadTime() != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  kafkaSourceDescription.getTopicPartition(),\n+                  kafkaSourceDescription.getStartReadTime());\n+        } else {\n+          startOffset = offsetConsumer.position(kafkaSourceDescription.getTopicPartition());\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public WatermarkEstimatorT newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return createWatermarkEstimatorFn.apply(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        @Restriction OffsetRange offsetRange)\n+        throws Exception {\n+      double numOfRecords =\n+          ((HasProgress) restrictionTracker(kafkaSourceDescription, offsetRange))\n+              .getProgress()\n+              .getWorkRemaining();\n+\n+      // Before processing elements, we don't have a good estimated size of records and offset gap.\n+      if (avgOffsetGap.containsKey(kafkaSourceDescription.getTopicPartition())) {\n+        numOfRecords =\n+            numOfRecords / (1 + avgOffsetGap.get(kafkaSourceDescription.getTopicPartition()).get());\n+      }\n+      return (!avgRecordSize.containsKey(kafkaSourceDescription.getTopicPartition())\n+              ? 1\n+              : avgRecordSize.get(kafkaSourceDescription.getTopicPartition()).get())\n+          * numOfRecords;\n+    }\n+\n+    @NewTracker\n+    public RestrictionTracker<OffsetRange, Long> restrictionTracker(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 719}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2ODg2MDA3OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzowOTo0MlrOGnytyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNTowNTowNFrOGoDCzQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM3ODU2OA==", "bodyText": "Why not numOfRecords * DEFAULT_MESSAGE_SIZE?\nWhere DEFAULT_MESSAGE_SIZE=100 or 1000?", "url": "https://github.com/apache/beam/pull/11749#discussion_r444378568", "createdAt": "2020-06-23T17:09:42Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,861 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaSourceDescription.Schemas;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.Manual;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.WallTime;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the <a href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The {@link WatermarkEstimator} is created by {@link #getCreateWatermarkEstimatorFn()}. The\n+ * estimated watermark is computed by this {@link WatermarkEstimator} based on output timestamps\n+ * computed by {@link #getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link #withProcessingTime()} as {@code extractTimestampFn} and {@link\n+ * #withMonotonicallyIncreasingWatermarkEstimator()} as {@link WatermarkEstimator}.\n+ */\n+@Experimental(Kind.PORTABILITY)\n+@AutoValue\n+abstract class ReadViaSDF<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<Instant, WatermarkEstimatorT> getCreateWatermarkEstimatorFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  @Nullable\n+  abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+  abstract Builder<K, V, WatermarkEstimatorT> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>> {\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setOffsetConsumerConfig(\n+        Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCreateWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimatorT> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setTimestampPolicyFactory(\n+        TimestampPolicyFactory<K, V> policy);\n+\n+    abstract ReadViaSDF<K, V, WatermarkEstimatorT> build();\n+  }\n+\n+  public static <K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      ReadViaSDF<K, V, WatermarkEstimatorT> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V, WatermarkEstimatorT>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setCommitOffsetEnabled(false)\n+        .build()\n+        .withProcessingTime()\n+        .withMonotonicallyIncreasingWatermarkEstimator();\n+  }\n+\n+  // Note that if the bootstrapServers is set here but also populated with the element, the element\n+  // will override the bootstrapServers from the config.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializer(\n+      Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigUpdates(\n+      Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreatWatermarkEstimatorFn(\n+      SerializableFunction<Instant, WatermarkEstimatorT> fn) {\n+    return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withWallTimeWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new WallTime(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withMonotonicallyIncreasingWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new MonotonicallyIncreasing(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withManualWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new Manual(state);\n+        });\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigOverrides(\n+      Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  ReadViaSDFExternally forExternalBuild() {\n+    return new ReadViaSDFExternally(this);\n+  }\n+\n+  private static class ReadViaSDFExternally<\n+          K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends PTransform<PCollection<Row>, PCollection<KV<K, V>>> {\n+\n+    private final ReadViaSDF<K, V, WatermarkEstimatorT> readViaSDF;\n+\n+    ReadViaSDFExternally(ReadViaSDF read) {\n+      readViaSDF = read;\n+    }\n+\n+    @Override\n+    public PCollection<KV<K, V>> expand(PCollection<Row> input) {\n+      return input\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<Row, KafkaSourceDescription>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element Row row, OutputReceiver<KafkaSourceDescription> outputReceiver) {\n+                      TopicPartition topicPartition =\n+                          new TopicPartition(\n+                              row.getString(Schemas.TOPIC), row.getInt32(Schemas.PARTITION));\n+                      Instant startReadTime =\n+                          row.getInt64(Schemas.START_READ_TIME) != null\n+                              ? Instant.ofEpochMilli(row.getInt64(Schemas.START_READ_TIME))\n+                              : null;\n+                      outputReceiver.output(\n+                          KafkaSourceDescription.of(\n+                              topicPartition,\n+                              row.getInt64(Schemas.START_READ_OFFSET),\n+                              startReadTime,\n+                              new ArrayList<>(row.getArray(Schemas.BOOTSTRAP_SERVERS))));\n+                    }\n+                  }))\n+          .apply(readViaSDF)\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<KafkaRecord<K, V>, KV<K, V>>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element KafkaRecord element, OutputReceiver<KV<K, V>> outputReceiver) {\n+                      outputReceiver.output(element.getKV());\n+                    }\n+                  }))\n+          .setCoder(KvCoder.<K, V>of(readViaSDF.getKeyCoder(), readViaSDF.getValueCoder()));\n+    }\n+  }\n+\n+  ReadViaSDF<K, V, WatermarkEstimatorT> withTimestampPolicyFactory(\n+      TimestampPolicyFactory<K, V> timestampPolicyFactory) {\n+    return toBuilder().setTimestampPolicyFactory(timestampPolicyFactory).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<KafkaSourceDescription> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input\n+            .apply(ParDo.of(new ReadFromKafkaDoFn<K, V, WatermarkEstimatorT>(this)))\n+            .setCoder(outputCoder);\n+    // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      throw new IllegalStateException(\"Offset committed is not supported yet\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link KafkaSourceDescription} and outputs {@link\n+   * KafkaRecord}. By default, a {@link MonotonicallyIncreasing} watermark estimator is used to\n+   * track watermark.\n+   */\n+  @VisibleForTesting\n+  @UnboundedPerElement\n+  static class ReadFromKafkaDoFn<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends DoFn<KafkaSourceDescription, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn(ReadViaSDF transform) {\n+      this.consumerConfig = transform.getConsumerConfig();\n+      this.offsetConsumerConfig = transform.getOffsetConsumerConfig();\n+      this.keyDeserializerProvider = transform.getKeyDeserializerProvider();\n+      this.valueDeserializerProvider = transform.getValueDeserializerProvider();\n+      this.consumerFactoryFn = transform.getConsumerFactoryFn();\n+      this.extractOutputTimestampFn = transform.getExtractOutputTimestampFn();\n+      this.createWatermarkEstimatorFn = transform.getCreateWatermarkEstimatorFn();\n+      this.timestampPolicyFactory = transform.getTimestampPolicyFactory();\n+    }\n+\n+    private final Map<String, Object> offsetConsumerConfig;\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn;\n+    private final SerializableFunction<Instant, WatermarkEstimatorT> createWatermarkEstimatorFn;\n+    private final TimestampPolicyFactory<K, V> timestampPolicyFactory;\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgRecordSize;\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgOffsetGap;\n+\n+    private static final Duration KAFKA_POLL_TIMEOUT = Duration.millis(1000);\n+\n+    @VisibleForTesting final DeserializerProvider keyDeserializerProvider;\n+    @VisibleForTesting final DeserializerProvider valueDeserializerProvider;\n+    @VisibleForTesting final Map<String, Object> consumerConfig;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private static class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+      private final Supplier<Long> memorizedBacklog;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+        memorizedBacklog =\n+            Suppliers.memoizeWithExpiration(\n+                () -> {\n+                  consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+                  return offsetConsumer.position(topicPartition);\n+                },\n+                5,\n+                TimeUnit.SECONDS);\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        return memorizedBacklog.get();\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element KafkaSourceDescription kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        consumerSpEL.evaluateAssign(\n+            offsetConsumer, ImmutableList.of(kafkaSourceDescription.getTopicPartition()));\n+        long startOffset;\n+        if (kafkaSourceDescription.getStartReadOffset() != null) {\n+          startOffset = kafkaSourceDescription.getStartReadOffset();\n+        } else if (kafkaSourceDescription.getStartReadTime() != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  kafkaSourceDescription.getTopicPartition(),\n+                  kafkaSourceDescription.getStartReadTime());\n+        } else {\n+          startOffset = offsetConsumer.position(kafkaSourceDescription.getTopicPartition());\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public WatermarkEstimatorT newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return createWatermarkEstimatorFn.apply(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        @Restriction OffsetRange offsetRange)\n+        throws Exception {\n+      double numOfRecords =\n+          ((HasProgress) restrictionTracker(kafkaSourceDescription, offsetRange))\n+              .getProgress()\n+              .getWorkRemaining();\n+\n+      // Before processing elements, we don't have a good estimated size of records and offset gap.\n+      if (avgOffsetGap.containsKey(kafkaSourceDescription.getTopicPartition())) {\n+        numOfRecords =\n+            numOfRecords / (1 + avgOffsetGap.get(kafkaSourceDescription.getTopicPartition()).get());\n+      }\n+      return (!avgRecordSize.containsKey(kafkaSourceDescription.getTopicPartition())\n+              ? 1", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 713}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY0NjA5Mw==", "bodyText": "I don't think it's easy to come up with a default size here. There is no avgSize and avgGap only when initial sizing, which is not really important for streaming.", "url": "https://github.com/apache/beam/pull/11749#discussion_r444646093", "createdAt": "2020-06-24T05:05:04Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,861 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaSourceDescription.Schemas;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.Manual;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.WallTime;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the <a href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The {@link WatermarkEstimator} is created by {@link #getCreateWatermarkEstimatorFn()}. The\n+ * estimated watermark is computed by this {@link WatermarkEstimator} based on output timestamps\n+ * computed by {@link #getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link #withProcessingTime()} as {@code extractTimestampFn} and {@link\n+ * #withMonotonicallyIncreasingWatermarkEstimator()} as {@link WatermarkEstimator}.\n+ */\n+@Experimental(Kind.PORTABILITY)\n+@AutoValue\n+abstract class ReadViaSDF<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<Instant, WatermarkEstimatorT> getCreateWatermarkEstimatorFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  @Nullable\n+  abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+  abstract Builder<K, V, WatermarkEstimatorT> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>> {\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setOffsetConsumerConfig(\n+        Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCreateWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimatorT> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setTimestampPolicyFactory(\n+        TimestampPolicyFactory<K, V> policy);\n+\n+    abstract ReadViaSDF<K, V, WatermarkEstimatorT> build();\n+  }\n+\n+  public static <K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      ReadViaSDF<K, V, WatermarkEstimatorT> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V, WatermarkEstimatorT>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setCommitOffsetEnabled(false)\n+        .build()\n+        .withProcessingTime()\n+        .withMonotonicallyIncreasingWatermarkEstimator();\n+  }\n+\n+  // Note that if the bootstrapServers is set here but also populated with the element, the element\n+  // will override the bootstrapServers from the config.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializer(\n+      Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigUpdates(\n+      Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreatWatermarkEstimatorFn(\n+      SerializableFunction<Instant, WatermarkEstimatorT> fn) {\n+    return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withWallTimeWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new WallTime(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withMonotonicallyIncreasingWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new MonotonicallyIncreasing(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withManualWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new Manual(state);\n+        });\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigOverrides(\n+      Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  ReadViaSDFExternally forExternalBuild() {\n+    return new ReadViaSDFExternally(this);\n+  }\n+\n+  private static class ReadViaSDFExternally<\n+          K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends PTransform<PCollection<Row>, PCollection<KV<K, V>>> {\n+\n+    private final ReadViaSDF<K, V, WatermarkEstimatorT> readViaSDF;\n+\n+    ReadViaSDFExternally(ReadViaSDF read) {\n+      readViaSDF = read;\n+    }\n+\n+    @Override\n+    public PCollection<KV<K, V>> expand(PCollection<Row> input) {\n+      return input\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<Row, KafkaSourceDescription>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element Row row, OutputReceiver<KafkaSourceDescription> outputReceiver) {\n+                      TopicPartition topicPartition =\n+                          new TopicPartition(\n+                              row.getString(Schemas.TOPIC), row.getInt32(Schemas.PARTITION));\n+                      Instant startReadTime =\n+                          row.getInt64(Schemas.START_READ_TIME) != null\n+                              ? Instant.ofEpochMilli(row.getInt64(Schemas.START_READ_TIME))\n+                              : null;\n+                      outputReceiver.output(\n+                          KafkaSourceDescription.of(\n+                              topicPartition,\n+                              row.getInt64(Schemas.START_READ_OFFSET),\n+                              startReadTime,\n+                              new ArrayList<>(row.getArray(Schemas.BOOTSTRAP_SERVERS))));\n+                    }\n+                  }))\n+          .apply(readViaSDF)\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<KafkaRecord<K, V>, KV<K, V>>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element KafkaRecord element, OutputReceiver<KV<K, V>> outputReceiver) {\n+                      outputReceiver.output(element.getKV());\n+                    }\n+                  }))\n+          .setCoder(KvCoder.<K, V>of(readViaSDF.getKeyCoder(), readViaSDF.getValueCoder()));\n+    }\n+  }\n+\n+  ReadViaSDF<K, V, WatermarkEstimatorT> withTimestampPolicyFactory(\n+      TimestampPolicyFactory<K, V> timestampPolicyFactory) {\n+    return toBuilder().setTimestampPolicyFactory(timestampPolicyFactory).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<KafkaSourceDescription> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input\n+            .apply(ParDo.of(new ReadFromKafkaDoFn<K, V, WatermarkEstimatorT>(this)))\n+            .setCoder(outputCoder);\n+    // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      throw new IllegalStateException(\"Offset committed is not supported yet\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link KafkaSourceDescription} and outputs {@link\n+   * KafkaRecord}. By default, a {@link MonotonicallyIncreasing} watermark estimator is used to\n+   * track watermark.\n+   */\n+  @VisibleForTesting\n+  @UnboundedPerElement\n+  static class ReadFromKafkaDoFn<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends DoFn<KafkaSourceDescription, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn(ReadViaSDF transform) {\n+      this.consumerConfig = transform.getConsumerConfig();\n+      this.offsetConsumerConfig = transform.getOffsetConsumerConfig();\n+      this.keyDeserializerProvider = transform.getKeyDeserializerProvider();\n+      this.valueDeserializerProvider = transform.getValueDeserializerProvider();\n+      this.consumerFactoryFn = transform.getConsumerFactoryFn();\n+      this.extractOutputTimestampFn = transform.getExtractOutputTimestampFn();\n+      this.createWatermarkEstimatorFn = transform.getCreateWatermarkEstimatorFn();\n+      this.timestampPolicyFactory = transform.getTimestampPolicyFactory();\n+    }\n+\n+    private final Map<String, Object> offsetConsumerConfig;\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn;\n+    private final SerializableFunction<Instant, WatermarkEstimatorT> createWatermarkEstimatorFn;\n+    private final TimestampPolicyFactory<K, V> timestampPolicyFactory;\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgRecordSize;\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgOffsetGap;\n+\n+    private static final Duration KAFKA_POLL_TIMEOUT = Duration.millis(1000);\n+\n+    @VisibleForTesting final DeserializerProvider keyDeserializerProvider;\n+    @VisibleForTesting final DeserializerProvider valueDeserializerProvider;\n+    @VisibleForTesting final Map<String, Object> consumerConfig;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private static class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+      private final Supplier<Long> memorizedBacklog;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+        memorizedBacklog =\n+            Suppliers.memoizeWithExpiration(\n+                () -> {\n+                  consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+                  return offsetConsumer.position(topicPartition);\n+                },\n+                5,\n+                TimeUnit.SECONDS);\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        return memorizedBacklog.get();\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element KafkaSourceDescription kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        consumerSpEL.evaluateAssign(\n+            offsetConsumer, ImmutableList.of(kafkaSourceDescription.getTopicPartition()));\n+        long startOffset;\n+        if (kafkaSourceDescription.getStartReadOffset() != null) {\n+          startOffset = kafkaSourceDescription.getStartReadOffset();\n+        } else if (kafkaSourceDescription.getStartReadTime() != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  kafkaSourceDescription.getTopicPartition(),\n+                  kafkaSourceDescription.getStartReadTime());\n+        } else {\n+          startOffset = offsetConsumer.position(kafkaSourceDescription.getTopicPartition());\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public WatermarkEstimatorT newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return createWatermarkEstimatorFn.apply(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        @Restriction OffsetRange offsetRange)\n+        throws Exception {\n+      double numOfRecords =\n+          ((HasProgress) restrictionTracker(kafkaSourceDescription, offsetRange))\n+              .getProgress()\n+              .getWorkRemaining();\n+\n+      // Before processing elements, we don't have a good estimated size of records and offset gap.\n+      if (avgOffsetGap.containsKey(kafkaSourceDescription.getTopicPartition())) {\n+        numOfRecords =\n+            numOfRecords / (1 + avgOffsetGap.get(kafkaSourceDescription.getTopicPartition()).get());\n+      }\n+      return (!avgRecordSize.containsKey(kafkaSourceDescription.getTopicPartition())\n+              ? 1", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM3ODU2OA=="}, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 713}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2ODg2MzQ3OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzoxMDozOFrOGnywBw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzoxMDozOFrOGnywBw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM3OTE0Mw==", "bodyText": "nit: numOfRecords -> numRecords", "url": "https://github.com/apache/beam/pull/11749#discussion_r444379143", "createdAt": "2020-06-23T17:10:38Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,861 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaSourceDescription.Schemas;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.Manual;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.WallTime;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the <a href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The {@link WatermarkEstimator} is created by {@link #getCreateWatermarkEstimatorFn()}. The\n+ * estimated watermark is computed by this {@link WatermarkEstimator} based on output timestamps\n+ * computed by {@link #getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link #withProcessingTime()} as {@code extractTimestampFn} and {@link\n+ * #withMonotonicallyIncreasingWatermarkEstimator()} as {@link WatermarkEstimator}.\n+ */\n+@Experimental(Kind.PORTABILITY)\n+@AutoValue\n+abstract class ReadViaSDF<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<Instant, WatermarkEstimatorT> getCreateWatermarkEstimatorFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  @Nullable\n+  abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+  abstract Builder<K, V, WatermarkEstimatorT> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>> {\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setOffsetConsumerConfig(\n+        Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCreateWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimatorT> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setTimestampPolicyFactory(\n+        TimestampPolicyFactory<K, V> policy);\n+\n+    abstract ReadViaSDF<K, V, WatermarkEstimatorT> build();\n+  }\n+\n+  public static <K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      ReadViaSDF<K, V, WatermarkEstimatorT> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V, WatermarkEstimatorT>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setCommitOffsetEnabled(false)\n+        .build()\n+        .withProcessingTime()\n+        .withMonotonicallyIncreasingWatermarkEstimator();\n+  }\n+\n+  // Note that if the bootstrapServers is set here but also populated with the element, the element\n+  // will override the bootstrapServers from the config.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializer(\n+      Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigUpdates(\n+      Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreatWatermarkEstimatorFn(\n+      SerializableFunction<Instant, WatermarkEstimatorT> fn) {\n+    return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withWallTimeWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new WallTime(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withMonotonicallyIncreasingWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new MonotonicallyIncreasing(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withManualWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new Manual(state);\n+        });\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigOverrides(\n+      Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  ReadViaSDFExternally forExternalBuild() {\n+    return new ReadViaSDFExternally(this);\n+  }\n+\n+  private static class ReadViaSDFExternally<\n+          K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends PTransform<PCollection<Row>, PCollection<KV<K, V>>> {\n+\n+    private final ReadViaSDF<K, V, WatermarkEstimatorT> readViaSDF;\n+\n+    ReadViaSDFExternally(ReadViaSDF read) {\n+      readViaSDF = read;\n+    }\n+\n+    @Override\n+    public PCollection<KV<K, V>> expand(PCollection<Row> input) {\n+      return input\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<Row, KafkaSourceDescription>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element Row row, OutputReceiver<KafkaSourceDescription> outputReceiver) {\n+                      TopicPartition topicPartition =\n+                          new TopicPartition(\n+                              row.getString(Schemas.TOPIC), row.getInt32(Schemas.PARTITION));\n+                      Instant startReadTime =\n+                          row.getInt64(Schemas.START_READ_TIME) != null\n+                              ? Instant.ofEpochMilli(row.getInt64(Schemas.START_READ_TIME))\n+                              : null;\n+                      outputReceiver.output(\n+                          KafkaSourceDescription.of(\n+                              topicPartition,\n+                              row.getInt64(Schemas.START_READ_OFFSET),\n+                              startReadTime,\n+                              new ArrayList<>(row.getArray(Schemas.BOOTSTRAP_SERVERS))));\n+                    }\n+                  }))\n+          .apply(readViaSDF)\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<KafkaRecord<K, V>, KV<K, V>>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element KafkaRecord element, OutputReceiver<KV<K, V>> outputReceiver) {\n+                      outputReceiver.output(element.getKV());\n+                    }\n+                  }))\n+          .setCoder(KvCoder.<K, V>of(readViaSDF.getKeyCoder(), readViaSDF.getValueCoder()));\n+    }\n+  }\n+\n+  ReadViaSDF<K, V, WatermarkEstimatorT> withTimestampPolicyFactory(\n+      TimestampPolicyFactory<K, V> timestampPolicyFactory) {\n+    return toBuilder().setTimestampPolicyFactory(timestampPolicyFactory).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<KafkaSourceDescription> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input\n+            .apply(ParDo.of(new ReadFromKafkaDoFn<K, V, WatermarkEstimatorT>(this)))\n+            .setCoder(outputCoder);\n+    // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      throw new IllegalStateException(\"Offset committed is not supported yet\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link KafkaSourceDescription} and outputs {@link\n+   * KafkaRecord}. By default, a {@link MonotonicallyIncreasing} watermark estimator is used to\n+   * track watermark.\n+   */\n+  @VisibleForTesting\n+  @UnboundedPerElement\n+  static class ReadFromKafkaDoFn<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends DoFn<KafkaSourceDescription, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn(ReadViaSDF transform) {\n+      this.consumerConfig = transform.getConsumerConfig();\n+      this.offsetConsumerConfig = transform.getOffsetConsumerConfig();\n+      this.keyDeserializerProvider = transform.getKeyDeserializerProvider();\n+      this.valueDeserializerProvider = transform.getValueDeserializerProvider();\n+      this.consumerFactoryFn = transform.getConsumerFactoryFn();\n+      this.extractOutputTimestampFn = transform.getExtractOutputTimestampFn();\n+      this.createWatermarkEstimatorFn = transform.getCreateWatermarkEstimatorFn();\n+      this.timestampPolicyFactory = transform.getTimestampPolicyFactory();\n+    }\n+\n+    private final Map<String, Object> offsetConsumerConfig;\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn;\n+    private final SerializableFunction<Instant, WatermarkEstimatorT> createWatermarkEstimatorFn;\n+    private final TimestampPolicyFactory<K, V> timestampPolicyFactory;\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgRecordSize;\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgOffsetGap;\n+\n+    private static final Duration KAFKA_POLL_TIMEOUT = Duration.millis(1000);\n+\n+    @VisibleForTesting final DeserializerProvider keyDeserializerProvider;\n+    @VisibleForTesting final DeserializerProvider valueDeserializerProvider;\n+    @VisibleForTesting final Map<String, Object> consumerConfig;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private static class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+      private final Supplier<Long> memorizedBacklog;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+        memorizedBacklog =\n+            Suppliers.memoizeWithExpiration(\n+                () -> {\n+                  consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+                  return offsetConsumer.position(topicPartition);\n+                },\n+                5,\n+                TimeUnit.SECONDS);\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        return memorizedBacklog.get();\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element KafkaSourceDescription kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        consumerSpEL.evaluateAssign(\n+            offsetConsumer, ImmutableList.of(kafkaSourceDescription.getTopicPartition()));\n+        long startOffset;\n+        if (kafkaSourceDescription.getStartReadOffset() != null) {\n+          startOffset = kafkaSourceDescription.getStartReadOffset();\n+        } else if (kafkaSourceDescription.getStartReadTime() != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  kafkaSourceDescription.getTopicPartition(),\n+                  kafkaSourceDescription.getStartReadTime());\n+        } else {\n+          startOffset = offsetConsumer.position(kafkaSourceDescription.getTopicPartition());\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public WatermarkEstimatorT newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return createWatermarkEstimatorFn.apply(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        @Restriction OffsetRange offsetRange)\n+        throws Exception {\n+      double numOfRecords =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 702}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2ODg3ODk3OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzoxNDo0N1rOGny5sA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzoxNjowN1rOGny82Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM4MTYxNg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  // If there is no future work, resume with max timeout and move to the next element.\n          \n          \n            \n                  // If there is no known work, resume with max timeout and move to the next element.", "url": "https://github.com/apache/beam/pull/11749#discussion_r444381616", "createdAt": "2020-06-23T17:14:47Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,861 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaSourceDescription.Schemas;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.Manual;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.WallTime;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the <a href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The {@link WatermarkEstimator} is created by {@link #getCreateWatermarkEstimatorFn()}. The\n+ * estimated watermark is computed by this {@link WatermarkEstimator} based on output timestamps\n+ * computed by {@link #getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link #withProcessingTime()} as {@code extractTimestampFn} and {@link\n+ * #withMonotonicallyIncreasingWatermarkEstimator()} as {@link WatermarkEstimator}.\n+ */\n+@Experimental(Kind.PORTABILITY)\n+@AutoValue\n+abstract class ReadViaSDF<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<Instant, WatermarkEstimatorT> getCreateWatermarkEstimatorFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  @Nullable\n+  abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+  abstract Builder<K, V, WatermarkEstimatorT> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>> {\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setOffsetConsumerConfig(\n+        Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCreateWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimatorT> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setTimestampPolicyFactory(\n+        TimestampPolicyFactory<K, V> policy);\n+\n+    abstract ReadViaSDF<K, V, WatermarkEstimatorT> build();\n+  }\n+\n+  public static <K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      ReadViaSDF<K, V, WatermarkEstimatorT> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V, WatermarkEstimatorT>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setCommitOffsetEnabled(false)\n+        .build()\n+        .withProcessingTime()\n+        .withMonotonicallyIncreasingWatermarkEstimator();\n+  }\n+\n+  // Note that if the bootstrapServers is set here but also populated with the element, the element\n+  // will override the bootstrapServers from the config.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializer(\n+      Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigUpdates(\n+      Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreatWatermarkEstimatorFn(\n+      SerializableFunction<Instant, WatermarkEstimatorT> fn) {\n+    return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withWallTimeWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new WallTime(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withMonotonicallyIncreasingWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new MonotonicallyIncreasing(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withManualWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new Manual(state);\n+        });\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigOverrides(\n+      Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  ReadViaSDFExternally forExternalBuild() {\n+    return new ReadViaSDFExternally(this);\n+  }\n+\n+  private static class ReadViaSDFExternally<\n+          K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends PTransform<PCollection<Row>, PCollection<KV<K, V>>> {\n+\n+    private final ReadViaSDF<K, V, WatermarkEstimatorT> readViaSDF;\n+\n+    ReadViaSDFExternally(ReadViaSDF read) {\n+      readViaSDF = read;\n+    }\n+\n+    @Override\n+    public PCollection<KV<K, V>> expand(PCollection<Row> input) {\n+      return input\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<Row, KafkaSourceDescription>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element Row row, OutputReceiver<KafkaSourceDescription> outputReceiver) {\n+                      TopicPartition topicPartition =\n+                          new TopicPartition(\n+                              row.getString(Schemas.TOPIC), row.getInt32(Schemas.PARTITION));\n+                      Instant startReadTime =\n+                          row.getInt64(Schemas.START_READ_TIME) != null\n+                              ? Instant.ofEpochMilli(row.getInt64(Schemas.START_READ_TIME))\n+                              : null;\n+                      outputReceiver.output(\n+                          KafkaSourceDescription.of(\n+                              topicPartition,\n+                              row.getInt64(Schemas.START_READ_OFFSET),\n+                              startReadTime,\n+                              new ArrayList<>(row.getArray(Schemas.BOOTSTRAP_SERVERS))));\n+                    }\n+                  }))\n+          .apply(readViaSDF)\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<KafkaRecord<K, V>, KV<K, V>>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element KafkaRecord element, OutputReceiver<KV<K, V>> outputReceiver) {\n+                      outputReceiver.output(element.getKV());\n+                    }\n+                  }))\n+          .setCoder(KvCoder.<K, V>of(readViaSDF.getKeyCoder(), readViaSDF.getValueCoder()));\n+    }\n+  }\n+\n+  ReadViaSDF<K, V, WatermarkEstimatorT> withTimestampPolicyFactory(\n+      TimestampPolicyFactory<K, V> timestampPolicyFactory) {\n+    return toBuilder().setTimestampPolicyFactory(timestampPolicyFactory).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<KafkaSourceDescription> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input\n+            .apply(ParDo.of(new ReadFromKafkaDoFn<K, V, WatermarkEstimatorT>(this)))\n+            .setCoder(outputCoder);\n+    // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      throw new IllegalStateException(\"Offset committed is not supported yet\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link KafkaSourceDescription} and outputs {@link\n+   * KafkaRecord}. By default, a {@link MonotonicallyIncreasing} watermark estimator is used to\n+   * track watermark.\n+   */\n+  @VisibleForTesting\n+  @UnboundedPerElement\n+  static class ReadFromKafkaDoFn<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends DoFn<KafkaSourceDescription, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn(ReadViaSDF transform) {\n+      this.consumerConfig = transform.getConsumerConfig();\n+      this.offsetConsumerConfig = transform.getOffsetConsumerConfig();\n+      this.keyDeserializerProvider = transform.getKeyDeserializerProvider();\n+      this.valueDeserializerProvider = transform.getValueDeserializerProvider();\n+      this.consumerFactoryFn = transform.getConsumerFactoryFn();\n+      this.extractOutputTimestampFn = transform.getExtractOutputTimestampFn();\n+      this.createWatermarkEstimatorFn = transform.getCreateWatermarkEstimatorFn();\n+      this.timestampPolicyFactory = transform.getTimestampPolicyFactory();\n+    }\n+\n+    private final Map<String, Object> offsetConsumerConfig;\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn;\n+    private final SerializableFunction<Instant, WatermarkEstimatorT> createWatermarkEstimatorFn;\n+    private final TimestampPolicyFactory<K, V> timestampPolicyFactory;\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgRecordSize;\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgOffsetGap;\n+\n+    private static final Duration KAFKA_POLL_TIMEOUT = Duration.millis(1000);\n+\n+    @VisibleForTesting final DeserializerProvider keyDeserializerProvider;\n+    @VisibleForTesting final DeserializerProvider valueDeserializerProvider;\n+    @VisibleForTesting final Map<String, Object> consumerConfig;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private static class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+      private final Supplier<Long> memorizedBacklog;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+        memorizedBacklog =\n+            Suppliers.memoizeWithExpiration(\n+                () -> {\n+                  consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+                  return offsetConsumer.position(topicPartition);\n+                },\n+                5,\n+                TimeUnit.SECONDS);\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        return memorizedBacklog.get();\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element KafkaSourceDescription kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        consumerSpEL.evaluateAssign(\n+            offsetConsumer, ImmutableList.of(kafkaSourceDescription.getTopicPartition()));\n+        long startOffset;\n+        if (kafkaSourceDescription.getStartReadOffset() != null) {\n+          startOffset = kafkaSourceDescription.getStartReadOffset();\n+        } else if (kafkaSourceDescription.getStartReadTime() != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  kafkaSourceDescription.getTopicPartition(),\n+                  kafkaSourceDescription.getStartReadTime());\n+        } else {\n+          startOffset = offsetConsumer.position(kafkaSourceDescription.getTopicPartition());\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public WatermarkEstimatorT newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return createWatermarkEstimatorFn.apply(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        @Restriction OffsetRange offsetRange)\n+        throws Exception {\n+      double numOfRecords =\n+          ((HasProgress) restrictionTracker(kafkaSourceDescription, offsetRange))\n+              .getProgress()\n+              .getWorkRemaining();\n+\n+      // Before processing elements, we don't have a good estimated size of records and offset gap.\n+      if (avgOffsetGap.containsKey(kafkaSourceDescription.getTopicPartition())) {\n+        numOfRecords =\n+            numOfRecords / (1 + avgOffsetGap.get(kafkaSourceDescription.getTopicPartition()).get());\n+      }\n+      return (!avgRecordSize.containsKey(kafkaSourceDescription.getTopicPartition())\n+              ? 1\n+              : avgRecordSize.get(kafkaSourceDescription.getTopicPartition()).get())\n+          * numOfRecords;\n+    }\n+\n+    @NewTracker\n+    public RestrictionTracker<OffsetRange, Long> restrictionTracker(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        @Restriction OffsetRange restriction) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      KafkaLatestOffsetEstimator offsetPoller =\n+          new KafkaLatestOffsetEstimator(\n+              consumerFactoryFn.apply(\n+                  KafkaIOUtils.getOffsetConsumerConfig(\n+                      \"tracker-\" + kafkaSourceDescription.getTopicPartition(),\n+                      offsetConsumerConfig,\n+                      updatedConsumerConfig)),\n+              kafkaSourceDescription.getTopicPartition());\n+      return new GrowableOffsetRangeTracker(restriction.getFrom(), offsetPoller);\n+    }\n+\n+    @ProcessElement\n+    public ProcessContinuation processElement(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        RestrictionTracker<OffsetRange, Long> tracker,\n+        WatermarkEstimator watermarkEstimator,\n+        OutputReceiver<KafkaRecord<K, V>> receiver) {\n+      // If there is no future work, resume with max timeout and move to the next element.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 741}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM4MjQyNQ==", "bodyText": "I know I suggested this but am wary now. I think it won't be as performant but checking to see if there are any messages seems like the safer bet (effectively the logic you had before).", "url": "https://github.com/apache/beam/pull/11749#discussion_r444382425", "createdAt": "2020-06-23T17:16:07Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,861 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaSourceDescription.Schemas;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.Manual;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.WallTime;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the <a href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The {@link WatermarkEstimator} is created by {@link #getCreateWatermarkEstimatorFn()}. The\n+ * estimated watermark is computed by this {@link WatermarkEstimator} based on output timestamps\n+ * computed by {@link #getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link #withProcessingTime()} as {@code extractTimestampFn} and {@link\n+ * #withMonotonicallyIncreasingWatermarkEstimator()} as {@link WatermarkEstimator}.\n+ */\n+@Experimental(Kind.PORTABILITY)\n+@AutoValue\n+abstract class ReadViaSDF<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<Instant, WatermarkEstimatorT> getCreateWatermarkEstimatorFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  @Nullable\n+  abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+  abstract Builder<K, V, WatermarkEstimatorT> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>> {\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setOffsetConsumerConfig(\n+        Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCreateWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimatorT> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setTimestampPolicyFactory(\n+        TimestampPolicyFactory<K, V> policy);\n+\n+    abstract ReadViaSDF<K, V, WatermarkEstimatorT> build();\n+  }\n+\n+  public static <K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      ReadViaSDF<K, V, WatermarkEstimatorT> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V, WatermarkEstimatorT>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setCommitOffsetEnabled(false)\n+        .build()\n+        .withProcessingTime()\n+        .withMonotonicallyIncreasingWatermarkEstimator();\n+  }\n+\n+  // Note that if the bootstrapServers is set here but also populated with the element, the element\n+  // will override the bootstrapServers from the config.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializer(\n+      Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigUpdates(\n+      Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreatWatermarkEstimatorFn(\n+      SerializableFunction<Instant, WatermarkEstimatorT> fn) {\n+    return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withWallTimeWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new WallTime(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withMonotonicallyIncreasingWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new MonotonicallyIncreasing(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withManualWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new Manual(state);\n+        });\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigOverrides(\n+      Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  ReadViaSDFExternally forExternalBuild() {\n+    return new ReadViaSDFExternally(this);\n+  }\n+\n+  private static class ReadViaSDFExternally<\n+          K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends PTransform<PCollection<Row>, PCollection<KV<K, V>>> {\n+\n+    private final ReadViaSDF<K, V, WatermarkEstimatorT> readViaSDF;\n+\n+    ReadViaSDFExternally(ReadViaSDF read) {\n+      readViaSDF = read;\n+    }\n+\n+    @Override\n+    public PCollection<KV<K, V>> expand(PCollection<Row> input) {\n+      return input\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<Row, KafkaSourceDescription>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element Row row, OutputReceiver<KafkaSourceDescription> outputReceiver) {\n+                      TopicPartition topicPartition =\n+                          new TopicPartition(\n+                              row.getString(Schemas.TOPIC), row.getInt32(Schemas.PARTITION));\n+                      Instant startReadTime =\n+                          row.getInt64(Schemas.START_READ_TIME) != null\n+                              ? Instant.ofEpochMilli(row.getInt64(Schemas.START_READ_TIME))\n+                              : null;\n+                      outputReceiver.output(\n+                          KafkaSourceDescription.of(\n+                              topicPartition,\n+                              row.getInt64(Schemas.START_READ_OFFSET),\n+                              startReadTime,\n+                              new ArrayList<>(row.getArray(Schemas.BOOTSTRAP_SERVERS))));\n+                    }\n+                  }))\n+          .apply(readViaSDF)\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<KafkaRecord<K, V>, KV<K, V>>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element KafkaRecord element, OutputReceiver<KV<K, V>> outputReceiver) {\n+                      outputReceiver.output(element.getKV());\n+                    }\n+                  }))\n+          .setCoder(KvCoder.<K, V>of(readViaSDF.getKeyCoder(), readViaSDF.getValueCoder()));\n+    }\n+  }\n+\n+  ReadViaSDF<K, V, WatermarkEstimatorT> withTimestampPolicyFactory(\n+      TimestampPolicyFactory<K, V> timestampPolicyFactory) {\n+    return toBuilder().setTimestampPolicyFactory(timestampPolicyFactory).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<KafkaSourceDescription> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input\n+            .apply(ParDo.of(new ReadFromKafkaDoFn<K, V, WatermarkEstimatorT>(this)))\n+            .setCoder(outputCoder);\n+    // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      throw new IllegalStateException(\"Offset committed is not supported yet\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link KafkaSourceDescription} and outputs {@link\n+   * KafkaRecord}. By default, a {@link MonotonicallyIncreasing} watermark estimator is used to\n+   * track watermark.\n+   */\n+  @VisibleForTesting\n+  @UnboundedPerElement\n+  static class ReadFromKafkaDoFn<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends DoFn<KafkaSourceDescription, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn(ReadViaSDF transform) {\n+      this.consumerConfig = transform.getConsumerConfig();\n+      this.offsetConsumerConfig = transform.getOffsetConsumerConfig();\n+      this.keyDeserializerProvider = transform.getKeyDeserializerProvider();\n+      this.valueDeserializerProvider = transform.getValueDeserializerProvider();\n+      this.consumerFactoryFn = transform.getConsumerFactoryFn();\n+      this.extractOutputTimestampFn = transform.getExtractOutputTimestampFn();\n+      this.createWatermarkEstimatorFn = transform.getCreateWatermarkEstimatorFn();\n+      this.timestampPolicyFactory = transform.getTimestampPolicyFactory();\n+    }\n+\n+    private final Map<String, Object> offsetConsumerConfig;\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn;\n+    private final SerializableFunction<Instant, WatermarkEstimatorT> createWatermarkEstimatorFn;\n+    private final TimestampPolicyFactory<K, V> timestampPolicyFactory;\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgRecordSize;\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgOffsetGap;\n+\n+    private static final Duration KAFKA_POLL_TIMEOUT = Duration.millis(1000);\n+\n+    @VisibleForTesting final DeserializerProvider keyDeserializerProvider;\n+    @VisibleForTesting final DeserializerProvider valueDeserializerProvider;\n+    @VisibleForTesting final Map<String, Object> consumerConfig;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private static class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+      private final Supplier<Long> memorizedBacklog;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+        memorizedBacklog =\n+            Suppliers.memoizeWithExpiration(\n+                () -> {\n+                  consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+                  return offsetConsumer.position(topicPartition);\n+                },\n+                5,\n+                TimeUnit.SECONDS);\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        return memorizedBacklog.get();\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element KafkaSourceDescription kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        consumerSpEL.evaluateAssign(\n+            offsetConsumer, ImmutableList.of(kafkaSourceDescription.getTopicPartition()));\n+        long startOffset;\n+        if (kafkaSourceDescription.getStartReadOffset() != null) {\n+          startOffset = kafkaSourceDescription.getStartReadOffset();\n+        } else if (kafkaSourceDescription.getStartReadTime() != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  kafkaSourceDescription.getTopicPartition(),\n+                  kafkaSourceDescription.getStartReadTime());\n+        } else {\n+          startOffset = offsetConsumer.position(kafkaSourceDescription.getTopicPartition());\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public WatermarkEstimatorT newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return createWatermarkEstimatorFn.apply(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        @Restriction OffsetRange offsetRange)\n+        throws Exception {\n+      double numOfRecords =\n+          ((HasProgress) restrictionTracker(kafkaSourceDescription, offsetRange))\n+              .getProgress()\n+              .getWorkRemaining();\n+\n+      // Before processing elements, we don't have a good estimated size of records and offset gap.\n+      if (avgOffsetGap.containsKey(kafkaSourceDescription.getTopicPartition())) {\n+        numOfRecords =\n+            numOfRecords / (1 + avgOffsetGap.get(kafkaSourceDescription.getTopicPartition()).get());\n+      }\n+      return (!avgRecordSize.containsKey(kafkaSourceDescription.getTopicPartition())\n+              ? 1\n+              : avgRecordSize.get(kafkaSourceDescription.getTopicPartition()).get())\n+          * numOfRecords;\n+    }\n+\n+    @NewTracker\n+    public RestrictionTracker<OffsetRange, Long> restrictionTracker(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        @Restriction OffsetRange restriction) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      KafkaLatestOffsetEstimator offsetPoller =\n+          new KafkaLatestOffsetEstimator(\n+              consumerFactoryFn.apply(\n+                  KafkaIOUtils.getOffsetConsumerConfig(\n+                      \"tracker-\" + kafkaSourceDescription.getTopicPartition(),\n+                      offsetConsumerConfig,\n+                      updatedConsumerConfig)),\n+              kafkaSourceDescription.getTopicPartition());\n+      return new GrowableOffsetRangeTracker(restriction.getFrom(), offsetPoller);\n+    }\n+\n+    @ProcessElement\n+    public ProcessContinuation processElement(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        RestrictionTracker<OffsetRange, Long> tracker,\n+        WatermarkEstimator watermarkEstimator,\n+        OutputReceiver<KafkaRecord<K, V>> receiver) {\n+      // If there is no future work, resume with max timeout and move to the next element.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM4MTYxNg=="}, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 741}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2ODg4NzQ3OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzoxNzowOFrOGny_MQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzoxNzowOFrOGny_MQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM4MzAyNQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        // When there is no records from the current TopicPartition temporarily, self-checkpoint\n          \n          \n            \n                        // and move to process the next element.\n          \n          \n            \n                        // When there are no records available for the current TopicPartition, self-checkpoint\n          \n          \n            \n                        // and move to process the next element.", "url": "https://github.com/apache/beam/pull/11749#discussion_r444383025", "createdAt": "2020-06-23T17:17:08Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,861 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaSourceDescription.Schemas;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.Manual;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.WallTime;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the <a href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The {@link WatermarkEstimator} is created by {@link #getCreateWatermarkEstimatorFn()}. The\n+ * estimated watermark is computed by this {@link WatermarkEstimator} based on output timestamps\n+ * computed by {@link #getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link #withProcessingTime()} as {@code extractTimestampFn} and {@link\n+ * #withMonotonicallyIncreasingWatermarkEstimator()} as {@link WatermarkEstimator}.\n+ */\n+@Experimental(Kind.PORTABILITY)\n+@AutoValue\n+abstract class ReadViaSDF<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<Instant, WatermarkEstimatorT> getCreateWatermarkEstimatorFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  @Nullable\n+  abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+  abstract Builder<K, V, WatermarkEstimatorT> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>> {\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setOffsetConsumerConfig(\n+        Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCreateWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimatorT> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setTimestampPolicyFactory(\n+        TimestampPolicyFactory<K, V> policy);\n+\n+    abstract ReadViaSDF<K, V, WatermarkEstimatorT> build();\n+  }\n+\n+  public static <K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      ReadViaSDF<K, V, WatermarkEstimatorT> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V, WatermarkEstimatorT>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setCommitOffsetEnabled(false)\n+        .build()\n+        .withProcessingTime()\n+        .withMonotonicallyIncreasingWatermarkEstimator();\n+  }\n+\n+  // Note that if the bootstrapServers is set here but also populated with the element, the element\n+  // will override the bootstrapServers from the config.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializer(\n+      Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigUpdates(\n+      Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreatWatermarkEstimatorFn(\n+      SerializableFunction<Instant, WatermarkEstimatorT> fn) {\n+    return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withWallTimeWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new WallTime(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withMonotonicallyIncreasingWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new MonotonicallyIncreasing(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withManualWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new Manual(state);\n+        });\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigOverrides(\n+      Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  ReadViaSDFExternally forExternalBuild() {\n+    return new ReadViaSDFExternally(this);\n+  }\n+\n+  private static class ReadViaSDFExternally<\n+          K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends PTransform<PCollection<Row>, PCollection<KV<K, V>>> {\n+\n+    private final ReadViaSDF<K, V, WatermarkEstimatorT> readViaSDF;\n+\n+    ReadViaSDFExternally(ReadViaSDF read) {\n+      readViaSDF = read;\n+    }\n+\n+    @Override\n+    public PCollection<KV<K, V>> expand(PCollection<Row> input) {\n+      return input\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<Row, KafkaSourceDescription>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element Row row, OutputReceiver<KafkaSourceDescription> outputReceiver) {\n+                      TopicPartition topicPartition =\n+                          new TopicPartition(\n+                              row.getString(Schemas.TOPIC), row.getInt32(Schemas.PARTITION));\n+                      Instant startReadTime =\n+                          row.getInt64(Schemas.START_READ_TIME) != null\n+                              ? Instant.ofEpochMilli(row.getInt64(Schemas.START_READ_TIME))\n+                              : null;\n+                      outputReceiver.output(\n+                          KafkaSourceDescription.of(\n+                              topicPartition,\n+                              row.getInt64(Schemas.START_READ_OFFSET),\n+                              startReadTime,\n+                              new ArrayList<>(row.getArray(Schemas.BOOTSTRAP_SERVERS))));\n+                    }\n+                  }))\n+          .apply(readViaSDF)\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<KafkaRecord<K, V>, KV<K, V>>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element KafkaRecord element, OutputReceiver<KV<K, V>> outputReceiver) {\n+                      outputReceiver.output(element.getKV());\n+                    }\n+                  }))\n+          .setCoder(KvCoder.<K, V>of(readViaSDF.getKeyCoder(), readViaSDF.getValueCoder()));\n+    }\n+  }\n+\n+  ReadViaSDF<K, V, WatermarkEstimatorT> withTimestampPolicyFactory(\n+      TimestampPolicyFactory<K, V> timestampPolicyFactory) {\n+    return toBuilder().setTimestampPolicyFactory(timestampPolicyFactory).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<KafkaSourceDescription> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input\n+            .apply(ParDo.of(new ReadFromKafkaDoFn<K, V, WatermarkEstimatorT>(this)))\n+            .setCoder(outputCoder);\n+    // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      throw new IllegalStateException(\"Offset committed is not supported yet\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link KafkaSourceDescription} and outputs {@link\n+   * KafkaRecord}. By default, a {@link MonotonicallyIncreasing} watermark estimator is used to\n+   * track watermark.\n+   */\n+  @VisibleForTesting\n+  @UnboundedPerElement\n+  static class ReadFromKafkaDoFn<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends DoFn<KafkaSourceDescription, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn(ReadViaSDF transform) {\n+      this.consumerConfig = transform.getConsumerConfig();\n+      this.offsetConsumerConfig = transform.getOffsetConsumerConfig();\n+      this.keyDeserializerProvider = transform.getKeyDeserializerProvider();\n+      this.valueDeserializerProvider = transform.getValueDeserializerProvider();\n+      this.consumerFactoryFn = transform.getConsumerFactoryFn();\n+      this.extractOutputTimestampFn = transform.getExtractOutputTimestampFn();\n+      this.createWatermarkEstimatorFn = transform.getCreateWatermarkEstimatorFn();\n+      this.timestampPolicyFactory = transform.getTimestampPolicyFactory();\n+    }\n+\n+    private final Map<String, Object> offsetConsumerConfig;\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn;\n+    private final SerializableFunction<Instant, WatermarkEstimatorT> createWatermarkEstimatorFn;\n+    private final TimestampPolicyFactory<K, V> timestampPolicyFactory;\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgRecordSize;\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgOffsetGap;\n+\n+    private static final Duration KAFKA_POLL_TIMEOUT = Duration.millis(1000);\n+\n+    @VisibleForTesting final DeserializerProvider keyDeserializerProvider;\n+    @VisibleForTesting final DeserializerProvider valueDeserializerProvider;\n+    @VisibleForTesting final Map<String, Object> consumerConfig;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private static class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+      private final Supplier<Long> memorizedBacklog;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+        memorizedBacklog =\n+            Suppliers.memoizeWithExpiration(\n+                () -> {\n+                  consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+                  return offsetConsumer.position(topicPartition);\n+                },\n+                5,\n+                TimeUnit.SECONDS);\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        return memorizedBacklog.get();\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element KafkaSourceDescription kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        consumerSpEL.evaluateAssign(\n+            offsetConsumer, ImmutableList.of(kafkaSourceDescription.getTopicPartition()));\n+        long startOffset;\n+        if (kafkaSourceDescription.getStartReadOffset() != null) {\n+          startOffset = kafkaSourceDescription.getStartReadOffset();\n+        } else if (kafkaSourceDescription.getStartReadTime() != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  kafkaSourceDescription.getTopicPartition(),\n+                  kafkaSourceDescription.getStartReadTime());\n+        } else {\n+          startOffset = offsetConsumer.position(kafkaSourceDescription.getTopicPartition());\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public WatermarkEstimatorT newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return createWatermarkEstimatorFn.apply(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        @Restriction OffsetRange offsetRange)\n+        throws Exception {\n+      double numOfRecords =\n+          ((HasProgress) restrictionTracker(kafkaSourceDescription, offsetRange))\n+              .getProgress()\n+              .getWorkRemaining();\n+\n+      // Before processing elements, we don't have a good estimated size of records and offset gap.\n+      if (avgOffsetGap.containsKey(kafkaSourceDescription.getTopicPartition())) {\n+        numOfRecords =\n+            numOfRecords / (1 + avgOffsetGap.get(kafkaSourceDescription.getTopicPartition()).get());\n+      }\n+      return (!avgRecordSize.containsKey(kafkaSourceDescription.getTopicPartition())\n+              ? 1\n+              : avgRecordSize.get(kafkaSourceDescription.getTopicPartition()).get())\n+          * numOfRecords;\n+    }\n+\n+    @NewTracker\n+    public RestrictionTracker<OffsetRange, Long> restrictionTracker(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        @Restriction OffsetRange restriction) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      KafkaLatestOffsetEstimator offsetPoller =\n+          new KafkaLatestOffsetEstimator(\n+              consumerFactoryFn.apply(\n+                  KafkaIOUtils.getOffsetConsumerConfig(\n+                      \"tracker-\" + kafkaSourceDescription.getTopicPartition(),\n+                      offsetConsumerConfig,\n+                      updatedConsumerConfig)),\n+              kafkaSourceDescription.getTopicPartition());\n+      return new GrowableOffsetRangeTracker(restriction.getFrom(), offsetPoller);\n+    }\n+\n+    @ProcessElement\n+    public ProcessContinuation processElement(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        RestrictionTracker<OffsetRange, Long> tracker,\n+        WatermarkEstimator watermarkEstimator,\n+        OutputReceiver<KafkaRecord<K, V>> receiver) {\n+      // If there is no future work, resume with max timeout and move to the next element.\n+      if (((HasProgress) tracker).getProgress().getWorkRemaining() <= 0.0) {\n+        return ProcessContinuation.resume().withResumeDelay(KAFKA_POLL_TIMEOUT);\n+      }\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      // If there is a timestampPolicyFactory, create the TimestampPolicy for current\n+      // TopicPartition.\n+      TimestampPolicy timestampPolicy = null;\n+      if (timestampPolicyFactory != null) {\n+        timestampPolicy =\n+            timestampPolicyFactory.createTimestampPolicy(\n+                kafkaSourceDescription.getTopicPartition(),\n+                Optional.ofNullable(watermarkEstimator.currentWatermark()));\n+      }\n+      try (Consumer<byte[], byte[]> consumer = consumerFactoryFn.apply(updatedConsumerConfig)) {\n+        consumerSpEL.evaluateAssign(\n+            consumer, ImmutableList.of(kafkaSourceDescription.getTopicPartition()));\n+        long startOffset = tracker.currentRestriction().getFrom();\n+        long expectedOffset = startOffset;\n+        consumer.seek(kafkaSourceDescription.getTopicPartition(), startOffset);\n+        ConsumerRecords<byte[], byte[]> rawRecords = ConsumerRecords.empty();\n+\n+        try {\n+          while (true) {\n+            rawRecords = consumer.poll(KAFKA_POLL_TIMEOUT.getMillis());\n+            // When there is no records from the current TopicPartition temporarily, self-checkpoint\n+            // and move to process the next element.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 768}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2ODg5Njg0OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzoxOTo1M1rOGnzFMQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNToxMjo1NFrOGoDLBQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM4NDU2MQ==", "bodyText": "If we update the expectedOffset to be currentOffset + 1, won't the expectedOffset - nextOffset typically be 0 when there are no gaps?", "url": "https://github.com/apache/beam/pull/11749#discussion_r444384561", "createdAt": "2020-06-23T17:19:53Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,861 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaSourceDescription.Schemas;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.Manual;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.WallTime;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the <a href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The {@link WatermarkEstimator} is created by {@link #getCreateWatermarkEstimatorFn()}. The\n+ * estimated watermark is computed by this {@link WatermarkEstimator} based on output timestamps\n+ * computed by {@link #getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link #withProcessingTime()} as {@code extractTimestampFn} and {@link\n+ * #withMonotonicallyIncreasingWatermarkEstimator()} as {@link WatermarkEstimator}.\n+ */\n+@Experimental(Kind.PORTABILITY)\n+@AutoValue\n+abstract class ReadViaSDF<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<Instant, WatermarkEstimatorT> getCreateWatermarkEstimatorFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  @Nullable\n+  abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+  abstract Builder<K, V, WatermarkEstimatorT> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>> {\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setOffsetConsumerConfig(\n+        Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCreateWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimatorT> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setTimestampPolicyFactory(\n+        TimestampPolicyFactory<K, V> policy);\n+\n+    abstract ReadViaSDF<K, V, WatermarkEstimatorT> build();\n+  }\n+\n+  public static <K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      ReadViaSDF<K, V, WatermarkEstimatorT> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V, WatermarkEstimatorT>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setCommitOffsetEnabled(false)\n+        .build()\n+        .withProcessingTime()\n+        .withMonotonicallyIncreasingWatermarkEstimator();\n+  }\n+\n+  // Note that if the bootstrapServers is set here but also populated with the element, the element\n+  // will override the bootstrapServers from the config.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializer(\n+      Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigUpdates(\n+      Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreatWatermarkEstimatorFn(\n+      SerializableFunction<Instant, WatermarkEstimatorT> fn) {\n+    return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withWallTimeWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new WallTime(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withMonotonicallyIncreasingWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new MonotonicallyIncreasing(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withManualWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new Manual(state);\n+        });\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigOverrides(\n+      Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  ReadViaSDFExternally forExternalBuild() {\n+    return new ReadViaSDFExternally(this);\n+  }\n+\n+  private static class ReadViaSDFExternally<\n+          K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends PTransform<PCollection<Row>, PCollection<KV<K, V>>> {\n+\n+    private final ReadViaSDF<K, V, WatermarkEstimatorT> readViaSDF;\n+\n+    ReadViaSDFExternally(ReadViaSDF read) {\n+      readViaSDF = read;\n+    }\n+\n+    @Override\n+    public PCollection<KV<K, V>> expand(PCollection<Row> input) {\n+      return input\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<Row, KafkaSourceDescription>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element Row row, OutputReceiver<KafkaSourceDescription> outputReceiver) {\n+                      TopicPartition topicPartition =\n+                          new TopicPartition(\n+                              row.getString(Schemas.TOPIC), row.getInt32(Schemas.PARTITION));\n+                      Instant startReadTime =\n+                          row.getInt64(Schemas.START_READ_TIME) != null\n+                              ? Instant.ofEpochMilli(row.getInt64(Schemas.START_READ_TIME))\n+                              : null;\n+                      outputReceiver.output(\n+                          KafkaSourceDescription.of(\n+                              topicPartition,\n+                              row.getInt64(Schemas.START_READ_OFFSET),\n+                              startReadTime,\n+                              new ArrayList<>(row.getArray(Schemas.BOOTSTRAP_SERVERS))));\n+                    }\n+                  }))\n+          .apply(readViaSDF)\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<KafkaRecord<K, V>, KV<K, V>>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element KafkaRecord element, OutputReceiver<KV<K, V>> outputReceiver) {\n+                      outputReceiver.output(element.getKV());\n+                    }\n+                  }))\n+          .setCoder(KvCoder.<K, V>of(readViaSDF.getKeyCoder(), readViaSDF.getValueCoder()));\n+    }\n+  }\n+\n+  ReadViaSDF<K, V, WatermarkEstimatorT> withTimestampPolicyFactory(\n+      TimestampPolicyFactory<K, V> timestampPolicyFactory) {\n+    return toBuilder().setTimestampPolicyFactory(timestampPolicyFactory).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<KafkaSourceDescription> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input\n+            .apply(ParDo.of(new ReadFromKafkaDoFn<K, V, WatermarkEstimatorT>(this)))\n+            .setCoder(outputCoder);\n+    // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      throw new IllegalStateException(\"Offset committed is not supported yet\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link KafkaSourceDescription} and outputs {@link\n+   * KafkaRecord}. By default, a {@link MonotonicallyIncreasing} watermark estimator is used to\n+   * track watermark.\n+   */\n+  @VisibleForTesting\n+  @UnboundedPerElement\n+  static class ReadFromKafkaDoFn<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends DoFn<KafkaSourceDescription, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn(ReadViaSDF transform) {\n+      this.consumerConfig = transform.getConsumerConfig();\n+      this.offsetConsumerConfig = transform.getOffsetConsumerConfig();\n+      this.keyDeserializerProvider = transform.getKeyDeserializerProvider();\n+      this.valueDeserializerProvider = transform.getValueDeserializerProvider();\n+      this.consumerFactoryFn = transform.getConsumerFactoryFn();\n+      this.extractOutputTimestampFn = transform.getExtractOutputTimestampFn();\n+      this.createWatermarkEstimatorFn = transform.getCreateWatermarkEstimatorFn();\n+      this.timestampPolicyFactory = transform.getTimestampPolicyFactory();\n+    }\n+\n+    private final Map<String, Object> offsetConsumerConfig;\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn;\n+    private final SerializableFunction<Instant, WatermarkEstimatorT> createWatermarkEstimatorFn;\n+    private final TimestampPolicyFactory<K, V> timestampPolicyFactory;\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgRecordSize;\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgOffsetGap;\n+\n+    private static final Duration KAFKA_POLL_TIMEOUT = Duration.millis(1000);\n+\n+    @VisibleForTesting final DeserializerProvider keyDeserializerProvider;\n+    @VisibleForTesting final DeserializerProvider valueDeserializerProvider;\n+    @VisibleForTesting final Map<String, Object> consumerConfig;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private static class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+      private final Supplier<Long> memorizedBacklog;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+        memorizedBacklog =\n+            Suppliers.memoizeWithExpiration(\n+                () -> {\n+                  consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+                  return offsetConsumer.position(topicPartition);\n+                },\n+                5,\n+                TimeUnit.SECONDS);\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        return memorizedBacklog.get();\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element KafkaSourceDescription kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        consumerSpEL.evaluateAssign(\n+            offsetConsumer, ImmutableList.of(kafkaSourceDescription.getTopicPartition()));\n+        long startOffset;\n+        if (kafkaSourceDescription.getStartReadOffset() != null) {\n+          startOffset = kafkaSourceDescription.getStartReadOffset();\n+        } else if (kafkaSourceDescription.getStartReadTime() != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  kafkaSourceDescription.getTopicPartition(),\n+                  kafkaSourceDescription.getStartReadTime());\n+        } else {\n+          startOffset = offsetConsumer.position(kafkaSourceDescription.getTopicPartition());\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public WatermarkEstimatorT newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return createWatermarkEstimatorFn.apply(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        @Restriction OffsetRange offsetRange)\n+        throws Exception {\n+      double numOfRecords =\n+          ((HasProgress) restrictionTracker(kafkaSourceDescription, offsetRange))\n+              .getProgress()\n+              .getWorkRemaining();\n+\n+      // Before processing elements, we don't have a good estimated size of records and offset gap.\n+      if (avgOffsetGap.containsKey(kafkaSourceDescription.getTopicPartition())) {\n+        numOfRecords =\n+            numOfRecords / (1 + avgOffsetGap.get(kafkaSourceDescription.getTopicPartition()).get());\n+      }\n+      return (!avgRecordSize.containsKey(kafkaSourceDescription.getTopicPartition())\n+              ? 1\n+              : avgRecordSize.get(kafkaSourceDescription.getTopicPartition()).get())\n+          * numOfRecords;\n+    }\n+\n+    @NewTracker\n+    public RestrictionTracker<OffsetRange, Long> restrictionTracker(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        @Restriction OffsetRange restriction) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      KafkaLatestOffsetEstimator offsetPoller =\n+          new KafkaLatestOffsetEstimator(\n+              consumerFactoryFn.apply(\n+                  KafkaIOUtils.getOffsetConsumerConfig(\n+                      \"tracker-\" + kafkaSourceDescription.getTopicPartition(),\n+                      offsetConsumerConfig,\n+                      updatedConsumerConfig)),\n+              kafkaSourceDescription.getTopicPartition());\n+      return new GrowableOffsetRangeTracker(restriction.getFrom(), offsetPoller);\n+    }\n+\n+    @ProcessElement\n+    public ProcessContinuation processElement(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        RestrictionTracker<OffsetRange, Long> tracker,\n+        WatermarkEstimator watermarkEstimator,\n+        OutputReceiver<KafkaRecord<K, V>> receiver) {\n+      // If there is no future work, resume with max timeout and move to the next element.\n+      if (((HasProgress) tracker).getProgress().getWorkRemaining() <= 0.0) {\n+        return ProcessContinuation.resume().withResumeDelay(KAFKA_POLL_TIMEOUT);\n+      }\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      // If there is a timestampPolicyFactory, create the TimestampPolicy for current\n+      // TopicPartition.\n+      TimestampPolicy timestampPolicy = null;\n+      if (timestampPolicyFactory != null) {\n+        timestampPolicy =\n+            timestampPolicyFactory.createTimestampPolicy(\n+                kafkaSourceDescription.getTopicPartition(),\n+                Optional.ofNullable(watermarkEstimator.currentWatermark()));\n+      }\n+      try (Consumer<byte[], byte[]> consumer = consumerFactoryFn.apply(updatedConsumerConfig)) {\n+        consumerSpEL.evaluateAssign(\n+            consumer, ImmutableList.of(kafkaSourceDescription.getTopicPartition()));\n+        long startOffset = tracker.currentRestriction().getFrom();\n+        long expectedOffset = startOffset;\n+        consumer.seek(kafkaSourceDescription.getTopicPartition(), startOffset);\n+        ConsumerRecords<byte[], byte[]> rawRecords = ConsumerRecords.empty();\n+\n+        try {\n+          while (true) {\n+            rawRecords = consumer.poll(KAFKA_POLL_TIMEOUT.getMillis());\n+            // When there is no records from the current TopicPartition temporarily, self-checkpoint\n+            // and move to process the next element.\n+            if (rawRecords.isEmpty()) {\n+              return ProcessContinuation.resume();\n+            }\n+            for (ConsumerRecord<byte[], byte[]> rawRecord : rawRecords) {\n+              if (!tracker.tryClaim(rawRecord.offset())) {\n+                return ProcessContinuation.stop();\n+              }\n+              KafkaRecord<K, V> kafkaRecord =\n+                  new KafkaRecord<>(\n+                      rawRecord.topic(),\n+                      rawRecord.partition(),\n+                      rawRecord.offset(),\n+                      consumerSpEL.getRecordTimestamp(rawRecord),\n+                      consumerSpEL.getRecordTimestampType(rawRecord),\n+                      ConsumerSpEL.hasHeaders() ? rawRecord.headers() : null,\n+                      keyDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.key()),\n+                      valueDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.value()));\n+              int recordSize =\n+                  (rawRecord.key() == null ? 0 : rawRecord.key().length)\n+                      + (rawRecord.value() == null ? 0 : rawRecord.value().length);\n+              avgOffsetGap\n+                  .computeIfAbsent(kafkaSourceDescription.getTopicPartition(), k -> new MovingAvg())\n+                  .update(expectedOffset - rawRecord.offset());\n+              avgRecordSize\n+                  .computeIfAbsent(kafkaSourceDescription.getTopicPartition(), k -> new MovingAvg())\n+                  .update(recordSize);\n+              expectedOffset = rawRecord.offset() + 1;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 795}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY0ODE5Nw==", "bodyText": "Yes, I think that should be the case. For example, at this moment we get a record with offset 1. Then we expect that the next offset with gap should be 2.", "url": "https://github.com/apache/beam/pull/11749#discussion_r444648197", "createdAt": "2020-06-24T05:12:54Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,861 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaSourceDescription.Schemas;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.Manual;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.WallTime;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the <a href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The {@link WatermarkEstimator} is created by {@link #getCreateWatermarkEstimatorFn()}. The\n+ * estimated watermark is computed by this {@link WatermarkEstimator} based on output timestamps\n+ * computed by {@link #getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link #withProcessingTime()} as {@code extractTimestampFn} and {@link\n+ * #withMonotonicallyIncreasingWatermarkEstimator()} as {@link WatermarkEstimator}.\n+ */\n+@Experimental(Kind.PORTABILITY)\n+@AutoValue\n+abstract class ReadViaSDF<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<Instant, WatermarkEstimatorT> getCreateWatermarkEstimatorFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  @Nullable\n+  abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+  abstract Builder<K, V, WatermarkEstimatorT> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>> {\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setOffsetConsumerConfig(\n+        Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCreateWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimatorT> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setTimestampPolicyFactory(\n+        TimestampPolicyFactory<K, V> policy);\n+\n+    abstract ReadViaSDF<K, V, WatermarkEstimatorT> build();\n+  }\n+\n+  public static <K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      ReadViaSDF<K, V, WatermarkEstimatorT> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V, WatermarkEstimatorT>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setCommitOffsetEnabled(false)\n+        .build()\n+        .withProcessingTime()\n+        .withMonotonicallyIncreasingWatermarkEstimator();\n+  }\n+\n+  // Note that if the bootstrapServers is set here but also populated with the element, the element\n+  // will override the bootstrapServers from the config.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializer(\n+      Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigUpdates(\n+      Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreatWatermarkEstimatorFn(\n+      SerializableFunction<Instant, WatermarkEstimatorT> fn) {\n+    return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withWallTimeWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new WallTime(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withMonotonicallyIncreasingWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new MonotonicallyIncreasing(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withManualWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new Manual(state);\n+        });\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigOverrides(\n+      Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  ReadViaSDFExternally forExternalBuild() {\n+    return new ReadViaSDFExternally(this);\n+  }\n+\n+  private static class ReadViaSDFExternally<\n+          K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends PTransform<PCollection<Row>, PCollection<KV<K, V>>> {\n+\n+    private final ReadViaSDF<K, V, WatermarkEstimatorT> readViaSDF;\n+\n+    ReadViaSDFExternally(ReadViaSDF read) {\n+      readViaSDF = read;\n+    }\n+\n+    @Override\n+    public PCollection<KV<K, V>> expand(PCollection<Row> input) {\n+      return input\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<Row, KafkaSourceDescription>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element Row row, OutputReceiver<KafkaSourceDescription> outputReceiver) {\n+                      TopicPartition topicPartition =\n+                          new TopicPartition(\n+                              row.getString(Schemas.TOPIC), row.getInt32(Schemas.PARTITION));\n+                      Instant startReadTime =\n+                          row.getInt64(Schemas.START_READ_TIME) != null\n+                              ? Instant.ofEpochMilli(row.getInt64(Schemas.START_READ_TIME))\n+                              : null;\n+                      outputReceiver.output(\n+                          KafkaSourceDescription.of(\n+                              topicPartition,\n+                              row.getInt64(Schemas.START_READ_OFFSET),\n+                              startReadTime,\n+                              new ArrayList<>(row.getArray(Schemas.BOOTSTRAP_SERVERS))));\n+                    }\n+                  }))\n+          .apply(readViaSDF)\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<KafkaRecord<K, V>, KV<K, V>>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element KafkaRecord element, OutputReceiver<KV<K, V>> outputReceiver) {\n+                      outputReceiver.output(element.getKV());\n+                    }\n+                  }))\n+          .setCoder(KvCoder.<K, V>of(readViaSDF.getKeyCoder(), readViaSDF.getValueCoder()));\n+    }\n+  }\n+\n+  ReadViaSDF<K, V, WatermarkEstimatorT> withTimestampPolicyFactory(\n+      TimestampPolicyFactory<K, V> timestampPolicyFactory) {\n+    return toBuilder().setTimestampPolicyFactory(timestampPolicyFactory).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<KafkaSourceDescription> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input\n+            .apply(ParDo.of(new ReadFromKafkaDoFn<K, V, WatermarkEstimatorT>(this)))\n+            .setCoder(outputCoder);\n+    // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      throw new IllegalStateException(\"Offset committed is not supported yet\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link KafkaSourceDescription} and outputs {@link\n+   * KafkaRecord}. By default, a {@link MonotonicallyIncreasing} watermark estimator is used to\n+   * track watermark.\n+   */\n+  @VisibleForTesting\n+  @UnboundedPerElement\n+  static class ReadFromKafkaDoFn<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends DoFn<KafkaSourceDescription, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn(ReadViaSDF transform) {\n+      this.consumerConfig = transform.getConsumerConfig();\n+      this.offsetConsumerConfig = transform.getOffsetConsumerConfig();\n+      this.keyDeserializerProvider = transform.getKeyDeserializerProvider();\n+      this.valueDeserializerProvider = transform.getValueDeserializerProvider();\n+      this.consumerFactoryFn = transform.getConsumerFactoryFn();\n+      this.extractOutputTimestampFn = transform.getExtractOutputTimestampFn();\n+      this.createWatermarkEstimatorFn = transform.getCreateWatermarkEstimatorFn();\n+      this.timestampPolicyFactory = transform.getTimestampPolicyFactory();\n+    }\n+\n+    private final Map<String, Object> offsetConsumerConfig;\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn;\n+    private final SerializableFunction<Instant, WatermarkEstimatorT> createWatermarkEstimatorFn;\n+    private final TimestampPolicyFactory<K, V> timestampPolicyFactory;\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgRecordSize;\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgOffsetGap;\n+\n+    private static final Duration KAFKA_POLL_TIMEOUT = Duration.millis(1000);\n+\n+    @VisibleForTesting final DeserializerProvider keyDeserializerProvider;\n+    @VisibleForTesting final DeserializerProvider valueDeserializerProvider;\n+    @VisibleForTesting final Map<String, Object> consumerConfig;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private static class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+      private final Supplier<Long> memorizedBacklog;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+        memorizedBacklog =\n+            Suppliers.memoizeWithExpiration(\n+                () -> {\n+                  consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+                  return offsetConsumer.position(topicPartition);\n+                },\n+                5,\n+                TimeUnit.SECONDS);\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        return memorizedBacklog.get();\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element KafkaSourceDescription kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        consumerSpEL.evaluateAssign(\n+            offsetConsumer, ImmutableList.of(kafkaSourceDescription.getTopicPartition()));\n+        long startOffset;\n+        if (kafkaSourceDescription.getStartReadOffset() != null) {\n+          startOffset = kafkaSourceDescription.getStartReadOffset();\n+        } else if (kafkaSourceDescription.getStartReadTime() != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  kafkaSourceDescription.getTopicPartition(),\n+                  kafkaSourceDescription.getStartReadTime());\n+        } else {\n+          startOffset = offsetConsumer.position(kafkaSourceDescription.getTopicPartition());\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public WatermarkEstimatorT newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return createWatermarkEstimatorFn.apply(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        @Restriction OffsetRange offsetRange)\n+        throws Exception {\n+      double numOfRecords =\n+          ((HasProgress) restrictionTracker(kafkaSourceDescription, offsetRange))\n+              .getProgress()\n+              .getWorkRemaining();\n+\n+      // Before processing elements, we don't have a good estimated size of records and offset gap.\n+      if (avgOffsetGap.containsKey(kafkaSourceDescription.getTopicPartition())) {\n+        numOfRecords =\n+            numOfRecords / (1 + avgOffsetGap.get(kafkaSourceDescription.getTopicPartition()).get());\n+      }\n+      return (!avgRecordSize.containsKey(kafkaSourceDescription.getTopicPartition())\n+              ? 1\n+              : avgRecordSize.get(kafkaSourceDescription.getTopicPartition()).get())\n+          * numOfRecords;\n+    }\n+\n+    @NewTracker\n+    public RestrictionTracker<OffsetRange, Long> restrictionTracker(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        @Restriction OffsetRange restriction) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      KafkaLatestOffsetEstimator offsetPoller =\n+          new KafkaLatestOffsetEstimator(\n+              consumerFactoryFn.apply(\n+                  KafkaIOUtils.getOffsetConsumerConfig(\n+                      \"tracker-\" + kafkaSourceDescription.getTopicPartition(),\n+                      offsetConsumerConfig,\n+                      updatedConsumerConfig)),\n+              kafkaSourceDescription.getTopicPartition());\n+      return new GrowableOffsetRangeTracker(restriction.getFrom(), offsetPoller);\n+    }\n+\n+    @ProcessElement\n+    public ProcessContinuation processElement(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        RestrictionTracker<OffsetRange, Long> tracker,\n+        WatermarkEstimator watermarkEstimator,\n+        OutputReceiver<KafkaRecord<K, V>> receiver) {\n+      // If there is no future work, resume with max timeout and move to the next element.\n+      if (((HasProgress) tracker).getProgress().getWorkRemaining() <= 0.0) {\n+        return ProcessContinuation.resume().withResumeDelay(KAFKA_POLL_TIMEOUT);\n+      }\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      // If there is a timestampPolicyFactory, create the TimestampPolicy for current\n+      // TopicPartition.\n+      TimestampPolicy timestampPolicy = null;\n+      if (timestampPolicyFactory != null) {\n+        timestampPolicy =\n+            timestampPolicyFactory.createTimestampPolicy(\n+                kafkaSourceDescription.getTopicPartition(),\n+                Optional.ofNullable(watermarkEstimator.currentWatermark()));\n+      }\n+      try (Consumer<byte[], byte[]> consumer = consumerFactoryFn.apply(updatedConsumerConfig)) {\n+        consumerSpEL.evaluateAssign(\n+            consumer, ImmutableList.of(kafkaSourceDescription.getTopicPartition()));\n+        long startOffset = tracker.currentRestriction().getFrom();\n+        long expectedOffset = startOffset;\n+        consumer.seek(kafkaSourceDescription.getTopicPartition(), startOffset);\n+        ConsumerRecords<byte[], byte[]> rawRecords = ConsumerRecords.empty();\n+\n+        try {\n+          while (true) {\n+            rawRecords = consumer.poll(KAFKA_POLL_TIMEOUT.getMillis());\n+            // When there is no records from the current TopicPartition temporarily, self-checkpoint\n+            // and move to process the next element.\n+            if (rawRecords.isEmpty()) {\n+              return ProcessContinuation.resume();\n+            }\n+            for (ConsumerRecord<byte[], byte[]> rawRecord : rawRecords) {\n+              if (!tracker.tryClaim(rawRecord.offset())) {\n+                return ProcessContinuation.stop();\n+              }\n+              KafkaRecord<K, V> kafkaRecord =\n+                  new KafkaRecord<>(\n+                      rawRecord.topic(),\n+                      rawRecord.partition(),\n+                      rawRecord.offset(),\n+                      consumerSpEL.getRecordTimestamp(rawRecord),\n+                      consumerSpEL.getRecordTimestampType(rawRecord),\n+                      ConsumerSpEL.hasHeaders() ? rawRecord.headers() : null,\n+                      keyDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.key()),\n+                      valueDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.value()));\n+              int recordSize =\n+                  (rawRecord.key() == null ? 0 : rawRecord.key().length)\n+                      + (rawRecord.value() == null ? 0 : rawRecord.value().length);\n+              avgOffsetGap\n+                  .computeIfAbsent(kafkaSourceDescription.getTopicPartition(), k -> new MovingAvg())\n+                  .update(expectedOffset - rawRecord.offset());\n+              avgRecordSize\n+                  .computeIfAbsent(kafkaSourceDescription.getTopicPartition(), k -> new MovingAvg())\n+                  .update(recordSize);\n+              expectedOffset = rawRecord.offset() + 1;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM4NDU2MQ=="}, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 795}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2ODkxMDUxOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzoyMzozMVrOGnzNyw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzoyMzozMVrOGnzNyw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM4Njc2Mw==", "bodyText": "It would be nice if instead we had one map/object that was able to handle both the offset gap and the record size.", "url": "https://github.com/apache/beam/pull/11749#discussion_r444386763", "createdAt": "2020-06-23T17:23:31Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,861 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaSourceDescription.Schemas;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.Manual;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.WallTime;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the <a href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The {@link WatermarkEstimator} is created by {@link #getCreateWatermarkEstimatorFn()}. The\n+ * estimated watermark is computed by this {@link WatermarkEstimator} based on output timestamps\n+ * computed by {@link #getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link #withProcessingTime()} as {@code extractTimestampFn} and {@link\n+ * #withMonotonicallyIncreasingWatermarkEstimator()} as {@link WatermarkEstimator}.\n+ */\n+@Experimental(Kind.PORTABILITY)\n+@AutoValue\n+abstract class ReadViaSDF<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<Instant, WatermarkEstimatorT> getCreateWatermarkEstimatorFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  @Nullable\n+  abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+  abstract Builder<K, V, WatermarkEstimatorT> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>> {\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setOffsetConsumerConfig(\n+        Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCreateWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimatorT> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setTimestampPolicyFactory(\n+        TimestampPolicyFactory<K, V> policy);\n+\n+    abstract ReadViaSDF<K, V, WatermarkEstimatorT> build();\n+  }\n+\n+  public static <K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      ReadViaSDF<K, V, WatermarkEstimatorT> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V, WatermarkEstimatorT>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setCommitOffsetEnabled(false)\n+        .build()\n+        .withProcessingTime()\n+        .withMonotonicallyIncreasingWatermarkEstimator();\n+  }\n+\n+  // Note that if the bootstrapServers is set here but also populated with the element, the element\n+  // will override the bootstrapServers from the config.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializer(\n+      Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigUpdates(\n+      Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreatWatermarkEstimatorFn(\n+      SerializableFunction<Instant, WatermarkEstimatorT> fn) {\n+    return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withWallTimeWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new WallTime(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withMonotonicallyIncreasingWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new MonotonicallyIncreasing(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withManualWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new Manual(state);\n+        });\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigOverrides(\n+      Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  ReadViaSDFExternally forExternalBuild() {\n+    return new ReadViaSDFExternally(this);\n+  }\n+\n+  private static class ReadViaSDFExternally<\n+          K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends PTransform<PCollection<Row>, PCollection<KV<K, V>>> {\n+\n+    private final ReadViaSDF<K, V, WatermarkEstimatorT> readViaSDF;\n+\n+    ReadViaSDFExternally(ReadViaSDF read) {\n+      readViaSDF = read;\n+    }\n+\n+    @Override\n+    public PCollection<KV<K, V>> expand(PCollection<Row> input) {\n+      return input\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<Row, KafkaSourceDescription>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element Row row, OutputReceiver<KafkaSourceDescription> outputReceiver) {\n+                      TopicPartition topicPartition =\n+                          new TopicPartition(\n+                              row.getString(Schemas.TOPIC), row.getInt32(Schemas.PARTITION));\n+                      Instant startReadTime =\n+                          row.getInt64(Schemas.START_READ_TIME) != null\n+                              ? Instant.ofEpochMilli(row.getInt64(Schemas.START_READ_TIME))\n+                              : null;\n+                      outputReceiver.output(\n+                          KafkaSourceDescription.of(\n+                              topicPartition,\n+                              row.getInt64(Schemas.START_READ_OFFSET),\n+                              startReadTime,\n+                              new ArrayList<>(row.getArray(Schemas.BOOTSTRAP_SERVERS))));\n+                    }\n+                  }))\n+          .apply(readViaSDF)\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<KafkaRecord<K, V>, KV<K, V>>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element KafkaRecord element, OutputReceiver<KV<K, V>> outputReceiver) {\n+                      outputReceiver.output(element.getKV());\n+                    }\n+                  }))\n+          .setCoder(KvCoder.<K, V>of(readViaSDF.getKeyCoder(), readViaSDF.getValueCoder()));\n+    }\n+  }\n+\n+  ReadViaSDF<K, V, WatermarkEstimatorT> withTimestampPolicyFactory(\n+      TimestampPolicyFactory<K, V> timestampPolicyFactory) {\n+    return toBuilder().setTimestampPolicyFactory(timestampPolicyFactory).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<KafkaSourceDescription> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input\n+            .apply(ParDo.of(new ReadFromKafkaDoFn<K, V, WatermarkEstimatorT>(this)))\n+            .setCoder(outputCoder);\n+    // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      throw new IllegalStateException(\"Offset committed is not supported yet\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link KafkaSourceDescription} and outputs {@link\n+   * KafkaRecord}. By default, a {@link MonotonicallyIncreasing} watermark estimator is used to\n+   * track watermark.\n+   */\n+  @VisibleForTesting\n+  @UnboundedPerElement\n+  static class ReadFromKafkaDoFn<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends DoFn<KafkaSourceDescription, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn(ReadViaSDF transform) {\n+      this.consumerConfig = transform.getConsumerConfig();\n+      this.offsetConsumerConfig = transform.getOffsetConsumerConfig();\n+      this.keyDeserializerProvider = transform.getKeyDeserializerProvider();\n+      this.valueDeserializerProvider = transform.getValueDeserializerProvider();\n+      this.consumerFactoryFn = transform.getConsumerFactoryFn();\n+      this.extractOutputTimestampFn = transform.getExtractOutputTimestampFn();\n+      this.createWatermarkEstimatorFn = transform.getCreateWatermarkEstimatorFn();\n+      this.timestampPolicyFactory = transform.getTimestampPolicyFactory();\n+    }\n+\n+    private final Map<String, Object> offsetConsumerConfig;\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn;\n+    private final SerializableFunction<Instant, WatermarkEstimatorT> createWatermarkEstimatorFn;\n+    private final TimestampPolicyFactory<K, V> timestampPolicyFactory;\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgRecordSize;\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgOffsetGap;\n+\n+    private static final Duration KAFKA_POLL_TIMEOUT = Duration.millis(1000);\n+\n+    @VisibleForTesting final DeserializerProvider keyDeserializerProvider;\n+    @VisibleForTesting final DeserializerProvider valueDeserializerProvider;\n+    @VisibleForTesting final Map<String, Object> consumerConfig;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private static class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+      private final Supplier<Long> memorizedBacklog;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+        memorizedBacklog =\n+            Suppliers.memoizeWithExpiration(\n+                () -> {\n+                  consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+                  return offsetConsumer.position(topicPartition);\n+                },\n+                5,\n+                TimeUnit.SECONDS);\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        return memorizedBacklog.get();\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element KafkaSourceDescription kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        consumerSpEL.evaluateAssign(\n+            offsetConsumer, ImmutableList.of(kafkaSourceDescription.getTopicPartition()));\n+        long startOffset;\n+        if (kafkaSourceDescription.getStartReadOffset() != null) {\n+          startOffset = kafkaSourceDescription.getStartReadOffset();\n+        } else if (kafkaSourceDescription.getStartReadTime() != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  kafkaSourceDescription.getTopicPartition(),\n+                  kafkaSourceDescription.getStartReadTime());\n+        } else {\n+          startOffset = offsetConsumer.position(kafkaSourceDescription.getTopicPartition());\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public WatermarkEstimatorT newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return createWatermarkEstimatorFn.apply(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        @Restriction OffsetRange offsetRange)\n+        throws Exception {\n+      double numOfRecords =\n+          ((HasProgress) restrictionTracker(kafkaSourceDescription, offsetRange))\n+              .getProgress()\n+              .getWorkRemaining();\n+\n+      // Before processing elements, we don't have a good estimated size of records and offset gap.\n+      if (avgOffsetGap.containsKey(kafkaSourceDescription.getTopicPartition())) {\n+        numOfRecords =\n+            numOfRecords / (1 + avgOffsetGap.get(kafkaSourceDescription.getTopicPartition()).get());\n+      }\n+      return (!avgRecordSize.containsKey(kafkaSourceDescription.getTopicPartition())\n+              ? 1\n+              : avgRecordSize.get(kafkaSourceDescription.getTopicPartition()).get())\n+          * numOfRecords;\n+    }\n+\n+    @NewTracker\n+    public RestrictionTracker<OffsetRange, Long> restrictionTracker(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        @Restriction OffsetRange restriction) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      KafkaLatestOffsetEstimator offsetPoller =\n+          new KafkaLatestOffsetEstimator(\n+              consumerFactoryFn.apply(\n+                  KafkaIOUtils.getOffsetConsumerConfig(\n+                      \"tracker-\" + kafkaSourceDescription.getTopicPartition(),\n+                      offsetConsumerConfig,\n+                      updatedConsumerConfig)),\n+              kafkaSourceDescription.getTopicPartition());\n+      return new GrowableOffsetRangeTracker(restriction.getFrom(), offsetPoller);\n+    }\n+\n+    @ProcessElement\n+    public ProcessContinuation processElement(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        RestrictionTracker<OffsetRange, Long> tracker,\n+        WatermarkEstimator watermarkEstimator,\n+        OutputReceiver<KafkaRecord<K, V>> receiver) {\n+      // If there is no future work, resume with max timeout and move to the next element.\n+      if (((HasProgress) tracker).getProgress().getWorkRemaining() <= 0.0) {\n+        return ProcessContinuation.resume().withResumeDelay(KAFKA_POLL_TIMEOUT);\n+      }\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      // If there is a timestampPolicyFactory, create the TimestampPolicy for current\n+      // TopicPartition.\n+      TimestampPolicy timestampPolicy = null;\n+      if (timestampPolicyFactory != null) {\n+        timestampPolicy =\n+            timestampPolicyFactory.createTimestampPolicy(\n+                kafkaSourceDescription.getTopicPartition(),\n+                Optional.ofNullable(watermarkEstimator.currentWatermark()));\n+      }\n+      try (Consumer<byte[], byte[]> consumer = consumerFactoryFn.apply(updatedConsumerConfig)) {\n+        consumerSpEL.evaluateAssign(\n+            consumer, ImmutableList.of(kafkaSourceDescription.getTopicPartition()));\n+        long startOffset = tracker.currentRestriction().getFrom();\n+        long expectedOffset = startOffset;\n+        consumer.seek(kafkaSourceDescription.getTopicPartition(), startOffset);\n+        ConsumerRecords<byte[], byte[]> rawRecords = ConsumerRecords.empty();\n+\n+        try {\n+          while (true) {\n+            rawRecords = consumer.poll(KAFKA_POLL_TIMEOUT.getMillis());\n+            // When there is no records from the current TopicPartition temporarily, self-checkpoint\n+            // and move to process the next element.\n+            if (rawRecords.isEmpty()) {\n+              return ProcessContinuation.resume();\n+            }\n+            for (ConsumerRecord<byte[], byte[]> rawRecord : rawRecords) {\n+              if (!tracker.tryClaim(rawRecord.offset())) {\n+                return ProcessContinuation.stop();\n+              }\n+              KafkaRecord<K, V> kafkaRecord =\n+                  new KafkaRecord<>(\n+                      rawRecord.topic(),\n+                      rawRecord.partition(),\n+                      rawRecord.offset(),\n+                      consumerSpEL.getRecordTimestamp(rawRecord),\n+                      consumerSpEL.getRecordTimestampType(rawRecord),\n+                      ConsumerSpEL.hasHeaders() ? rawRecord.headers() : null,\n+                      keyDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.key()),\n+                      valueDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.value()));\n+              int recordSize =\n+                  (rawRecord.key() == null ? 0 : rawRecord.key().length)\n+                      + (rawRecord.value() == null ? 0 : rawRecord.value().length);\n+              avgOffsetGap", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 789}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2ODkxODM1OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzoyNTo0MVrOGnzSwQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNToxNToyN1rOGoDNkg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM4ODAzMw==", "bodyText": "The backlog check time doesn't accurately reflect when the value was memoized. Not sure if this will cause an issue.", "url": "https://github.com/apache/beam/pull/11749#discussion_r444388033", "createdAt": "2020-06-23T17:25:41Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,861 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaSourceDescription.Schemas;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.Manual;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.WallTime;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the <a href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The {@link WatermarkEstimator} is created by {@link #getCreateWatermarkEstimatorFn()}. The\n+ * estimated watermark is computed by this {@link WatermarkEstimator} based on output timestamps\n+ * computed by {@link #getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link #withProcessingTime()} as {@code extractTimestampFn} and {@link\n+ * #withMonotonicallyIncreasingWatermarkEstimator()} as {@link WatermarkEstimator}.\n+ */\n+@Experimental(Kind.PORTABILITY)\n+@AutoValue\n+abstract class ReadViaSDF<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<Instant, WatermarkEstimatorT> getCreateWatermarkEstimatorFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  @Nullable\n+  abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+  abstract Builder<K, V, WatermarkEstimatorT> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>> {\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setOffsetConsumerConfig(\n+        Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCreateWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimatorT> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setTimestampPolicyFactory(\n+        TimestampPolicyFactory<K, V> policy);\n+\n+    abstract ReadViaSDF<K, V, WatermarkEstimatorT> build();\n+  }\n+\n+  public static <K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      ReadViaSDF<K, V, WatermarkEstimatorT> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V, WatermarkEstimatorT>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setCommitOffsetEnabled(false)\n+        .build()\n+        .withProcessingTime()\n+        .withMonotonicallyIncreasingWatermarkEstimator();\n+  }\n+\n+  // Note that if the bootstrapServers is set here but also populated with the element, the element\n+  // will override the bootstrapServers from the config.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializer(\n+      Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigUpdates(\n+      Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreatWatermarkEstimatorFn(\n+      SerializableFunction<Instant, WatermarkEstimatorT> fn) {\n+    return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withWallTimeWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new WallTime(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withMonotonicallyIncreasingWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new MonotonicallyIncreasing(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withManualWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new Manual(state);\n+        });\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigOverrides(\n+      Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  ReadViaSDFExternally forExternalBuild() {\n+    return new ReadViaSDFExternally(this);\n+  }\n+\n+  private static class ReadViaSDFExternally<\n+          K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends PTransform<PCollection<Row>, PCollection<KV<K, V>>> {\n+\n+    private final ReadViaSDF<K, V, WatermarkEstimatorT> readViaSDF;\n+\n+    ReadViaSDFExternally(ReadViaSDF read) {\n+      readViaSDF = read;\n+    }\n+\n+    @Override\n+    public PCollection<KV<K, V>> expand(PCollection<Row> input) {\n+      return input\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<Row, KafkaSourceDescription>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element Row row, OutputReceiver<KafkaSourceDescription> outputReceiver) {\n+                      TopicPartition topicPartition =\n+                          new TopicPartition(\n+                              row.getString(Schemas.TOPIC), row.getInt32(Schemas.PARTITION));\n+                      Instant startReadTime =\n+                          row.getInt64(Schemas.START_READ_TIME) != null\n+                              ? Instant.ofEpochMilli(row.getInt64(Schemas.START_READ_TIME))\n+                              : null;\n+                      outputReceiver.output(\n+                          KafkaSourceDescription.of(\n+                              topicPartition,\n+                              row.getInt64(Schemas.START_READ_OFFSET),\n+                              startReadTime,\n+                              new ArrayList<>(row.getArray(Schemas.BOOTSTRAP_SERVERS))));\n+                    }\n+                  }))\n+          .apply(readViaSDF)\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<KafkaRecord<K, V>, KV<K, V>>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element KafkaRecord element, OutputReceiver<KV<K, V>> outputReceiver) {\n+                      outputReceiver.output(element.getKV());\n+                    }\n+                  }))\n+          .setCoder(KvCoder.<K, V>of(readViaSDF.getKeyCoder(), readViaSDF.getValueCoder()));\n+    }\n+  }\n+\n+  ReadViaSDF<K, V, WatermarkEstimatorT> withTimestampPolicyFactory(\n+      TimestampPolicyFactory<K, V> timestampPolicyFactory) {\n+    return toBuilder().setTimestampPolicyFactory(timestampPolicyFactory).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<KafkaSourceDescription> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input\n+            .apply(ParDo.of(new ReadFromKafkaDoFn<K, V, WatermarkEstimatorT>(this)))\n+            .setCoder(outputCoder);\n+    // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      throw new IllegalStateException(\"Offset committed is not supported yet\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link KafkaSourceDescription} and outputs {@link\n+   * KafkaRecord}. By default, a {@link MonotonicallyIncreasing} watermark estimator is used to\n+   * track watermark.\n+   */\n+  @VisibleForTesting\n+  @UnboundedPerElement\n+  static class ReadFromKafkaDoFn<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends DoFn<KafkaSourceDescription, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn(ReadViaSDF transform) {\n+      this.consumerConfig = transform.getConsumerConfig();\n+      this.offsetConsumerConfig = transform.getOffsetConsumerConfig();\n+      this.keyDeserializerProvider = transform.getKeyDeserializerProvider();\n+      this.valueDeserializerProvider = transform.getValueDeserializerProvider();\n+      this.consumerFactoryFn = transform.getConsumerFactoryFn();\n+      this.extractOutputTimestampFn = transform.getExtractOutputTimestampFn();\n+      this.createWatermarkEstimatorFn = transform.getCreateWatermarkEstimatorFn();\n+      this.timestampPolicyFactory = transform.getTimestampPolicyFactory();\n+    }\n+\n+    private final Map<String, Object> offsetConsumerConfig;\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn;\n+    private final SerializableFunction<Instant, WatermarkEstimatorT> createWatermarkEstimatorFn;\n+    private final TimestampPolicyFactory<K, V> timestampPolicyFactory;\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgRecordSize;\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgOffsetGap;\n+\n+    private static final Duration KAFKA_POLL_TIMEOUT = Duration.millis(1000);\n+\n+    @VisibleForTesting final DeserializerProvider keyDeserializerProvider;\n+    @VisibleForTesting final DeserializerProvider valueDeserializerProvider;\n+    @VisibleForTesting final Map<String, Object> consumerConfig;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private static class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+      private final Supplier<Long> memorizedBacklog;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+        memorizedBacklog =\n+            Suppliers.memoizeWithExpiration(\n+                () -> {\n+                  consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+                  return offsetConsumer.position(topicPartition);\n+                },\n+                5,\n+                TimeUnit.SECONDS);\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        return memorizedBacklog.get();\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element KafkaSourceDescription kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        consumerSpEL.evaluateAssign(\n+            offsetConsumer, ImmutableList.of(kafkaSourceDescription.getTopicPartition()));\n+        long startOffset;\n+        if (kafkaSourceDescription.getStartReadOffset() != null) {\n+          startOffset = kafkaSourceDescription.getStartReadOffset();\n+        } else if (kafkaSourceDescription.getStartReadTime() != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  kafkaSourceDescription.getTopicPartition(),\n+                  kafkaSourceDescription.getStartReadTime());\n+        } else {\n+          startOffset = offsetConsumer.position(kafkaSourceDescription.getTopicPartition());\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public WatermarkEstimatorT newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return createWatermarkEstimatorFn.apply(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        @Restriction OffsetRange offsetRange)\n+        throws Exception {\n+      double numOfRecords =\n+          ((HasProgress) restrictionTracker(kafkaSourceDescription, offsetRange))\n+              .getProgress()\n+              .getWorkRemaining();\n+\n+      // Before processing elements, we don't have a good estimated size of records and offset gap.\n+      if (avgOffsetGap.containsKey(kafkaSourceDescription.getTopicPartition())) {\n+        numOfRecords =\n+            numOfRecords / (1 + avgOffsetGap.get(kafkaSourceDescription.getTopicPartition()).get());\n+      }\n+      return (!avgRecordSize.containsKey(kafkaSourceDescription.getTopicPartition())\n+              ? 1\n+              : avgRecordSize.get(kafkaSourceDescription.getTopicPartition()).get())\n+          * numOfRecords;\n+    }\n+\n+    @NewTracker\n+    public RestrictionTracker<OffsetRange, Long> restrictionTracker(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        @Restriction OffsetRange restriction) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      KafkaLatestOffsetEstimator offsetPoller =\n+          new KafkaLatestOffsetEstimator(\n+              consumerFactoryFn.apply(\n+                  KafkaIOUtils.getOffsetConsumerConfig(\n+                      \"tracker-\" + kafkaSourceDescription.getTopicPartition(),\n+                      offsetConsumerConfig,\n+                      updatedConsumerConfig)),\n+              kafkaSourceDescription.getTopicPartition());\n+      return new GrowableOffsetRangeTracker(restriction.getFrom(), offsetPoller);\n+    }\n+\n+    @ProcessElement\n+    public ProcessContinuation processElement(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        RestrictionTracker<OffsetRange, Long> tracker,\n+        WatermarkEstimator watermarkEstimator,\n+        OutputReceiver<KafkaRecord<K, V>> receiver) {\n+      // If there is no future work, resume with max timeout and move to the next element.\n+      if (((HasProgress) tracker).getProgress().getWorkRemaining() <= 0.0) {\n+        return ProcessContinuation.resume().withResumeDelay(KAFKA_POLL_TIMEOUT);\n+      }\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      // If there is a timestampPolicyFactory, create the TimestampPolicy for current\n+      // TopicPartition.\n+      TimestampPolicy timestampPolicy = null;\n+      if (timestampPolicyFactory != null) {\n+        timestampPolicy =\n+            timestampPolicyFactory.createTimestampPolicy(\n+                kafkaSourceDescription.getTopicPartition(),\n+                Optional.ofNullable(watermarkEstimator.currentWatermark()));\n+      }\n+      try (Consumer<byte[], byte[]> consumer = consumerFactoryFn.apply(updatedConsumerConfig)) {\n+        consumerSpEL.evaluateAssign(\n+            consumer, ImmutableList.of(kafkaSourceDescription.getTopicPartition()));\n+        long startOffset = tracker.currentRestriction().getFrom();\n+        long expectedOffset = startOffset;\n+        consumer.seek(kafkaSourceDescription.getTopicPartition(), startOffset);\n+        ConsumerRecords<byte[], byte[]> rawRecords = ConsumerRecords.empty();\n+\n+        try {\n+          while (true) {\n+            rawRecords = consumer.poll(KAFKA_POLL_TIMEOUT.getMillis());\n+            // When there is no records from the current TopicPartition temporarily, self-checkpoint\n+            // and move to process the next element.\n+            if (rawRecords.isEmpty()) {\n+              return ProcessContinuation.resume();\n+            }\n+            for (ConsumerRecord<byte[], byte[]> rawRecord : rawRecords) {\n+              if (!tracker.tryClaim(rawRecord.offset())) {\n+                return ProcessContinuation.stop();\n+              }\n+              KafkaRecord<K, V> kafkaRecord =\n+                  new KafkaRecord<>(\n+                      rawRecord.topic(),\n+                      rawRecord.partition(),\n+                      rawRecord.offset(),\n+                      consumerSpEL.getRecordTimestamp(rawRecord),\n+                      consumerSpEL.getRecordTimestampType(rawRecord),\n+                      ConsumerSpEL.hasHeaders() ? rawRecord.headers() : null,\n+                      keyDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.key()),\n+                      valueDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.value()));\n+              int recordSize =\n+                  (rawRecord.key() == null ? 0 : rawRecord.key().length)\n+                      + (rawRecord.value() == null ? 0 : rawRecord.value().length);\n+              avgOffsetGap\n+                  .computeIfAbsent(kafkaSourceDescription.getTopicPartition(), k -> new MovingAvg())\n+                  .update(expectedOffset - rawRecord.offset());\n+              avgRecordSize\n+                  .computeIfAbsent(kafkaSourceDescription.getTopicPartition(), k -> new MovingAvg())\n+                  .update(recordSize);\n+              expectedOffset = rawRecord.offset() + 1;\n+              Instant outputTimestamp;\n+              // The outputTimestamp and watermark will be computed by timestampPolicy, where the\n+              // WatermarkEstimator should be a Manual one.\n+              if (timestampPolicy != null) {\n+                checkState(watermarkEstimator instanceof ManualWatermarkEstimator);\n+                TimestampPolicyContext context =\n+                    new TimestampPolicyContext(\n+                        (long) ((HasProgress) tracker).getProgress().getWorkRemaining(),\n+                        Instant.now());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 804}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY0ODg1MA==", "bodyText": "If the timeout of the supplier is 5s, it wouldn't have a significant impact.", "url": "https://github.com/apache/beam/pull/11749#discussion_r444648850", "createdAt": "2020-06-24T05:15:27Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,861 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaSourceDescription.Schemas;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.Manual;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.WallTime;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the <a href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The {@link WatermarkEstimator} is created by {@link #getCreateWatermarkEstimatorFn()}. The\n+ * estimated watermark is computed by this {@link WatermarkEstimator} based on output timestamps\n+ * computed by {@link #getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link #withProcessingTime()} as {@code extractTimestampFn} and {@link\n+ * #withMonotonicallyIncreasingWatermarkEstimator()} as {@link WatermarkEstimator}.\n+ */\n+@Experimental(Kind.PORTABILITY)\n+@AutoValue\n+abstract class ReadViaSDF<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<Instant, WatermarkEstimatorT> getCreateWatermarkEstimatorFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  @Nullable\n+  abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+  abstract Builder<K, V, WatermarkEstimatorT> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>> {\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setOffsetConsumerConfig(\n+        Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCreateWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimatorT> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setTimestampPolicyFactory(\n+        TimestampPolicyFactory<K, V> policy);\n+\n+    abstract ReadViaSDF<K, V, WatermarkEstimatorT> build();\n+  }\n+\n+  public static <K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      ReadViaSDF<K, V, WatermarkEstimatorT> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V, WatermarkEstimatorT>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setCommitOffsetEnabled(false)\n+        .build()\n+        .withProcessingTime()\n+        .withMonotonicallyIncreasingWatermarkEstimator();\n+  }\n+\n+  // Note that if the bootstrapServers is set here but also populated with the element, the element\n+  // will override the bootstrapServers from the config.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializer(\n+      Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigUpdates(\n+      Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreatWatermarkEstimatorFn(\n+      SerializableFunction<Instant, WatermarkEstimatorT> fn) {\n+    return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withWallTimeWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new WallTime(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withMonotonicallyIncreasingWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new MonotonicallyIncreasing(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withManualWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new Manual(state);\n+        });\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigOverrides(\n+      Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  ReadViaSDFExternally forExternalBuild() {\n+    return new ReadViaSDFExternally(this);\n+  }\n+\n+  private static class ReadViaSDFExternally<\n+          K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends PTransform<PCollection<Row>, PCollection<KV<K, V>>> {\n+\n+    private final ReadViaSDF<K, V, WatermarkEstimatorT> readViaSDF;\n+\n+    ReadViaSDFExternally(ReadViaSDF read) {\n+      readViaSDF = read;\n+    }\n+\n+    @Override\n+    public PCollection<KV<K, V>> expand(PCollection<Row> input) {\n+      return input\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<Row, KafkaSourceDescription>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element Row row, OutputReceiver<KafkaSourceDescription> outputReceiver) {\n+                      TopicPartition topicPartition =\n+                          new TopicPartition(\n+                              row.getString(Schemas.TOPIC), row.getInt32(Schemas.PARTITION));\n+                      Instant startReadTime =\n+                          row.getInt64(Schemas.START_READ_TIME) != null\n+                              ? Instant.ofEpochMilli(row.getInt64(Schemas.START_READ_TIME))\n+                              : null;\n+                      outputReceiver.output(\n+                          KafkaSourceDescription.of(\n+                              topicPartition,\n+                              row.getInt64(Schemas.START_READ_OFFSET),\n+                              startReadTime,\n+                              new ArrayList<>(row.getArray(Schemas.BOOTSTRAP_SERVERS))));\n+                    }\n+                  }))\n+          .apply(readViaSDF)\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<KafkaRecord<K, V>, KV<K, V>>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element KafkaRecord element, OutputReceiver<KV<K, V>> outputReceiver) {\n+                      outputReceiver.output(element.getKV());\n+                    }\n+                  }))\n+          .setCoder(KvCoder.<K, V>of(readViaSDF.getKeyCoder(), readViaSDF.getValueCoder()));\n+    }\n+  }\n+\n+  ReadViaSDF<K, V, WatermarkEstimatorT> withTimestampPolicyFactory(\n+      TimestampPolicyFactory<K, V> timestampPolicyFactory) {\n+    return toBuilder().setTimestampPolicyFactory(timestampPolicyFactory).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<KafkaSourceDescription> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input\n+            .apply(ParDo.of(new ReadFromKafkaDoFn<K, V, WatermarkEstimatorT>(this)))\n+            .setCoder(outputCoder);\n+    // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      throw new IllegalStateException(\"Offset committed is not supported yet\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link KafkaSourceDescription} and outputs {@link\n+   * KafkaRecord}. By default, a {@link MonotonicallyIncreasing} watermark estimator is used to\n+   * track watermark.\n+   */\n+  @VisibleForTesting\n+  @UnboundedPerElement\n+  static class ReadFromKafkaDoFn<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends DoFn<KafkaSourceDescription, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn(ReadViaSDF transform) {\n+      this.consumerConfig = transform.getConsumerConfig();\n+      this.offsetConsumerConfig = transform.getOffsetConsumerConfig();\n+      this.keyDeserializerProvider = transform.getKeyDeserializerProvider();\n+      this.valueDeserializerProvider = transform.getValueDeserializerProvider();\n+      this.consumerFactoryFn = transform.getConsumerFactoryFn();\n+      this.extractOutputTimestampFn = transform.getExtractOutputTimestampFn();\n+      this.createWatermarkEstimatorFn = transform.getCreateWatermarkEstimatorFn();\n+      this.timestampPolicyFactory = transform.getTimestampPolicyFactory();\n+    }\n+\n+    private final Map<String, Object> offsetConsumerConfig;\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn;\n+    private final SerializableFunction<Instant, WatermarkEstimatorT> createWatermarkEstimatorFn;\n+    private final TimestampPolicyFactory<K, V> timestampPolicyFactory;\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgRecordSize;\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgOffsetGap;\n+\n+    private static final Duration KAFKA_POLL_TIMEOUT = Duration.millis(1000);\n+\n+    @VisibleForTesting final DeserializerProvider keyDeserializerProvider;\n+    @VisibleForTesting final DeserializerProvider valueDeserializerProvider;\n+    @VisibleForTesting final Map<String, Object> consumerConfig;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private static class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+      private final Supplier<Long> memorizedBacklog;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+        memorizedBacklog =\n+            Suppliers.memoizeWithExpiration(\n+                () -> {\n+                  consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+                  return offsetConsumer.position(topicPartition);\n+                },\n+                5,\n+                TimeUnit.SECONDS);\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        return memorizedBacklog.get();\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element KafkaSourceDescription kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        consumerSpEL.evaluateAssign(\n+            offsetConsumer, ImmutableList.of(kafkaSourceDescription.getTopicPartition()));\n+        long startOffset;\n+        if (kafkaSourceDescription.getStartReadOffset() != null) {\n+          startOffset = kafkaSourceDescription.getStartReadOffset();\n+        } else if (kafkaSourceDescription.getStartReadTime() != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  kafkaSourceDescription.getTopicPartition(),\n+                  kafkaSourceDescription.getStartReadTime());\n+        } else {\n+          startOffset = offsetConsumer.position(kafkaSourceDescription.getTopicPartition());\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public WatermarkEstimatorT newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return createWatermarkEstimatorFn.apply(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        @Restriction OffsetRange offsetRange)\n+        throws Exception {\n+      double numOfRecords =\n+          ((HasProgress) restrictionTracker(kafkaSourceDescription, offsetRange))\n+              .getProgress()\n+              .getWorkRemaining();\n+\n+      // Before processing elements, we don't have a good estimated size of records and offset gap.\n+      if (avgOffsetGap.containsKey(kafkaSourceDescription.getTopicPartition())) {\n+        numOfRecords =\n+            numOfRecords / (1 + avgOffsetGap.get(kafkaSourceDescription.getTopicPartition()).get());\n+      }\n+      return (!avgRecordSize.containsKey(kafkaSourceDescription.getTopicPartition())\n+              ? 1\n+              : avgRecordSize.get(kafkaSourceDescription.getTopicPartition()).get())\n+          * numOfRecords;\n+    }\n+\n+    @NewTracker\n+    public RestrictionTracker<OffsetRange, Long> restrictionTracker(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        @Restriction OffsetRange restriction) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      KafkaLatestOffsetEstimator offsetPoller =\n+          new KafkaLatestOffsetEstimator(\n+              consumerFactoryFn.apply(\n+                  KafkaIOUtils.getOffsetConsumerConfig(\n+                      \"tracker-\" + kafkaSourceDescription.getTopicPartition(),\n+                      offsetConsumerConfig,\n+                      updatedConsumerConfig)),\n+              kafkaSourceDescription.getTopicPartition());\n+      return new GrowableOffsetRangeTracker(restriction.getFrom(), offsetPoller);\n+    }\n+\n+    @ProcessElement\n+    public ProcessContinuation processElement(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        RestrictionTracker<OffsetRange, Long> tracker,\n+        WatermarkEstimator watermarkEstimator,\n+        OutputReceiver<KafkaRecord<K, V>> receiver) {\n+      // If there is no future work, resume with max timeout and move to the next element.\n+      if (((HasProgress) tracker).getProgress().getWorkRemaining() <= 0.0) {\n+        return ProcessContinuation.resume().withResumeDelay(KAFKA_POLL_TIMEOUT);\n+      }\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      // If there is a timestampPolicyFactory, create the TimestampPolicy for current\n+      // TopicPartition.\n+      TimestampPolicy timestampPolicy = null;\n+      if (timestampPolicyFactory != null) {\n+        timestampPolicy =\n+            timestampPolicyFactory.createTimestampPolicy(\n+                kafkaSourceDescription.getTopicPartition(),\n+                Optional.ofNullable(watermarkEstimator.currentWatermark()));\n+      }\n+      try (Consumer<byte[], byte[]> consumer = consumerFactoryFn.apply(updatedConsumerConfig)) {\n+        consumerSpEL.evaluateAssign(\n+            consumer, ImmutableList.of(kafkaSourceDescription.getTopicPartition()));\n+        long startOffset = tracker.currentRestriction().getFrom();\n+        long expectedOffset = startOffset;\n+        consumer.seek(kafkaSourceDescription.getTopicPartition(), startOffset);\n+        ConsumerRecords<byte[], byte[]> rawRecords = ConsumerRecords.empty();\n+\n+        try {\n+          while (true) {\n+            rawRecords = consumer.poll(KAFKA_POLL_TIMEOUT.getMillis());\n+            // When there is no records from the current TopicPartition temporarily, self-checkpoint\n+            // and move to process the next element.\n+            if (rawRecords.isEmpty()) {\n+              return ProcessContinuation.resume();\n+            }\n+            for (ConsumerRecord<byte[], byte[]> rawRecord : rawRecords) {\n+              if (!tracker.tryClaim(rawRecord.offset())) {\n+                return ProcessContinuation.stop();\n+              }\n+              KafkaRecord<K, V> kafkaRecord =\n+                  new KafkaRecord<>(\n+                      rawRecord.topic(),\n+                      rawRecord.partition(),\n+                      rawRecord.offset(),\n+                      consumerSpEL.getRecordTimestamp(rawRecord),\n+                      consumerSpEL.getRecordTimestampType(rawRecord),\n+                      ConsumerSpEL.hasHeaders() ? rawRecord.headers() : null,\n+                      keyDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.key()),\n+                      valueDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.value()));\n+              int recordSize =\n+                  (rawRecord.key() == null ? 0 : rawRecord.key().length)\n+                      + (rawRecord.value() == null ? 0 : rawRecord.value().length);\n+              avgOffsetGap\n+                  .computeIfAbsent(kafkaSourceDescription.getTopicPartition(), k -> new MovingAvg())\n+                  .update(expectedOffset - rawRecord.offset());\n+              avgRecordSize\n+                  .computeIfAbsent(kafkaSourceDescription.getTopicPartition(), k -> new MovingAvg())\n+                  .update(recordSize);\n+              expectedOffset = rawRecord.offset() + 1;\n+              Instant outputTimestamp;\n+              // The outputTimestamp and watermark will be computed by timestampPolicy, where the\n+              // WatermarkEstimator should be a Manual one.\n+              if (timestampPolicy != null) {\n+                checkState(watermarkEstimator instanceof ManualWatermarkEstimator);\n+                TimestampPolicyContext context =\n+                    new TimestampPolicyContext(\n+                        (long) ((HasProgress) tracker).getProgress().getWorkRemaining(),\n+                        Instant.now());", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM4ODAzMw=="}, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 804}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc2ODkyMTQwOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yM1QxNzoyNjozN1rOGnzUvw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQwNTozNDoyOFrOGoDiJw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM4ODU0Mw==", "bodyText": "I'm guessing that you don't actually need this and are using it for debugging.\nAny reason why higher level logging that the process element call failed wouldn't be enough?", "url": "https://github.com/apache/beam/pull/11749#discussion_r444388543", "createdAt": "2020-06-23T17:26:37Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,861 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaSourceDescription.Schemas;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.Manual;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.WallTime;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the <a href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The {@link WatermarkEstimator} is created by {@link #getCreateWatermarkEstimatorFn()}. The\n+ * estimated watermark is computed by this {@link WatermarkEstimator} based on output timestamps\n+ * computed by {@link #getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link #withProcessingTime()} as {@code extractTimestampFn} and {@link\n+ * #withMonotonicallyIncreasingWatermarkEstimator()} as {@link WatermarkEstimator}.\n+ */\n+@Experimental(Kind.PORTABILITY)\n+@AutoValue\n+abstract class ReadViaSDF<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<Instant, WatermarkEstimatorT> getCreateWatermarkEstimatorFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  @Nullable\n+  abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+  abstract Builder<K, V, WatermarkEstimatorT> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>> {\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setOffsetConsumerConfig(\n+        Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCreateWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimatorT> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setTimestampPolicyFactory(\n+        TimestampPolicyFactory<K, V> policy);\n+\n+    abstract ReadViaSDF<K, V, WatermarkEstimatorT> build();\n+  }\n+\n+  public static <K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      ReadViaSDF<K, V, WatermarkEstimatorT> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V, WatermarkEstimatorT>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setCommitOffsetEnabled(false)\n+        .build()\n+        .withProcessingTime()\n+        .withMonotonicallyIncreasingWatermarkEstimator();\n+  }\n+\n+  // Note that if the bootstrapServers is set here but also populated with the element, the element\n+  // will override the bootstrapServers from the config.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializer(\n+      Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigUpdates(\n+      Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreatWatermarkEstimatorFn(\n+      SerializableFunction<Instant, WatermarkEstimatorT> fn) {\n+    return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withWallTimeWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new WallTime(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withMonotonicallyIncreasingWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new MonotonicallyIncreasing(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withManualWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new Manual(state);\n+        });\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigOverrides(\n+      Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  ReadViaSDFExternally forExternalBuild() {\n+    return new ReadViaSDFExternally(this);\n+  }\n+\n+  private static class ReadViaSDFExternally<\n+          K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends PTransform<PCollection<Row>, PCollection<KV<K, V>>> {\n+\n+    private final ReadViaSDF<K, V, WatermarkEstimatorT> readViaSDF;\n+\n+    ReadViaSDFExternally(ReadViaSDF read) {\n+      readViaSDF = read;\n+    }\n+\n+    @Override\n+    public PCollection<KV<K, V>> expand(PCollection<Row> input) {\n+      return input\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<Row, KafkaSourceDescription>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element Row row, OutputReceiver<KafkaSourceDescription> outputReceiver) {\n+                      TopicPartition topicPartition =\n+                          new TopicPartition(\n+                              row.getString(Schemas.TOPIC), row.getInt32(Schemas.PARTITION));\n+                      Instant startReadTime =\n+                          row.getInt64(Schemas.START_READ_TIME) != null\n+                              ? Instant.ofEpochMilli(row.getInt64(Schemas.START_READ_TIME))\n+                              : null;\n+                      outputReceiver.output(\n+                          KafkaSourceDescription.of(\n+                              topicPartition,\n+                              row.getInt64(Schemas.START_READ_OFFSET),\n+                              startReadTime,\n+                              new ArrayList<>(row.getArray(Schemas.BOOTSTRAP_SERVERS))));\n+                    }\n+                  }))\n+          .apply(readViaSDF)\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<KafkaRecord<K, V>, KV<K, V>>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element KafkaRecord element, OutputReceiver<KV<K, V>> outputReceiver) {\n+                      outputReceiver.output(element.getKV());\n+                    }\n+                  }))\n+          .setCoder(KvCoder.<K, V>of(readViaSDF.getKeyCoder(), readViaSDF.getValueCoder()));\n+    }\n+  }\n+\n+  ReadViaSDF<K, V, WatermarkEstimatorT> withTimestampPolicyFactory(\n+      TimestampPolicyFactory<K, V> timestampPolicyFactory) {\n+    return toBuilder().setTimestampPolicyFactory(timestampPolicyFactory).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<KafkaSourceDescription> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input\n+            .apply(ParDo.of(new ReadFromKafkaDoFn<K, V, WatermarkEstimatorT>(this)))\n+            .setCoder(outputCoder);\n+    // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      throw new IllegalStateException(\"Offset committed is not supported yet\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link KafkaSourceDescription} and outputs {@link\n+   * KafkaRecord}. By default, a {@link MonotonicallyIncreasing} watermark estimator is used to\n+   * track watermark.\n+   */\n+  @VisibleForTesting\n+  @UnboundedPerElement\n+  static class ReadFromKafkaDoFn<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends DoFn<KafkaSourceDescription, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn(ReadViaSDF transform) {\n+      this.consumerConfig = transform.getConsumerConfig();\n+      this.offsetConsumerConfig = transform.getOffsetConsumerConfig();\n+      this.keyDeserializerProvider = transform.getKeyDeserializerProvider();\n+      this.valueDeserializerProvider = transform.getValueDeserializerProvider();\n+      this.consumerFactoryFn = transform.getConsumerFactoryFn();\n+      this.extractOutputTimestampFn = transform.getExtractOutputTimestampFn();\n+      this.createWatermarkEstimatorFn = transform.getCreateWatermarkEstimatorFn();\n+      this.timestampPolicyFactory = transform.getTimestampPolicyFactory();\n+    }\n+\n+    private final Map<String, Object> offsetConsumerConfig;\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn;\n+    private final SerializableFunction<Instant, WatermarkEstimatorT> createWatermarkEstimatorFn;\n+    private final TimestampPolicyFactory<K, V> timestampPolicyFactory;\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgRecordSize;\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgOffsetGap;\n+\n+    private static final Duration KAFKA_POLL_TIMEOUT = Duration.millis(1000);\n+\n+    @VisibleForTesting final DeserializerProvider keyDeserializerProvider;\n+    @VisibleForTesting final DeserializerProvider valueDeserializerProvider;\n+    @VisibleForTesting final Map<String, Object> consumerConfig;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private static class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+      private final Supplier<Long> memorizedBacklog;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+        memorizedBacklog =\n+            Suppliers.memoizeWithExpiration(\n+                () -> {\n+                  consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+                  return offsetConsumer.position(topicPartition);\n+                },\n+                5,\n+                TimeUnit.SECONDS);\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        return memorizedBacklog.get();\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element KafkaSourceDescription kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        consumerSpEL.evaluateAssign(\n+            offsetConsumer, ImmutableList.of(kafkaSourceDescription.getTopicPartition()));\n+        long startOffset;\n+        if (kafkaSourceDescription.getStartReadOffset() != null) {\n+          startOffset = kafkaSourceDescription.getStartReadOffset();\n+        } else if (kafkaSourceDescription.getStartReadTime() != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  kafkaSourceDescription.getTopicPartition(),\n+                  kafkaSourceDescription.getStartReadTime());\n+        } else {\n+          startOffset = offsetConsumer.position(kafkaSourceDescription.getTopicPartition());\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public WatermarkEstimatorT newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return createWatermarkEstimatorFn.apply(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        @Restriction OffsetRange offsetRange)\n+        throws Exception {\n+      double numOfRecords =\n+          ((HasProgress) restrictionTracker(kafkaSourceDescription, offsetRange))\n+              .getProgress()\n+              .getWorkRemaining();\n+\n+      // Before processing elements, we don't have a good estimated size of records and offset gap.\n+      if (avgOffsetGap.containsKey(kafkaSourceDescription.getTopicPartition())) {\n+        numOfRecords =\n+            numOfRecords / (1 + avgOffsetGap.get(kafkaSourceDescription.getTopicPartition()).get());\n+      }\n+      return (!avgRecordSize.containsKey(kafkaSourceDescription.getTopicPartition())\n+              ? 1\n+              : avgRecordSize.get(kafkaSourceDescription.getTopicPartition()).get())\n+          * numOfRecords;\n+    }\n+\n+    @NewTracker\n+    public RestrictionTracker<OffsetRange, Long> restrictionTracker(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        @Restriction OffsetRange restriction) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      KafkaLatestOffsetEstimator offsetPoller =\n+          new KafkaLatestOffsetEstimator(\n+              consumerFactoryFn.apply(\n+                  KafkaIOUtils.getOffsetConsumerConfig(\n+                      \"tracker-\" + kafkaSourceDescription.getTopicPartition(),\n+                      offsetConsumerConfig,\n+                      updatedConsumerConfig)),\n+              kafkaSourceDescription.getTopicPartition());\n+      return new GrowableOffsetRangeTracker(restriction.getFrom(), offsetPoller);\n+    }\n+\n+    @ProcessElement\n+    public ProcessContinuation processElement(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        RestrictionTracker<OffsetRange, Long> tracker,\n+        WatermarkEstimator watermarkEstimator,\n+        OutputReceiver<KafkaRecord<K, V>> receiver) {\n+      // If there is no future work, resume with max timeout and move to the next element.\n+      if (((HasProgress) tracker).getProgress().getWorkRemaining() <= 0.0) {\n+        return ProcessContinuation.resume().withResumeDelay(KAFKA_POLL_TIMEOUT);\n+      }\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      // If there is a timestampPolicyFactory, create the TimestampPolicy for current\n+      // TopicPartition.\n+      TimestampPolicy timestampPolicy = null;\n+      if (timestampPolicyFactory != null) {\n+        timestampPolicy =\n+            timestampPolicyFactory.createTimestampPolicy(\n+                kafkaSourceDescription.getTopicPartition(),\n+                Optional.ofNullable(watermarkEstimator.currentWatermark()));\n+      }\n+      try (Consumer<byte[], byte[]> consumer = consumerFactoryFn.apply(updatedConsumerConfig)) {\n+        consumerSpEL.evaluateAssign(\n+            consumer, ImmutableList.of(kafkaSourceDescription.getTopicPartition()));\n+        long startOffset = tracker.currentRestriction().getFrom();\n+        long expectedOffset = startOffset;\n+        consumer.seek(kafkaSourceDescription.getTopicPartition(), startOffset);\n+        ConsumerRecords<byte[], byte[]> rawRecords = ConsumerRecords.empty();\n+\n+        try {\n+          while (true) {\n+            rawRecords = consumer.poll(KAFKA_POLL_TIMEOUT.getMillis());\n+            // When there is no records from the current TopicPartition temporarily, self-checkpoint\n+            // and move to process the next element.\n+            if (rawRecords.isEmpty()) {\n+              return ProcessContinuation.resume();\n+            }\n+            for (ConsumerRecord<byte[], byte[]> rawRecord : rawRecords) {\n+              if (!tracker.tryClaim(rawRecord.offset())) {\n+                return ProcessContinuation.stop();\n+              }\n+              KafkaRecord<K, V> kafkaRecord =\n+                  new KafkaRecord<>(\n+                      rawRecord.topic(),\n+                      rawRecord.partition(),\n+                      rawRecord.offset(),\n+                      consumerSpEL.getRecordTimestamp(rawRecord),\n+                      consumerSpEL.getRecordTimestampType(rawRecord),\n+                      ConsumerSpEL.hasHeaders() ? rawRecord.headers() : null,\n+                      keyDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.key()),\n+                      valueDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.value()));\n+              int recordSize =\n+                  (rawRecord.key() == null ? 0 : rawRecord.key().length)\n+                      + (rawRecord.value() == null ? 0 : rawRecord.value().length);\n+              avgOffsetGap\n+                  .computeIfAbsent(kafkaSourceDescription.getTopicPartition(), k -> new MovingAvg())\n+                  .update(expectedOffset - rawRecord.offset());\n+              avgRecordSize\n+                  .computeIfAbsent(kafkaSourceDescription.getTopicPartition(), k -> new MovingAvg())\n+                  .update(recordSize);\n+              expectedOffset = rawRecord.offset() + 1;\n+              Instant outputTimestamp;\n+              // The outputTimestamp and watermark will be computed by timestampPolicy, where the\n+              // WatermarkEstimator should be a Manual one.\n+              if (timestampPolicy != null) {\n+                checkState(watermarkEstimator instanceof ManualWatermarkEstimator);\n+                TimestampPolicyContext context =\n+                    new TimestampPolicyContext(\n+                        (long) ((HasProgress) tracker).getProgress().getWorkRemaining(),\n+                        Instant.now());\n+                outputTimestamp = timestampPolicy.getTimestampForRecord(context, kafkaRecord);\n+                ((ManualWatermarkEstimator) watermarkEstimator)\n+                    .setWatermark(timestampPolicy.getWatermark(context));\n+              } else {\n+                outputTimestamp = extractOutputTimestampFn.apply(kafkaRecord);\n+              }\n+              receiver.outputWithTimestamp(kafkaRecord, outputTimestamp);\n+            }\n+          }\n+        } catch (Exception anyException) {\n+          LOG.error(\"{}: Exception while reading from Kafka\", this, anyException);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 815}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDY1NDExOQ==", "bodyText": "Nope, I just forgot that we have higher level logging.", "url": "https://github.com/apache/beam/pull/11749#discussion_r444654119", "createdAt": "2020-06-24T05:34:28Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadViaSDF.java", "diffHunk": "@@ -0,0 +1,861 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkArgument;\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import com.google.auto.value.AutoValue;\n+import java.util.ArrayList;\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.annotations.Experimental.Kind;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.CoderRegistry;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaSourceDescription.Schemas;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.options.ExperimentalOptions;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.PTransform;\n+import org.apache.beam.sdk.transforms.ParDo;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.Manual;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.WallTime;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.PCollection;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.apache.kafka.common.utils.AppInfoParser;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A {@link PTransform} that takes a PCollection of {@link KafkaSourceDescription} as input and\n+ * outputs a PCollection of {@link KafkaRecord}. The core implementation is based on {@code\n+ * SplittableDoFn}. For more details about the concept of {@code SplittableDoFn}, please refer to\n+ * the <a href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadViaSDF} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadViaSDF}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadViaSDF#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadViaSDF#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadViaSDF#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadViaSDF#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadViaSDF#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadViaSDF#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadViaSDF} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadViaSDF}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadViaSDF#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadViaSDF#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadViaSDF#withExtractOutputTimestampFn(SerializableFunction)} asks for a function\n+ * which takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadViaSDF#withProcessingTime()}, {@link ReadViaSDF#withCreateTime()} and {@link\n+ * ReadViaSDF#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadViaSDF} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn.KafkaLatestOffsetEstimator} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The {@link WatermarkEstimator} is created by {@link #getCreateWatermarkEstimatorFn()}. The\n+ * estimated watermark is computed by this {@link WatermarkEstimator} based on output timestamps\n+ * computed by {@link #getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link #withProcessingTime()} as {@code extractTimestampFn} and {@link\n+ * #withMonotonicallyIncreasingWatermarkEstimator()} as {@link WatermarkEstimator}.\n+ */\n+@Experimental(Kind.PORTABILITY)\n+@AutoValue\n+abstract class ReadViaSDF<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+    extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadViaSDF.class);\n+\n+  abstract Map<String, Object> getConsumerConfig();\n+\n+  @Nullable\n+  abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+  @Nullable\n+  abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+  @Nullable\n+  abstract DeserializerProvider getValueDeserializerProvider();\n+\n+  @Nullable\n+  abstract Coder<K> getKeyCoder();\n+\n+  @Nullable\n+  abstract Coder<V> getValueCoder();\n+\n+  abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      getConsumerFactoryFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+  @Nullable\n+  abstract SerializableFunction<Instant, WatermarkEstimatorT> getCreateWatermarkEstimatorFn();\n+\n+  abstract boolean isCommitOffsetEnabled();\n+\n+  @Nullable\n+  abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+  abstract Builder<K, V, WatermarkEstimatorT> toBuilder();\n+\n+  @AutoValue.Builder\n+  abstract static class Builder<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>> {\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerConfig(Map<String, Object> config);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setOffsetConsumerConfig(\n+        Map<String, Object> offsetConsumerConfig);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueDeserializerProvider(\n+        DeserializerProvider deserializerProvider);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setKeyCoder(Coder<K> keyCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setValueCoder(Coder<V> valueCoder);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCreateWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimatorT> fn);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+    abstract Builder<K, V, WatermarkEstimatorT> setTimestampPolicyFactory(\n+        TimestampPolicyFactory<K, V> policy);\n+\n+    abstract ReadViaSDF<K, V, WatermarkEstimatorT> build();\n+  }\n+\n+  public static <K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      ReadViaSDF<K, V, WatermarkEstimatorT> read() {\n+    return new AutoValue_ReadViaSDF.Builder<K, V, WatermarkEstimatorT>()\n+        .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+        .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+        .setCommitOffsetEnabled(false)\n+        .build()\n+        .withProcessingTime()\n+        .withMonotonicallyIncreasingWatermarkEstimator();\n+  }\n+\n+  // Note that if the bootstrapServers is set here but also populated with the element, the element\n+  // will override the bootstrapServers from the config.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withBootstrapServers(String bootstrapServers) {\n+    return withConsumerConfigUpdates(\n+        ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerProvider(\n+      DeserializerProvider<K> deserializerProvider) {\n+    return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerProvider(\n+      DeserializerProvider<V> deserializerProvider) {\n+    return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializer(\n+      Class<? extends Deserializer<K>> keyDeserializer) {\n+    return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializer(\n+      Class<? extends Deserializer<V>> valueDeserializer) {\n+    return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withKeyDeserializerAndCoder(\n+      Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+    return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withValueDeserializerAndCoder(\n+      Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+    return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerFactoryFn(\n+      SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+    return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigUpdates(\n+      Map<String, Object> configUpdates) {\n+    Map<String, Object> config =\n+        KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+    return toBuilder().setConsumerConfig(config).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withExtractOutputTimestampFn(\n+      SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+    return toBuilder().setExtractOutputTimestampFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreatWatermarkEstimatorFn(\n+      SerializableFunction<Instant, WatermarkEstimatorT> fn) {\n+    return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withLogAppendTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useLogAppendTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withProcessingTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useProcessingTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withCreateTime() {\n+    return withExtractOutputTimestampFn(ExtractOutputTimestampFns.useCreateTime());\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withWallTimeWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new WallTime(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withMonotonicallyIncreasingWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new MonotonicallyIncreasing(state);\n+        });\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withManualWatermarkEstimator() {\n+    return withCreatWatermarkEstimatorFn(\n+        state -> {\n+          return (WatermarkEstimatorT) new Manual(state);\n+        });\n+  }\n+\n+  // If a transactional producer is used and it's desired to only read records from committed\n+  // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the default\n+  // value.\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withReadCommitted() {\n+    return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> commitOffsets() {\n+    return toBuilder().setCommitOffsetEnabled(true).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withOffsetConsumerConfigOverrides(\n+      Map<String, Object> offsetConsumerConfig) {\n+    return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+  }\n+\n+  public ReadViaSDF<K, V, WatermarkEstimatorT> withConsumerConfigOverrides(\n+      Map<String, Object> consumerConfig) {\n+    return toBuilder().setConsumerConfig(consumerConfig).build();\n+  }\n+\n+  ReadViaSDFExternally forExternalBuild() {\n+    return new ReadViaSDFExternally(this);\n+  }\n+\n+  private static class ReadViaSDFExternally<\n+          K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends PTransform<PCollection<Row>, PCollection<KV<K, V>>> {\n+\n+    private final ReadViaSDF<K, V, WatermarkEstimatorT> readViaSDF;\n+\n+    ReadViaSDFExternally(ReadViaSDF read) {\n+      readViaSDF = read;\n+    }\n+\n+    @Override\n+    public PCollection<KV<K, V>> expand(PCollection<Row> input) {\n+      return input\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<Row, KafkaSourceDescription>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element Row row, OutputReceiver<KafkaSourceDescription> outputReceiver) {\n+                      TopicPartition topicPartition =\n+                          new TopicPartition(\n+                              row.getString(Schemas.TOPIC), row.getInt32(Schemas.PARTITION));\n+                      Instant startReadTime =\n+                          row.getInt64(Schemas.START_READ_TIME) != null\n+                              ? Instant.ofEpochMilli(row.getInt64(Schemas.START_READ_TIME))\n+                              : null;\n+                      outputReceiver.output(\n+                          KafkaSourceDescription.of(\n+                              topicPartition,\n+                              row.getInt64(Schemas.START_READ_OFFSET),\n+                              startReadTime,\n+                              new ArrayList<>(row.getArray(Schemas.BOOTSTRAP_SERVERS))));\n+                    }\n+                  }))\n+          .apply(readViaSDF)\n+          .apply(\n+              ParDo.of(\n+                  new DoFn<KafkaRecord<K, V>, KV<K, V>>() {\n+                    @ProcessElement\n+                    public void processElement(\n+                        @Element KafkaRecord element, OutputReceiver<KV<K, V>> outputReceiver) {\n+                      outputReceiver.output(element.getKV());\n+                    }\n+                  }))\n+          .setCoder(KvCoder.<K, V>of(readViaSDF.getKeyCoder(), readViaSDF.getValueCoder()));\n+    }\n+  }\n+\n+  ReadViaSDF<K, V, WatermarkEstimatorT> withTimestampPolicyFactory(\n+      TimestampPolicyFactory<K, V> timestampPolicyFactory) {\n+    return toBuilder().setTimestampPolicyFactory(timestampPolicyFactory).build();\n+  }\n+\n+  @Override\n+  public PCollection<KafkaRecord<K, V>> expand(PCollection<KafkaSourceDescription> input) {\n+    checkArgument(\n+        ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+        \"The ReadFromKafkaViaSDF can only used when beam_fn_api is enabled.\");\n+\n+    checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+    checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+    ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+    if (!consumerSpEL.hasOffsetsForTimes()) {\n+      LOG.warn(\n+          \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+              + \"may not be supported in next release of Apache Beam. \"\n+              + \"Please upgrade your Kafka client version.\",\n+          AppInfoParser.getVersion());\n+    }\n+\n+    if (isCommitOffsetEnabled()) {\n+      if (configuredKafkaCommit()) {\n+        LOG.info(\n+            \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+      }\n+    }\n+\n+    if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+      LOG.warn(\n+          \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescription during runtime. Otherwise, the pipeline will fail.\");\n+    }\n+\n+    CoderRegistry coderRegistry = input.getPipeline().getCoderRegistry();\n+    Coder<K> keyCoder = getKeyCoder(coderRegistry);\n+    Coder<V> valueCoder = getValueCoder(coderRegistry);\n+    Coder<KafkaRecord<K, V>> outputCoder = KafkaRecordCoder.of(keyCoder, valueCoder);\n+    PCollection<KafkaRecord<K, V>> output =\n+        input\n+            .apply(ParDo.of(new ReadFromKafkaDoFn<K, V, WatermarkEstimatorT>(this)))\n+            .setCoder(outputCoder);\n+    // TODO(BEAM-10123): Add CommitOffsetTransform to expansion.\n+    if (isCommitOffsetEnabled() && !configuredKafkaCommit()) {\n+      throw new IllegalStateException(\"Offset committed is not supported yet\");\n+    }\n+    return output;\n+  }\n+\n+  private Coder<K> getKeyCoder(CoderRegistry coderRegistry) {\n+    return (getKeyCoder() != null)\n+        ? getKeyCoder()\n+        : getKeyDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private Coder<V> getValueCoder(CoderRegistry coderRegistry) {\n+    return (getValueCoder() != null)\n+        ? getValueCoder()\n+        : getValueDeserializerProvider().getCoder(coderRegistry);\n+  }\n+\n+  private boolean configuredKafkaCommit() {\n+    return getConsumerConfig().get(\"isolation.level\") == \"read_committed\"\n+        || Boolean.TRUE.equals(getConsumerConfig().get(ConsumerConfig.ENABLE_AUTO_COMMIT_CONFIG));\n+  }\n+\n+  static class ExtractOutputTimestampFns<K, V> {\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useProcessingTime() {\n+      return record -> Instant.now();\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useCreateTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.CREATE_TIME,\n+            \"Kafka record's timestamp is not 'CREATE_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+\n+    public static <K, V> SerializableFunction<KafkaRecord<K, V>, Instant> useLogAppendTime() {\n+      return record -> {\n+        checkArgument(\n+            record.getTimestampType() == KafkaTimestampType.LOG_APPEND_TIME,\n+            \"Kafka record's timestamp is not 'LOG_APPEND_TIME' \"\n+                + \"(topic: %s, partition %s, offset %s, timestamp type '%s')\",\n+            record.getTopic(),\n+            record.getPartition(),\n+            record.getOffset(),\n+            record.getTimestampType());\n+        return new Instant(record.getTimestamp());\n+      };\n+    }\n+  }\n+\n+  /**\n+   * A SplittableDoFn which reads from {@link KafkaSourceDescription} and outputs {@link\n+   * KafkaRecord}. By default, a {@link MonotonicallyIncreasing} watermark estimator is used to\n+   * track watermark.\n+   */\n+  @VisibleForTesting\n+  @UnboundedPerElement\n+  static class ReadFromKafkaDoFn<K, V, WatermarkEstimatorT extends WatermarkEstimator<Instant>>\n+      extends DoFn<KafkaSourceDescription, KafkaRecord<K, V>> {\n+\n+    ReadFromKafkaDoFn(ReadViaSDF transform) {\n+      this.consumerConfig = transform.getConsumerConfig();\n+      this.offsetConsumerConfig = transform.getOffsetConsumerConfig();\n+      this.keyDeserializerProvider = transform.getKeyDeserializerProvider();\n+      this.valueDeserializerProvider = transform.getValueDeserializerProvider();\n+      this.consumerFactoryFn = transform.getConsumerFactoryFn();\n+      this.extractOutputTimestampFn = transform.getExtractOutputTimestampFn();\n+      this.createWatermarkEstimatorFn = transform.getCreateWatermarkEstimatorFn();\n+      this.timestampPolicyFactory = transform.getTimestampPolicyFactory();\n+    }\n+\n+    private final Map<String, Object> offsetConsumerConfig;\n+\n+    private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        consumerFactoryFn;\n+    private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn;\n+    private final SerializableFunction<Instant, WatermarkEstimatorT> createWatermarkEstimatorFn;\n+    private final TimestampPolicyFactory<K, V> timestampPolicyFactory;\n+\n+    // Variables that are initialized when bundle is started and closed when FinishBundle is called.\n+    private transient ConsumerSpEL consumerSpEL = null;\n+    private transient Deserializer<K> keyDeserializerInstance = null;\n+    private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgRecordSize;\n+    private transient HashMap<TopicPartition, KafkaIOUtils.MovingAvg> avgOffsetGap;\n+\n+    private static final Duration KAFKA_POLL_TIMEOUT = Duration.millis(1000);\n+\n+    @VisibleForTesting final DeserializerProvider keyDeserializerProvider;\n+    @VisibleForTesting final DeserializerProvider valueDeserializerProvider;\n+    @VisibleForTesting final Map<String, Object> consumerConfig;\n+\n+    /**\n+     * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+     * fetch backlog.\n+     */\n+    private static class KafkaLatestOffsetEstimator\n+        implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+\n+      private final Consumer<byte[], byte[]> offsetConsumer;\n+      private final TopicPartition topicPartition;\n+      private final ConsumerSpEL consumerSpEL;\n+      private final Supplier<Long> memorizedBacklog;\n+\n+      KafkaLatestOffsetEstimator(\n+          Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+        this.offsetConsumer = offsetConsumer;\n+        this.topicPartition = topicPartition;\n+        this.consumerSpEL = new ConsumerSpEL();\n+        this.consumerSpEL.evaluateAssign(\n+            this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+        memorizedBacklog =\n+            Suppliers.memoizeWithExpiration(\n+                () -> {\n+                  consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+                  return offsetConsumer.position(topicPartition);\n+                },\n+                5,\n+                TimeUnit.SECONDS);\n+      }\n+\n+      @Override\n+      protected void finalize() {\n+        try {\n+          Closeables.close(offsetConsumer, true);\n+        } catch (Exception anyException) {\n+          LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+        }\n+      }\n+\n+      @Override\n+      public long estimate() {\n+        return memorizedBacklog.get();\n+      }\n+    }\n+\n+    @GetInitialRestriction\n+    public OffsetRange initialRestriction(@Element KafkaSourceDescription kafkaSourceDescription) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      try (Consumer<byte[], byte[]> offsetConsumer =\n+          consumerFactoryFn.apply(\n+              KafkaIOUtils.getOffsetConsumerConfig(\n+                  \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+        consumerSpEL.evaluateAssign(\n+            offsetConsumer, ImmutableList.of(kafkaSourceDescription.getTopicPartition()));\n+        long startOffset;\n+        if (kafkaSourceDescription.getStartReadOffset() != null) {\n+          startOffset = kafkaSourceDescription.getStartReadOffset();\n+        } else if (kafkaSourceDescription.getStartReadTime() != null) {\n+          startOffset =\n+              consumerSpEL.offsetForTime(\n+                  offsetConsumer,\n+                  kafkaSourceDescription.getTopicPartition(),\n+                  kafkaSourceDescription.getStartReadTime());\n+        } else {\n+          startOffset = offsetConsumer.position(kafkaSourceDescription.getTopicPartition());\n+        }\n+        return new OffsetRange(startOffset, Long.MAX_VALUE);\n+      }\n+    }\n+\n+    @GetInitialWatermarkEstimatorState\n+    public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+      return currentElementTimestamp;\n+    }\n+\n+    @NewWatermarkEstimator\n+    public WatermarkEstimatorT newWatermarkEstimator(\n+        @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+      return createWatermarkEstimatorFn.apply(watermarkEstimatorState);\n+    }\n+\n+    @GetSize\n+    public double getSize(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        @Restriction OffsetRange offsetRange)\n+        throws Exception {\n+      double numOfRecords =\n+          ((HasProgress) restrictionTracker(kafkaSourceDescription, offsetRange))\n+              .getProgress()\n+              .getWorkRemaining();\n+\n+      // Before processing elements, we don't have a good estimated size of records and offset gap.\n+      if (avgOffsetGap.containsKey(kafkaSourceDescription.getTopicPartition())) {\n+        numOfRecords =\n+            numOfRecords / (1 + avgOffsetGap.get(kafkaSourceDescription.getTopicPartition()).get());\n+      }\n+      return (!avgRecordSize.containsKey(kafkaSourceDescription.getTopicPartition())\n+              ? 1\n+              : avgRecordSize.get(kafkaSourceDescription.getTopicPartition()).get())\n+          * numOfRecords;\n+    }\n+\n+    @NewTracker\n+    public RestrictionTracker<OffsetRange, Long> restrictionTracker(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        @Restriction OffsetRange restriction) {\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      KafkaLatestOffsetEstimator offsetPoller =\n+          new KafkaLatestOffsetEstimator(\n+              consumerFactoryFn.apply(\n+                  KafkaIOUtils.getOffsetConsumerConfig(\n+                      \"tracker-\" + kafkaSourceDescription.getTopicPartition(),\n+                      offsetConsumerConfig,\n+                      updatedConsumerConfig)),\n+              kafkaSourceDescription.getTopicPartition());\n+      return new GrowableOffsetRangeTracker(restriction.getFrom(), offsetPoller);\n+    }\n+\n+    @ProcessElement\n+    public ProcessContinuation processElement(\n+        @Element KafkaSourceDescription kafkaSourceDescription,\n+        RestrictionTracker<OffsetRange, Long> tracker,\n+        WatermarkEstimator watermarkEstimator,\n+        OutputReceiver<KafkaRecord<K, V>> receiver) {\n+      // If there is no future work, resume with max timeout and move to the next element.\n+      if (((HasProgress) tracker).getProgress().getWorkRemaining() <= 0.0) {\n+        return ProcessContinuation.resume().withResumeDelay(KAFKA_POLL_TIMEOUT);\n+      }\n+      Map<String, Object> updatedConsumerConfig =\n+          overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescription);\n+      // If there is a timestampPolicyFactory, create the TimestampPolicy for current\n+      // TopicPartition.\n+      TimestampPolicy timestampPolicy = null;\n+      if (timestampPolicyFactory != null) {\n+        timestampPolicy =\n+            timestampPolicyFactory.createTimestampPolicy(\n+                kafkaSourceDescription.getTopicPartition(),\n+                Optional.ofNullable(watermarkEstimator.currentWatermark()));\n+      }\n+      try (Consumer<byte[], byte[]> consumer = consumerFactoryFn.apply(updatedConsumerConfig)) {\n+        consumerSpEL.evaluateAssign(\n+            consumer, ImmutableList.of(kafkaSourceDescription.getTopicPartition()));\n+        long startOffset = tracker.currentRestriction().getFrom();\n+        long expectedOffset = startOffset;\n+        consumer.seek(kafkaSourceDescription.getTopicPartition(), startOffset);\n+        ConsumerRecords<byte[], byte[]> rawRecords = ConsumerRecords.empty();\n+\n+        try {\n+          while (true) {\n+            rawRecords = consumer.poll(KAFKA_POLL_TIMEOUT.getMillis());\n+            // When there is no records from the current TopicPartition temporarily, self-checkpoint\n+            // and move to process the next element.\n+            if (rawRecords.isEmpty()) {\n+              return ProcessContinuation.resume();\n+            }\n+            for (ConsumerRecord<byte[], byte[]> rawRecord : rawRecords) {\n+              if (!tracker.tryClaim(rawRecord.offset())) {\n+                return ProcessContinuation.stop();\n+              }\n+              KafkaRecord<K, V> kafkaRecord =\n+                  new KafkaRecord<>(\n+                      rawRecord.topic(),\n+                      rawRecord.partition(),\n+                      rawRecord.offset(),\n+                      consumerSpEL.getRecordTimestamp(rawRecord),\n+                      consumerSpEL.getRecordTimestampType(rawRecord),\n+                      ConsumerSpEL.hasHeaders() ? rawRecord.headers() : null,\n+                      keyDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.key()),\n+                      valueDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.value()));\n+              int recordSize =\n+                  (rawRecord.key() == null ? 0 : rawRecord.key().length)\n+                      + (rawRecord.value() == null ? 0 : rawRecord.value().length);\n+              avgOffsetGap\n+                  .computeIfAbsent(kafkaSourceDescription.getTopicPartition(), k -> new MovingAvg())\n+                  .update(expectedOffset - rawRecord.offset());\n+              avgRecordSize\n+                  .computeIfAbsent(kafkaSourceDescription.getTopicPartition(), k -> new MovingAvg())\n+                  .update(recordSize);\n+              expectedOffset = rawRecord.offset() + 1;\n+              Instant outputTimestamp;\n+              // The outputTimestamp and watermark will be computed by timestampPolicy, where the\n+              // WatermarkEstimator should be a Manual one.\n+              if (timestampPolicy != null) {\n+                checkState(watermarkEstimator instanceof ManualWatermarkEstimator);\n+                TimestampPolicyContext context =\n+                    new TimestampPolicyContext(\n+                        (long) ((HasProgress) tracker).getProgress().getWorkRemaining(),\n+                        Instant.now());\n+                outputTimestamp = timestampPolicy.getTimestampForRecord(context, kafkaRecord);\n+                ((ManualWatermarkEstimator) watermarkEstimator)\n+                    .setWatermark(timestampPolicy.getWatermark(context));\n+              } else {\n+                outputTimestamp = extractOutputTimestampFn.apply(kafkaRecord);\n+              }\n+              receiver.outputWithTimestamp(kafkaRecord, outputTimestamp);\n+            }\n+          }\n+        } catch (Exception anyException) {\n+          LOG.error(\"{}: Exception while reading from Kafka\", this, anyException);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDM4ODU0Mw=="}, "originalCommit": {"oid": "e66b075b3a9d802e9fd3c3147e1651d99e30ad27"}, "originalPosition": 815}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MjIwMDg5OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxMzo1Njo0NFrOGoTT1Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxNjoyMjoyMFrOGoZrRg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDkxMjU5Nw==", "bodyText": "I really would like to avoid calling this ReadAll because it is not consistent with the ongoing work where we call ReadAll transforms the ones from PCollection<Read>", "url": "https://github.com/apache/beam/pull/11749#discussion_r444912597", "createdAt": "2020-06-24T13:56:44Z", "author": {"login": "iemejia"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1051,33 +1261,341 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     }\n   }\n \n-  ////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-  private static final Logger LOG = LoggerFactory.getLogger(KafkaIO.class);\n-\n   /**\n-   * Returns a new config map which is merge of current config and updates. Verifies the updates do\n-   * not includes ignored properties.\n+   * A {@link PTransform} to read from Kafka. See {@link KafkaIO} for more information on usage and\n+   * configuration.\n    */\n-  private static Map<String, Object> updateKafkaProperties(\n-      Map<String, Object> currentConfig,\n-      Map<String, String> ignoredProperties,\n-      Map<String, Object> updates) {\n+  @Experimental(Kind.PORTABILITY)\n+  @AutoValue\n+  public abstract static class ReadAll<K, V>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 489}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTAxNjkwMg==", "bodyText": "Thanks for starting the discussion. If taking x-lang usage into consideration, Read is not a good choice as input for these DoFn.", "url": "https://github.com/apache/beam/pull/11749#discussion_r445016902", "createdAt": "2020-06-24T16:22:20Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1051,33 +1261,341 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     }\n   }\n \n-  ////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-  private static final Logger LOG = LoggerFactory.getLogger(KafkaIO.class);\n-\n   /**\n-   * Returns a new config map which is merge of current config and updates. Verifies the updates do\n-   * not includes ignored properties.\n+   * A {@link PTransform} to read from Kafka. See {@link KafkaIO} for more information on usage and\n+   * configuration.\n    */\n-  private static Map<String, Object> updateKafkaProperties(\n-      Map<String, Object> currentConfig,\n-      Map<String, String> ignoredProperties,\n-      Map<String, Object> updates) {\n+  @Experimental(Kind.PORTABILITY)\n+  @AutoValue\n+  public abstract static class ReadAll<K, V>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NDkxMjU5Nw=="}, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 489}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzI1Mjk5OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaSourceDescription.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODoyMTo1M1rOGod4oQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQyMToxMDoxN1rOGojP8Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA4NTg1Nw==", "bodyText": "Did you mean to make this one snake_case as well?\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n              @SchemaFieldName(\"bootstrapServers\")\n          \n          \n            \n              @SchemaFieldName(\"bootstrap_servers\")", "url": "https://github.com/apache/beam/pull/11749#discussion_r445085857", "createdAt": "2020-06-24T18:21:53Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaSourceDescription.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.util.List;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.schemas.AutoValueSchema;\n+import org.apache.beam.sdk.schemas.NoSuchSchemaException;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.schemas.SchemaRegistry;\n+import org.apache.beam.sdk.schemas.annotations.DefaultSchema;\n+import org.apache.beam.sdk.schemas.annotations.SchemaFieldName;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.kafka.common.TopicPartition;\n+import org.joda.time.Instant;\n+\n+/**\n+ * An AutoValue object which represents a Kafka source description. Note that this object should be\n+ * encoded/decoded with equivalent {@link Schema} as a {@link Row} when crossing the wire.\n+ */\n+@DefaultSchema(AutoValueSchema.class)\n+@AutoValue\n+public abstract class KafkaSourceDescription implements Serializable {\n+  @SchemaFieldName(\"topic\")\n+  abstract String getTopic();\n+\n+  @SchemaFieldName(\"partition\")\n+  abstract Integer getPartition();\n+\n+  @SchemaFieldName(\"start_read_offset\")\n+  @Nullable\n+  abstract Long getStartReadOffset();\n+\n+  @SchemaFieldName(\"start_read_time\")\n+  @Nullable\n+  abstract Instant getStartReadTime();\n+\n+  @SchemaFieldName(\"bootstrapServers\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 58}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE3Mzc0NQ==", "bodyText": "+1\nThe default in the inferred schema should be camel-case with the first letter lower-case so this would be a no-op as written (same with topic and partition, but there's value in making them explicit if you want).", "url": "https://github.com/apache/beam/pull/11749#discussion_r445173745", "createdAt": "2020-06-24T21:10:17Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaSourceDescription.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.util.List;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.schemas.AutoValueSchema;\n+import org.apache.beam.sdk.schemas.NoSuchSchemaException;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.schemas.SchemaRegistry;\n+import org.apache.beam.sdk.schemas.annotations.DefaultSchema;\n+import org.apache.beam.sdk.schemas.annotations.SchemaFieldName;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.kafka.common.TopicPartition;\n+import org.joda.time.Instant;\n+\n+/**\n+ * An AutoValue object which represents a Kafka source description. Note that this object should be\n+ * encoded/decoded with equivalent {@link Schema} as a {@link Row} when crossing the wire.\n+ */\n+@DefaultSchema(AutoValueSchema.class)\n+@AutoValue\n+public abstract class KafkaSourceDescription implements Serializable {\n+  @SchemaFieldName(\"topic\")\n+  abstract String getTopic();\n+\n+  @SchemaFieldName(\"partition\")\n+  abstract Integer getPartition();\n+\n+  @SchemaFieldName(\"start_read_offset\")\n+  @Nullable\n+  abstract Long getStartReadOffset();\n+\n+  @SchemaFieldName(\"start_read_time\")\n+  @Nullable\n+  abstract Instant getStartReadTime();\n+\n+  @SchemaFieldName(\"bootstrapServers\")", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA4NTg1Nw=="}, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 58}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzI2NTUyOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaSourceDescription.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODoyNTozN1rOGoeAtw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODoyNTozN1rOGoeAtw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA4NzkyNw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * An AutoValue object which represents a Kafka source description. Note that this object should be\n          \n          \n            \n             * encoded/decoded with equivalent {@link Schema} as a {@link Row} when crossing the wire.\n          \n          \n            \n             * Represents a Kafka source description.\n          \n          \n            \n             *\n          \n          \n            \n             * <p>Note that this object should be encoded/decoded with its corresponding {@link #getCoder schema coder}.", "url": "https://github.com/apache/beam/pull/11749#discussion_r445087927", "createdAt": "2020-06-24T18:25:37Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaSourceDescription.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.util.List;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.schemas.AutoValueSchema;\n+import org.apache.beam.sdk.schemas.NoSuchSchemaException;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.schemas.SchemaRegistry;\n+import org.apache.beam.sdk.schemas.annotations.DefaultSchema;\n+import org.apache.beam.sdk.schemas.annotations.SchemaFieldName;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.kafka.common.TopicPartition;\n+import org.joda.time.Instant;\n+\n+/**\n+ * An AutoValue object which represents a Kafka source description. Note that this object should be\n+ * encoded/decoded with equivalent {@link Schema} as a {@link Row} when crossing the wire.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 39}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzI3NTg3OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODoyODo0MlrOGoeHVA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODoyODo0MlrOGoeHVA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA4OTYyMA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * query Kafka topics from BigQuery table and read these topics via {@link ReadAll}.\n          \n          \n            \n             * query Kafka topics from a BigQuery table and read these topics via {@link ReadAll}.", "url": "https://github.com/apache/beam/pull/11749#discussion_r445089620", "createdAt": "2020-06-24T18:28:42Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -198,6 +213,154 @@\n  *    ...\n  * }</pre>\n  *\n+ * <h2>Read from Kafka as a {@link DoFn}</h2>\n+ *\n+ * {@link ReadAll} is the {@link PTransform} that takes a PCollection of {@link\n+ * KafkaSourceDescription} as input and outputs a PCollection of {@link KafkaRecord}. The core\n+ * implementation is based on {@code SplittableDoFn}. For more details about the concept of {@code\n+ * SplittableDoFn}, please refer to the <a\n+ * href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadAll} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadAll}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 66}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzI3OTcxOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODoyOTo1MFrOGoeJzA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODoyOTo1MFrOGoeJzA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA5MDI1Mg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n          \n          \n            \n             * Note that the {@code bootstrapServers} can also be populated from the {@link KafkaSourceDescriptor}:", "url": "https://github.com/apache/beam/pull/11749#discussion_r445090252", "createdAt": "2020-06-24T18:29:50Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -198,6 +213,154 @@\n  *    ...\n  * }</pre>\n  *\n+ * <h2>Read from Kafka as a {@link DoFn}</h2>\n+ *\n+ * {@link ReadAll} is the {@link PTransform} that takes a PCollection of {@link\n+ * KafkaSourceDescription} as input and outputs a PCollection of {@link KafkaRecord}. The core\n+ * implementation is based on {@code SplittableDoFn}. For more details about the concept of {@code\n+ * SplittableDoFn}, please refer to the <a\n+ * href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadAll} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadAll}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadAll#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadAll#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadAll#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadAll#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadAll#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadAll#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadAll#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadAll#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadAll} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 99}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzI4MjgwOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODozMDo0N1rOGoeLyQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODozMDo0N1rOGoeLyQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA5MDc2MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             *\n          \n          \n            \n             * }</pre>\n          \n          \n            \n             *", "url": "https://github.com/apache/beam/pull/11749#discussion_r445090761", "createdAt": "2020-06-24T18:30:47Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -198,6 +213,154 @@\n  *    ...\n  * }</pre>\n  *\n+ * <h2>Read from Kafka as a {@link DoFn}</h2>\n+ *\n+ * {@link ReadAll} is the {@link PTransform} that takes a PCollection of {@link\n+ * KafkaSourceDescription} as input and outputs a PCollection of {@link KafkaRecord}. The core\n+ * implementation is based on {@code SplittableDoFn}. For more details about the concept of {@code\n+ * SplittableDoFn}, please refer to the <a\n+ * href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadAll} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadAll}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadAll#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadAll#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadAll#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadAll#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadAll#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadAll#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadAll#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadAll#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadAll} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 98}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzI4MzY1OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODozMTowNVrOGoeMZg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODozMTowNVrOGoeMZg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA5MDkxOA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * pipeline\n          \n          \n            \n             * <pre>{@code\n          \n          \n            \n             * pipeline", "url": "https://github.com/apache/beam/pull/11749#discussion_r445090918", "createdAt": "2020-06-24T18:31:05Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -198,6 +213,154 @@\n  *    ...\n  * }</pre>\n  *\n+ * <h2>Read from Kafka as a {@link DoFn}</h2>\n+ *\n+ * {@link ReadAll} is the {@link PTransform} that takes a PCollection of {@link\n+ * KafkaSourceDescription} as input and outputs a PCollection of {@link KafkaRecord}. The core\n+ * implementation is based on {@code SplittableDoFn}. For more details about the concept of {@code\n+ * SplittableDoFn}, please refer to the <a\n+ * href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadAll} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadAll}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadAll#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadAll#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadAll#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadAll#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadAll#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadAll#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadAll#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadAll#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadAll} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 100}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzI4NjIwOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODozMTo1NlrOGoeOFw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODozMTo1NlrOGoeOFw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA5MTM1MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             *", "url": "https://github.com/apache/beam/pull/11749#discussion_r445091351", "createdAt": "2020-06-24T18:31:56Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -198,6 +213,154 @@\n  *    ...\n  * }</pre>\n  *\n+ * <h2>Read from Kafka as a {@link DoFn}</h2>\n+ *\n+ * {@link ReadAll} is the {@link PTransform} that takes a PCollection of {@link\n+ * KafkaSourceDescription} as input and outputs a PCollection of {@link KafkaRecord}. The core\n+ * implementation is based on {@code SplittableDoFn}. For more details about the concept of {@code\n+ * SplittableDoFn}, please refer to the <a\n+ * href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadAll} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadAll}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadAll#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadAll#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadAll#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadAll#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadAll#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadAll#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadAll#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadAll#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadAll} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadAll}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadAll#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadAll#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadAll#withExtractOutputTimestampFn(SerializableFunction)} asks for a function which\n+ * takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadAll#withProcessingTime()}, {@link ReadAll#withCreateTime()} and {@link\n+ * ReadAll#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadAll} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 144}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzI5MjQ3OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODozMzo1NFrOGoeSFg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODozMzo1NFrOGoeSFg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA5MjM3NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             *", "url": "https://github.com/apache/beam/pull/11749#discussion_r445092374", "createdAt": "2020-06-24T18:33:54Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -198,6 +213,154 @@\n  *    ...\n  * }</pre>\n  *\n+ * <h2>Read from Kafka as a {@link DoFn}</h2>\n+ *\n+ * {@link ReadAll} is the {@link PTransform} that takes a PCollection of {@link\n+ * KafkaSourceDescription} as input and outputs a PCollection of {@link KafkaRecord}. The core\n+ * implementation is based on {@code SplittableDoFn}. For more details about the concept of {@code\n+ * SplittableDoFn}, please refer to the <a\n+ * href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadAll} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadAll}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadAll#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadAll#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadAll#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadAll#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadAll#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadAll#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadAll#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadAll#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadAll} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 110}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzI5NDU4OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODozNDozN1rOGoeThw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODozNDozN1rOGoeThw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA5Mjc0Mw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * .apply(Create.of(\n          \n          \n            \n             *    KafkaSourceDescription.of(\n          \n          \n            \n             *      new TopicPartition(\"topic\", 1),\n          \n          \n            \n             *      null,\n          \n          \n            \n             *      null,\n          \n          \n            \n             *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n          \n          \n            \n             * .apply(KafkaIO.readAll()\n          \n          \n            \n             *         .withKeyDeserializer(LongDeserializer.class).\n          \n          \n            \n             *         .withValueDeserializer(StringDeserializer.class));\n          \n          \n            \n             *  .apply(Create.of(\n          \n          \n            \n             *    KafkaSourceDescription.of(\n          \n          \n            \n             *      new TopicPartition(\"topic\", 1),\n          \n          \n            \n             *      null,\n          \n          \n            \n             *      null,\n          \n          \n            \n             *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n          \n          \n            \n             *  .apply(KafkaIO.readAll()\n          \n          \n            \n             *         .withKeyDeserializer(LongDeserializer.class).\n          \n          \n            \n             *         .withValueDeserializer(StringDeserializer.class));", "url": "https://github.com/apache/beam/pull/11749#discussion_r445092743", "createdAt": "2020-06-24T18:34:37Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -198,6 +213,154 @@\n  *    ...\n  * }</pre>\n  *\n+ * <h2>Read from Kafka as a {@link DoFn}</h2>\n+ *\n+ * {@link ReadAll} is the {@link PTransform} that takes a PCollection of {@link\n+ * KafkaSourceDescription} as input and outputs a PCollection of {@link KafkaRecord}. The core\n+ * implementation is based on {@code SplittableDoFn}. For more details about the concept of {@code\n+ * SplittableDoFn}, please refer to the <a\n+ * href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadAll} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadAll}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadAll#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadAll#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadAll#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadAll#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadAll#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadAll#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadAll#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadAll#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadAll} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 109}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzMwNTA5OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODozNzo0N1rOGoeaPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODozNzo0N1rOGoeaPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA5NDQ2MA==", "bodyText": "nit: appropriate appropriate -> appropriate on \n  \n    \n      beam/sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java\n    \n    \n         Line 156\n      in\n      f98104a\n    \n    \n    \n    \n\n        \n          \n            * beginning by setting appropriate appropriate properties in {@link ConsumerConfig}, through {@link", "url": "https://github.com/apache/beam/pull/11749#discussion_r445094460", "createdAt": "2020-06-24T18:37:47Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -198,6 +213,154 @@\n  *    ...", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 51}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzMxMDM1OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaSourceDescription.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODozOToyN1rOGoedmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwMDowNDo0OFrOGonGHw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA5NTMyMA==", "bodyText": "Do you think the name KafkaSourceDescriptor would be more appropriate?", "url": "https://github.com/apache/beam/pull/11749#discussion_r445095320", "createdAt": "2020-06-24T18:39:27Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaSourceDescription.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.util.List;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.schemas.AutoValueSchema;\n+import org.apache.beam.sdk.schemas.NoSuchSchemaException;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.schemas.SchemaRegistry;\n+import org.apache.beam.sdk.schemas.annotations.DefaultSchema;\n+import org.apache.beam.sdk.schemas.annotations.SchemaFieldName;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.kafka.common.TopicPartition;\n+import org.joda.time.Instant;\n+\n+/**\n+ * An AutoValue object which represents a Kafka source description. Note that this object should be\n+ * encoded/decoded with equivalent {@link Schema} as a {@link Row} when crossing the wire.\n+ */\n+@DefaultSchema(AutoValueSchema.class)\n+@AutoValue\n+public abstract class KafkaSourceDescription implements Serializable {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTIzNjc2Nw==", "bodyText": "It seems like Descriptor makes more sense.", "url": "https://github.com/apache/beam/pull/11749#discussion_r445236767", "createdAt": "2020-06-25T00:04:48Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaSourceDescription.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.util.List;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.schemas.AutoValueSchema;\n+import org.apache.beam.sdk.schemas.NoSuchSchemaException;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.schemas.SchemaRegistry;\n+import org.apache.beam.sdk.schemas.annotations.DefaultSchema;\n+import org.apache.beam.sdk.schemas.annotations.SchemaFieldName;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.kafka.common.TopicPartition;\n+import org.joda.time.Instant;\n+\n+/**\n+ * An AutoValue object which represents a Kafka source description. Note that this object should be\n+ * encoded/decoded with equivalent {@link Schema} as a {@link Row} when crossing the wire.\n+ */\n+@DefaultSchema(AutoValueSchema.class)\n+@AutoValue\n+public abstract class KafkaSourceDescription implements Serializable {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA5NTMyMA=="}, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 43}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzMxMjM4OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODo0MDowM1rOGoee2Q==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODo0MDowM1rOGoee2Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA5NTY0MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * that if {@code isolation.level} is set to \"read_committed\" or {@link\n          \n          \n            \n             * that if the {@code isolation.level} is set to \"read_committed\" or {@link", "url": "https://github.com/apache/beam/pull/11749#discussion_r445095641", "createdAt": "2020-06-24T18:40:03Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -198,6 +213,154 @@\n  *    ...\n  * }</pre>\n  *\n+ * <h2>Read from Kafka as a {@link DoFn}</h2>\n+ *\n+ * {@link ReadAll} is the {@link PTransform} that takes a PCollection of {@link\n+ * KafkaSourceDescription} as input and outputs a PCollection of {@link KafkaRecord}. The core\n+ * implementation is based on {@code SplittableDoFn}. For more details about the concept of {@code\n+ * SplittableDoFn}, please refer to the <a\n+ * href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadAll} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadAll}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadAll#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadAll#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadAll#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadAll#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadAll#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadAll#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadAll#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadAll#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadAll} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadAll}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadAll#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 119}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzMxOTUwOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODo0MjowNVrOGoejPA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODo0MjowNVrOGoejPA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA5Njc2NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             *   <li>{@link ReadAll#isCommitOffsetEnabled()} means the same as {@link\n          \n          \n            \n             *   <li>{@link ReadAll#isCommitOffsetEnabled()} has the same meaning as {@link", "url": "https://github.com/apache/beam/pull/11749#discussion_r445096764", "createdAt": "2020-06-24T18:42:05Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -198,6 +213,154 @@\n  *    ...\n  * }</pre>\n  *\n+ * <h2>Read from Kafka as a {@link DoFn}</h2>\n+ *\n+ * {@link ReadAll} is the {@link PTransform} that takes a PCollection of {@link\n+ * KafkaSourceDescription} as input and outputs a PCollection of {@link KafkaRecord}. The core\n+ * implementation is based on {@code SplittableDoFn}. For more details about the concept of {@code\n+ * SplittableDoFn}, please refer to the <a\n+ * href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadAll} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadAll}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadAll#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadAll#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadAll#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadAll#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadAll#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadAll#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadAll#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadAll#isCommitOffsetEnabled()} means the same as {@link", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 85}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzMzNjM5OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODo0NzowNFrOGoetzg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODo0NzowNFrOGoetzg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA5OTQ3MA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * <p>{@link ReadAll#withExtractOutputTimestampFn(SerializableFunction)} asks for a function which\n          \n          \n            \n             * takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n          \n          \n            \n             * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n          \n          \n            \n             * ReadAll#withProcessingTime()}, {@link ReadAll#withCreateTime()} and {@link\n          \n          \n            \n             * ReadAll#withLogAppendTime()}.\n          \n          \n            \n             * <p>{@link ReadAll#withExtractOutputTimestampFn(SerializableFunction)} is used to compute the {@code output timestamp} for a given {@link KafkaRecord}. There are three built-in types: {@link\n          \n          \n            \n             * ReadAll#withProcessingTime()}, {@link ReadAll#withCreateTime()} and {@link\n          \n          \n            \n             * ReadAll#withLogAppendTime()}.", "url": "https://github.com/apache/beam/pull/11749#discussion_r445099470", "createdAt": "2020-06-24T18:47:04Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -198,6 +213,154 @@\n  *    ...\n  * }</pre>\n  *\n+ * <h2>Read from Kafka as a {@link DoFn}</h2>\n+ *\n+ * {@link ReadAll} is the {@link PTransform} that takes a PCollection of {@link\n+ * KafkaSourceDescription} as input and outputs a PCollection of {@link KafkaRecord}. The core\n+ * implementation is based on {@code SplittableDoFn}. For more details about the concept of {@code\n+ * SplittableDoFn}, please refer to the <a\n+ * href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadAll} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadAll}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadAll#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadAll#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadAll#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadAll#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadAll#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadAll#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadAll#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadAll#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadAll} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadAll}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadAll#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadAll#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadAll#withExtractOutputTimestampFn(SerializableFunction)} asks for a function which\n+ * takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadAll#withProcessingTime()}, {@link ReadAll#withCreateTime()} and {@link\n+ * ReadAll#withLogAppendTime()}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 127}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzMzODg3OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODo0Nzo0NlrOGoevYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODo0Nzo0NlrOGoevYg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTA5OTg3NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * <p>For example, to create a {@link ReadAll} with these configurations:\n          \n          \n            \n             * <p>For example, to create a {@link ReadAll} with this additional configuration:", "url": "https://github.com/apache/beam/pull/11749#discussion_r445099874", "createdAt": "2020-06-24T18:47:46Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -198,6 +213,154 @@\n  *    ...\n  * }</pre>\n  *\n+ * <h2>Read from Kafka as a {@link DoFn}</h2>\n+ *\n+ * {@link ReadAll} is the {@link PTransform} that takes a PCollection of {@link\n+ * KafkaSourceDescription} as input and outputs a PCollection of {@link KafkaRecord}. The core\n+ * implementation is based on {@code SplittableDoFn}. For more details about the concept of {@code\n+ * SplittableDoFn}, please refer to the <a\n+ * href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadAll} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadAll}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadAll#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadAll#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadAll#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadAll#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadAll#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadAll#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadAll#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadAll#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadAll} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadAll}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadAll#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadAll#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadAll#withExtractOutputTimestampFn(SerializableFunction)} asks for a function which\n+ * takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadAll#withProcessingTime()}, {@link ReadAll#withCreateTime()} and {@link\n+ * ReadAll#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadAll} with these configurations:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 129}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzM0NzM3OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODo1MDoxNVrOGoe0oA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODo1MDoxNVrOGoe0oA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTEwMTIxNg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * <h4>Initialize Restriction</h4>\n          \n          \n            \n             * <h4>Initial Restriction</h4>", "url": "https://github.com/apache/beam/pull/11749#discussion_r445101216", "createdAt": "2020-06-24T18:50:15Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -198,6 +213,154 @@\n  *    ...\n  * }</pre>\n  *\n+ * <h2>Read from Kafka as a {@link DoFn}</h2>\n+ *\n+ * {@link ReadAll} is the {@link PTransform} that takes a PCollection of {@link\n+ * KafkaSourceDescription} as input and outputs a PCollection of {@link KafkaRecord}. The core\n+ * implementation is based on {@code SplittableDoFn}. For more details about the concept of {@code\n+ * SplittableDoFn}, please refer to the <a\n+ * href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadAll} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadAll}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadAll#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadAll#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadAll#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadAll#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadAll#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadAll#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadAll#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadAll#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadAll} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadAll}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadAll#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadAll#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadAll#withExtractOutputTimestampFn(SerializableFunction)} asks for a function which\n+ * takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadAll#withProcessingTime()}, {@link ReadAll#withCreateTime()} and {@link\n+ * ReadAll#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadAll} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 154}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzM1MjE2OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODo1MTo0NFrOGoe3sg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODo1MTo0NFrOGoe3sg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTEwMjAwMg==", "bodyText": "This is an implementation detail, I'm not sure we want to share it as part of the Javadoc that we want users to read when they look at KafkaIO. It would make sense to have this on the ReadFromKafkaViaDoFn class though.", "url": "https://github.com/apache/beam/pull/11749#discussion_r445102002", "createdAt": "2020-06-24T18:51:44Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -198,6 +213,154 @@\n  *    ...\n  * }</pre>\n  *\n+ * <h2>Read from Kafka as a {@link DoFn}</h2>\n+ *\n+ * {@link ReadAll} is the {@link PTransform} that takes a PCollection of {@link\n+ * KafkaSourceDescription} as input and outputs a PCollection of {@link KafkaRecord}. The core\n+ * implementation is based on {@code SplittableDoFn}. For more details about the concept of {@code\n+ * SplittableDoFn}, please refer to the <a\n+ * href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadAll} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadAll}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadAll#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadAll#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadAll#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadAll#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadAll#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadAll#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadAll#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadAll#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadAll} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadAll}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadAll#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadAll#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadAll#withExtractOutputTimestampFn(SerializableFunction)} asks for a function which\n+ * takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadAll#withProcessingTime()}, {@link ReadAll#withCreateTime()} and {@link\n+ * ReadAll#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadAll} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 152}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzM3NDQ2OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODo1ODoxNVrOGofF5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQxNzowMzo1MlrOGpD00g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTEwNTYzNg==", "bodyText": "Should we be defining endReadOffset and endReadTime which are optional as well?\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n          \n          \n            \n             * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n          \n          \n            \n             * Long.MAX_VALUE}. For the start of the range:\n          \n          \n            \n             *\n          \n          \n            \n             * <ul>\n          \n          \n            \n             *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n          \n          \n            \n             *       start.\n          \n          \n            \n             *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n          \n          \n            \n             *       based on this time.\n          \n          \n            \n             *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n          \n          \n            \n             *       Consumer#position(TopicPartition)} as the start.\n          \n          \n            \n             * </ul>\n          \n          \n            \n             * {@link The initial range for\n          \n          \n            \n             * a {@link KafkaSourceDescription} is defined by {@code [startOffset, Long.MAX_VALUE)} where {@code startOffset} is defined as:\n          \n          \n            \n             *\n          \n          \n            \n             * <ul>\n          \n          \n            \n             *   <li>the {@code startReadOffset} if {@link KafkaSourceDescription#getStartReadOffset} is set.\n          \n          \n            \n             *   <li>the first offset with a greater or equivalent timestamp if {@link KafkaSourceDescription#getStartReadTimestamp} is set.\n          \n          \n            \n             *   <li>the {@code last committed offset + 1} for the {@link\n          \n          \n            \n             *       Consumer#position(TopicPartition) topic partition}.\n          \n          \n            \n             * </ul>", "url": "https://github.com/apache/beam/pull/11749#discussion_r445105636", "createdAt": "2020-06-24T18:58:15Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -198,6 +213,154 @@\n  *    ...\n  * }</pre>\n  *\n+ * <h2>Read from Kafka as a {@link DoFn}</h2>\n+ *\n+ * {@link ReadAll} is the {@link PTransform} that takes a PCollection of {@link\n+ * KafkaSourceDescription} as input and outputs a PCollection of {@link KafkaRecord}. The core\n+ * implementation is based on {@code SplittableDoFn}. For more details about the concept of {@code\n+ * SplittableDoFn}, please refer to the <a\n+ * href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadAll} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadAll}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadAll#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadAll#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadAll#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadAll#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadAll#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadAll#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadAll#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadAll#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadAll} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadAll}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadAll#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadAll#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadAll#withExtractOutputTimestampFn(SerializableFunction)} asks for a function which\n+ * takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadAll#withProcessingTime()}, {@link ReadAll#withCreateTime()} and {@link\n+ * ReadAll#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadAll} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTI0MjgzNw==", "bodyText": "Are endReadOffset and endReadTime for batch reading case? Is endReadTime for process time or for event time?", "url": "https://github.com/apache/beam/pull/11749#discussion_r445242837", "createdAt": "2020-06-25T00:27:22Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -198,6 +213,154 @@\n  *    ...\n  * }</pre>\n  *\n+ * <h2>Read from Kafka as a {@link DoFn}</h2>\n+ *\n+ * {@link ReadAll} is the {@link PTransform} that takes a PCollection of {@link\n+ * KafkaSourceDescription} as input and outputs a PCollection of {@link KafkaRecord}. The core\n+ * implementation is based on {@code SplittableDoFn}. For more details about the concept of {@code\n+ * SplittableDoFn}, please refer to the <a\n+ * href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadAll} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadAll}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadAll#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadAll#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadAll#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadAll#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadAll#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadAll#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadAll#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadAll#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadAll} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadAll}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadAll#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadAll#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadAll#withExtractOutputTimestampFn(SerializableFunction)} asks for a function which\n+ * takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadAll#withProcessingTime()}, {@link ReadAll#withCreateTime()} and {@link\n+ * ReadAll#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadAll} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTEwNTYzNg=="}, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 167}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTcwNzQ3NA==", "bodyText": "endReadTime is the same time domain as startReadTime.\nHaving an end would be primarily for batch but could also be useful in streaming pipelines.", "url": "https://github.com/apache/beam/pull/11749#discussion_r445707474", "createdAt": "2020-06-25T17:03:52Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -198,6 +213,154 @@\n  *    ...\n  * }</pre>\n  *\n+ * <h2>Read from Kafka as a {@link DoFn}</h2>\n+ *\n+ * {@link ReadAll} is the {@link PTransform} that takes a PCollection of {@link\n+ * KafkaSourceDescription} as input and outputs a PCollection of {@link KafkaRecord}. The core\n+ * implementation is based on {@code SplittableDoFn}. For more details about the concept of {@code\n+ * SplittableDoFn}, please refer to the <a\n+ * href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadAll} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadAll}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadAll#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadAll#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadAll#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadAll#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadAll#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadAll#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadAll#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadAll#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadAll} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadAll}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadAll#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadAll#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadAll#withExtractOutputTimestampFn(SerializableFunction)} asks for a function which\n+ * takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadAll#withProcessingTime()}, {@link ReadAll#withCreateTime()} and {@link\n+ * ReadAll#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadAll} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTEwNTYzNg=="}, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 167}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzM3NTc1OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODo1ODo0M1rOGofGtQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxODo1ODo0M1rOGofGtQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTEwNTg0NQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * <p>There is no initial split for now.\n          \n          \n            \n             * <p>TODO(BEAM-YYY): Add support for initial splitting.", "url": "https://github.com/apache/beam/pull/11749#discussion_r445105845", "createdAt": "2020-06-24T18:58:43Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -198,6 +213,154 @@\n  *    ...\n  * }</pre>\n  *\n+ * <h2>Read from Kafka as a {@link DoFn}</h2>\n+ *\n+ * {@link ReadAll} is the {@link PTransform} that takes a PCollection of {@link\n+ * KafkaSourceDescription} as input and outputs a PCollection of {@link KafkaRecord}. The core\n+ * implementation is based on {@code SplittableDoFn}. For more details about the concept of {@code\n+ * SplittableDoFn}, please refer to the <a\n+ * href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadAll} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadAll}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadAll#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadAll#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadAll#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadAll#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadAll#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadAll#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadAll#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadAll#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadAll} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadAll}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadAll#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadAll#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadAll#withExtractOutputTimestampFn(SerializableFunction)} asks for a function which\n+ * takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadAll#withProcessingTime()}, {@link ReadAll#withCreateTime()} and {@link\n+ * ReadAll#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadAll} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 171}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzM4MDI4OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxOTowMDowNVrOGofJow==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxOTowMDowNVrOGofJow==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTEwNjU5NQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * <h4>Initial Split</h4>\n          \n          \n            \n             * <h4>Splitting</h4>", "url": "https://github.com/apache/beam/pull/11749#discussion_r445106595", "createdAt": "2020-06-24T19:00:05Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -198,6 +213,154 @@\n  *    ...\n  * }</pre>\n  *\n+ * <h2>Read from Kafka as a {@link DoFn}</h2>\n+ *\n+ * {@link ReadAll} is the {@link PTransform} that takes a PCollection of {@link\n+ * KafkaSourceDescription} as input and outputs a PCollection of {@link KafkaRecord}. The core\n+ * implementation is based on {@code SplittableDoFn}. For more details about the concept of {@code\n+ * SplittableDoFn}, please refer to the <a\n+ * href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadAll} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadAll}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadAll#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadAll#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadAll#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadAll#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadAll#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadAll#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadAll#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadAll#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadAll} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadAll}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadAll#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadAll#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadAll#withExtractOutputTimestampFn(SerializableFunction)} asks for a function which\n+ * takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadAll#withProcessingTime()}, {@link ReadAll#withCreateTime()} and {@link\n+ * ReadAll#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadAll} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 169}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzUxMjIyOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxOTozOTo1NFrOGogbDQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQxOTozOTo1NFrOGogbDQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTEyNzQzNw==", "bodyText": "Move to ReadFromKafkaViaDoFn", "url": "https://github.com/apache/beam/pull/11749#discussion_r445127437", "createdAt": "2020-06-24T19:39:54Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -198,6 +213,154 @@\n  *    ...\n  * }</pre>\n  *\n+ * <h2>Read from Kafka as a {@link DoFn}</h2>\n+ *\n+ * {@link ReadAll} is the {@link PTransform} that takes a PCollection of {@link\n+ * KafkaSourceDescription} as input and outputs a PCollection of {@link KafkaRecord}. The core\n+ * implementation is based on {@code SplittableDoFn}. For more details about the concept of {@code\n+ * SplittableDoFn}, please refer to the <a\n+ * href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadAll} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from BigQuery table and read these topics via {@link ReadAll}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadAll#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadAll#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadAll#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadAll#getKeyCoder()} is the same as {@link KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadAll#getValueCoder()} is the same as {@link KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadAll#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadAll#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadAll#isCommitOffsetEnabled()} means the same as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadAll} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescription.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from {@link KafkaSourceDescription}:\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadAll}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadAll#commitOffsets()} enables committing offset after processing the record. Note\n+ * that if {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadAll#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadAll#withExtractOutputTimestampFn(SerializableFunction)} asks for a function which\n+ * takes a {@link KafkaRecord} as input and outputs outputTimestamp. This function is used to\n+ * produce output timestamp per {@link KafkaRecord}. There are three built-in types: {@link\n+ * ReadAll#withProcessingTime()}, {@link ReadAll#withCreateTime()} and {@link\n+ * ReadAll#withLogAppendTime()}.\n+ *\n+ * <p>For example, to create a {@link ReadAll} with these configurations:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ * .apply(Create.of(\n+ *    KafkaSourceDescription.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ * .apply(KafkaIO.readAll()\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class)\n+ *          .withProcessingTime()\n+ *          .commitOffsets());\n+ *\n+ * }</pre>\n+ *\n+ * <h3>Read from {@link KafkaSourceDescription}</h3>\n+ *\n+ * {@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescription}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initialize Restriction</h4>\n+ *\n+ * {@link ReadFromKafkaDoFn#initialRestriction(KafkaSourceDescription)} creates an initial range for\n+ * a input element {@link KafkaSourceDescription}. The end of range will be initialized as {@code\n+ * Long.MAX_VALUE}. For the start of the range:\n+ *\n+ * <ul>\n+ *   <li>If {@code startReadOffset} in {@link KafkaSourceDescription} is set, use this offset as\n+ *       start.\n+ *   <li>If {@code startReadTime} in {@link KafkaSourceDescription} is set, seek the start offset\n+ *       based on this time.\n+ *   <li>Otherwise, the last committed offset + 1 will be returned by {@link\n+ *       Consumer#position(TopicPartition)} as the start.\n+ * </ul>\n+ *\n+ * <h4>Initial Split</h4>\n+ *\n+ * <p>There is no initial split for now.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescription} and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescription}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn#restrictionTracker(KafkaSourceDescription, OffsetRange)} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescription,\n+ * OffsetRange).} A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka\n+ * records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * The {@link WatermarkEstimator} is created by {@link ReadAll#getCreateWatermarkEstimatorFn()}. The\n+ * estimated watermark is computed by this {@link WatermarkEstimator} based on output timestamps\n+ * computed by {@link ReadAll#getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link ReadAll#withProcessingTime()} as {@code extractTimestampFn} and\n+ * {@link ReadAll#withMonotonicallyIncreasingWatermarkEstimator()} as {@link WatermarkEstimator}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 200}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzY2ODgwOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQyMDoyODozOVrOGoh9WA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQyMDoyODozOVrOGoh9WA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE1MjYwMA==", "bodyText": "I think we can keep the original comment here since we support the timestamp policy fn now.", "url": "https://github.com/apache/beam/pull/11749#discussion_r445152600", "createdAt": "2020-06-24T20:28:39Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -729,12 +902,15 @@ public void setValueDeserializer(String valueDeserializer) {\n \n     /**\n      * Provide custom {@link TimestampPolicyFactory} to set event times and watermark for each\n-     * partition. {@link TimestampPolicyFactory#createTimestampPolicy(TopicPartition, Optional)} is\n-     * invoked for each partition when the reader starts.\n+     * partition when beam_fn_api is disabled. {@link", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 282}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzY2OTcxOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQyMDoyODo1N1rOGoh98g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQyMDoyODo1N1rOGoh98g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE1Mjc1NA==", "bodyText": "I don't think we need this suggestion anymore.", "url": "https://github.com/apache/beam/pull/11749#discussion_r445152754", "createdAt": "2020-06-24T20:28:57Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -729,12 +902,15 @@ public void setValueDeserializer(String valueDeserializer) {\n \n     /**\n      * Provide custom {@link TimestampPolicyFactory} to set event times and watermark for each\n-     * partition. {@link TimestampPolicyFactory#createTimestampPolicy(TopicPartition, Optional)} is\n-     * invoked for each partition when the reader starts.\n+     * partition when beam_fn_api is disabled. {@link\n+     * TimestampPolicyFactory#createTimestampPolicy(TopicPartition, Optional)} is invoked for each\n+     * partition when the reader starts.\n      *\n      * @see #withLogAppendTime()\n      * @see #withCreateTime(Duration)\n      * @see #withProcessingTime()\n+     *     <p>For the pipeline with beam_fn_api is enabled, you should use {@link", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 289}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzY3NzY4OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQyMDozMToyMlrOGoiC1A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQyMDozMToyMlrOGoiC1A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE1NDAwNA==", "bodyText": "Should this be javadoc?\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                // If a transactional producer is used and it's desired to only read records from committed\n          \n          \n            \n                // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the\n          \n          \n            \n                // default\n          \n          \n            \n                // value.\n          \n          \n            \n                // If a transactional producer is used and it's desired to only read records from committed\n          \n          \n            \n                // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the\n          \n          \n            \n                // default value.", "url": "https://github.com/apache/beam/pull/11749#discussion_r445154004", "createdAt": "2020-06-24T20:31:22Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1051,33 +1261,341 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     }\n   }\n \n-  ////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-  private static final Logger LOG = LoggerFactory.getLogger(KafkaIO.class);\n-\n   /**\n-   * Returns a new config map which is merge of current config and updates. Verifies the updates do\n-   * not includes ignored properties.\n+   * A {@link PTransform} to read from Kafka. See {@link KafkaIO} for more information on usage and\n+   * configuration.\n    */\n-  private static Map<String, Object> updateKafkaProperties(\n-      Map<String, Object> currentConfig,\n-      Map<String, String> ignoredProperties,\n-      Map<String, Object> updates) {\n+  @Experimental(Kind.PORTABILITY)\n+  @AutoValue\n+  public abstract static class ReadAll<K, V>\n+      extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(ReadAll.class);\n+\n+    abstract Map<String, Object> getConsumerConfig();\n+\n+    @Nullable\n+    abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+    @Nullable\n+    abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+    @Nullable\n+    abstract DeserializerProvider getValueDeserializerProvider();\n+\n+    @Nullable\n+    abstract Coder<K> getKeyCoder();\n+\n+    @Nullable\n+    abstract Coder<V> getValueCoder();\n+\n+    abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        getConsumerFactoryFn();\n+\n+    @Nullable\n+    abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+    @Nullable\n+    abstract SerializableFunction<Instant, WatermarkEstimator<Instant>>\n+        getCreateWatermarkEstimatorFn();\n+\n+    abstract boolean isCommitOffsetEnabled();\n+\n+    @Nullable\n+    abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+    abstract ReadAll.Builder<K, V> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<K, V> {\n+      abstract ReadAll.Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+      abstract ReadAll.Builder<K, V> setOffsetConsumerConfig(\n+          Map<String, Object> offsetConsumerConfig);\n+\n+      abstract ReadAll.Builder<K, V> setConsumerFactoryFn(\n+          SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+      abstract ReadAll.Builder<K, V> setKeyDeserializerProvider(\n+          DeserializerProvider deserializerProvider);\n+\n+      abstract ReadAll.Builder<K, V> setValueDeserializerProvider(\n+          DeserializerProvider deserializerProvider);\n+\n+      abstract ReadAll.Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+      abstract ReadAll.Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+      abstract ReadAll.Builder<K, V> setExtractOutputTimestampFn(\n+          SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+      abstract ReadAll.Builder<K, V> setCreateWatermarkEstimatorFn(\n+          SerializableFunction<Instant, WatermarkEstimator<Instant>> fn);\n+\n+      abstract ReadAll.Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+      abstract ReadAll.Builder<K, V> setTimestampPolicyFactory(TimestampPolicyFactory<K, V> policy);\n+\n+      abstract ReadAll<K, V> build();\n+    }\n \n-    for (String key : updates.keySet()) {\n+    public static <K, V> ReadAll<K, V> read() {\n+      return new AutoValue_KafkaIO_ReadAll.Builder<K, V>()\n+          .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+          .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+          .setCommitOffsetEnabled(false)\n+          .build()\n+          .withProcessingTime()\n+          .withMonotonicallyIncreasingWatermarkEstimator();\n+    }\n+\n+    // Note that if the bootstrapServers is set here but also populated with the element, the\n+    // element\n+    // will override the bootstrapServers from the config.\n+    public ReadAll<K, V> withBootstrapServers(String bootstrapServers) {\n+      return withConsumerConfigUpdates(\n+          ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+    }\n+\n+    public ReadAll<K, V> withKeyDeserializerProvider(DeserializerProvider<K> deserializerProvider) {\n+      return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+    }\n+\n+    public ReadAll<K, V> withValueDeserializerProvider(\n+        DeserializerProvider<V> deserializerProvider) {\n+      return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+    }\n+\n+    public ReadAll<K, V> withKeyDeserializer(Class<? extends Deserializer<K>> keyDeserializer) {\n+      return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+    }\n+\n+    public ReadAll<K, V> withValueDeserializer(Class<? extends Deserializer<V>> valueDeserializer) {\n+      return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+    }\n+\n+    public ReadAll<K, V> withKeyDeserializerAndCoder(\n+        Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+      return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+    }\n+\n+    public ReadAll<K, V> withValueDeserializerAndCoder(\n+        Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+      return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+    }\n+\n+    public ReadAll<K, V> withConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+      return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+    }\n+\n+    public ReadAll<K, V> withConsumerConfigUpdates(Map<String, Object> configUpdates) {\n+      Map<String, Object> config =\n+          KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+      return toBuilder().setConsumerConfig(config).build();\n+    }\n+\n+    public ReadAll<K, V> withExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+      return toBuilder().setExtractOutputTimestampFn(fn).build();\n+    }\n+\n+    public ReadAll<K, V> withCreatWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimator<Instant>> fn) {\n+      return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+    }\n+\n+    public ReadAll<K, V> withLogAppendTime() {\n+      return withExtractOutputTimestampFn(ReadAll.ExtractOutputTimestampFns.useLogAppendTime());\n+    }\n+\n+    public ReadAll<K, V> withProcessingTime() {\n+      return withExtractOutputTimestampFn(ReadAll.ExtractOutputTimestampFns.useProcessingTime());\n+    }\n+\n+    public ReadAll<K, V> withCreateTime() {\n+      return withExtractOutputTimestampFn(ReadAll.ExtractOutputTimestampFns.useCreateTime());\n+    }\n+\n+    public ReadAll<K, V> withWallTimeWatermarkEstimator() {\n+      return withCreatWatermarkEstimatorFn(\n+          state -> {\n+            return new WallTime(state);\n+          });\n+    }\n+\n+    public ReadAll<K, V> withMonotonicallyIncreasingWatermarkEstimator() {\n+      return withCreatWatermarkEstimatorFn(\n+          state -> {\n+            return new MonotonicallyIncreasing(state);\n+          });\n+    }\n+\n+    public ReadAll<K, V> withManualWatermarkEstimator() {\n+      return withCreatWatermarkEstimatorFn(\n+          state -> {\n+            return new Manual(state);\n+          });\n+    }\n+\n+    // If a transactional producer is used and it's desired to only read records from committed\n+    // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the\n+    // default\n+    // value.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 664}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzY5OTI3OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQyMDozODowMFrOGoiQYg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNVQwMDowMDo1MlrOGonB4Q==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE1NzQ3NA==", "bodyText": "We should move this to the external transform builder and remove the forExternalBuild method.", "url": "https://github.com/apache/beam/pull/11749#discussion_r445157474", "createdAt": "2020-06-24T20:38:00Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1051,33 +1261,341 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     }\n   }\n \n-  ////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-  private static final Logger LOG = LoggerFactory.getLogger(KafkaIO.class);\n-\n   /**\n-   * Returns a new config map which is merge of current config and updates. Verifies the updates do\n-   * not includes ignored properties.\n+   * A {@link PTransform} to read from Kafka. See {@link KafkaIO} for more information on usage and\n+   * configuration.\n    */\n-  private static Map<String, Object> updateKafkaProperties(\n-      Map<String, Object> currentConfig,\n-      Map<String, String> ignoredProperties,\n-      Map<String, Object> updates) {\n+  @Experimental(Kind.PORTABILITY)\n+  @AutoValue\n+  public abstract static class ReadAll<K, V>\n+      extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(ReadAll.class);\n+\n+    abstract Map<String, Object> getConsumerConfig();\n+\n+    @Nullable\n+    abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+    @Nullable\n+    abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+    @Nullable\n+    abstract DeserializerProvider getValueDeserializerProvider();\n+\n+    @Nullable\n+    abstract Coder<K> getKeyCoder();\n+\n+    @Nullable\n+    abstract Coder<V> getValueCoder();\n+\n+    abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        getConsumerFactoryFn();\n+\n+    @Nullable\n+    abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+    @Nullable\n+    abstract SerializableFunction<Instant, WatermarkEstimator<Instant>>\n+        getCreateWatermarkEstimatorFn();\n+\n+    abstract boolean isCommitOffsetEnabled();\n+\n+    @Nullable\n+    abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+    abstract ReadAll.Builder<K, V> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<K, V> {\n+      abstract ReadAll.Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+      abstract ReadAll.Builder<K, V> setOffsetConsumerConfig(\n+          Map<String, Object> offsetConsumerConfig);\n+\n+      abstract ReadAll.Builder<K, V> setConsumerFactoryFn(\n+          SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+      abstract ReadAll.Builder<K, V> setKeyDeserializerProvider(\n+          DeserializerProvider deserializerProvider);\n+\n+      abstract ReadAll.Builder<K, V> setValueDeserializerProvider(\n+          DeserializerProvider deserializerProvider);\n+\n+      abstract ReadAll.Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+      abstract ReadAll.Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+      abstract ReadAll.Builder<K, V> setExtractOutputTimestampFn(\n+          SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+      abstract ReadAll.Builder<K, V> setCreateWatermarkEstimatorFn(\n+          SerializableFunction<Instant, WatermarkEstimator<Instant>> fn);\n+\n+      abstract ReadAll.Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+      abstract ReadAll.Builder<K, V> setTimestampPolicyFactory(TimestampPolicyFactory<K, V> policy);\n+\n+      abstract ReadAll<K, V> build();\n+    }\n \n-    for (String key : updates.keySet()) {\n+    public static <K, V> ReadAll<K, V> read() {\n+      return new AutoValue_KafkaIO_ReadAll.Builder<K, V>()\n+          .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+          .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+          .setCommitOffsetEnabled(false)\n+          .build()\n+          .withProcessingTime()\n+          .withMonotonicallyIncreasingWatermarkEstimator();\n+    }\n+\n+    // Note that if the bootstrapServers is set here but also populated with the element, the\n+    // element\n+    // will override the bootstrapServers from the config.\n+    public ReadAll<K, V> withBootstrapServers(String bootstrapServers) {\n+      return withConsumerConfigUpdates(\n+          ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+    }\n+\n+    public ReadAll<K, V> withKeyDeserializerProvider(DeserializerProvider<K> deserializerProvider) {\n+      return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+    }\n+\n+    public ReadAll<K, V> withValueDeserializerProvider(\n+        DeserializerProvider<V> deserializerProvider) {\n+      return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+    }\n+\n+    public ReadAll<K, V> withKeyDeserializer(Class<? extends Deserializer<K>> keyDeserializer) {\n+      return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+    }\n+\n+    public ReadAll<K, V> withValueDeserializer(Class<? extends Deserializer<V>> valueDeserializer) {\n+      return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+    }\n+\n+    public ReadAll<K, V> withKeyDeserializerAndCoder(\n+        Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+      return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+    }\n+\n+    public ReadAll<K, V> withValueDeserializerAndCoder(\n+        Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+      return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+    }\n+\n+    public ReadAll<K, V> withConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+      return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+    }\n+\n+    public ReadAll<K, V> withConsumerConfigUpdates(Map<String, Object> configUpdates) {\n+      Map<String, Object> config =\n+          KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+      return toBuilder().setConsumerConfig(config).build();\n+    }\n+\n+    public ReadAll<K, V> withExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+      return toBuilder().setExtractOutputTimestampFn(fn).build();\n+    }\n+\n+    public ReadAll<K, V> withCreatWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimator<Instant>> fn) {\n+      return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+    }\n+\n+    public ReadAll<K, V> withLogAppendTime() {\n+      return withExtractOutputTimestampFn(ReadAll.ExtractOutputTimestampFns.useLogAppendTime());\n+    }\n+\n+    public ReadAll<K, V> withProcessingTime() {\n+      return withExtractOutputTimestampFn(ReadAll.ExtractOutputTimestampFns.useProcessingTime());\n+    }\n+\n+    public ReadAll<K, V> withCreateTime() {\n+      return withExtractOutputTimestampFn(ReadAll.ExtractOutputTimestampFns.useCreateTime());\n+    }\n+\n+    public ReadAll<K, V> withWallTimeWatermarkEstimator() {\n+      return withCreatWatermarkEstimatorFn(\n+          state -> {\n+            return new WallTime(state);\n+          });\n+    }\n+\n+    public ReadAll<K, V> withMonotonicallyIncreasingWatermarkEstimator() {\n+      return withCreatWatermarkEstimatorFn(\n+          state -> {\n+            return new MonotonicallyIncreasing(state);\n+          });\n+    }\n+\n+    public ReadAll<K, V> withManualWatermarkEstimator() {\n+      return withCreatWatermarkEstimatorFn(\n+          state -> {\n+            return new Manual(state);\n+          });\n+    }\n+\n+    // If a transactional producer is used and it's desired to only read records from committed\n+    // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the\n+    // default\n+    // value.\n+    public ReadAll<K, V> withReadCommitted() {\n+      return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+    }\n+\n+    public ReadAll<K, V> commitOffsets() {\n+      return toBuilder().setCommitOffsetEnabled(true).build();\n+    }\n+\n+    public ReadAll<K, V> withOffsetConsumerConfigOverrides(\n+        Map<String, Object> offsetConsumerConfig) {\n+      return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+    }\n+\n+    public ReadAll<K, V> withConsumerConfigOverrides(Map<String, Object> consumerConfig) {\n+      return toBuilder().setConsumerConfig(consumerConfig).build();\n+    }\n+\n+    ReadAllFromRow forExternalBuild() {\n+      return new ReadAllFromRow(this);\n+    }\n+\n+    // This transform is used in cross-language case. The input Row should be encoded with an\n+    // equivalent schema as KafkaSourceDescription.\n+    private static class ReadAllFromRow<K, V>\n+        extends PTransform<PCollection<Row>, PCollection<KV<K, V>>> {\n+\n+      private final ReadAll<K, V> readViaSDF;\n+\n+      ReadAllFromRow(ReadAll read) {\n+        readViaSDF = read;\n+      }\n+\n+      @Override\n+      public PCollection<KV<K, V>> expand(PCollection<Row> input) {\n+        return input\n+            .apply(Convert.fromRows(KafkaSourceDescription.class))\n+            .apply(readViaSDF)\n+            .apply(\n+                ParDo.of(\n+                    new DoFn<KafkaRecord<K, V>, KV<K, V>>() {\n+                      @ProcessElement\n+                      public void processElement(\n+                          @Element KafkaRecord element, OutputReceiver<KV<K, V>> outputReceiver) {\n+                        outputReceiver.output(element.getKV());\n+                      }\n+                    }))\n+            .setCoder(KvCoder.<K, V>of(readViaSDF.getKeyCoder(), readViaSDF.getValueCoder()));\n+      }\n+    }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 713}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTIzNTY4MQ==", "bodyText": "I plan to have a separate PR to introduce external transform builder for ReadAll(). The buildExternal will be like:\nreturn build().forExternalBuild()", "url": "https://github.com/apache/beam/pull/11749#discussion_r445235681", "createdAt": "2020-06-25T00:00:52Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1051,33 +1261,341 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     }\n   }\n \n-  ////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-  private static final Logger LOG = LoggerFactory.getLogger(KafkaIO.class);\n-\n   /**\n-   * Returns a new config map which is merge of current config and updates. Verifies the updates do\n-   * not includes ignored properties.\n+   * A {@link PTransform} to read from Kafka. See {@link KafkaIO} for more information on usage and\n+   * configuration.\n    */\n-  private static Map<String, Object> updateKafkaProperties(\n-      Map<String, Object> currentConfig,\n-      Map<String, String> ignoredProperties,\n-      Map<String, Object> updates) {\n+  @Experimental(Kind.PORTABILITY)\n+  @AutoValue\n+  public abstract static class ReadAll<K, V>\n+      extends PTransform<PCollection<KafkaSourceDescription>, PCollection<KafkaRecord<K, V>>> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(ReadAll.class);\n+\n+    abstract Map<String, Object> getConsumerConfig();\n+\n+    @Nullable\n+    abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+    @Nullable\n+    abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+    @Nullable\n+    abstract DeserializerProvider getValueDeserializerProvider();\n+\n+    @Nullable\n+    abstract Coder<K> getKeyCoder();\n+\n+    @Nullable\n+    abstract Coder<V> getValueCoder();\n+\n+    abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        getConsumerFactoryFn();\n+\n+    @Nullable\n+    abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+    @Nullable\n+    abstract SerializableFunction<Instant, WatermarkEstimator<Instant>>\n+        getCreateWatermarkEstimatorFn();\n+\n+    abstract boolean isCommitOffsetEnabled();\n+\n+    @Nullable\n+    abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+    abstract ReadAll.Builder<K, V> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<K, V> {\n+      abstract ReadAll.Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+      abstract ReadAll.Builder<K, V> setOffsetConsumerConfig(\n+          Map<String, Object> offsetConsumerConfig);\n+\n+      abstract ReadAll.Builder<K, V> setConsumerFactoryFn(\n+          SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+      abstract ReadAll.Builder<K, V> setKeyDeserializerProvider(\n+          DeserializerProvider deserializerProvider);\n+\n+      abstract ReadAll.Builder<K, V> setValueDeserializerProvider(\n+          DeserializerProvider deserializerProvider);\n+\n+      abstract ReadAll.Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+      abstract ReadAll.Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+      abstract ReadAll.Builder<K, V> setExtractOutputTimestampFn(\n+          SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+      abstract ReadAll.Builder<K, V> setCreateWatermarkEstimatorFn(\n+          SerializableFunction<Instant, WatermarkEstimator<Instant>> fn);\n+\n+      abstract ReadAll.Builder<K, V> setCommitOffsetEnabled(boolean commitOffsetEnabled);\n+\n+      abstract ReadAll.Builder<K, V> setTimestampPolicyFactory(TimestampPolicyFactory<K, V> policy);\n+\n+      abstract ReadAll<K, V> build();\n+    }\n \n-    for (String key : updates.keySet()) {\n+    public static <K, V> ReadAll<K, V> read() {\n+      return new AutoValue_KafkaIO_ReadAll.Builder<K, V>()\n+          .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+          .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+          .setCommitOffsetEnabled(false)\n+          .build()\n+          .withProcessingTime()\n+          .withMonotonicallyIncreasingWatermarkEstimator();\n+    }\n+\n+    // Note that if the bootstrapServers is set here but also populated with the element, the\n+    // element\n+    // will override the bootstrapServers from the config.\n+    public ReadAll<K, V> withBootstrapServers(String bootstrapServers) {\n+      return withConsumerConfigUpdates(\n+          ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+    }\n+\n+    public ReadAll<K, V> withKeyDeserializerProvider(DeserializerProvider<K> deserializerProvider) {\n+      return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+    }\n+\n+    public ReadAll<K, V> withValueDeserializerProvider(\n+        DeserializerProvider<V> deserializerProvider) {\n+      return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+    }\n+\n+    public ReadAll<K, V> withKeyDeserializer(Class<? extends Deserializer<K>> keyDeserializer) {\n+      return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+    }\n+\n+    public ReadAll<K, V> withValueDeserializer(Class<? extends Deserializer<V>> valueDeserializer) {\n+      return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+    }\n+\n+    public ReadAll<K, V> withKeyDeserializerAndCoder(\n+        Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+      return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+    }\n+\n+    public ReadAll<K, V> withValueDeserializerAndCoder(\n+        Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+      return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+    }\n+\n+    public ReadAll<K, V> withConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+      return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+    }\n+\n+    public ReadAll<K, V> withConsumerConfigUpdates(Map<String, Object> configUpdates) {\n+      Map<String, Object> config =\n+          KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+      return toBuilder().setConsumerConfig(config).build();\n+    }\n+\n+    public ReadAll<K, V> withExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+      return toBuilder().setExtractOutputTimestampFn(fn).build();\n+    }\n+\n+    public ReadAll<K, V> withCreatWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimator<Instant>> fn) {\n+      return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+    }\n+\n+    public ReadAll<K, V> withLogAppendTime() {\n+      return withExtractOutputTimestampFn(ReadAll.ExtractOutputTimestampFns.useLogAppendTime());\n+    }\n+\n+    public ReadAll<K, V> withProcessingTime() {\n+      return withExtractOutputTimestampFn(ReadAll.ExtractOutputTimestampFns.useProcessingTime());\n+    }\n+\n+    public ReadAll<K, V> withCreateTime() {\n+      return withExtractOutputTimestampFn(ReadAll.ExtractOutputTimestampFns.useCreateTime());\n+    }\n+\n+    public ReadAll<K, V> withWallTimeWatermarkEstimator() {\n+      return withCreatWatermarkEstimatorFn(\n+          state -> {\n+            return new WallTime(state);\n+          });\n+    }\n+\n+    public ReadAll<K, V> withMonotonicallyIncreasingWatermarkEstimator() {\n+      return withCreatWatermarkEstimatorFn(\n+          state -> {\n+            return new MonotonicallyIncreasing(state);\n+          });\n+    }\n+\n+    public ReadAll<K, V> withManualWatermarkEstimator() {\n+      return withCreatWatermarkEstimatorFn(\n+          state -> {\n+            return new Manual(state);\n+          });\n+    }\n+\n+    // If a transactional producer is used and it's desired to only read records from committed\n+    // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the\n+    // default\n+    // value.\n+    public ReadAll<K, V> withReadCommitted() {\n+      return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+    }\n+\n+    public ReadAll<K, V> commitOffsets() {\n+      return toBuilder().setCommitOffsetEnabled(true).build();\n+    }\n+\n+    public ReadAll<K, V> withOffsetConsumerConfigOverrides(\n+        Map<String, Object> offsetConsumerConfig) {\n+      return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+    }\n+\n+    public ReadAll<K, V> withConsumerConfigOverrides(Map<String, Object> consumerConfig) {\n+      return toBuilder().setConsumerConfig(consumerConfig).build();\n+    }\n+\n+    ReadAllFromRow forExternalBuild() {\n+      return new ReadAllFromRow(this);\n+    }\n+\n+    // This transform is used in cross-language case. The input Row should be encoded with an\n+    // equivalent schema as KafkaSourceDescription.\n+    private static class ReadAllFromRow<K, V>\n+        extends PTransform<PCollection<Row>, PCollection<KV<K, V>>> {\n+\n+      private final ReadAll<K, V> readViaSDF;\n+\n+      ReadAllFromRow(ReadAll read) {\n+        readViaSDF = read;\n+      }\n+\n+      @Override\n+      public PCollection<KV<K, V>> expand(PCollection<Row> input) {\n+        return input\n+            .apply(Convert.fromRows(KafkaSourceDescription.class))\n+            .apply(readViaSDF)\n+            .apply(\n+                ParDo.of(\n+                    new DoFn<KafkaRecord<K, V>, KV<K, V>>() {\n+                      @ProcessElement\n+                      public void processElement(\n+                          @Element KafkaRecord element, OutputReceiver<KV<K, V>> outputReceiver) {\n+                        outputReceiver.output(element.getKV());\n+                      }\n+                    }))\n+            .setCoder(KvCoder.<K, V>of(readViaSDF.getKeyCoder(), readViaSDF.getValueCoder()));\n+      }\n+    }", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE1NzQ3NA=="}, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 713}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzcwNzc1OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQyMDo0MDo1MFrOGoiV2w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQyMDo0MDo1MFrOGoiV2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE1ODg3NQ==", "bodyText": "This is going to grow indefinitely, please use an LRU cache with a limited number of elements (e.g. 1000).", "url": "https://github.com/apache/beam/pull/11749#discussion_r445158875", "createdAt": "2020-06-24T20:40:50Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java", "diffHunk": "@@ -0,0 +1,339 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.kafka.KafkaIO.ReadAll;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A SplittableDoFn which reads from {@link KafkaSourceDescription} and outputs {@link KafkaRecord}.\n+ * By default, a {@link MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+ */\n+@UnboundedPerElement\n+class ReadFromKafkaDoFn<K, V> extends DoFn<KafkaSourceDescription, KafkaRecord<K, V>> {\n+\n+  ReadFromKafkaDoFn(ReadAll transform) {\n+    this.consumerConfig = transform.getConsumerConfig();\n+    this.offsetConsumerConfig = transform.getOffsetConsumerConfig();\n+    this.keyDeserializerProvider = transform.getKeyDeserializerProvider();\n+    this.valueDeserializerProvider = transform.getValueDeserializerProvider();\n+    this.consumerFactoryFn = transform.getConsumerFactoryFn();\n+    this.extractOutputTimestampFn = transform.getExtractOutputTimestampFn();\n+    this.createWatermarkEstimatorFn = transform.getCreateWatermarkEstimatorFn();\n+    this.timestampPolicyFactory = transform.getTimestampPolicyFactory();\n+  }\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadFromKafkaDoFn.class);\n+\n+  private final Map<String, Object> offsetConsumerConfig;\n+\n+  private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      consumerFactoryFn;\n+  private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn;\n+  private final SerializableFunction<Instant, WatermarkEstimator<Instant>>\n+      createWatermarkEstimatorFn;\n+  private final TimestampPolicyFactory<K, V> timestampPolicyFactory;\n+\n+  // Valid between bundle start and bundle finish.\n+  private transient ConsumerSpEL consumerSpEL = null;\n+  private transient Deserializer<K> keyDeserializerInstance = null;\n+  private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+  private transient HashMap<TopicPartition, AverageRecordSize> avgRecordSize;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 90}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3MzgxMzI5OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": false, "comments": {"totalCount": 3, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQyMToxNDo1M1rOGojYJA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNlQxODozNzoyNFrOGpq78w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE3NTg0NA==", "bodyText": "This should happen automatically because there's a schema registered for KafkaSourceDescription. Was that not working?", "url": "https://github.com/apache/beam/pull/11749#discussion_r445175844", "createdAt": "2020-06-24T21:14:53Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -906,19 +1082,91 @@ public void setValueDeserializer(String valueDeserializer) {\n       Coder<K> keyCoder = getKeyCoder(coderRegistry);\n       Coder<V> valueCoder = getValueCoder(coderRegistry);\n \n-      // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n-      Unbounded<KafkaRecord<K, V>> unbounded =\n-          org.apache.beam.sdk.io.Read.from(\n-              toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+      // The Read will be expanded into SDF transform when \"beam_fn_api\" is enabled and\n+      // \"beam_fn_api_use_deprecated_read\" is not enabled.\n+      if (!ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\")\n+          || ExperimentalOptions.hasExperiment(\n+              input.getPipeline().getOptions(), \"beam_fn_api_use_deprecated_read\")) {\n+        // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n+        Unbounded<KafkaRecord<K, V>> unbounded =\n+            org.apache.beam.sdk.io.Read.from(\n+                toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+\n+        PTransform<PBegin, PCollection<KafkaRecord<K, V>>> transform = unbounded;\n+\n+        if (getMaxNumRecords() < Long.MAX_VALUE || getMaxReadTime() != null) {\n+          transform =\n+              unbounded.withMaxReadTime(getMaxReadTime()).withMaxNumRecords(getMaxNumRecords());\n+        }\n \n-      PTransform<PBegin, PCollection<KafkaRecord<K, V>>> transform = unbounded;\n+        return input.getPipeline().apply(transform);\n+      }\n+      ReadAll<K, V> readTransform =\n+          ReadAll.<K, V>read()\n+              .withConsumerConfigOverrides(getConsumerConfig())\n+              .withOffsetConsumerConfigOverrides(getOffsetConsumerConfig())\n+              .withConsumerFactoryFn(getConsumerFactoryFn())\n+              .withKeyDeserializerProvider(getKeyDeserializerProvider())\n+              .withValueDeserializerProvider(getValueDeserializerProvider())\n+              .withManualWatermarkEstimator()\n+              .withTimestampPolicyFactory(getTimestampPolicyFactory());\n+      if (isCommitOffsetsInFinalizeEnabled()) {\n+        readTransform = readTransform.commitOffsets();\n+      }\n+      PCollection<KafkaSourceDescription> output =\n+          input\n+              .getPipeline()\n+              .apply(Impulse.create())\n+              .apply(ParDo.of(new GenerateKafkaSourceDescription(this)));\n+      try {\n+        output.setCoder(KafkaSourceDescription.getCoder(input.getPipeline().getSchemaRegistry()));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 354}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTIyNTk0Nw==", "bodyText": "It works with setSchema but I want to make it explicitly because it's possible that an user writes a DoFn which produces KafkaSourceDescription.", "url": "https://github.com/apache/beam/pull/11749#discussion_r445225947", "createdAt": "2020-06-24T23:28:00Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -906,19 +1082,91 @@ public void setValueDeserializer(String valueDeserializer) {\n       Coder<K> keyCoder = getKeyCoder(coderRegistry);\n       Coder<V> valueCoder = getValueCoder(coderRegistry);\n \n-      // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n-      Unbounded<KafkaRecord<K, V>> unbounded =\n-          org.apache.beam.sdk.io.Read.from(\n-              toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+      // The Read will be expanded into SDF transform when \"beam_fn_api\" is enabled and\n+      // \"beam_fn_api_use_deprecated_read\" is not enabled.\n+      if (!ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\")\n+          || ExperimentalOptions.hasExperiment(\n+              input.getPipeline().getOptions(), \"beam_fn_api_use_deprecated_read\")) {\n+        // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n+        Unbounded<KafkaRecord<K, V>> unbounded =\n+            org.apache.beam.sdk.io.Read.from(\n+                toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+\n+        PTransform<PBegin, PCollection<KafkaRecord<K, V>>> transform = unbounded;\n+\n+        if (getMaxNumRecords() < Long.MAX_VALUE || getMaxReadTime() != null) {\n+          transform =\n+              unbounded.withMaxReadTime(getMaxReadTime()).withMaxNumRecords(getMaxNumRecords());\n+        }\n \n-      PTransform<PBegin, PCollection<KafkaRecord<K, V>>> transform = unbounded;\n+        return input.getPipeline().apply(transform);\n+      }\n+      ReadAll<K, V> readTransform =\n+          ReadAll.<K, V>read()\n+              .withConsumerConfigOverrides(getConsumerConfig())\n+              .withOffsetConsumerConfigOverrides(getOffsetConsumerConfig())\n+              .withConsumerFactoryFn(getConsumerFactoryFn())\n+              .withKeyDeserializerProvider(getKeyDeserializerProvider())\n+              .withValueDeserializerProvider(getValueDeserializerProvider())\n+              .withManualWatermarkEstimator()\n+              .withTimestampPolicyFactory(getTimestampPolicyFactory());\n+      if (isCommitOffsetsInFinalizeEnabled()) {\n+        readTransform = readTransform.commitOffsets();\n+      }\n+      PCollection<KafkaSourceDescription> output =\n+          input\n+              .getPipeline()\n+              .apply(Impulse.create())\n+              .apply(ParDo.of(new GenerateKafkaSourceDescription(this)));\n+      try {\n+        output.setCoder(KafkaSourceDescription.getCoder(input.getPipeline().getSchemaRegistry()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE3NTg0NA=="}, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 354}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NjM0ODI3NQ==", "bodyText": "I don't think you should need setSchema either, we should always use the SchemaCoder for a PCollection where T has a Schema registered (unless the PCollection has a specific coder set, or T also has a default coder set):\n\n  \n    \n      beam/sdks/java/core/src/main/java/org/apache/beam/sdk/values/PCollection.java\n    \n    \n        Lines 154 to 166\n      in\n      b83c06d\n    \n    \n    \n    \n\n        \n          \n           // If there is a schema registered for the type, attempt to create a SchemaCoder. \n        \n\n        \n          \n           if (token != null) { \n        \n\n        \n          \n             try { \n        \n\n        \n          \n               SchemaCoder<T> schemaCoder = \n        \n\n        \n          \n                   SchemaCoder.of( \n        \n\n        \n          \n                       schemaRegistry.getSchema(token), \n        \n\n        \n          \n                       token, \n        \n\n        \n          \n                       schemaRegistry.getToRowFunction(token), \n        \n\n        \n          \n                       schemaRegistry.getFromRowFunction(token)); \n        \n\n        \n          \n               return new CoderOrFailure<>(schemaCoder, null); \n        \n\n        \n          \n             } catch (NoSuchSchemaException esc) { \n        \n\n        \n          \n               // No schema. \n        \n\n        \n          \n             } \n        \n    \n  \n\n\nI'm not sure I follow why a DoFn that produces KafkaSourceDescription requires any special logic", "url": "https://github.com/apache/beam/pull/11749#discussion_r446348275", "createdAt": "2020-06-26T18:37:24Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -906,19 +1082,91 @@ public void setValueDeserializer(String valueDeserializer) {\n       Coder<K> keyCoder = getKeyCoder(coderRegistry);\n       Coder<V> valueCoder = getValueCoder(coderRegistry);\n \n-      // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n-      Unbounded<KafkaRecord<K, V>> unbounded =\n-          org.apache.beam.sdk.io.Read.from(\n-              toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+      // The Read will be expanded into SDF transform when \"beam_fn_api\" is enabled and\n+      // \"beam_fn_api_use_deprecated_read\" is not enabled.\n+      if (!ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\")\n+          || ExperimentalOptions.hasExperiment(\n+              input.getPipeline().getOptions(), \"beam_fn_api_use_deprecated_read\")) {\n+        // Handles unbounded source to bounded conversion if maxNumRecords or maxReadTime is set.\n+        Unbounded<KafkaRecord<K, V>> unbounded =\n+            org.apache.beam.sdk.io.Read.from(\n+                toBuilder().setKeyCoder(keyCoder).setValueCoder(valueCoder).build().makeSource());\n+\n+        PTransform<PBegin, PCollection<KafkaRecord<K, V>>> transform = unbounded;\n+\n+        if (getMaxNumRecords() < Long.MAX_VALUE || getMaxReadTime() != null) {\n+          transform =\n+              unbounded.withMaxReadTime(getMaxReadTime()).withMaxNumRecords(getMaxNumRecords());\n+        }\n \n-      PTransform<PBegin, PCollection<KafkaRecord<K, V>>> transform = unbounded;\n+        return input.getPipeline().apply(transform);\n+      }\n+      ReadAll<K, V> readTransform =\n+          ReadAll.<K, V>read()\n+              .withConsumerConfigOverrides(getConsumerConfig())\n+              .withOffsetConsumerConfigOverrides(getOffsetConsumerConfig())\n+              .withConsumerFactoryFn(getConsumerFactoryFn())\n+              .withKeyDeserializerProvider(getKeyDeserializerProvider())\n+              .withValueDeserializerProvider(getValueDeserializerProvider())\n+              .withManualWatermarkEstimator()\n+              .withTimestampPolicyFactory(getTimestampPolicyFactory());\n+      if (isCommitOffsetsInFinalizeEnabled()) {\n+        readTransform = readTransform.commitOffsets();\n+      }\n+      PCollection<KafkaSourceDescription> output =\n+          input\n+              .getPipeline()\n+              .apply(Impulse.create())\n+              .apply(ParDo.of(new GenerateKafkaSourceDescription(this)));\n+      try {\n+        output.setCoder(KafkaSourceDescription.getCoder(input.getPipeline().getSchemaRegistry()));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE3NTg0NA=="}, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 354}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3Mzg0NzQ0OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaSourceDescription.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQyMToyNjo0OFrOGojtUA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQyMzoyODo0MFrOGomciw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE4MTI2NA==", "bodyText": "This will get pulled into the generated schema which I don't think is your intention. You should change the name so it's not a getter, or add @SchemaIgnore", "url": "https://github.com/apache/beam/pull/11749#discussion_r445181264", "createdAt": "2020-06-24T21:26:48Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaSourceDescription.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.util.List;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.schemas.AutoValueSchema;\n+import org.apache.beam.sdk.schemas.NoSuchSchemaException;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.schemas.SchemaRegistry;\n+import org.apache.beam.sdk.schemas.annotations.DefaultSchema;\n+import org.apache.beam.sdk.schemas.annotations.SchemaFieldName;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.kafka.common.TopicPartition;\n+import org.joda.time.Instant;\n+\n+/**\n+ * An AutoValue object which represents a Kafka source description. Note that this object should be\n+ * encoded/decoded with equivalent {@link Schema} as a {@link Row} when crossing the wire.\n+ */\n+@DefaultSchema(AutoValueSchema.class)\n+@AutoValue\n+public abstract class KafkaSourceDescription implements Serializable {\n+  @SchemaFieldName(\"topic\")\n+  abstract String getTopic();\n+\n+  @SchemaFieldName(\"partition\")\n+  abstract Integer getPartition();\n+\n+  @SchemaFieldName(\"start_read_offset\")\n+  @Nullable\n+  abstract Long getStartReadOffset();\n+\n+  @SchemaFieldName(\"start_read_time\")\n+  @Nullable\n+  abstract Instant getStartReadTime();\n+\n+  @SchemaFieldName(\"bootstrapServers\")\n+  @Nullable\n+  abstract List<String> getBootStrapServers();\n+\n+  private TopicPartition topicPartition = null;\n+\n+  public TopicPartition getTopicPartition() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTIyNjEyMw==", "bodyText": "Thanks!", "url": "https://github.com/apache/beam/pull/11749#discussion_r445226123", "createdAt": "2020-06-24T23:28:40Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaSourceDescription.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.util.List;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.schemas.AutoValueSchema;\n+import org.apache.beam.sdk.schemas.NoSuchSchemaException;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.schemas.SchemaRegistry;\n+import org.apache.beam.sdk.schemas.annotations.DefaultSchema;\n+import org.apache.beam.sdk.schemas.annotations.SchemaFieldName;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.kafka.common.TopicPartition;\n+import org.joda.time.Instant;\n+\n+/**\n+ * An AutoValue object which represents a Kafka source description. Note that this object should be\n+ * encoded/decoded with equivalent {@link Schema} as a {@link Row} when crossing the wire.\n+ */\n+@DefaultSchema(AutoValueSchema.class)\n+@AutoValue\n+public abstract class KafkaSourceDescription implements Serializable {\n+  @SchemaFieldName(\"topic\")\n+  abstract String getTopic();\n+\n+  @SchemaFieldName(\"partition\")\n+  abstract Integer getPartition();\n+\n+  @SchemaFieldName(\"start_read_offset\")\n+  @Nullable\n+  abstract Long getStartReadOffset();\n+\n+  @SchemaFieldName(\"start_read_time\")\n+  @Nullable\n+  abstract Instant getStartReadTime();\n+\n+  @SchemaFieldName(\"bootstrapServers\")\n+  @Nullable\n+  abstract List<String> getBootStrapServers();\n+\n+  private TopicPartition topicPartition = null;\n+\n+  public TopicPartition getTopicPartition() {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE4MTI2NA=="}, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjc3Mzg1MjA3OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaSourceDescription.java", "isResolved": false, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQyMToyODoyM1rOGojwQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNi0yNFQyMToyODoyM1rOGojwQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0NTE4MjAxNw==", "bodyText": "I don't think you you should need this function. By registering a schema for KafkaSourceDescription Beam we will default to using SchemaCoder", "url": "https://github.com/apache/beam/pull/11749#discussion_r445182017", "createdAt": "2020-06-24T21:28:23Z", "author": {"login": "TheNeuralBit"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaSourceDescription.java", "diffHunk": "@@ -0,0 +1,92 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import com.google.auto.value.AutoValue;\n+import java.io.Serializable;\n+import java.util.List;\n+import javax.annotation.Nullable;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.schemas.AutoValueSchema;\n+import org.apache.beam.sdk.schemas.NoSuchSchemaException;\n+import org.apache.beam.sdk.schemas.Schema;\n+import org.apache.beam.sdk.schemas.SchemaCoder;\n+import org.apache.beam.sdk.schemas.SchemaRegistry;\n+import org.apache.beam.sdk.schemas.annotations.DefaultSchema;\n+import org.apache.beam.sdk.schemas.annotations.SchemaFieldName;\n+import org.apache.beam.sdk.values.Row;\n+import org.apache.beam.sdk.values.TypeDescriptor;\n+import org.apache.kafka.common.TopicPartition;\n+import org.joda.time.Instant;\n+\n+/**\n+ * An AutoValue object which represents a Kafka source description. Note that this object should be\n+ * encoded/decoded with equivalent {@link Schema} as a {@link Row} when crossing the wire.\n+ */\n+@DefaultSchema(AutoValueSchema.class)\n+@AutoValue\n+public abstract class KafkaSourceDescription implements Serializable {\n+  @SchemaFieldName(\"topic\")\n+  abstract String getTopic();\n+\n+  @SchemaFieldName(\"partition\")\n+  abstract Integer getPartition();\n+\n+  @SchemaFieldName(\"start_read_offset\")\n+  @Nullable\n+  abstract Long getStartReadOffset();\n+\n+  @SchemaFieldName(\"start_read_time\")\n+  @Nullable\n+  abstract Instant getStartReadTime();\n+\n+  @SchemaFieldName(\"bootstrapServers\")\n+  @Nullable\n+  abstract List<String> getBootStrapServers();\n+\n+  private TopicPartition topicPartition = null;\n+\n+  public TopicPartition getTopicPartition() {\n+    if (topicPartition == null) {\n+      topicPartition = new TopicPartition(getTopic(), getPartition());\n+    }\n+    return topicPartition;\n+  }\n+\n+  public static KafkaSourceDescription of(\n+      TopicPartition topicPartition,\n+      Long startReadOffset,\n+      Instant startReadTime,\n+      List<String> bootstrapServers) {\n+    return new AutoValue_KafkaSourceDescription(\n+        topicPartition.topic(),\n+        topicPartition.partition(),\n+        startReadOffset,\n+        startReadTime,\n+        bootstrapServers);\n+  }\n+\n+  public static Coder<KafkaSourceDescription> getCoder(SchemaRegistry registry)\n+      throws NoSuchSchemaException {\n+    return SchemaCoder.of(\n+        registry.getSchema(KafkaSourceDescription.class),\n+        TypeDescriptor.of(KafkaSourceDescription.class),\n+        registry.getToRowFunction(KafkaSourceDescription.class),\n+        registry.getFromRowFunction(KafkaSourceDescription.class));\n+  }", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b95c9d143422c6db7d5b4e38514242c9e25e5f5e"}, "originalPosition": 91}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMDcyNzQzOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMTo0NToxMVrOG8bvLQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMTo0NToxMVrOG8bvLQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAyMjE4OQ==", "bodyText": "use an unorderered list for the options: <ul><li></ul>", "url": "https://github.com/apache/beam/pull/11749#discussion_r466022189", "createdAt": "2020-08-05T21:45:11Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -198,6 +209,102 @@\n  *    ...\n  * }</pre>\n  *\n+ * <h2>Read from Kafka as a {@link DoFn}</h2>\n+ *\n+ * {@link ReadSourceDescriptors} is the {@link PTransform} that takes a PCollection of {@link\n+ * KafkaSourceDescriptor} as input and outputs a PCollection of {@link KafkaRecord}. The core\n+ * implementation is based on {@code SplittableDoFn}. For more details about the concept of {@code\n+ * SplittableDoFn}, please refer to the <a\n+ * href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadSourceDescriptors} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from a BigQuery table and read these topics via {@link ReadSourceDescriptors}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadSourceDescriptors#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadSourceDescriptors#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadSourceDescriptors#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadSourceDescriptors#getKeyCoder()} is the same as {@link\n+ *       KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadSourceDescriptors#getValueCoder()} is the same as {@link\n+ *       KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadSourceDescriptors#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadSourceDescriptors#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadSourceDescriptors#isCommitOffsetEnabled()} has the same meaning as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadSourceDescriptors} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescriptor.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from the {@link\n+ * KafkaSourceDescriptor}:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(\n+ *    KafkaSourceDescriptor.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ *  .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadSourceDescriptors}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadSourceDescriptors#commitOffsets()} enables committing offset after processing the\n+ * record. Note that if the {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadSourceDescriptors#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadSourceDescriptors#withExtractOutputTimestampFn(SerializableFunction)} is used to\n+ * compute the {@code output timestamp} for a given {@link KafkaRecord}. There are three built-in\n+ * types: {@link ReadSourceDescriptors#withProcessingTime()}, {@link\n+ * ReadSourceDescriptors#withCreateTime()} and {@link ReadSourceDescriptors#withLogAppendTime()}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d228f9c9e1835d4b83ef0d3e32091d071746c2c6"}, "originalPosition": 136}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMDcyOTc0OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMTo0NTo1OFrOG8bwiQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMTo0NTo1OFrOG8bwiQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAyMjUzNw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * compute the {@code output timestamp} for a given {@link KafkaRecord}. There are three built-in\n          \n          \n            \n             * compute the {@code output timestamp} for a given {@link KafkaRecord} and controls the watermark advancement. There are three built-in", "url": "https://github.com/apache/beam/pull/11749#discussion_r466022537", "createdAt": "2020-08-05T21:45:58Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -198,6 +209,102 @@\n  *    ...\n  * }</pre>\n  *\n+ * <h2>Read from Kafka as a {@link DoFn}</h2>\n+ *\n+ * {@link ReadSourceDescriptors} is the {@link PTransform} that takes a PCollection of {@link\n+ * KafkaSourceDescriptor} as input and outputs a PCollection of {@link KafkaRecord}. The core\n+ * implementation is based on {@code SplittableDoFn}. For more details about the concept of {@code\n+ * SplittableDoFn}, please refer to the <a\n+ * href=\"https://beam.apache.org/blog/splittable-do-fn/\">blog post</a> and <a\n+ * href=\"https://s.apache.org/beam-fn-api\">design doc</a>. The major difference from {@link\n+ * KafkaIO.Read} is, {@link ReadSourceDescriptors} doesn't require source descriptions(e.g., {@link\n+ * KafkaIO.Read#getTopicPartitions()}, {@link KafkaIO.Read#getTopics()}, {@link\n+ * KafkaIO.Read#getStartReadTime()}, etc.) during the pipeline construction time. Instead, the\n+ * pipeline can populate these source descriptions during runtime. For example, the pipeline can\n+ * query Kafka topics from a BigQuery table and read these topics via {@link ReadSourceDescriptors}.\n+ *\n+ * <h3>Common Kafka Consumer Configurations</h3>\n+ *\n+ * <p>Most Kafka consumer configurations are similar to {@link KafkaIO.Read}:\n+ *\n+ * <ul>\n+ *   <li>{@link ReadSourceDescriptors#getConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerConfig()}.\n+ *   <li>{@link ReadSourceDescriptors#getConsumerFactoryFn()} is the same as {@link\n+ *       KafkaIO.Read#getConsumerFactoryFn()}.\n+ *   <li>{@link ReadSourceDescriptors#getOffsetConsumerConfig()} is the same as {@link\n+ *       KafkaIO.Read#getOffsetConsumerConfig()}.\n+ *   <li>{@link ReadSourceDescriptors#getKeyCoder()} is the same as {@link\n+ *       KafkaIO.Read#getKeyCoder()}.\n+ *   <li>{@link ReadSourceDescriptors#getValueCoder()} is the same as {@link\n+ *       KafkaIO.Read#getValueCoder()}.\n+ *   <li>{@link ReadSourceDescriptors#getKeyDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getKeyDeserializerProvider()}.\n+ *   <li>{@link ReadSourceDescriptors#getValueDeserializerProvider()} is the same as {@link\n+ *       KafkaIO.Read#getValueDeserializerProvider()}.\n+ *   <li>{@link ReadSourceDescriptors#isCommitOffsetEnabled()} has the same meaning as {@link\n+ *       KafkaIO.Read#isCommitOffsetsInFinalizeEnabled()}.\n+ * </ul>\n+ *\n+ * <p>For example, to create a basic {@link ReadSourceDescriptors} transform:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(KafkaSourceDescriptor.of(new TopicPartition(\"topic\", 1)))\n+ *  .apply(KafkaIO.readAll()\n+ *          .withBootstrapServers(\"broker_1:9092,broker_2:9092\")\n+ *          .withKeyDeserializer(LongDeserializer.class).\n+ *          .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * Note that the {@code bootstrapServers} can also be populated from the {@link\n+ * KafkaSourceDescriptor}:\n+ *\n+ * <pre>{@code\n+ * pipeline\n+ *  .apply(Create.of(\n+ *    KafkaSourceDescriptor.of(\n+ *      new TopicPartition(\"topic\", 1),\n+ *      null,\n+ *      null,\n+ *      ImmutableList.of(\"broker_1:9092\", \"broker_2:9092\"))\n+ *  .apply(KafkaIO.readAll()\n+ *         .withKeyDeserializer(LongDeserializer.class).\n+ *         .withValueDeserializer(StringDeserializer.class));\n+ * }</pre>\n+ *\n+ * <h3>Configurations of {@link ReadSourceDescriptors}</h3>\n+ *\n+ * <p>Except configurations of Kafka Consumer, there are some other configurations which are related\n+ * to processing records.\n+ *\n+ * <p>{@link ReadSourceDescriptors#commitOffsets()} enables committing offset after processing the\n+ * record. Note that if the {@code isolation.level} is set to \"read_committed\" or {@link\n+ * ConsumerConfig#ENABLE_AUTO_COMMIT_CONFIG} is set in the consumer config, the {@link\n+ * ReadSourceDescriptors#commitOffsets()} will be ignored.\n+ *\n+ * <p>{@link ReadSourceDescriptors#withExtractOutputTimestampFn(SerializableFunction)} is used to\n+ * compute the {@code output timestamp} for a given {@link KafkaRecord}. There are three built-in", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d228f9c9e1835d4b83ef0d3e32091d071746c2c6"}, "originalPosition": 134}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMDc1NDMyOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMTo1NDo1OFrOG8b_iw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMTo1NDo1OFrOG8b_iw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAyNjM3OQ==", "bodyText": "In the javadoc, we should state:\n\nwhat the defaults are if nothing is configured\nall the withYYY methods, we should state that the KafkaSourceDescriptor.x takes precendence and if unset then we default to using this method.\nall the KafkaSourceDescriptor methods should state what they override in the ReadSourceDescriptors transform\nthat the watermark is controlled by one of withCreateWatermarkEstimatorFn / withWallTimeWatermarkEstimator / ...\nthat the output timestamp is controlled by one of withProcessingTime / withLogAppendTime / ...\n\nsome of this Javadoc should go on KafkaIO while others makes more sense to place on the methods.", "url": "https://github.com/apache/beam/pull/11749#discussion_r466026379", "createdAt": "2020-08-05T21:54:58Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1051,33 +1198,352 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     }\n   }\n \n-  ////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-  private static final Logger LOG = LoggerFactory.getLogger(KafkaIO.class);\n-\n   /**\n-   * Returns a new config map which is merge of current config and updates. Verifies the updates do\n-   * not includes ignored properties.\n+   * A {@link PTransform} to read from Kafka. See {@link KafkaIO} for more information on usage and", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d228f9c9e1835d4b83ef0d3e32091d071746c2c6"}, "originalPosition": 411}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMDc4MjczOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMjowNToxNlrOG8cQfg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDoyNzowM1rOG9DF3A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAzMDcxOA==", "bodyText": "It is unfortunate that Read doesn't have the documentation for these methods that we could copy.", "url": "https://github.com/apache/beam/pull/11749#discussion_r466030718", "createdAt": "2020-08-05T22:05:16Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1051,33 +1198,352 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     }\n   }\n \n-  ////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-  private static final Logger LOG = LoggerFactory.getLogger(KafkaIO.class);\n-\n   /**\n-   * Returns a new config map which is merge of current config and updates. Verifies the updates do\n-   * not includes ignored properties.\n+   * A {@link PTransform} to read from Kafka. See {@link KafkaIO} for more information on usage and\n+   * configuration.\n    */\n-  private static Map<String, Object> updateKafkaProperties(\n-      Map<String, Object> currentConfig,\n-      Map<String, String> ignoredProperties,\n-      Map<String, Object> updates) {\n+  @Experimental(Kind.PORTABILITY)\n+  @AutoValue\n+  public abstract static class ReadSourceDescriptors<K, V>\n+      extends PTransform<PCollection<KafkaSourceDescriptor>, PCollection<KafkaRecord<K, V>>> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(ReadSourceDescriptors.class);\n+\n+    abstract Map<String, Object> getConsumerConfig();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d228f9c9e1835d4b83ef0d3e32091d071746c2c6"}, "originalPosition": 425}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY2Njk3Mg==", "bodyText": "Most of the documentations are at KafkaIO level.", "url": "https://github.com/apache/beam/pull/11749#discussion_r466666972", "createdAt": "2020-08-06T20:27:03Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1051,33 +1198,352 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     }\n   }\n \n-  ////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-  private static final Logger LOG = LoggerFactory.getLogger(KafkaIO.class);\n-\n   /**\n-   * Returns a new config map which is merge of current config and updates. Verifies the updates do\n-   * not includes ignored properties.\n+   * A {@link PTransform} to read from Kafka. See {@link KafkaIO} for more information on usage and\n+   * configuration.\n    */\n-  private static Map<String, Object> updateKafkaProperties(\n-      Map<String, Object> currentConfig,\n-      Map<String, String> ignoredProperties,\n-      Map<String, Object> updates) {\n+  @Experimental(Kind.PORTABILITY)\n+  @AutoValue\n+  public abstract static class ReadSourceDescriptors<K, V>\n+      extends PTransform<PCollection<KafkaSourceDescriptor>, PCollection<KafkaRecord<K, V>>> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(ReadSourceDescriptors.class);\n+\n+    abstract Map<String, Object> getConsumerConfig();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAzMDcxOA=="}, "originalCommit": {"oid": "d228f9c9e1835d4b83ef0d3e32091d071746c2c6"}, "originalPosition": 425}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMDc4Njg0OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMjowNzowNVrOG8cTGQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMjowNzowNVrOG8cTGQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAzMTM4NQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                        \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescriptor during runtime. Otherwise, the pipeline will fail.\");\n          \n          \n            \n                        \"The bootstrapServers is not set. It must be populated through the KafkaSourceDescriptor during runtime otherwise the pipeline will fail.\");", "url": "https://github.com/apache/beam/pull/11749#discussion_r466031385", "createdAt": "2020-08-05T22:07:05Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1051,33 +1198,352 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     }\n   }\n \n-  ////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-  private static final Logger LOG = LoggerFactory.getLogger(KafkaIO.class);\n-\n   /**\n-   * Returns a new config map which is merge of current config and updates. Verifies the updates do\n-   * not includes ignored properties.\n+   * A {@link PTransform} to read from Kafka. See {@link KafkaIO} for more information on usage and\n+   * configuration.\n    */\n-  private static Map<String, Object> updateKafkaProperties(\n-      Map<String, Object> currentConfig,\n-      Map<String, String> ignoredProperties,\n-      Map<String, Object> updates) {\n+  @Experimental(Kind.PORTABILITY)\n+  @AutoValue\n+  public abstract static class ReadSourceDescriptors<K, V>\n+      extends PTransform<PCollection<KafkaSourceDescriptor>, PCollection<KafkaRecord<K, V>>> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(ReadSourceDescriptors.class);\n+\n+    abstract Map<String, Object> getConsumerConfig();\n+\n+    @Nullable\n+    abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+    @Nullable\n+    abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+    @Nullable\n+    abstract DeserializerProvider getValueDeserializerProvider();\n+\n+    @Nullable\n+    abstract Coder<K> getKeyCoder();\n+\n+    @Nullable\n+    abstract Coder<V> getValueCoder();\n+\n+    abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        getConsumerFactoryFn();\n+\n+    @Nullable\n+    abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+    @Nullable\n+    abstract SerializableFunction<Instant, WatermarkEstimator<Instant>>\n+        getCreateWatermarkEstimatorFn();\n+\n+    abstract boolean isCommitOffsetEnabled();\n+\n+    @Nullable\n+    abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+    abstract ReadSourceDescriptors.Builder<K, V> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<K, V> {\n+      abstract ReadSourceDescriptors.Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setOffsetConsumerConfig(\n+          Map<String, Object> offsetConsumerConfig);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setConsumerFactoryFn(\n+          SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setKeyDeserializerProvider(\n+          DeserializerProvider deserializerProvider);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setValueDeserializerProvider(\n+          DeserializerProvider deserializerProvider);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setExtractOutputTimestampFn(\n+          SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setCreateWatermarkEstimatorFn(\n+          SerializableFunction<Instant, WatermarkEstimator<Instant>> fn);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setCommitOffsetEnabled(\n+          boolean commitOffsetEnabled);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setTimestampPolicyFactory(\n+          TimestampPolicyFactory<K, V> policy);\n+\n+      abstract ReadSourceDescriptors<K, V> build();\n+    }\n+\n+    public static <K, V> ReadSourceDescriptors<K, V> read() {\n+      return new AutoValue_KafkaIO_ReadSourceDescriptors.Builder<K, V>()\n+          .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+          .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+          .setCommitOffsetEnabled(false)\n+          .build()\n+          .withProcessingTime()\n+          .withMonotonicallyIncreasingWatermarkEstimator();\n+    }\n+\n+    // Note that if the bootstrapServers is set here but also populated with the element, the\n+    // element\n+    // will override the bootstrapServers from the config.\n+    public ReadSourceDescriptors<K, V> withBootstrapServers(String bootstrapServers) {\n+      return withConsumerConfigUpdates(\n+          ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withKeyDeserializerProvider(\n+        DeserializerProvider<K> deserializerProvider) {\n+      return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withValueDeserializerProvider(\n+        DeserializerProvider<V> deserializerProvider) {\n+      return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withKeyDeserializer(\n+        Class<? extends Deserializer<K>> keyDeserializer) {\n+      return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withValueDeserializer(\n+        Class<? extends Deserializer<V>> valueDeserializer) {\n+      return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withKeyDeserializerAndCoder(\n+        Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+      return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withValueDeserializerAndCoder(\n+        Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+      return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+      return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withConsumerConfigUpdates(\n+        Map<String, Object> configUpdates) {\n+      Map<String, Object> config =\n+          KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+      return toBuilder().setConsumerConfig(config).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+      return toBuilder().setExtractOutputTimestampFn(fn).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withCreatWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimator<Instant>> fn) {\n+      return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withLogAppendTime() {\n+      return withExtractOutputTimestampFn(\n+          ReadSourceDescriptors.ExtractOutputTimestampFns.useLogAppendTime());\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withProcessingTime() {\n+      return withExtractOutputTimestampFn(\n+          ReadSourceDescriptors.ExtractOutputTimestampFns.useProcessingTime());\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withCreateTime() {\n+      return withExtractOutputTimestampFn(\n+          ReadSourceDescriptors.ExtractOutputTimestampFns.useCreateTime());\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withWallTimeWatermarkEstimator() {\n+      return withCreatWatermarkEstimatorFn(\n+          state -> {\n+            return new WallTime(state);\n+          });\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withMonotonicallyIncreasingWatermarkEstimator() {\n+      return withCreatWatermarkEstimatorFn(\n+          state -> {\n+            return new MonotonicallyIncreasing(state);\n+          });\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withManualWatermarkEstimator() {\n+      return withCreatWatermarkEstimatorFn(\n+          state -> {\n+            return new Manual(state);\n+          });\n+    }\n+\n+    // If a transactional producer is used and it's desired to only read records from committed\n+    // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the\n+    // default value.\n+    public ReadSourceDescriptors<K, V> withReadCommitted() {\n+      return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+    }\n+\n+    public ReadSourceDescriptors<K, V> commitOffsets() {\n+      return toBuilder().setCommitOffsetEnabled(true).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withOffsetConsumerConfigOverrides(\n+        Map<String, Object> offsetConsumerConfig) {\n+      return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withConsumerConfigOverrides(\n+        Map<String, Object> consumerConfig) {\n+      return toBuilder().setConsumerConfig(consumerConfig).build();\n+    }\n+\n+    // TODO(BEAM-10320): Create external build transform for ReadSourceDescriptors().\n+    ReadAllFromRow forExternalBuild() {\n+      return new ReadAllFromRow(this);\n+    }\n+\n+    // This transform is used in cross-language case. The input Row should be encoded with an\n+    // equivalent schema as KafkaSourceDescriptor.\n+    private static class ReadAllFromRow<K, V>\n+        extends PTransform<PCollection<Row>, PCollection<KV<K, V>>> {\n+\n+      private final ReadSourceDescriptors<K, V> readViaSDF;\n+\n+      ReadAllFromRow(ReadSourceDescriptors read) {\n+        readViaSDF = read;\n+      }\n+\n+      @Override\n+      public PCollection<KV<K, V>> expand(PCollection<Row> input) {\n+        return input\n+            .apply(Convert.fromRows(KafkaSourceDescriptor.class))\n+            .apply(readViaSDF)\n+            .apply(\n+                ParDo.of(\n+                    new DoFn<KafkaRecord<K, V>, KV<K, V>>() {\n+                      @ProcessElement\n+                      public void processElement(\n+                          @Element KafkaRecord element, OutputReceiver<KV<K, V>> outputReceiver) {\n+                        outputReceiver.output(element.getKV());\n+                      }\n+                    }))\n+            .setCoder(KvCoder.<K, V>of(readViaSDF.getKeyCoder(), readViaSDF.getValueCoder()));\n+      }\n+    }\n+\n+    ReadSourceDescriptors<K, V> withTimestampPolicyFactory(\n+        TimestampPolicyFactory<K, V> timestampPolicyFactory) {\n+      return toBuilder().setTimestampPolicyFactory(timestampPolicyFactory).build();\n+    }\n \n-    for (String key : updates.keySet()) {\n+    @Override\n+    public PCollection<KafkaRecord<K, V>> expand(PCollection<KafkaSourceDescriptor> input) {\n       checkArgument(\n-          !ignoredProperties.containsKey(key),\n-          \"No need to configure '%s'. %s\",\n-          key,\n-          ignoredProperties.get(key));\n+          ExperimentalOptions.hasExperiment(input.getPipeline().getOptions(), \"beam_fn_api\"),\n+          \"The ReadSourceDescriptors can only used when beam_fn_api is enabled.\");\n+\n+      checkArgument(getKeyDeserializerProvider() != null, \"withKeyDeserializer() is required\");\n+      checkArgument(getValueDeserializerProvider() != null, \"withValueDeserializer() is required\");\n+\n+      ConsumerSpEL consumerSpEL = new ConsumerSpEL();\n+      if (!consumerSpEL.hasOffsetsForTimes()) {\n+        LOG.warn(\n+            \"Kafka client version {} is too old. Versions before 0.10.1.0 are deprecated and \"\n+                + \"may not be supported in next release of Apache Beam. \"\n+                + \"Please upgrade your Kafka client version.\",\n+            AppInfoParser.getVersion());\n+      }\n+\n+      if (isCommitOffsetEnabled()) {\n+        if (configuredKafkaCommit()) {\n+          LOG.info(\n+              \"Either read_committed or auto_commit is set together with commitOffsetEnabled but you \"\n+                  + \"only need one of them. The commitOffsetEnabled is going to be ignored\");\n+        }\n+      }\n+\n+      if (getConsumerConfig().get(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG) == null) {\n+        LOG.warn(\n+            \"The bootstrapServers is not set. Then it must be populated through KafkaSourceDescriptor during runtime. Otherwise, the pipeline will fail.\");", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d228f9c9e1835d4b83ef0d3e32091d071746c2c6"}, "originalPosition": 693}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMDc5MjQ3OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMjowOTozMVrOG8cWng==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDo0MDowNVrOG9DfUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAzMjI4Ng==", "bodyText": "This looks like it is done.", "url": "https://github.com/apache/beam/pull/11749#discussion_r466032286", "createdAt": "2020-08-05T22:09:31Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1051,33 +1198,352 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     }\n   }\n \n-  ////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-  private static final Logger LOG = LoggerFactory.getLogger(KafkaIO.class);\n-\n   /**\n-   * Returns a new config map which is merge of current config and updates. Verifies the updates do\n-   * not includes ignored properties.\n+   * A {@link PTransform} to read from Kafka. See {@link KafkaIO} for more information on usage and\n+   * configuration.\n    */\n-  private static Map<String, Object> updateKafkaProperties(\n-      Map<String, Object> currentConfig,\n-      Map<String, String> ignoredProperties,\n-      Map<String, Object> updates) {\n+  @Experimental(Kind.PORTABILITY)\n+  @AutoValue\n+  public abstract static class ReadSourceDescriptors<K, V>\n+      extends PTransform<PCollection<KafkaSourceDescriptor>, PCollection<KafkaRecord<K, V>>> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(ReadSourceDescriptors.class);\n+\n+    abstract Map<String, Object> getConsumerConfig();\n+\n+    @Nullable\n+    abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+    @Nullable\n+    abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+    @Nullable\n+    abstract DeserializerProvider getValueDeserializerProvider();\n+\n+    @Nullable\n+    abstract Coder<K> getKeyCoder();\n+\n+    @Nullable\n+    abstract Coder<V> getValueCoder();\n+\n+    abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        getConsumerFactoryFn();\n+\n+    @Nullable\n+    abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+    @Nullable\n+    abstract SerializableFunction<Instant, WatermarkEstimator<Instant>>\n+        getCreateWatermarkEstimatorFn();\n+\n+    abstract boolean isCommitOffsetEnabled();\n+\n+    @Nullable\n+    abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+    abstract ReadSourceDescriptors.Builder<K, V> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<K, V> {\n+      abstract ReadSourceDescriptors.Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setOffsetConsumerConfig(\n+          Map<String, Object> offsetConsumerConfig);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setConsumerFactoryFn(\n+          SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setKeyDeserializerProvider(\n+          DeserializerProvider deserializerProvider);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setValueDeserializerProvider(\n+          DeserializerProvider deserializerProvider);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setExtractOutputTimestampFn(\n+          SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setCreateWatermarkEstimatorFn(\n+          SerializableFunction<Instant, WatermarkEstimator<Instant>> fn);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setCommitOffsetEnabled(\n+          boolean commitOffsetEnabled);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setTimestampPolicyFactory(\n+          TimestampPolicyFactory<K, V> policy);\n+\n+      abstract ReadSourceDescriptors<K, V> build();\n+    }\n+\n+    public static <K, V> ReadSourceDescriptors<K, V> read() {\n+      return new AutoValue_KafkaIO_ReadSourceDescriptors.Builder<K, V>()\n+          .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+          .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+          .setCommitOffsetEnabled(false)\n+          .build()\n+          .withProcessingTime()\n+          .withMonotonicallyIncreasingWatermarkEstimator();\n+    }\n+\n+    // Note that if the bootstrapServers is set here but also populated with the element, the\n+    // element\n+    // will override the bootstrapServers from the config.\n+    public ReadSourceDescriptors<K, V> withBootstrapServers(String bootstrapServers) {\n+      return withConsumerConfigUpdates(\n+          ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withKeyDeserializerProvider(\n+        DeserializerProvider<K> deserializerProvider) {\n+      return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withValueDeserializerProvider(\n+        DeserializerProvider<V> deserializerProvider) {\n+      return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withKeyDeserializer(\n+        Class<? extends Deserializer<K>> keyDeserializer) {\n+      return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withValueDeserializer(\n+        Class<? extends Deserializer<V>> valueDeserializer) {\n+      return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withKeyDeserializerAndCoder(\n+        Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+      return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withValueDeserializerAndCoder(\n+        Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+      return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+      return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withConsumerConfigUpdates(\n+        Map<String, Object> configUpdates) {\n+      Map<String, Object> config =\n+          KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+      return toBuilder().setConsumerConfig(config).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+      return toBuilder().setExtractOutputTimestampFn(fn).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withCreatWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimator<Instant>> fn) {\n+      return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withLogAppendTime() {\n+      return withExtractOutputTimestampFn(\n+          ReadSourceDescriptors.ExtractOutputTimestampFns.useLogAppendTime());\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withProcessingTime() {\n+      return withExtractOutputTimestampFn(\n+          ReadSourceDescriptors.ExtractOutputTimestampFns.useProcessingTime());\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withCreateTime() {\n+      return withExtractOutputTimestampFn(\n+          ReadSourceDescriptors.ExtractOutputTimestampFns.useCreateTime());\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withWallTimeWatermarkEstimator() {\n+      return withCreatWatermarkEstimatorFn(\n+          state -> {\n+            return new WallTime(state);\n+          });\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withMonotonicallyIncreasingWatermarkEstimator() {\n+      return withCreatWatermarkEstimatorFn(\n+          state -> {\n+            return new MonotonicallyIncreasing(state);\n+          });\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withManualWatermarkEstimator() {\n+      return withCreatWatermarkEstimatorFn(\n+          state -> {\n+            return new Manual(state);\n+          });\n+    }\n+\n+    // If a transactional producer is used and it's desired to only read records from committed\n+    // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the\n+    // default value.\n+    public ReadSourceDescriptors<K, V> withReadCommitted() {\n+      return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+    }\n+\n+    public ReadSourceDescriptors<K, V> commitOffsets() {\n+      return toBuilder().setCommitOffsetEnabled(true).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withOffsetConsumerConfigOverrides(\n+        Map<String, Object> offsetConsumerConfig) {\n+      return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withConsumerConfigOverrides(\n+        Map<String, Object> consumerConfig) {\n+      return toBuilder().setConsumerConfig(consumerConfig).build();\n+    }\n+\n+    // TODO(BEAM-10320): Create external build transform for ReadSourceDescriptors().", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d228f9c9e1835d4b83ef0d3e32091d071746c2c6"}, "originalPosition": 621}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY3MzQ4OA==", "bodyText": "Yes you are right. Thanks!", "url": "https://github.com/apache/beam/pull/11749#discussion_r466673488", "createdAt": "2020-08-06T20:40:05Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1051,33 +1198,352 @@ public void populateDisplayData(DisplayData.Builder builder) {\n     }\n   }\n \n-  ////////////////////////////////////////////////////////////////////////////////////////////////\n-\n-  private static final Logger LOG = LoggerFactory.getLogger(KafkaIO.class);\n-\n   /**\n-   * Returns a new config map which is merge of current config and updates. Verifies the updates do\n-   * not includes ignored properties.\n+   * A {@link PTransform} to read from Kafka. See {@link KafkaIO} for more information on usage and\n+   * configuration.\n    */\n-  private static Map<String, Object> updateKafkaProperties(\n-      Map<String, Object> currentConfig,\n-      Map<String, String> ignoredProperties,\n-      Map<String, Object> updates) {\n+  @Experimental(Kind.PORTABILITY)\n+  @AutoValue\n+  public abstract static class ReadSourceDescriptors<K, V>\n+      extends PTransform<PCollection<KafkaSourceDescriptor>, PCollection<KafkaRecord<K, V>>> {\n+\n+    private static final Logger LOG = LoggerFactory.getLogger(ReadSourceDescriptors.class);\n+\n+    abstract Map<String, Object> getConsumerConfig();\n+\n+    @Nullable\n+    abstract Map<String, Object> getOffsetConsumerConfig();\n+\n+    @Nullable\n+    abstract DeserializerProvider getKeyDeserializerProvider();\n+\n+    @Nullable\n+    abstract DeserializerProvider getValueDeserializerProvider();\n+\n+    @Nullable\n+    abstract Coder<K> getKeyCoder();\n+\n+    @Nullable\n+    abstract Coder<V> getValueCoder();\n+\n+    abstract SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+        getConsumerFactoryFn();\n+\n+    @Nullable\n+    abstract SerializableFunction<KafkaRecord<K, V>, Instant> getExtractOutputTimestampFn();\n+\n+    @Nullable\n+    abstract SerializableFunction<Instant, WatermarkEstimator<Instant>>\n+        getCreateWatermarkEstimatorFn();\n+\n+    abstract boolean isCommitOffsetEnabled();\n+\n+    @Nullable\n+    abstract TimestampPolicyFactory<K, V> getTimestampPolicyFactory();\n+\n+    abstract ReadSourceDescriptors.Builder<K, V> toBuilder();\n+\n+    @AutoValue.Builder\n+    abstract static class Builder<K, V> {\n+      abstract ReadSourceDescriptors.Builder<K, V> setConsumerConfig(Map<String, Object> config);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setOffsetConsumerConfig(\n+          Map<String, Object> offsetConsumerConfig);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setConsumerFactoryFn(\n+          SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setKeyDeserializerProvider(\n+          DeserializerProvider deserializerProvider);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setValueDeserializerProvider(\n+          DeserializerProvider deserializerProvider);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setKeyCoder(Coder<K> keyCoder);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setValueCoder(Coder<V> valueCoder);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setExtractOutputTimestampFn(\n+          SerializableFunction<KafkaRecord<K, V>, Instant> fn);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setCreateWatermarkEstimatorFn(\n+          SerializableFunction<Instant, WatermarkEstimator<Instant>> fn);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setCommitOffsetEnabled(\n+          boolean commitOffsetEnabled);\n+\n+      abstract ReadSourceDescriptors.Builder<K, V> setTimestampPolicyFactory(\n+          TimestampPolicyFactory<K, V> policy);\n+\n+      abstract ReadSourceDescriptors<K, V> build();\n+    }\n+\n+    public static <K, V> ReadSourceDescriptors<K, V> read() {\n+      return new AutoValue_KafkaIO_ReadSourceDescriptors.Builder<K, V>()\n+          .setConsumerFactoryFn(KafkaIOUtils.KAFKA_CONSUMER_FACTORY_FN)\n+          .setConsumerConfig(KafkaIOUtils.DEFAULT_CONSUMER_PROPERTIES)\n+          .setCommitOffsetEnabled(false)\n+          .build()\n+          .withProcessingTime()\n+          .withMonotonicallyIncreasingWatermarkEstimator();\n+    }\n+\n+    // Note that if the bootstrapServers is set here but also populated with the element, the\n+    // element\n+    // will override the bootstrapServers from the config.\n+    public ReadSourceDescriptors<K, V> withBootstrapServers(String bootstrapServers) {\n+      return withConsumerConfigUpdates(\n+          ImmutableMap.of(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, bootstrapServers));\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withKeyDeserializerProvider(\n+        DeserializerProvider<K> deserializerProvider) {\n+      return toBuilder().setKeyDeserializerProvider(deserializerProvider).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withValueDeserializerProvider(\n+        DeserializerProvider<V> deserializerProvider) {\n+      return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withKeyDeserializer(\n+        Class<? extends Deserializer<K>> keyDeserializer) {\n+      return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withValueDeserializer(\n+        Class<? extends Deserializer<V>> valueDeserializer) {\n+      return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withKeyDeserializerAndCoder(\n+        Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n+      return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withValueDeserializerAndCoder(\n+        Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n+      return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withConsumerFactoryFn(\n+        SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n+      return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withConsumerConfigUpdates(\n+        Map<String, Object> configUpdates) {\n+      Map<String, Object> config =\n+          KafkaIOUtils.updateKafkaProperties(getConsumerConfig(), configUpdates);\n+      return toBuilder().setConsumerConfig(config).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withExtractOutputTimestampFn(\n+        SerializableFunction<KafkaRecord<K, V>, Instant> fn) {\n+      return toBuilder().setExtractOutputTimestampFn(fn).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withCreatWatermarkEstimatorFn(\n+        SerializableFunction<Instant, WatermarkEstimator<Instant>> fn) {\n+      return toBuilder().setCreateWatermarkEstimatorFn(fn).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withLogAppendTime() {\n+      return withExtractOutputTimestampFn(\n+          ReadSourceDescriptors.ExtractOutputTimestampFns.useLogAppendTime());\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withProcessingTime() {\n+      return withExtractOutputTimestampFn(\n+          ReadSourceDescriptors.ExtractOutputTimestampFns.useProcessingTime());\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withCreateTime() {\n+      return withExtractOutputTimestampFn(\n+          ReadSourceDescriptors.ExtractOutputTimestampFns.useCreateTime());\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withWallTimeWatermarkEstimator() {\n+      return withCreatWatermarkEstimatorFn(\n+          state -> {\n+            return new WallTime(state);\n+          });\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withMonotonicallyIncreasingWatermarkEstimator() {\n+      return withCreatWatermarkEstimatorFn(\n+          state -> {\n+            return new MonotonicallyIncreasing(state);\n+          });\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withManualWatermarkEstimator() {\n+      return withCreatWatermarkEstimatorFn(\n+          state -> {\n+            return new Manual(state);\n+          });\n+    }\n+\n+    // If a transactional producer is used and it's desired to only read records from committed\n+    // transaction, it's recommended to set read_committed. Otherwise, read_uncommitted is the\n+    // default value.\n+    public ReadSourceDescriptors<K, V> withReadCommitted() {\n+      return withConsumerConfigUpdates(ImmutableMap.of(\"isolation.level\", \"read_committed\"));\n+    }\n+\n+    public ReadSourceDescriptors<K, V> commitOffsets() {\n+      return toBuilder().setCommitOffsetEnabled(true).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withOffsetConsumerConfigOverrides(\n+        Map<String, Object> offsetConsumerConfig) {\n+      return toBuilder().setOffsetConsumerConfig(offsetConsumerConfig).build();\n+    }\n+\n+    public ReadSourceDescriptors<K, V> withConsumerConfigOverrides(\n+        Map<String, Object> consumerConfig) {\n+      return toBuilder().setConsumerConfig(consumerConfig).build();\n+    }\n+\n+    // TODO(BEAM-10320): Create external build transform for ReadSourceDescriptors().", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAzMjI4Ng=="}, "originalCommit": {"oid": "d228f9c9e1835d4b83ef0d3e32091d071746c2c6"}, "originalPosition": 621}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMDgwNDg2OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMjoxNDo1NlrOG8ceSA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMjoxNDo1NlrOG8ceSA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAzNDI0OA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * <p>The initial range for a {@link KafkaSourceDescriptor } is defined by {@code [startOffset,\n          \n          \n            \n             * <p>The initial range for a {@link KafkaSourceDescriptor} is defined by {@code [startOffset,", "url": "https://github.com/apache/beam/pull/11749#discussion_r466034248", "createdAt": "2020-08-05T22:14:56Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java", "diffHunk": "@@ -0,0 +1,403 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.kafka.KafkaIO.ReadSourceDescriptors;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.CacheBuilder;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.CacheLoader;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LoadingCache;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A SplittableDoFn which reads from {@link KafkaSourceDescriptor} and outputs {@link KafkaRecord}.\n+ * By default, a {@link MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+ *\n+ * <p>{@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescriptor}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initial Restriction</h4>\n+ *\n+ * <p>The initial range for a {@link KafkaSourceDescriptor } is defined by {@code [startOffset,", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d228f9c9e1835d4b83ef0d3e32091d071746c2c6"}, "originalPosition": 71}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMDgxMDUxOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java", "isResolved": true, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMjoxNzoxN1rOG8chrA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMTowMDo1MVrOG9EHaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAzNTExNg==", "bodyText": "Most of my comments above about Javadoc can be copied directly from the comments you have made here.", "url": "https://github.com/apache/beam/pull/11749#discussion_r466035116", "createdAt": "2020-08-05T22:17:17Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java", "diffHunk": "@@ -0,0 +1,403 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.kafka.KafkaIO.ReadSourceDescriptors;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.CacheBuilder;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.CacheLoader;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LoadingCache;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A SplittableDoFn which reads from {@link KafkaSourceDescriptor} and outputs {@link KafkaRecord}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d228f9c9e1835d4b83ef0d3e32091d071746c2c6"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY4Mzc1Mg==", "bodyText": "I would prefer that we leave the documentation here and link it from KafkaIO.ReadSourceDescriptors.", "url": "https://github.com/apache/beam/pull/11749#discussion_r466683752", "createdAt": "2020-08-06T21:00:51Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java", "diffHunk": "@@ -0,0 +1,403 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.kafka.KafkaIO.ReadSourceDescriptors;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.CacheBuilder;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.CacheLoader;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LoadingCache;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A SplittableDoFn which reads from {@link KafkaSourceDescriptor} and outputs {@link KafkaRecord}.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAzNTExNg=="}, "originalCommit": {"oid": "d228f9c9e1835d4b83ef0d3e32091d071746c2c6"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMDgxMzg2OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMjoxODo0OVrOG8cjpA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNlQyMDo1MzoxNlrOG9D4-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAzNTYyMA==", "bodyText": "We should state the order of preference for how we start reading (offset/timestamp/last commit offset) in this block somewhere.", "url": "https://github.com/apache/beam/pull/11749#discussion_r466035620", "createdAt": "2020-08-05T22:18:49Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -198,6 +209,102 @@\n  *    ...\n  * }</pre>\n  *\n+ * <h2>Read from Kafka as a {@link DoFn}</h2>\n+ *\n+ * {@link ReadSourceDescriptors} is the {@link PTransform} that takes a PCollection of {@link", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d228f9c9e1835d4b83ef0d3e32091d071746c2c6"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjY4MDA1OQ==", "bodyText": "The order is explained in the javadoc of ReadFromKafkaDoFn, [Initial Restriction] section.", "url": "https://github.com/apache/beam/pull/11749#discussion_r466680059", "createdAt": "2020-08-06T20:53:16Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -198,6 +209,102 @@\n  *    ...\n  * }</pre>\n  *\n+ * <h2>Read from Kafka as a {@link DoFn}</h2>\n+ *\n+ * {@link ReadSourceDescriptors} is the {@link PTransform} that takes a PCollection of {@link", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAzNTYyMA=="}, "originalCommit": {"oid": "d228f9c9e1835d4b83ef0d3e32091d071746c2c6"}, "originalPosition": 61}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMDgxNTM3OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMjoxOToyOVrOG8ckig==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMjoxOToyOVrOG8ckig==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAzNTg1MA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * checkpoint at current {@link KafkaSourceDescriptor } and move to process the next element. These\n          \n          \n            \n             * checkpoint the current {@link KafkaSourceDescriptor} and move to process the next element. These", "url": "https://github.com/apache/beam/pull/11749#discussion_r466035850", "createdAt": "2020-08-05T22:19:29Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java", "diffHunk": "@@ -0,0 +1,403 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.kafka.KafkaIO.ReadSourceDescriptors;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.CacheBuilder;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.CacheLoader;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LoadingCache;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A SplittableDoFn which reads from {@link KafkaSourceDescriptor} and outputs {@link KafkaRecord}.\n+ * By default, a {@link MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+ *\n+ * <p>{@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescriptor}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initial Restriction</h4>\n+ *\n+ * <p>The initial range for a {@link KafkaSourceDescriptor } is defined by {@code [startOffset,\n+ * Long.MAX_VALUE)} where {@code startOffset} is defined as:\n+ *\n+ * <ul>\n+ *   <li>the {@code startReadOffset} if {@link KafkaSourceDescriptor#getStartReadOffset} is set.\n+ *   <li>the first offset with a greater or equivalent timestamp if {@link\n+ *       KafkaSourceDescriptor#getStartReadTime()} is set.\n+ *   <li>the {@code last committed offset + 1} for the {@link Consumer#position(TopicPartition)\n+ *       topic partition}.\n+ * </ul>\n+ *\n+ * <h4>Splitting</h4>\n+ *\n+ * <p>TODO(BEAM-10319): Add support for initial splitting.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescriptor } and move to process the next element. These", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d228f9c9e1835d4b83ef0d3e32091d071746c2c6"}, "originalPosition": 92}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMDgxNzExOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMjoyMDowNlrOG8cllA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMjoyMDowNlrOG8cllA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAzNjExNg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * KafkaSourceDescriptor }. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n          \n          \n            \n             * KafkaSourceDescriptor}. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in", "url": "https://github.com/apache/beam/pull/11749#discussion_r466036116", "createdAt": "2020-08-05T22:20:06Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java", "diffHunk": "@@ -0,0 +1,403 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.kafka.KafkaIO.ReadSourceDescriptors;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.CacheBuilder;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.CacheLoader;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LoadingCache;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A SplittableDoFn which reads from {@link KafkaSourceDescriptor} and outputs {@link KafkaRecord}.\n+ * By default, a {@link MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+ *\n+ * <p>{@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescriptor}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initial Restriction</h4>\n+ *\n+ * <p>The initial range for a {@link KafkaSourceDescriptor } is defined by {@code [startOffset,\n+ * Long.MAX_VALUE)} where {@code startOffset} is defined as:\n+ *\n+ * <ul>\n+ *   <li>the {@code startReadOffset} if {@link KafkaSourceDescriptor#getStartReadOffset} is set.\n+ *   <li>the first offset with a greater or equivalent timestamp if {@link\n+ *       KafkaSourceDescriptor#getStartReadTime()} is set.\n+ *   <li>the {@code last committed offset + 1} for the {@link Consumer#position(TopicPartition)\n+ *       topic partition}.\n+ * </ul>\n+ *\n+ * <h4>Splitting</h4>\n+ *\n+ * <p>TODO(BEAM-10319): Add support for initial splitting.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescriptor } and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescriptor }. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d228f9c9e1835d4b83ef0d3e32091d071746c2c6"}, "originalPosition": 98}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMDgxNzY2OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMjoyMDoyMFrOG8cl5w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMjoyMDoyMFrOG8cl5w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAzNjE5OQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescriptor, OffsetRange).}\n          \n          \n            \n             * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescriptor, OffsetRange)}.", "url": "https://github.com/apache/beam/pull/11749#discussion_r466036199", "createdAt": "2020-08-05T22:20:20Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java", "diffHunk": "@@ -0,0 +1,403 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.kafka.KafkaIO.ReadSourceDescriptors;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.CacheBuilder;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.CacheLoader;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LoadingCache;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A SplittableDoFn which reads from {@link KafkaSourceDescriptor} and outputs {@link KafkaRecord}.\n+ * By default, a {@link MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+ *\n+ * <p>{@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescriptor}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initial Restriction</h4>\n+ *\n+ * <p>The initial range for a {@link KafkaSourceDescriptor } is defined by {@code [startOffset,\n+ * Long.MAX_VALUE)} where {@code startOffset} is defined as:\n+ *\n+ * <ul>\n+ *   <li>the {@code startReadOffset} if {@link KafkaSourceDescriptor#getStartReadOffset} is set.\n+ *   <li>the first offset with a greater or equivalent timestamp if {@link\n+ *       KafkaSourceDescriptor#getStartReadTime()} is set.\n+ *   <li>the {@code last committed offset + 1} for the {@link Consumer#position(TopicPartition)\n+ *       topic partition}.\n+ * </ul>\n+ *\n+ * <h4>Splitting</h4>\n+ *\n+ * <p>TODO(BEAM-10319): Add support for initial splitting.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescriptor } and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescriptor }. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn#restrictionTracker(KafkaSourceDescriptor, OffsetRange)} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescriptor, OffsetRange).}", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d228f9c9e1835d4b83ef0d3e32091d071746c2c6"}, "originalPosition": 103}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMDgxOTEzOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMjoyMDo0OVrOG8cmuA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMjoyMDo0OVrOG8cmuA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAzNjQwOA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * ReadSourceDescriptors#withMonotonicallyIncreasingWatermarkEstimator()} as {@link\n          \n          \n            \n             * ReadSourceDescriptors#withMonotonicallyIncreasingWatermarkEstimator()} as the {@link", "url": "https://github.com/apache/beam/pull/11749#discussion_r466036408", "createdAt": "2020-08-05T22:20:49Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java", "diffHunk": "@@ -0,0 +1,403 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.kafka.KafkaIO.ReadSourceDescriptors;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.CacheBuilder;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.CacheLoader;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LoadingCache;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A SplittableDoFn which reads from {@link KafkaSourceDescriptor} and outputs {@link KafkaRecord}.\n+ * By default, a {@link MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+ *\n+ * <p>{@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescriptor}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initial Restriction</h4>\n+ *\n+ * <p>The initial range for a {@link KafkaSourceDescriptor } is defined by {@code [startOffset,\n+ * Long.MAX_VALUE)} where {@code startOffset} is defined as:\n+ *\n+ * <ul>\n+ *   <li>the {@code startReadOffset} if {@link KafkaSourceDescriptor#getStartReadOffset} is set.\n+ *   <li>the first offset with a greater or equivalent timestamp if {@link\n+ *       KafkaSourceDescriptor#getStartReadTime()} is set.\n+ *   <li>the {@code last committed offset + 1} for the {@link Consumer#position(TopicPartition)\n+ *       topic partition}.\n+ * </ul>\n+ *\n+ * <h4>Splitting</h4>\n+ *\n+ * <p>TODO(BEAM-10319): Add support for initial splitting.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescriptor } and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescriptor }. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn#restrictionTracker(KafkaSourceDescriptor, OffsetRange)} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescriptor, OffsetRange).}\n+ * A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * <p>The {@link WatermarkEstimator} is created by {@link\n+ * ReadSourceDescriptors#getCreateWatermarkEstimatorFn()}. The estimated watermark is computed by\n+ * this {@link WatermarkEstimator} based on output timestamps computed by {@link\n+ * ReadSourceDescriptors#getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link ReadSourceDescriptors#withProcessingTime()} as {@code\n+ * extractTimestampFn} and {@link\n+ * ReadSourceDescriptors#withMonotonicallyIncreasingWatermarkEstimator()} as {@link", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d228f9c9e1835d4b83ef0d3e32091d071746c2c6"}, "originalPosition": 114}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMDgxOTUwOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMjoyMTowMFrOG8cm8A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMjoyMTowMFrOG8cm8A==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAzNjQ2NA==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n             * configuration is using {@link ReadSourceDescriptors#withProcessingTime()} as {@code\n          \n          \n            \n             * configuration is using {@link ReadSourceDescriptors#withProcessingTime()} as the {@code", "url": "https://github.com/apache/beam/pull/11749#discussion_r466036464", "createdAt": "2020-08-05T22:21:00Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java", "diffHunk": "@@ -0,0 +1,403 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.kafka.KafkaIO.ReadSourceDescriptors;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.CacheBuilder;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.CacheLoader;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LoadingCache;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A SplittableDoFn which reads from {@link KafkaSourceDescriptor} and outputs {@link KafkaRecord}.\n+ * By default, a {@link MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+ *\n+ * <p>{@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescriptor}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initial Restriction</h4>\n+ *\n+ * <p>The initial range for a {@link KafkaSourceDescriptor } is defined by {@code [startOffset,\n+ * Long.MAX_VALUE)} where {@code startOffset} is defined as:\n+ *\n+ * <ul>\n+ *   <li>the {@code startReadOffset} if {@link KafkaSourceDescriptor#getStartReadOffset} is set.\n+ *   <li>the first offset with a greater or equivalent timestamp if {@link\n+ *       KafkaSourceDescriptor#getStartReadTime()} is set.\n+ *   <li>the {@code last committed offset + 1} for the {@link Consumer#position(TopicPartition)\n+ *       topic partition}.\n+ * </ul>\n+ *\n+ * <h4>Splitting</h4>\n+ *\n+ * <p>TODO(BEAM-10319): Add support for initial splitting.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescriptor } and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescriptor }. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn#restrictionTracker(KafkaSourceDescriptor, OffsetRange)} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescriptor, OffsetRange).}\n+ * A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * <p>The {@link WatermarkEstimator} is created by {@link\n+ * ReadSourceDescriptors#getCreateWatermarkEstimatorFn()}. The estimated watermark is computed by\n+ * this {@link WatermarkEstimator} based on output timestamps computed by {@link\n+ * ReadSourceDescriptors#getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link ReadSourceDescriptors#withProcessingTime()} as {@code", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d228f9c9e1835d4b83ef0d3e32091d071746c2c6"}, "originalPosition": 112}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMDgyNzY2OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMjoyNDoxN1rOG8crmA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMjoyNDoxN1rOG8crmA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAzNzY1Ng==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                      // WatermarkEstimator should be a Manual one.\n          \n          \n            \n                      // WatermarkEstimator should be a manual one.", "url": "https://github.com/apache/beam/pull/11749#discussion_r466037656", "createdAt": "2020-08-05T22:24:17Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java", "diffHunk": "@@ -0,0 +1,403 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.kafka.KafkaIO.ReadSourceDescriptors;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.CacheBuilder;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.CacheLoader;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LoadingCache;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A SplittableDoFn which reads from {@link KafkaSourceDescriptor} and outputs {@link KafkaRecord}.\n+ * By default, a {@link MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+ *\n+ * <p>{@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescriptor}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initial Restriction</h4>\n+ *\n+ * <p>The initial range for a {@link KafkaSourceDescriptor } is defined by {@code [startOffset,\n+ * Long.MAX_VALUE)} where {@code startOffset} is defined as:\n+ *\n+ * <ul>\n+ *   <li>the {@code startReadOffset} if {@link KafkaSourceDescriptor#getStartReadOffset} is set.\n+ *   <li>the first offset with a greater or equivalent timestamp if {@link\n+ *       KafkaSourceDescriptor#getStartReadTime()} is set.\n+ *   <li>the {@code last committed offset + 1} for the {@link Consumer#position(TopicPartition)\n+ *       topic partition}.\n+ * </ul>\n+ *\n+ * <h4>Splitting</h4>\n+ *\n+ * <p>TODO(BEAM-10319): Add support for initial splitting.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescriptor } and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescriptor }. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn#restrictionTracker(KafkaSourceDescriptor, OffsetRange)} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescriptor, OffsetRange).}\n+ * A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * <p>The {@link WatermarkEstimator} is created by {@link\n+ * ReadSourceDescriptors#getCreateWatermarkEstimatorFn()}. The estimated watermark is computed by\n+ * this {@link WatermarkEstimator} based on output timestamps computed by {@link\n+ * ReadSourceDescriptors#getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link ReadSourceDescriptors#withProcessingTime()} as {@code\n+ * extractTimestampFn} and {@link\n+ * ReadSourceDescriptors#withMonotonicallyIncreasingWatermarkEstimator()} as {@link\n+ * WatermarkEstimator}.\n+ */\n+@UnboundedPerElement\n+class ReadFromKafkaDoFn<K, V> extends DoFn<KafkaSourceDescriptor, KafkaRecord<K, V>> {\n+\n+  ReadFromKafkaDoFn(ReadSourceDescriptors transform) {\n+    this.consumerConfig = transform.getConsumerConfig();\n+    this.offsetConsumerConfig = transform.getOffsetConsumerConfig();\n+    this.keyDeserializerProvider = transform.getKeyDeserializerProvider();\n+    this.valueDeserializerProvider = transform.getValueDeserializerProvider();\n+    this.consumerFactoryFn = transform.getConsumerFactoryFn();\n+    this.extractOutputTimestampFn = transform.getExtractOutputTimestampFn();\n+    this.createWatermarkEstimatorFn = transform.getCreateWatermarkEstimatorFn();\n+    this.timestampPolicyFactory = transform.getTimestampPolicyFactory();\n+  }\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadFromKafkaDoFn.class);\n+\n+  private final Map<String, Object> offsetConsumerConfig;\n+\n+  private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      consumerFactoryFn;\n+  private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn;\n+  private final SerializableFunction<Instant, WatermarkEstimator<Instant>>\n+      createWatermarkEstimatorFn;\n+  private final TimestampPolicyFactory<K, V> timestampPolicyFactory;\n+\n+  // Valid between bundle start and bundle finish.\n+  private transient ConsumerSpEL consumerSpEL = null;\n+  private transient Deserializer<K> keyDeserializerInstance = null;\n+  private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+  private transient LoadingCache<TopicPartition, AverageRecordSize> avgRecordSize;\n+\n+  private static final Duration KAFKA_POLL_TIMEOUT = Duration.millis(1000);\n+\n+  @VisibleForTesting final DeserializerProvider keyDeserializerProvider;\n+  @VisibleForTesting final DeserializerProvider valueDeserializerProvider;\n+  @VisibleForTesting final Map<String, Object> consumerConfig;\n+\n+  /**\n+   * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+   * fetch backlog.\n+   */\n+  private static class KafkaLatestOffsetEstimator\n+      implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+\n+    private final Consumer<byte[], byte[]> offsetConsumer;\n+    private final TopicPartition topicPartition;\n+    private final ConsumerSpEL consumerSpEL;\n+    private final Supplier<Long> memoizedBacklog;\n+\n+    KafkaLatestOffsetEstimator(\n+        Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+      this.offsetConsumer = offsetConsumer;\n+      this.topicPartition = topicPartition;\n+      this.consumerSpEL = new ConsumerSpEL();\n+      this.consumerSpEL.evaluateAssign(this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+      memoizedBacklog =\n+          Suppliers.memoizeWithExpiration(\n+              () -> {\n+                consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+                return offsetConsumer.position(topicPartition);\n+              },\n+              5,\n+              TimeUnit.SECONDS);\n+    }\n+\n+    @Override\n+    protected void finalize() {\n+      try {\n+        Closeables.close(offsetConsumer, true);\n+      } catch (Exception anyException) {\n+        LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+      }\n+    }\n+\n+    @Override\n+    public long estimate() {\n+      return memoizedBacklog.get();\n+    }\n+  }\n+\n+  @GetInitialRestriction\n+  public OffsetRange initialRestriction(@Element KafkaSourceDescriptor kafkaSourceDescriptor) {\n+    Map<String, Object> updatedConsumerConfig =\n+        overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescriptor);\n+    try (Consumer<byte[], byte[]> offsetConsumer =\n+        consumerFactoryFn.apply(\n+            KafkaIOUtils.getOffsetConsumerConfig(\n+                \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+      consumerSpEL.evaluateAssign(\n+          offsetConsumer, ImmutableList.of(kafkaSourceDescriptor.getTopicPartition()));\n+      long startOffset;\n+      if (kafkaSourceDescriptor.getStartReadOffset() != null) {\n+        startOffset = kafkaSourceDescriptor.getStartReadOffset();\n+      } else if (kafkaSourceDescriptor.getStartReadTime() != null) {\n+        startOffset =\n+            consumerSpEL.offsetForTime(\n+                offsetConsumer,\n+                kafkaSourceDescriptor.getTopicPartition(),\n+                kafkaSourceDescriptor.getStartReadTime());\n+      } else {\n+        startOffset = offsetConsumer.position(kafkaSourceDescriptor.getTopicPartition());\n+      }\n+      return new OffsetRange(startOffset, Long.MAX_VALUE);\n+    }\n+  }\n+\n+  @GetInitialWatermarkEstimatorState\n+  public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+    return currentElementTimestamp;\n+  }\n+\n+  @NewWatermarkEstimator\n+  public WatermarkEstimator<Instant> newWatermarkEstimator(\n+      @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+    return createWatermarkEstimatorFn.apply(watermarkEstimatorState);\n+  }\n+\n+  @GetSize\n+  public double getSize(\n+      @Element KafkaSourceDescriptor kafkaSourceDescriptor, @Restriction OffsetRange offsetRange)\n+      throws Exception {\n+    double numRecords =\n+        restrictionTracker(kafkaSourceDescriptor, offsetRange).getProgress().getWorkRemaining();\n+    // Before processing elements, we don't have a good estimated size of records and offset gap.\n+    if (!avgRecordSize.asMap().containsKey(kafkaSourceDescriptor.getTopicPartition())) {\n+      return numRecords;\n+    }\n+    return avgRecordSize.get(kafkaSourceDescriptor.getTopicPartition()).getTotalSize(numRecords);\n+  }\n+\n+  @NewTracker\n+  public GrowableOffsetRangeTracker restrictionTracker(\n+      @Element KafkaSourceDescriptor kafkaSourceDescriptor, @Restriction OffsetRange restriction) {\n+    Map<String, Object> updatedConsumerConfig =\n+        overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescriptor);\n+    KafkaLatestOffsetEstimator offsetPoller =\n+        new KafkaLatestOffsetEstimator(\n+            consumerFactoryFn.apply(\n+                KafkaIOUtils.getOffsetConsumerConfig(\n+                    \"tracker-\" + kafkaSourceDescriptor.getTopicPartition(),\n+                    offsetConsumerConfig,\n+                    updatedConsumerConfig)),\n+            kafkaSourceDescriptor.getTopicPartition());\n+    return new GrowableOffsetRangeTracker(restriction.getFrom(), offsetPoller);\n+  }\n+\n+  @ProcessElement\n+  public ProcessContinuation processElement(\n+      @Element KafkaSourceDescriptor kafkaSourceDescriptor,\n+      RestrictionTracker<OffsetRange, Long> tracker,\n+      WatermarkEstimator watermarkEstimator,\n+      OutputReceiver<KafkaRecord<K, V>> receiver) {\n+    // If there is no future work, resume with max timeout and move to the next element.\n+    Map<String, Object> updatedConsumerConfig =\n+        overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescriptor);\n+    // If there is a timestampPolicyFactory, create the TimestampPolicy for current\n+    // TopicPartition.\n+    TimestampPolicy timestampPolicy = null;\n+    if (timestampPolicyFactory != null) {\n+      timestampPolicy =\n+          timestampPolicyFactory.createTimestampPolicy(\n+              kafkaSourceDescriptor.getTopicPartition(),\n+              Optional.ofNullable(watermarkEstimator.currentWatermark()));\n+    }\n+    try (Consumer<byte[], byte[]> consumer = consumerFactoryFn.apply(updatedConsumerConfig)) {\n+      consumerSpEL.evaluateAssign(\n+          consumer, ImmutableList.of(kafkaSourceDescriptor.getTopicPartition()));\n+      long startOffset = tracker.currentRestriction().getFrom();\n+      long expectedOffset = startOffset;\n+      consumer.seek(kafkaSourceDescriptor.getTopicPartition(), startOffset);\n+      ConsumerRecords<byte[], byte[]> rawRecords = ConsumerRecords.empty();\n+\n+      while (true) {\n+        rawRecords = consumer.poll(KAFKA_POLL_TIMEOUT.getMillis());\n+        // When there are no records available for the current TopicPartition, self-checkpoint\n+        // and move to process the next element.\n+        if (rawRecords.isEmpty()) {\n+          return ProcessContinuation.resume();\n+        }\n+        for (ConsumerRecord<byte[], byte[]> rawRecord : rawRecords) {\n+          if (!tracker.tryClaim(rawRecord.offset())) {\n+            return ProcessContinuation.stop();\n+          }\n+          KafkaRecord<K, V> kafkaRecord =\n+              new KafkaRecord<>(\n+                  rawRecord.topic(),\n+                  rawRecord.partition(),\n+                  rawRecord.offset(),\n+                  consumerSpEL.getRecordTimestamp(rawRecord),\n+                  consumerSpEL.getRecordTimestampType(rawRecord),\n+                  ConsumerSpEL.hasHeaders() ? rawRecord.headers() : null,\n+                  keyDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.key()),\n+                  valueDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.value()));\n+          int recordSize =\n+              (rawRecord.key() == null ? 0 : rawRecord.key().length)\n+                  + (rawRecord.value() == null ? 0 : rawRecord.value().length);\n+          avgRecordSize\n+              .getUnchecked(kafkaSourceDescriptor.getTopicPartition())\n+              .update(recordSize, rawRecord.offset() - expectedOffset);\n+          expectedOffset = rawRecord.offset() + 1;\n+          Instant outputTimestamp;\n+          // The outputTimestamp and watermark will be computed by timestampPolicy, where the\n+          // WatermarkEstimator should be a Manual one.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d228f9c9e1835d4b83ef0d3e32091d071746c2c6"}, "originalPosition": 320}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxMDgzMzM1OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMjoyNjozMVrOG8cuvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wNVQyMjoyNjozMVrOG8cuvQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NjAzODQ2MQ==", "bodyText": "slf4j has special logic to format exceptions.\n\n  \n    \n  \n    \n\n  \n  This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                  LOG.warn(\"Fail to close resource during finishing bundle: {}\", anyException.getMessage());\n          \n          \n            \n                  LOG.warn(\"Fail to close resource during finishing bundle.\", anyException);", "url": "https://github.com/apache/beam/pull/11749#discussion_r466038461", "createdAt": "2020-08-05T22:26:31Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/ReadFromKafkaDoFn.java", "diffHunk": "@@ -0,0 +1,403 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.sdk.io.kafka;\n+\n+import static org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Preconditions.checkState;\n+\n+import java.util.HashMap;\n+import java.util.Map;\n+import java.util.Optional;\n+import java.util.concurrent.TimeUnit;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.io.kafka.KafkaIO.ReadSourceDescriptors;\n+import org.apache.beam.sdk.io.kafka.KafkaIOUtils.MovingAvg;\n+import org.apache.beam.sdk.io.kafka.KafkaUnboundedReader.TimestampPolicyContext;\n+import org.apache.beam.sdk.io.range.OffsetRange;\n+import org.apache.beam.sdk.transforms.DoFn;\n+import org.apache.beam.sdk.transforms.DoFn.UnboundedPerElement;\n+import org.apache.beam.sdk.transforms.SerializableFunction;\n+import org.apache.beam.sdk.transforms.splittabledofn.GrowableOffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.ManualWatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.OffsetRangeTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker;\n+import org.apache.beam.sdk.transforms.splittabledofn.RestrictionTracker.HasProgress;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimator;\n+import org.apache.beam.sdk.transforms.splittabledofn.WatermarkEstimators.MonotonicallyIncreasing;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.annotations.VisibleForTesting;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Supplier;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.base.Suppliers;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.CacheBuilder;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.CacheLoader;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.cache.LoadingCache;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableList;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.io.Closeables;\n+import org.apache.kafka.clients.consumer.Consumer;\n+import org.apache.kafka.clients.consumer.ConsumerConfig;\n+import org.apache.kafka.clients.consumer.ConsumerRecord;\n+import org.apache.kafka.clients.consumer.ConsumerRecords;\n+import org.apache.kafka.common.TopicPartition;\n+import org.apache.kafka.common.serialization.Deserializer;\n+import org.joda.time.Duration;\n+import org.joda.time.Instant;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+\n+/**\n+ * A SplittableDoFn which reads from {@link KafkaSourceDescriptor} and outputs {@link KafkaRecord}.\n+ * By default, a {@link MonotonicallyIncreasing} watermark estimator is used to track watermark.\n+ *\n+ * <p>{@link ReadFromKafkaDoFn} implements the logic of reading from Kafka. The element is a {@link\n+ * KafkaSourceDescriptor}, and the restriction is an {@link OffsetRange} which represents record\n+ * offset. A {@link GrowableOffsetRangeTracker} is used to track an {@link OffsetRange} ended with\n+ * {@code Long.MAX_VALUE}. For a finite range, a {@link OffsetRangeTracker} is created.\n+ *\n+ * <h4>Initial Restriction</h4>\n+ *\n+ * <p>The initial range for a {@link KafkaSourceDescriptor } is defined by {@code [startOffset,\n+ * Long.MAX_VALUE)} where {@code startOffset} is defined as:\n+ *\n+ * <ul>\n+ *   <li>the {@code startReadOffset} if {@link KafkaSourceDescriptor#getStartReadOffset} is set.\n+ *   <li>the first offset with a greater or equivalent timestamp if {@link\n+ *       KafkaSourceDescriptor#getStartReadTime()} is set.\n+ *   <li>the {@code last committed offset + 1} for the {@link Consumer#position(TopicPartition)\n+ *       topic partition}.\n+ * </ul>\n+ *\n+ * <h4>Splitting</h4>\n+ *\n+ * <p>TODO(BEAM-10319): Add support for initial splitting.\n+ *\n+ * <h4>Checkpoint and Resume Processing</h4>\n+ *\n+ * <p>There are 2 types of checkpoint here: self-checkpoint which invokes by the DoFn and\n+ * system-checkpoint which is issued by the runner via {@link\n+ * org.apache.beam.model.fnexecution.v1.BeamFnApi.ProcessBundleSplitRequest}. Every time the\n+ * consumer gets empty response from {@link Consumer#poll(long)}, {@link ReadFromKafkaDoFn} will\n+ * checkpoint at current {@link KafkaSourceDescriptor } and move to process the next element. These\n+ * deferred elements will be resumed by the runner as soon as possible.\n+ *\n+ * <h4>Progress and Size</h4>\n+ *\n+ * <p>The progress is provided by {@link GrowableOffsetRangeTracker} or per {@link\n+ * KafkaSourceDescriptor }. For an infinite {@link OffsetRange}, a Kafka {@link Consumer} is used in\n+ * the {@link GrowableOffsetRangeTracker} as the {@link\n+ * GrowableOffsetRangeTracker.RangeEndEstimator} to poll the latest offset. Please refer to {@link\n+ * ReadFromKafkaDoFn#restrictionTracker(KafkaSourceDescriptor, OffsetRange)} for details.\n+ *\n+ * <p>The size is computed by {@link ReadFromKafkaDoFn#getSize(KafkaSourceDescriptor, OffsetRange).}\n+ * A {@link KafkaIOUtils.MovingAvg} is used to track the average size of kafka records.\n+ *\n+ * <h4>Track Watermark</h4>\n+ *\n+ * <p>The {@link WatermarkEstimator} is created by {@link\n+ * ReadSourceDescriptors#getCreateWatermarkEstimatorFn()}. The estimated watermark is computed by\n+ * this {@link WatermarkEstimator} based on output timestamps computed by {@link\n+ * ReadSourceDescriptors#getExtractOutputTimestampFn()} (SerializableFunction)}. The default\n+ * configuration is using {@link ReadSourceDescriptors#withProcessingTime()} as {@code\n+ * extractTimestampFn} and {@link\n+ * ReadSourceDescriptors#withMonotonicallyIncreasingWatermarkEstimator()} as {@link\n+ * WatermarkEstimator}.\n+ */\n+@UnboundedPerElement\n+class ReadFromKafkaDoFn<K, V> extends DoFn<KafkaSourceDescriptor, KafkaRecord<K, V>> {\n+\n+  ReadFromKafkaDoFn(ReadSourceDescriptors transform) {\n+    this.consumerConfig = transform.getConsumerConfig();\n+    this.offsetConsumerConfig = transform.getOffsetConsumerConfig();\n+    this.keyDeserializerProvider = transform.getKeyDeserializerProvider();\n+    this.valueDeserializerProvider = transform.getValueDeserializerProvider();\n+    this.consumerFactoryFn = transform.getConsumerFactoryFn();\n+    this.extractOutputTimestampFn = transform.getExtractOutputTimestampFn();\n+    this.createWatermarkEstimatorFn = transform.getCreateWatermarkEstimatorFn();\n+    this.timestampPolicyFactory = transform.getTimestampPolicyFactory();\n+  }\n+\n+  private static final Logger LOG = LoggerFactory.getLogger(ReadFromKafkaDoFn.class);\n+\n+  private final Map<String, Object> offsetConsumerConfig;\n+\n+  private final SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>>\n+      consumerFactoryFn;\n+  private final SerializableFunction<KafkaRecord<K, V>, Instant> extractOutputTimestampFn;\n+  private final SerializableFunction<Instant, WatermarkEstimator<Instant>>\n+      createWatermarkEstimatorFn;\n+  private final TimestampPolicyFactory<K, V> timestampPolicyFactory;\n+\n+  // Valid between bundle start and bundle finish.\n+  private transient ConsumerSpEL consumerSpEL = null;\n+  private transient Deserializer<K> keyDeserializerInstance = null;\n+  private transient Deserializer<V> valueDeserializerInstance = null;\n+\n+  private transient LoadingCache<TopicPartition, AverageRecordSize> avgRecordSize;\n+\n+  private static final Duration KAFKA_POLL_TIMEOUT = Duration.millis(1000);\n+\n+  @VisibleForTesting final DeserializerProvider keyDeserializerProvider;\n+  @VisibleForTesting final DeserializerProvider valueDeserializerProvider;\n+  @VisibleForTesting final Map<String, Object> consumerConfig;\n+\n+  /**\n+   * A {@link GrowableOffsetRangeTracker.RangeEndEstimator} which uses a Kafka {@link Consumer} to\n+   * fetch backlog.\n+   */\n+  private static class KafkaLatestOffsetEstimator\n+      implements GrowableOffsetRangeTracker.RangeEndEstimator {\n+\n+    private final Consumer<byte[], byte[]> offsetConsumer;\n+    private final TopicPartition topicPartition;\n+    private final ConsumerSpEL consumerSpEL;\n+    private final Supplier<Long> memoizedBacklog;\n+\n+    KafkaLatestOffsetEstimator(\n+        Consumer<byte[], byte[]> offsetConsumer, TopicPartition topicPartition) {\n+      this.offsetConsumer = offsetConsumer;\n+      this.topicPartition = topicPartition;\n+      this.consumerSpEL = new ConsumerSpEL();\n+      this.consumerSpEL.evaluateAssign(this.offsetConsumer, ImmutableList.of(this.topicPartition));\n+      memoizedBacklog =\n+          Suppliers.memoizeWithExpiration(\n+              () -> {\n+                consumerSpEL.evaluateSeek2End(offsetConsumer, topicPartition);\n+                return offsetConsumer.position(topicPartition);\n+              },\n+              5,\n+              TimeUnit.SECONDS);\n+    }\n+\n+    @Override\n+    protected void finalize() {\n+      try {\n+        Closeables.close(offsetConsumer, true);\n+      } catch (Exception anyException) {\n+        LOG.warn(\"Failed to close offset consumer for {}\", topicPartition);\n+      }\n+    }\n+\n+    @Override\n+    public long estimate() {\n+      return memoizedBacklog.get();\n+    }\n+  }\n+\n+  @GetInitialRestriction\n+  public OffsetRange initialRestriction(@Element KafkaSourceDescriptor kafkaSourceDescriptor) {\n+    Map<String, Object> updatedConsumerConfig =\n+        overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescriptor);\n+    try (Consumer<byte[], byte[]> offsetConsumer =\n+        consumerFactoryFn.apply(\n+            KafkaIOUtils.getOffsetConsumerConfig(\n+                \"initialOffset\", offsetConsumerConfig, updatedConsumerConfig))) {\n+      consumerSpEL.evaluateAssign(\n+          offsetConsumer, ImmutableList.of(kafkaSourceDescriptor.getTopicPartition()));\n+      long startOffset;\n+      if (kafkaSourceDescriptor.getStartReadOffset() != null) {\n+        startOffset = kafkaSourceDescriptor.getStartReadOffset();\n+      } else if (kafkaSourceDescriptor.getStartReadTime() != null) {\n+        startOffset =\n+            consumerSpEL.offsetForTime(\n+                offsetConsumer,\n+                kafkaSourceDescriptor.getTopicPartition(),\n+                kafkaSourceDescriptor.getStartReadTime());\n+      } else {\n+        startOffset = offsetConsumer.position(kafkaSourceDescriptor.getTopicPartition());\n+      }\n+      return new OffsetRange(startOffset, Long.MAX_VALUE);\n+    }\n+  }\n+\n+  @GetInitialWatermarkEstimatorState\n+  public Instant getInitialWatermarkEstimatorState(@Timestamp Instant currentElementTimestamp) {\n+    return currentElementTimestamp;\n+  }\n+\n+  @NewWatermarkEstimator\n+  public WatermarkEstimator<Instant> newWatermarkEstimator(\n+      @WatermarkEstimatorState Instant watermarkEstimatorState) {\n+    return createWatermarkEstimatorFn.apply(watermarkEstimatorState);\n+  }\n+\n+  @GetSize\n+  public double getSize(\n+      @Element KafkaSourceDescriptor kafkaSourceDescriptor, @Restriction OffsetRange offsetRange)\n+      throws Exception {\n+    double numRecords =\n+        restrictionTracker(kafkaSourceDescriptor, offsetRange).getProgress().getWorkRemaining();\n+    // Before processing elements, we don't have a good estimated size of records and offset gap.\n+    if (!avgRecordSize.asMap().containsKey(kafkaSourceDescriptor.getTopicPartition())) {\n+      return numRecords;\n+    }\n+    return avgRecordSize.get(kafkaSourceDescriptor.getTopicPartition()).getTotalSize(numRecords);\n+  }\n+\n+  @NewTracker\n+  public GrowableOffsetRangeTracker restrictionTracker(\n+      @Element KafkaSourceDescriptor kafkaSourceDescriptor, @Restriction OffsetRange restriction) {\n+    Map<String, Object> updatedConsumerConfig =\n+        overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescriptor);\n+    KafkaLatestOffsetEstimator offsetPoller =\n+        new KafkaLatestOffsetEstimator(\n+            consumerFactoryFn.apply(\n+                KafkaIOUtils.getOffsetConsumerConfig(\n+                    \"tracker-\" + kafkaSourceDescriptor.getTopicPartition(),\n+                    offsetConsumerConfig,\n+                    updatedConsumerConfig)),\n+            kafkaSourceDescriptor.getTopicPartition());\n+    return new GrowableOffsetRangeTracker(restriction.getFrom(), offsetPoller);\n+  }\n+\n+  @ProcessElement\n+  public ProcessContinuation processElement(\n+      @Element KafkaSourceDescriptor kafkaSourceDescriptor,\n+      RestrictionTracker<OffsetRange, Long> tracker,\n+      WatermarkEstimator watermarkEstimator,\n+      OutputReceiver<KafkaRecord<K, V>> receiver) {\n+    // If there is no future work, resume with max timeout and move to the next element.\n+    Map<String, Object> updatedConsumerConfig =\n+        overrideBootstrapServersConfig(consumerConfig, kafkaSourceDescriptor);\n+    // If there is a timestampPolicyFactory, create the TimestampPolicy for current\n+    // TopicPartition.\n+    TimestampPolicy timestampPolicy = null;\n+    if (timestampPolicyFactory != null) {\n+      timestampPolicy =\n+          timestampPolicyFactory.createTimestampPolicy(\n+              kafkaSourceDescriptor.getTopicPartition(),\n+              Optional.ofNullable(watermarkEstimator.currentWatermark()));\n+    }\n+    try (Consumer<byte[], byte[]> consumer = consumerFactoryFn.apply(updatedConsumerConfig)) {\n+      consumerSpEL.evaluateAssign(\n+          consumer, ImmutableList.of(kafkaSourceDescriptor.getTopicPartition()));\n+      long startOffset = tracker.currentRestriction().getFrom();\n+      long expectedOffset = startOffset;\n+      consumer.seek(kafkaSourceDescriptor.getTopicPartition(), startOffset);\n+      ConsumerRecords<byte[], byte[]> rawRecords = ConsumerRecords.empty();\n+\n+      while (true) {\n+        rawRecords = consumer.poll(KAFKA_POLL_TIMEOUT.getMillis());\n+        // When there are no records available for the current TopicPartition, self-checkpoint\n+        // and move to process the next element.\n+        if (rawRecords.isEmpty()) {\n+          return ProcessContinuation.resume();\n+        }\n+        for (ConsumerRecord<byte[], byte[]> rawRecord : rawRecords) {\n+          if (!tracker.tryClaim(rawRecord.offset())) {\n+            return ProcessContinuation.stop();\n+          }\n+          KafkaRecord<K, V> kafkaRecord =\n+              new KafkaRecord<>(\n+                  rawRecord.topic(),\n+                  rawRecord.partition(),\n+                  rawRecord.offset(),\n+                  consumerSpEL.getRecordTimestamp(rawRecord),\n+                  consumerSpEL.getRecordTimestampType(rawRecord),\n+                  ConsumerSpEL.hasHeaders() ? rawRecord.headers() : null,\n+                  keyDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.key()),\n+                  valueDeserializerInstance.deserialize(rawRecord.topic(), rawRecord.value()));\n+          int recordSize =\n+              (rawRecord.key() == null ? 0 : rawRecord.key().length)\n+                  + (rawRecord.value() == null ? 0 : rawRecord.value().length);\n+          avgRecordSize\n+              .getUnchecked(kafkaSourceDescriptor.getTopicPartition())\n+              .update(recordSize, rawRecord.offset() - expectedOffset);\n+          expectedOffset = rawRecord.offset() + 1;\n+          Instant outputTimestamp;\n+          // The outputTimestamp and watermark will be computed by timestampPolicy, where the\n+          // WatermarkEstimator should be a Manual one.\n+          if (timestampPolicy != null) {\n+            checkState(watermarkEstimator instanceof ManualWatermarkEstimator);\n+            TimestampPolicyContext context =\n+                new TimestampPolicyContext(\n+                    (long) ((HasProgress) tracker).getProgress().getWorkRemaining(), Instant.now());\n+            outputTimestamp = timestampPolicy.getTimestampForRecord(context, kafkaRecord);\n+            ((ManualWatermarkEstimator) watermarkEstimator)\n+                .setWatermark(timestampPolicy.getWatermark(context));\n+          } else {\n+            outputTimestamp = extractOutputTimestampFn.apply(kafkaRecord);\n+          }\n+          receiver.outputWithTimestamp(kafkaRecord, outputTimestamp);\n+        }\n+      }\n+    }\n+  }\n+\n+  @GetRestrictionCoder\n+  public Coder<OffsetRange> restrictionCoder() {\n+    return new OffsetRange.Coder();\n+  }\n+\n+  @Setup\n+  public void setup() throws Exception {\n+    // Start to track record size and offset gap per bundle.\n+    avgRecordSize =\n+        CacheBuilder.newBuilder()\n+            .maximumSize(1000L)\n+            .build(\n+                new CacheLoader<TopicPartition, AverageRecordSize>() {\n+                  @Override\n+                  public AverageRecordSize load(TopicPartition topicPartition) throws Exception {\n+                    return new AverageRecordSize();\n+                  }\n+                });\n+    consumerSpEL = new ConsumerSpEL();\n+    keyDeserializerInstance = keyDeserializerProvider.getDeserializer(consumerConfig, true);\n+    valueDeserializerInstance = valueDeserializerProvider.getDeserializer(consumerConfig, false);\n+  }\n+\n+  @Teardown\n+  public void teardown() throws Exception {\n+    try {\n+      Closeables.close(keyDeserializerInstance, true);\n+      Closeables.close(valueDeserializerInstance, true);\n+    } catch (Exception anyException) {\n+      LOG.warn(\"Fail to close resource during finishing bundle: {}\", anyException.getMessage());", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d228f9c9e1835d4b83ef0d3e32091d071746c2c6"}, "originalPosition": 367}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxODA2NDk3OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNzowMTo0N1rOG9hPaA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNzowMTo0N1rOG9hPaA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE2MDkzNg==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Sets the bootstrap servers for the Kafka consumer. If the bootstrapServers is set here but\n          \n          \n            \n                 * also populated with the {@link KafkaSourceDescriptor}, the {@link\n          \n          \n            \n                 * KafkaSourceDescriptor#getBootStrapServers()} will override the bootstrapServers from the\n          \n          \n            \n                 * config.\n          \n          \n            \n                 * Sets the bootstrap servers to use for the Kafka consumer if unspecified via\n          \n          \n            \n                 * KafkaSourceDescriptor#getBootStrapServers()}.", "url": "https://github.com/apache/beam/pull/11749#discussion_r467160936", "createdAt": "2020-08-07T17:01:47Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1288,9 +1294,12 @@ public void populateDisplayData(DisplayData.Builder builder) {\n           .withMonotonicallyIncreasingWatermarkEstimator();\n     }\n \n-    // Note that if the bootstrapServers is set here but also populated with the element, the\n-    // element\n-    // will override the bootstrapServers from the config.\n+    /**\n+     * Sets the bootstrap servers for the Kafka consumer. If the bootstrapServers is set here but\n+     * also populated with the {@link KafkaSourceDescriptor}, the {@link\n+     * KafkaSourceDescriptor#getBootStrapServers()} will override the bootstrapServers from the\n+     * config.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24042b8ea789ae16849369848e3269e230e35f39"}, "originalPosition": 41}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxODA3MTQ0OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNzowMzo1NlrOG9hTXw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QyMjo0NzowMFrOG9qxVQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE2MTk1MQ==", "bodyText": "This comment seems incorrect, it looks like it should be a clone of the key one above but stating that we are deserializing the value.", "url": "https://github.com/apache/beam/pull/11749#discussion_r467161951", "createdAt": "2020-08-07T17:03:56Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1306,112 +1315,206 @@ public void populateDisplayData(DisplayData.Builder builder) {\n       return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} to interpret key bytes read from Kafka.\n+     *\n+     * <p>In addition, Beam also needs a {@link Coder} to serialize and deserialize key objects at\n+     * runtime. KafkaIO tries to infer a coder for the key based on the {@link Deserializer} class,\n+     * however in case that fails, you can use {@link #withKeyDeserializerAndCoder(Class, Coder)} to\n+     * provide the key coder explicitly.\n+     */\n     public ReadSourceDescriptors<K, V> withKeyDeserializer(\n         Class<? extends Deserializer<K>> keyDeserializer) {\n       return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} for interpreting key bytes read from Kafka along with a", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24042b8ea789ae16849369848e3269e230e35f39"}, "originalPosition": 64}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzMxNzA3Nw==", "bodyText": "Done, thanks!", "url": "https://github.com/apache/beam/pull/11749#discussion_r467317077", "createdAt": "2020-08-07T22:47:00Z", "author": {"login": "boyuanzz"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1306,112 +1315,206 @@ public void populateDisplayData(DisplayData.Builder builder) {\n       return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} to interpret key bytes read from Kafka.\n+     *\n+     * <p>In addition, Beam also needs a {@link Coder} to serialize and deserialize key objects at\n+     * runtime. KafkaIO tries to infer a coder for the key based on the {@link Deserializer} class,\n+     * however in case that fails, you can use {@link #withKeyDeserializerAndCoder(Class, Coder)} to\n+     * provide the key coder explicitly.\n+     */\n     public ReadSourceDescriptors<K, V> withKeyDeserializer(\n         Class<? extends Deserializer<K>> keyDeserializer) {\n       return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} for interpreting key bytes read from Kafka along with a", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE2MTk1MQ=="}, "originalCommit": {"oid": "24042b8ea789ae16849369848e3269e230e35f39"}, "originalPosition": 64}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxODA3NDgxOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNzowNDo0NVrOG9hVQQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNzowNDo0NVrOG9hVQQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE2MjQzMw==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * <p>Use this method only if your pipeline doesn't work with plain {@link\n          \n          \n            \n                 * #withKeyDeserializer(Class)}.\n          \n          \n            \n                 * <p>Use this method to override the coder inference performed within {@link\n          \n          \n            \n                 * #withKeyDeserializer(Class)}.", "url": "https://github.com/apache/beam/pull/11749#discussion_r467162433", "createdAt": "2020-08-07T17:04:45Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1306,112 +1315,206 @@ public void populateDisplayData(DisplayData.Builder builder) {\n       return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} to interpret key bytes read from Kafka.\n+     *\n+     * <p>In addition, Beam also needs a {@link Coder} to serialize and deserialize key objects at\n+     * runtime. KafkaIO tries to infer a coder for the key based on the {@link Deserializer} class,\n+     * however in case that fails, you can use {@link #withKeyDeserializerAndCoder(Class, Coder)} to\n+     * provide the key coder explicitly.\n+     */\n     public ReadSourceDescriptors<K, V> withKeyDeserializer(\n         Class<? extends Deserializer<K>> keyDeserializer) {\n       return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} for interpreting key bytes read from Kafka along with a\n+     * {@link Coder} for helping the Beam runner materialize key objects at runtime if necessary.\n+     *\n+     * <p>Use this method only if your pipeline doesn't work with plain {@link\n+     * #withKeyDeserializer(Class)}.\n+     */\n     public ReadSourceDescriptors<K, V> withValueDeserializer(\n         Class<? extends Deserializer<V>> valueDeserializer) {\n       return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} for interpreting key bytes read from Kafka along with a\n+     * {@link Coder} for helping the Beam runner materialize key objects at runtime if necessary.\n+     *\n+     * <p>Use this method only if your pipeline doesn't work with plain {@link\n+     * #withKeyDeserializer(Class)}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24042b8ea789ae16849369848e3269e230e35f39"}, "originalPosition": 80}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxODA3NTQ5OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNzowNDo1OFrOG9hVsQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNzowNDo1OFrOG9hVsQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE2MjU0NQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * <p>Use this method only if your pipeline doesn't work with plain {@link\n          \n          \n            \n                 * #withValueDeserializer(Class)}.\n          \n          \n            \n                 * <p>Use this method to override the coder inference performed within {@link\n          \n          \n            \n                 * #withValueDeserializer(Class)}.", "url": "https://github.com/apache/beam/pull/11749#discussion_r467162545", "createdAt": "2020-08-07T17:04:58Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1306,112 +1315,206 @@ public void populateDisplayData(DisplayData.Builder builder) {\n       return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} to interpret key bytes read from Kafka.\n+     *\n+     * <p>In addition, Beam also needs a {@link Coder} to serialize and deserialize key objects at\n+     * runtime. KafkaIO tries to infer a coder for the key based on the {@link Deserializer} class,\n+     * however in case that fails, you can use {@link #withKeyDeserializerAndCoder(Class, Coder)} to\n+     * provide the key coder explicitly.\n+     */\n     public ReadSourceDescriptors<K, V> withKeyDeserializer(\n         Class<? extends Deserializer<K>> keyDeserializer) {\n       return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} for interpreting key bytes read from Kafka along with a\n+     * {@link Coder} for helping the Beam runner materialize key objects at runtime if necessary.\n+     *\n+     * <p>Use this method only if your pipeline doesn't work with plain {@link\n+     * #withKeyDeserializer(Class)}.\n+     */\n     public ReadSourceDescriptors<K, V> withValueDeserializer(\n         Class<? extends Deserializer<V>> valueDeserializer) {\n       return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} for interpreting key bytes read from Kafka along with a\n+     * {@link Coder} for helping the Beam runner materialize key objects at runtime if necessary.\n+     *\n+     * <p>Use this method only if your pipeline doesn't work with plain {@link\n+     * #withKeyDeserializer(Class)}.\n+     */\n     public ReadSourceDescriptors<K, V> withKeyDeserializerAndCoder(\n         Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n       return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} for interpreting value bytes read from Kafka along with a\n+     * {@link Coder} for helping the Beam runner materialize value objects at runtime if necessary.\n+     *\n+     * <p>Use this method only if your pipeline doesn't work with plain {@link\n+     * #withValueDeserializer(Class)}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24042b8ea789ae16849369848e3269e230e35f39"}, "originalPosition": 92}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMjkxODA4ODMwOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "isResolved": true, "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNzowOToxMlrOG9hdhw==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0wN1QxNzowOToxMlrOG9hdhw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ2NzE2NDU1MQ==", "bodyText": "This file contains bidirectional Unicode text that may be interpreted or compiled differently than what appears below. To review, open the file in an editor that reveals hidden Unicode characters. Learn more about bidirectional Unicode characters\n\n\n  \n\n\n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                 * Update configuration for the backend main consumer. Note that the default consumer properties\n          \n          \n            \n                 * will not be completely overridden. This method only updates the value which has the same key.\n          \n          \n            \n                 *\n          \n          \n            \n                 * <p>In {@link ReadFromKafkaDoFn}, there're two consumers running in the backend actually:<br>\n          \n          \n            \n                 * 1. the main consumer, which reads data from kafka;<br>\n          \n          \n            \n                 * 2. the secondary offset consumer, which is used to estimate backlog, by fetching latest\n          \n          \n            \n                 * offset;<br>\n          \n          \n            \n                 *\n          \n          \n            \n                 * <p>By default, main consumer uses the configuration from {@link\n          \n          \n            \n                 * KafkaIOUtils#DEFAULT_CONSUMER_PROPERTIES}.\n          \n          \n            \n                 * Updates configuration for the main consumer. This method merges updates from the provided map with      \n          \n          \n            \n                 * with any prior updates using {@link\n          \n          \n            \n                 * KafkaIOUtils#DEFAULT_CONSUMER_PROPERTIES} as the starting configuration.\n          \n          \n            \n                 *\n          \n          \n            \n                 * <p>In {@link ReadFromKafkaDoFn}, there're two consumers running in the backend:\n          \n          \n            \n                 * <ol>\n          \n          \n            \n                 *   <li>the main consumer which reads data from kafka.\n          \n          \n            \n                 *   <li>the secondary offset consumer which is used to estimate the backlog by fetching the latest offset.\n          \n          \n            \n                 * </ol>\n          \n          \n            \n                 *\n          \n          \n            \n                 * <p>See {@link #withConsumerConfigOverrides} for overriding the configuration instead of updating it.\n          \n          \n            \n                 *\n          \n          \n            \n                 * <p>See {@link #withOffsetConsumerConfigOverrides} for configuring the secondary offset consumer.", "url": "https://github.com/apache/beam/pull/11749#discussion_r467164551", "createdAt": "2020-08-07T17:09:12Z", "author": {"login": "lukecwik"}, "path": "sdks/java/io/kafka/src/main/java/org/apache/beam/sdk/io/kafka/KafkaIO.java", "diffHunk": "@@ -1306,112 +1315,206 @@ public void populateDisplayData(DisplayData.Builder builder) {\n       return toBuilder().setValueDeserializerProvider(deserializerProvider).build();\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} to interpret key bytes read from Kafka.\n+     *\n+     * <p>In addition, Beam also needs a {@link Coder} to serialize and deserialize key objects at\n+     * runtime. KafkaIO tries to infer a coder for the key based on the {@link Deserializer} class,\n+     * however in case that fails, you can use {@link #withKeyDeserializerAndCoder(Class, Coder)} to\n+     * provide the key coder explicitly.\n+     */\n     public ReadSourceDescriptors<K, V> withKeyDeserializer(\n         Class<? extends Deserializer<K>> keyDeserializer) {\n       return withKeyDeserializerProvider(LocalDeserializerProvider.of(keyDeserializer));\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} for interpreting key bytes read from Kafka along with a\n+     * {@link Coder} for helping the Beam runner materialize key objects at runtime if necessary.\n+     *\n+     * <p>Use this method only if your pipeline doesn't work with plain {@link\n+     * #withKeyDeserializer(Class)}.\n+     */\n     public ReadSourceDescriptors<K, V> withValueDeserializer(\n         Class<? extends Deserializer<V>> valueDeserializer) {\n       return withValueDeserializerProvider(LocalDeserializerProvider.of(valueDeserializer));\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} for interpreting key bytes read from Kafka along with a\n+     * {@link Coder} for helping the Beam runner materialize key objects at runtime if necessary.\n+     *\n+     * <p>Use this method only if your pipeline doesn't work with plain {@link\n+     * #withKeyDeserializer(Class)}.\n+     */\n     public ReadSourceDescriptors<K, V> withKeyDeserializerAndCoder(\n         Class<? extends Deserializer<K>> keyDeserializer, Coder<K> keyCoder) {\n       return withKeyDeserializer(keyDeserializer).toBuilder().setKeyCoder(keyCoder).build();\n     }\n \n+    /**\n+     * Sets a Kafka {@link Deserializer} for interpreting value bytes read from Kafka along with a\n+     * {@link Coder} for helping the Beam runner materialize value objects at runtime if necessary.\n+     *\n+     * <p>Use this method only if your pipeline doesn't work with plain {@link\n+     * #withValueDeserializer(Class)}.\n+     */\n     public ReadSourceDescriptors<K, V> withValueDeserializerAndCoder(\n         Class<? extends Deserializer<V>> valueDeserializer, Coder<V> valueCoder) {\n       return withValueDeserializer(valueDeserializer).toBuilder().setValueCoder(valueCoder).build();\n     }\n \n+    /**\n+     * A factory to create Kafka {@link Consumer} from consumer configuration. This is useful for\n+     * supporting another version of Kafka consumer. Default is {@link KafkaConsumer}.\n+     */\n     public ReadSourceDescriptors<K, V> withConsumerFactoryFn(\n         SerializableFunction<Map<String, Object>, Consumer<byte[], byte[]>> consumerFactoryFn) {\n       return toBuilder().setConsumerFactoryFn(consumerFactoryFn).build();\n     }\n \n+    /**\n+     * Update configuration for the backend main consumer. Note that the default consumer properties\n+     * will not be completely overridden. This method only updates the value which has the same key.\n+     *\n+     * <p>In {@link ReadFromKafkaDoFn}, there're two consumers running in the backend actually:<br>\n+     * 1. the main consumer, which reads data from kafka;<br>\n+     * 2. the secondary offset consumer, which is used to estimate backlog, by fetching latest\n+     * offset;<br>\n+     *\n+     * <p>By default, main consumer uses the configuration from {@link\n+     * KafkaIOUtils#DEFAULT_CONSUMER_PROPERTIES}.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "24042b8ea789ae16849369848e3269e230e35f39"}, "originalPosition": 118}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3807, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}