{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDQzMDI3NjQ5", "number": 12157, "title": "[BEAM-7587] Spark portable streaming", "bodyText": "These changes will allow streaming data on the portable Spark runner.\n\nThank you for your contribution! Follow this checklist to help us incorporate your contribution quickly and easily:\n\n Choose reviewer(s) and mention them in a comment (R: @username).\n Format the pull request title like [BEAM-XXX] Fixes bug in ApproximateQuantiles, where you replace BEAM-XXX with the appropriate JIRA issue, if applicable. This will automatically link the pull request to the issue.\n Update CHANGES.md with noteworthy changes.\n If this contribution is large, please file an Apache Individual Contributor License Agreement.\n\nSee the Contributor Guide for more tips on how to make review process smoother.\nPost-Commit Tests Status (on master branch)\n\n\n\nLang\nSDK\nDataflow\nFlink\nSamza\nSpark\nTwister2\n\n\n\n\nGo\n\n---\n\n---\n\n---\n\n\nJava\n\n\n\n\n\n\n\n\nPython\n\n\n\n---\n\n---\n\n\nXLang\n---\n---\n\n---\n---\n\n\n\n\nPre-Commit Tests Status (on master branch)\n\n\n\n---\nJava\nPython\nGo\nWebsite\n\n\n\n\nNon-portable\n\n\n\n\n\n\nPortable\n---\n\n---\n---\n\n\n\nSee .test-infra/jenkins/README for trigger phrase, status and link of all Jenkins jobs.", "createdAt": "2020-07-01T22:28:32Z", "url": "https://github.com/apache/beam/pull/12157", "merged": true, "mergeCommit": {"oid": "10361a3e138532cffd086c98016235c4cd2abcf8"}, "closed": true, "closedAt": "2020-08-22T16:59:06Z", "author": {"login": "annaqin418"}, "timelineItems": {"totalCount": 55, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpPPAAABcwyiDrAFqTQ0MTI0NDY5Mg==", "endCursor": "Y3Vyc29yOnYyOpPPAAABdBcbT-AFqTQ3MjkzODM3NA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQxMjQ0Njky", "url": "https://github.com/apache/beam/pull/12157#pullrequestreview-441244692", "createdAt": "2020-07-01T22:52:52Z", "commit": {"oid": "a305406ee36064a0a35779a5669832068221cb55"}, "state": "COMMENTED", "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQyMjo1MzoxMVrOGr30Gg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wMVQyMjo1NDozNlrOGr31-w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODY1NjQxMA==", "bodyText": "Import classes instead of packages to avoid bloating the namespace.", "url": "https://github.com/apache/beam/pull/12157#discussion_r448656410", "createdAt": "2020-07-01T22:53:11Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/SparkPipelineRunner.java", "diffHunk": "@@ -40,9 +41,7 @@\n import org.apache.beam.runners.jobsubmission.PortablePipelineRunner;\n import org.apache.beam.runners.spark.aggregators.AggregatorsAccumulator;\n import org.apache.beam.runners.spark.metrics.MetricsAccumulator;\n-import org.apache.beam.runners.spark.translation.SparkBatchPortablePipelineTranslator;\n-import org.apache.beam.runners.spark.translation.SparkContextFactory;\n-import org.apache.beam.runners.spark.translation.SparkTranslationContext;\n+import org.apache.beam.runners.spark.translation.*;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a305406ee36064a0a35779a5669832068221cb55"}, "originalPosition": 15}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ0ODY1Njg5MQ==", "bodyText": "Why are these access changes necessary?", "url": "https://github.com/apache/beam/pull/12157#discussion_r448656891", "createdAt": "2020-07-01T22:54:36Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkTranslationContext.java", "diffHunk": "@@ -35,15 +35,15 @@\n  * compute them after translation.\n  */\n public class SparkTranslationContext {\n-  private final JavaSparkContext jsc;\n+  protected final JavaSparkContext jsc;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a305406ee36064a0a35779a5669832068221cb55"}, "originalPosition": 5}]}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDQzNDU3ODU3", "url": "https://github.com/apache/beam/pull/12157#pullrequestreview-443457857", "createdAt": "2020-07-06T22:59:34Z", "commit": {"oid": "d2d3f3ef0e67e52bececf67a34f61b7123de1c47"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNlQyMjo1OTozNVrOGtppxg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wNy0wNlQyMjo1OTozNVrOGtppxg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ1MDUyMTU0Mg==", "bodyText": "This option only applies to Flink, it has no meaning in Spark. \n  \n    \n      beam/runners/flink/src/main/java/org/apache/beam/runners/flink/FlinkPipelineOptions.java\n    \n    \n         Line 74\n      in\n      103facb\n    \n    \n    \n    \n\n        \n          \n           Integer getParallelism();", "url": "https://github.com/apache/beam/pull/12157#discussion_r450521542", "createdAt": "2020-07-06T22:59:35Z", "author": {"login": "ibzib"}, "path": "runners/spark/job-server/build.gradle", "diffHunk": "@@ -82,13 +82,21 @@ runShadow {\n     jvmArgs += [\"-Dorg.slf4j.simpleLogger.defaultLogLevel=${project.property('logLevel')}\"]\n }\n \n-def portableValidatesRunnerTask(String name) {\n+def portableValidatesRunnerTask(String name, Boolean streaming) {\n+  def pipelineOptions = [\n+          // Limit resource consumption via parallelism\n+          \"--parallelism=2\",", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "d2d3f3ef0e67e52bececf67a34f61b7123de1c47"}, "originalPosition": 8}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "a588f1146881ae10f0e997ed07db02ee311e4500", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/a588f1146881ae10f0e997ed07db02ee311e4500", "committedDate": "2020-07-17T20:36:51Z", "message": "Merge branch 'BEAM-7587-spark-portable-streaming-0' of github.com:annaqin418/beam into BEAM-7587-spark-portable-streaming-0"}, "afterCommit": {"oid": "679dd939d8fae4ef191891e1fb1f78362799d683", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/679dd939d8fae4ef191891e1fb1f78362799d683", "committedDate": "2020-07-17T21:30:17Z", "message": "flatten v0"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "679dd939d8fae4ef191891e1fb1f78362799d683", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/679dd939d8fae4ef191891e1fb1f78362799d683", "committedDate": "2020-07-17T21:30:17Z", "message": "flatten v0"}, "afterCommit": {"oid": "ecbe519c5cdb569156a8805221747fc4710ebd3e", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/ecbe519c5cdb569156a8805221747fc4710ebd3e", "committedDate": "2020-07-21T16:54:26Z", "message": "no-output sink"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ecbe519c5cdb569156a8805221747fc4710ebd3e", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/ecbe519c5cdb569156a8805221747fc4710ebd3e", "committedDate": "2020-07-21T16:54:26Z", "message": "no-output sink"}, "afterCommit": {"oid": "5fafd4dd77d9e7472360d5818106518774c2e10e", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/5fafd4dd77d9e7472360d5818106518774c2e10e", "committedDate": "2020-08-03T22:14:31Z", "message": "start streaming context in portable runner; modified impulse and exec stage"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "5fafd4dd77d9e7472360d5818106518774c2e10e", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/5fafd4dd77d9e7472360d5818106518774c2e10e", "committedDate": "2020-08-03T22:14:31Z", "message": "start streaming context in portable runner; modified impulse and exec stage"}, "afterCommit": {"oid": "37c2c94a9be216fdd724654ee2ce35c8415f8c61", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/37c2c94a9be216fdd724654ee2ce35c8415f8c61", "committedDate": "2020-08-11T17:08:02Z", "message": "reshuffle v0"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "37c2c94a9be216fdd724654ee2ce35c8415f8c61", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/37c2c94a9be216fdd724654ee2ce35c8415f8c61", "committedDate": "2020-08-11T17:08:02Z", "message": "reshuffle v0"}, "afterCommit": {"oid": "603b91685fa1283bb9f07bda6c93f710e8829a64", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/603b91685fa1283bb9f07bda6c93f710e8829a64", "committedDate": "2020-08-11T22:17:50Z", "message": "added jenkins job, changed watermark timestamp"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "603b91685fa1283bb9f07bda6c93f710e8829a64", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/603b91685fa1283bb9f07bda6c93f710e8829a64", "committedDate": "2020-08-11T22:17:50Z", "message": "added jenkins job, changed watermark timestamp"}, "afterCommit": {"oid": "cfd50c3c366d153bb0e2151beb18ee3b207498f5", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/cfd50c3c366d153bb0e2151beb18ee3b207498f5", "committedDate": "2020-08-13T01:56:20Z", "message": "timeout option for streaming v0.1"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "cfd50c3c366d153bb0e2151beb18ee3b207498f5", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/cfd50c3c366d153bb0e2151beb18ee3b207498f5", "committedDate": "2020-08-13T01:56:20Z", "message": "timeout option for streaming v0.1"}, "afterCommit": {"oid": "9e8c7b48be4996016437650aa5180e4a6c7b0219", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/9e8c7b48be4996016437650aa5180e4a6c7b0219", "committedDate": "2020-08-13T22:54:16Z", "message": "spotless, checkstyle, spotbugs"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "9e8c7b48be4996016437650aa5180e4a6c7b0219", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/9e8c7b48be4996016437650aa5180e4a6c7b0219", "committedDate": "2020-08-13T22:54:16Z", "message": "spotless, checkstyle, spotbugs"}, "afterCommit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/b3fba9cb53d5c0e1e5950d5065736966af25b97a", "committedDate": "2020-08-14T18:43:44Z", "message": "separate build task for streaming, updated registrar test"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY3OTA4NzQ5", "url": "https://github.com/apache/beam/pull/12157#pullrequestreview-467908749", "createdAt": "2020-08-14T22:39:01Z", "commit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a"}, "state": "COMMENTED", "comments": {"totalCount": 23, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNFQyMjozOTowMVrOHBFDOg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xNVQwMDoyMjozMFrOHBGRCg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg5MzM3MA==", "bodyText": "Since the batch and streaming configurations only differ in three fields (pipelineOptions, testCategories, and testFilter),  they should share the rest to avoid code duplication.\nFYI createPortableValidatesRunnerTask is implicitly creating a PortableValidatesRunnerConfiguration object, to share code you will probably have to create one explicitly.\n\n  \n    \n      beam/buildSrc/src/main/groovy/org/apache/beam/gradle/BeamModulePlugin.groovy\n    \n    \n         Line 245\n      in\n      71c7760\n    \n    \n    \n    \n\n        \n          \n           class PortableValidatesRunnerConfiguration {", "url": "https://github.com/apache/beam/pull/12157#discussion_r470893370", "createdAt": "2020-08-14T22:39:01Z", "author": {"login": "ibzib"}, "path": "runners/spark/job-server/build.gradle", "diffHunk": "@@ -82,53 +82,116 @@ runShadow {\n     jvmArgs += [\"-Dorg.slf4j.simpleLogger.defaultLogLevel=${project.property('logLevel')}\"]\n }\n \n-def portableValidatesRunnerTask(String name) {\n-  createPortableValidatesRunnerTask(\n-    name: \"validatesPortableRunner${name}\",\n-    jobServerDriver: \"org.apache.beam.runners.spark.SparkJobServerDriver\",\n-    jobServerConfig: \"--job-host=localhost,--job-port=0,--artifact-port=0,--expansion-port=0\",\n-    testClasspathConfiguration: configurations.validatesPortableRunner,\n-    numParallelTests: 4,\n-    environment: BeamModulePlugin.PortableValidatesRunnerConfiguration.Environment.EMBEDDED,\n-    systemProperties: [\n-      \"beam.spark.test.reuseSparkContext\": \"false\",\n-      \"spark.ui.enabled\": \"false\",\n-      \"spark.ui.showConsoleProgress\": \"false\",\n-    ],\n-    testCategories: {\n-      includeCategories 'org.apache.beam.sdk.testing.ValidatesRunner'\n-      excludeCategories 'org.apache.beam.sdk.testing.FlattenWithHeterogeneousCoders'\n-      excludeCategories 'org.apache.beam.sdk.testing.LargeKeys$Above100MB'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesCommittedMetrics'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesCustomWindowMerging'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesFailureMessage'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesGaugeMetrics'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesParDoLifecycle'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesMapState'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesSetState'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesOrderedListState'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesTimerMap'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesKeyInParDo'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesOnWindowExpiration'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesTestStream'\n-      //SplitableDoFnTests\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesBoundedSplittableParDo'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesSplittableParDoWithWindowedSideInputs'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesUnboundedSplittableParDo'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesStrictTimerOrdering'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesBundleFinalizer'\n-    },\n-    testFilter: {\n-      // TODO(BEAM-10094)\n-      excludeTestsMatching 'org.apache.beam.sdk.transforms.FlattenTest.testFlattenWithDifferentInputAndOutputCoders2'\n-    },\n-  )\n+def portableValidatesRunnerTask(String name, Boolean streaming) {\n+  def pipelineOptions = []\n+  if (streaming) {\n+    pipelineOptions += \"--streaming\"\n+    pipelineOptions += \"--timeout=20000\"\n+    // exclude unsupported tests\n+    createPortableValidatesRunnerTask(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a"}, "originalPosition": 51}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg5NDU0MQ==", "bodyText": "Nit: reorder this to make it more readable.\nboolean isStreaming = pipelineOptions.isStreaming() || hasUnboundedPCollections(pipeline);\nif (isStreaming) {\n...\n} else {\n...\n}", "url": "https://github.com/apache/beam/pull/12157#discussion_r470894541", "createdAt": "2020-08-14T22:44:23Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/SparkPipelineRunner.java", "diffHunk": "@@ -71,7 +79,15 @@ public SparkPipelineRunner(SparkPipelineOptions pipelineOptions) {\n \n   @Override\n   public PortablePipelineResult run(RunnerApi.Pipeline pipeline, JobInfo jobInfo) {\n-    SparkBatchPortablePipelineTranslator translator = new SparkBatchPortablePipelineTranslator();\n+    boolean isStreaming;\n+    SparkPortablePipelineTranslator translator;\n+    if (!pipelineOptions.isStreaming() && !hasUnboundedPCollections(pipeline)) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a"}, "originalPosition": 37}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg5NjY1OA==", "bodyText": "Prefer to go through the logger rather than using printStackTrace.\nAll the LOG methods take an exception as an additional argument, like this: \n  \n    \n      beam/runners/spark/src/main/java/org/apache/beam/runners/spark/SparkJobServerDriver.java\n    \n    \n         Line 71\n      in\n      e725118\n    \n    \n    \n    \n\n        \n          \n           LOG.error(\"Unable to parse command line arguments.\", e);", "url": "https://github.com/apache/beam/pull/12157#discussion_r470896658", "createdAt": "2020-08-14T22:53:46Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/SparkPipelineRunner.java", "diffHunk": "@@ -106,39 +123,95 @@ public PortablePipelineResult run(RunnerApi.Pipeline pipeline, JobInfo jobInfo)\n         pipelineOptions.getFilesToStage().size());\n     LOG.debug(\"Staging files: {}\", pipelineOptions.getFilesToStage());\n \n+    PortablePipelineResult result;\n     final JavaSparkContext jsc = SparkContextFactory.getSparkContext(pipelineOptions);\n+\n     LOG.info(String.format(\"Running job %s on Spark master %s\", jobInfo.jobId(), jsc.master()));\n-    AggregatorsAccumulator.init(pipelineOptions, jsc);\n \n+    // Initialize accumulators.\n+    AggregatorsAccumulator.init(pipelineOptions, jsc);\n     MetricsEnvironment.setMetricsSupported(true);\n     MetricsAccumulator.init(pipelineOptions, jsc);\n \n     final SparkTranslationContext context =\n-        new SparkTranslationContext(jsc, pipelineOptions, jobInfo);\n+        translator.createTranslationContext(jsc, pipelineOptions, jobInfo);\n     final ExecutorService executorService = Executors.newSingleThreadExecutor();\n-    final Future<?> submissionFuture =\n-        executorService.submit(\n-            () -> {\n-              translator.translate(fusedPipeline, context);\n-              LOG.info(\n-                  String.format(\n-                      \"Job %s: Pipeline translated successfully. Computing outputs\",\n-                      jobInfo.jobId()));\n-              context.computeOutputs();\n-              LOG.info(String.format(\"Job %s finished.\", jobInfo.jobId()));\n-            });\n-\n-    PortablePipelineResult result =\n-        new SparkPipelineResult.PortableBatchMode(submissionFuture, jsc);\n+\n+    LOG.info(String.format(\"Running job %s on Spark master %s\", jobInfo.jobId(), jsc.master()));\n+\n+    if (isStreaming) {\n+      final JavaStreamingContext jssc =\n+          ((SparkStreamingTranslationContext) context).getStreamingContext();\n+\n+      jssc.addStreamingListener(\n+          new JavaStreamingListenerWrapper(\n+              new AggregatorsAccumulator.AccumulatorCheckpointingSparkListener()));\n+      jssc.addStreamingListener(\n+          new JavaStreamingListenerWrapper(\n+              new MetricsAccumulator.AccumulatorCheckpointingSparkListener()));\n+\n+      // register user-defined listeners.\n+      for (JavaStreamingListener listener :\n+          pipelineOptions.as(SparkContextOptions.class).getListeners()) {\n+        LOG.info(\"Registered listener {}.\" + listener.getClass().getSimpleName());\n+        jssc.addStreamingListener(new JavaStreamingListenerWrapper(listener));\n+      }\n+\n+      // register Watermarks listener to broadcast the advanced WMs.\n+      jssc.addStreamingListener(\n+          new JavaStreamingListenerWrapper(\n+              new GlobalWatermarkHolder.WatermarkAdvancingStreamingListener()));\n+\n+      jssc.checkpoint(pipelineOptions.getCheckpointDir());\n+\n+      // Obtain timeout from options.\n+      Long timeout = pipelineOptions.as(SparkPortableStreamingPipelineOptions.class).getTimeout();\n+\n+      final Future<?> submissionFuture =\n+          executorService.submit(\n+              () -> {\n+                translator.translate(fusedPipeline, context);\n+                LOG.info(\n+                    String.format(\n+                        \"Job %s: Pipeline translated successfully. Computing outputs\",\n+                        jobInfo.jobId()));\n+                context.computeOutputs();\n+\n+                jssc.start();\n+                try {\n+                  jssc.awaitTerminationOrTimeout(timeout);\n+                } catch (InterruptedException e) {\n+                  LOG.warn(\"Streaming context interrupted, shutting down.\");\n+                  e.printStackTrace();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a"}, "originalPosition": 134}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg5NzY0NA==", "bodyText": "If -1L has a special meaning (ie infinity), document it here.", "url": "https://github.com/apache/beam/pull/12157#discussion_r470897644", "createdAt": "2020-08-14T22:58:23Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/SparkPortableStreamingPipelineOptions.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark;\n+\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PortablePipelineOptions;\n+\n+/**\n+ * Spark runner {@link PipelineOptions} handles Spark execution-related configurations, such as the\n+ * master address, batch-interval, and other user-related knobs.\n+ */\n+@Experimental\n+public interface SparkPortableStreamingPipelineOptions\n+    extends SparkPipelineOptions, PortablePipelineOptions, PipelineOptions {\n+  @Description(\"Timeout for testing Spark portable streaming, in milliseconds.\")", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg5ODA1OQ==", "bodyText": "Is -1L already treated as a special case by the Spark streaming context, or do we need to handle it specially in Beam?", "url": "https://github.com/apache/beam/pull/12157#discussion_r470898059", "createdAt": "2020-08-14T23:00:22Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/SparkPortableStreamingPipelineOptions.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark;\n+\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PortablePipelineOptions;\n+\n+/**\n+ * Spark runner {@link PipelineOptions} handles Spark execution-related configurations, such as the\n+ * master address, batch-interval, and other user-related knobs.\n+ */\n+@Experimental\n+public interface SparkPortableStreamingPipelineOptions\n+    extends SparkPipelineOptions, PortablePipelineOptions, PipelineOptions {\n+  @Description(\"Timeout for testing Spark portable streaming, in milliseconds.\")\n+  @Default.Long(-1L)", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg5ODg4Nw==", "bodyText": "Pipeline options all occupy the same namespace. We should give this option a name more specific to its purpose to prevent collisions.\nAlso, this is just personal preference, but I like when folks suffix their variable names with the units (here, Ms or Millis or Milliseconds) when using raw numbers as time values.", "url": "https://github.com/apache/beam/pull/12157#discussion_r470898887", "createdAt": "2020-08-14T23:04:14Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/SparkPortableStreamingPipelineOptions.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark;\n+\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PortablePipelineOptions;\n+\n+/**\n+ * Spark runner {@link PipelineOptions} handles Spark execution-related configurations, such as the\n+ * master address, batch-interval, and other user-related knobs.\n+ */\n+@Experimental\n+public interface SparkPortableStreamingPipelineOptions\n+    extends SparkPipelineOptions, PortablePipelineOptions, PipelineOptions {\n+  @Description(\"Timeout for testing Spark portable streaming, in milliseconds.\")\n+  @Default.Long(-1L)\n+  Long getTimeout();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a"}, "originalPosition": 35}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg5OTI3NA==", "bodyText": "Change this.", "url": "https://github.com/apache/beam/pull/12157#discussion_r470899274", "createdAt": "2020-08-14T23:05:52Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/SparkPortableStreamingPipelineOptions.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark;\n+\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PortablePipelineOptions;\n+\n+/**\n+ * Spark runner {@link PipelineOptions} handles Spark execution-related configurations, such as the", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a"}, "originalPosition": 27}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDg5OTQzNQ==", "bodyText": "We figured out we don't need to extend PipelineOptions directly. Remove it.", "url": "https://github.com/apache/beam/pull/12157#discussion_r470899435", "createdAt": "2020-08-14T23:06:42Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/SparkPortableStreamingPipelineOptions.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark;\n+\n+import org.apache.beam.sdk.annotations.Experimental;\n+import org.apache.beam.sdk.options.Default;\n+import org.apache.beam.sdk.options.Description;\n+import org.apache.beam.sdk.options.PipelineOptions;\n+import org.apache.beam.sdk.options.PortablePipelineOptions;\n+\n+/**\n+ * Spark runner {@link PipelineOptions} handles Spark execution-related configurations, such as the\n+ * master address, batch-interval, and other user-related knobs.\n+ */\n+@Experimental\n+public interface SparkPortableStreamingPipelineOptions\n+    extends SparkPipelineOptions, PortablePipelineOptions, PipelineOptions {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a"}, "originalPosition": 32}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkwMTU4Mg==", "bodyText": "Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n            /** Translates a bounded portable pipeline into a Spark job. */\n          \n          \n            \n            /** Translates an unbounded portable pipeline into a Spark job. */", "url": "https://github.com/apache/beam/pull/12157#discussion_r470901582", "createdAt": "2020-08-14T23:17:05Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkStreamingPortablePipelineTranslator.java", "diffHunk": "@@ -0,0 +1,393 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark.translation;\n+\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.createOutputMap;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowingStrategy;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.PTransformTranslation;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode.PTransformNode;\n+import org.apache.beam.runners.core.construction.graph.QueryablePipeline;\n+import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n+import org.apache.beam.runners.fnexecution.wire.WireCoders;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n+import org.apache.beam.runners.spark.coders.CoderHelpers;\n+import org.apache.beam.runners.spark.metrics.MetricsAccumulator;\n+import org.apache.beam.runners.spark.stateful.SparkGroupAlsoByWindowViaWindowSet;\n+import org.apache.beam.runners.spark.translation.streaming.UnboundedDataset;\n+import org.apache.beam.runners.spark.util.GlobalWatermarkHolder;\n+import org.apache.beam.runners.spark.util.SparkCompat;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.transforms.join.RawUnionValue;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.TimestampedValue;\n+import org.apache.beam.sdk.values.WindowingStrategy;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.BiMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Iterables;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaInputDStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Tuple2;\n+\n+/** Translates a bounded portable pipeline into a Spark job. */", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a"}, "originalPosition": 74}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkwMjMxNw==", "bodyText": "Super nit: this line doesn't deserve a newline following it.", "url": "https://github.com/apache/beam/pull/12157#discussion_r470902317", "createdAt": "2020-08-14T23:20:28Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkStreamingPortablePipelineTranslator.java", "diffHunk": "@@ -0,0 +1,393 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark.translation;\n+\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.createOutputMap;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowingStrategy;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.PTransformTranslation;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode.PTransformNode;\n+import org.apache.beam.runners.core.construction.graph.QueryablePipeline;\n+import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n+import org.apache.beam.runners.fnexecution.wire.WireCoders;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n+import org.apache.beam.runners.spark.coders.CoderHelpers;\n+import org.apache.beam.runners.spark.metrics.MetricsAccumulator;\n+import org.apache.beam.runners.spark.stateful.SparkGroupAlsoByWindowViaWindowSet;\n+import org.apache.beam.runners.spark.translation.streaming.UnboundedDataset;\n+import org.apache.beam.runners.spark.util.GlobalWatermarkHolder;\n+import org.apache.beam.runners.spark.util.SparkCompat;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.transforms.join.RawUnionValue;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.TimestampedValue;\n+import org.apache.beam.sdk.values.WindowingStrategy;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.BiMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Iterables;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaInputDStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Tuple2;\n+\n+/** Translates a bounded portable pipeline into a Spark job. */\n+public class SparkStreamingPortablePipelineTranslator\n+    implements SparkPortablePipelineTranslator<SparkStreamingTranslationContext> {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SparkStreamingPortablePipelineTranslator.class);\n+\n+  private final ImmutableMap<String, PTransformTranslator> urnToTransformTranslator;\n+\n+  interface PTransformTranslator {\n+\n+    /** Translates transformNode from Beam into the Spark context. */\n+    void translate(\n+        PTransformNode transformNode,\n+        RunnerApi.Pipeline pipeline,\n+        SparkStreamingTranslationContext context);\n+  }\n+\n+  @Override\n+  public Set<String> knownUrns() {\n+    return urnToTransformTranslator.keySet();\n+  }\n+\n+  public SparkStreamingPortablePipelineTranslator() {\n+    ImmutableMap.Builder<String, PTransformTranslator> translatorMap = ImmutableMap.builder();\n+    translatorMap.put(\n+        PTransformTranslation.IMPULSE_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateImpulse);\n+    translatorMap.put(\n+        PTransformTranslation.GROUP_BY_KEY_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateGroupByKey);\n+    translatorMap.put(\n+        ExecutableStage.URN, SparkStreamingPortablePipelineTranslator::translateExecutableStage);\n+    translatorMap.put(\n+        PTransformTranslation.FLATTEN_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateFlatten);\n+    translatorMap.put(\n+        PTransformTranslation.RESHUFFLE_URN,\n+        SparkStreamingPortablePipelineTranslator::translateReshuffle);\n+    this.urnToTransformTranslator = translatorMap.build();\n+  }\n+\n+  /** Translates pipeline from Beam into the Spark context. */\n+  @Override\n+  public void translate(\n+      final RunnerApi.Pipeline pipeline, SparkStreamingTranslationContext context) {\n+    QueryablePipeline p =\n+        QueryablePipeline.forTransforms(\n+            pipeline.getRootTransformIdsList(), pipeline.getComponents());\n+    for (PipelineNode.PTransformNode transformNode : p.getTopologicallyOrderedTransforms()) {\n+      urnToTransformTranslator\n+          .getOrDefault(\n+              transformNode.getTransform().getSpec().getUrn(),\n+              SparkStreamingPortablePipelineTranslator::urnNotFound)\n+          .translate(transformNode, pipeline, context);\n+    }\n+  }\n+\n+  private static void urnNotFound(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    throw new IllegalArgumentException(\n+        String.format(\n+            \"Transform %s has unknown URN %s\",\n+            transformNode.getId(), transformNode.getTransform().getSpec().getUrn()));\n+  }\n+\n+  private static void translateImpulse(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+\n+    TimestampedValue<byte[]> tsValue = TimestampedValue.atMinimumTimestamp(new byte[0]);\n+    Iterable<TimestampedValue<byte[]>> timestampedValues = Collections.singletonList(tsValue);\n+    Iterable<WindowedValue<byte[]>> windowedValues =\n+        StreamSupport.stream(timestampedValues.spliterator(), false)\n+            .map(\n+                timestampedValue ->\n+                    WindowedValue.of(\n+                        timestampedValue.getValue(),\n+                        timestampedValue.getTimestamp(),\n+                        GlobalWindow.INSTANCE,\n+                        PaneInfo.NO_FIRING))\n+            .collect(Collectors.toList());\n+\n+    ByteArrayCoder coder = ByteArrayCoder.of();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a"}, "originalPosition": 160}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkwMjcxNw==", "bodyText": "Nit: variable naming. An RDD containing an empty byte array is not empty itself.", "url": "https://github.com/apache/beam/pull/12157#discussion_r470902717", "createdAt": "2020-08-14T23:22:27Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkStreamingPortablePipelineTranslator.java", "diffHunk": "@@ -0,0 +1,393 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark.translation;\n+\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.createOutputMap;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowingStrategy;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.PTransformTranslation;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode.PTransformNode;\n+import org.apache.beam.runners.core.construction.graph.QueryablePipeline;\n+import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n+import org.apache.beam.runners.fnexecution.wire.WireCoders;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n+import org.apache.beam.runners.spark.coders.CoderHelpers;\n+import org.apache.beam.runners.spark.metrics.MetricsAccumulator;\n+import org.apache.beam.runners.spark.stateful.SparkGroupAlsoByWindowViaWindowSet;\n+import org.apache.beam.runners.spark.translation.streaming.UnboundedDataset;\n+import org.apache.beam.runners.spark.util.GlobalWatermarkHolder;\n+import org.apache.beam.runners.spark.util.SparkCompat;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.transforms.join.RawUnionValue;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.TimestampedValue;\n+import org.apache.beam.sdk.values.WindowingStrategy;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.BiMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Iterables;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaInputDStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Tuple2;\n+\n+/** Translates a bounded portable pipeline into a Spark job. */\n+public class SparkStreamingPortablePipelineTranslator\n+    implements SparkPortablePipelineTranslator<SparkStreamingTranslationContext> {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SparkStreamingPortablePipelineTranslator.class);\n+\n+  private final ImmutableMap<String, PTransformTranslator> urnToTransformTranslator;\n+\n+  interface PTransformTranslator {\n+\n+    /** Translates transformNode from Beam into the Spark context. */\n+    void translate(\n+        PTransformNode transformNode,\n+        RunnerApi.Pipeline pipeline,\n+        SparkStreamingTranslationContext context);\n+  }\n+\n+  @Override\n+  public Set<String> knownUrns() {\n+    return urnToTransformTranslator.keySet();\n+  }\n+\n+  public SparkStreamingPortablePipelineTranslator() {\n+    ImmutableMap.Builder<String, PTransformTranslator> translatorMap = ImmutableMap.builder();\n+    translatorMap.put(\n+        PTransformTranslation.IMPULSE_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateImpulse);\n+    translatorMap.put(\n+        PTransformTranslation.GROUP_BY_KEY_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateGroupByKey);\n+    translatorMap.put(\n+        ExecutableStage.URN, SparkStreamingPortablePipelineTranslator::translateExecutableStage);\n+    translatorMap.put(\n+        PTransformTranslation.FLATTEN_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateFlatten);\n+    translatorMap.put(\n+        PTransformTranslation.RESHUFFLE_URN,\n+        SparkStreamingPortablePipelineTranslator::translateReshuffle);\n+    this.urnToTransformTranslator = translatorMap.build();\n+  }\n+\n+  /** Translates pipeline from Beam into the Spark context. */\n+  @Override\n+  public void translate(\n+      final RunnerApi.Pipeline pipeline, SparkStreamingTranslationContext context) {\n+    QueryablePipeline p =\n+        QueryablePipeline.forTransforms(\n+            pipeline.getRootTransformIdsList(), pipeline.getComponents());\n+    for (PipelineNode.PTransformNode transformNode : p.getTopologicallyOrderedTransforms()) {\n+      urnToTransformTranslator\n+          .getOrDefault(\n+              transformNode.getTransform().getSpec().getUrn(),\n+              SparkStreamingPortablePipelineTranslator::urnNotFound)\n+          .translate(transformNode, pipeline, context);\n+    }\n+  }\n+\n+  private static void urnNotFound(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    throw new IllegalArgumentException(\n+        String.format(\n+            \"Transform %s has unknown URN %s\",\n+            transformNode.getId(), transformNode.getTransform().getSpec().getUrn()));\n+  }\n+\n+  private static void translateImpulse(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+\n+    TimestampedValue<byte[]> tsValue = TimestampedValue.atMinimumTimestamp(new byte[0]);\n+    Iterable<TimestampedValue<byte[]>> timestampedValues = Collections.singletonList(tsValue);\n+    Iterable<WindowedValue<byte[]>> windowedValues =\n+        StreamSupport.stream(timestampedValues.spliterator(), false)\n+            .map(\n+                timestampedValue ->\n+                    WindowedValue.of(\n+                        timestampedValue.getValue(),\n+                        timestampedValue.getTimestamp(),\n+                        GlobalWindow.INSTANCE,\n+                        PaneInfo.NO_FIRING))\n+            .collect(Collectors.toList());\n+\n+    ByteArrayCoder coder = ByteArrayCoder.of();\n+\n+    WindowedValue.FullWindowedValueCoder<byte[]> windowCoder =\n+        WindowedValue.FullWindowedValueCoder.of(coder, GlobalWindow.Coder.INSTANCE);\n+    JavaRDD<WindowedValue<byte[]>> emptyRDD =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a"}, "originalPosition": 164}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkwNDA0NA==", "bodyText": "Nit: this comment is obvious from the code.", "url": "https://github.com/apache/beam/pull/12157#discussion_r470904044", "createdAt": "2020-08-14T23:29:40Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkStreamingPortablePipelineTranslator.java", "diffHunk": "@@ -0,0 +1,393 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark.translation;\n+\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.createOutputMap;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowingStrategy;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.PTransformTranslation;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode.PTransformNode;\n+import org.apache.beam.runners.core.construction.graph.QueryablePipeline;\n+import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n+import org.apache.beam.runners.fnexecution.wire.WireCoders;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n+import org.apache.beam.runners.spark.coders.CoderHelpers;\n+import org.apache.beam.runners.spark.metrics.MetricsAccumulator;\n+import org.apache.beam.runners.spark.stateful.SparkGroupAlsoByWindowViaWindowSet;\n+import org.apache.beam.runners.spark.translation.streaming.UnboundedDataset;\n+import org.apache.beam.runners.spark.util.GlobalWatermarkHolder;\n+import org.apache.beam.runners.spark.util.SparkCompat;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.transforms.join.RawUnionValue;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.TimestampedValue;\n+import org.apache.beam.sdk.values.WindowingStrategy;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.BiMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Iterables;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaInputDStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Tuple2;\n+\n+/** Translates a bounded portable pipeline into a Spark job. */\n+public class SparkStreamingPortablePipelineTranslator\n+    implements SparkPortablePipelineTranslator<SparkStreamingTranslationContext> {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SparkStreamingPortablePipelineTranslator.class);\n+\n+  private final ImmutableMap<String, PTransformTranslator> urnToTransformTranslator;\n+\n+  interface PTransformTranslator {\n+\n+    /** Translates transformNode from Beam into the Spark context. */\n+    void translate(\n+        PTransformNode transformNode,\n+        RunnerApi.Pipeline pipeline,\n+        SparkStreamingTranslationContext context);\n+  }\n+\n+  @Override\n+  public Set<String> knownUrns() {\n+    return urnToTransformTranslator.keySet();\n+  }\n+\n+  public SparkStreamingPortablePipelineTranslator() {\n+    ImmutableMap.Builder<String, PTransformTranslator> translatorMap = ImmutableMap.builder();\n+    translatorMap.put(\n+        PTransformTranslation.IMPULSE_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateImpulse);\n+    translatorMap.put(\n+        PTransformTranslation.GROUP_BY_KEY_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateGroupByKey);\n+    translatorMap.put(\n+        ExecutableStage.URN, SparkStreamingPortablePipelineTranslator::translateExecutableStage);\n+    translatorMap.put(\n+        PTransformTranslation.FLATTEN_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateFlatten);\n+    translatorMap.put(\n+        PTransformTranslation.RESHUFFLE_URN,\n+        SparkStreamingPortablePipelineTranslator::translateReshuffle);\n+    this.urnToTransformTranslator = translatorMap.build();\n+  }\n+\n+  /** Translates pipeline from Beam into the Spark context. */\n+  @Override\n+  public void translate(\n+      final RunnerApi.Pipeline pipeline, SparkStreamingTranslationContext context) {\n+    QueryablePipeline p =\n+        QueryablePipeline.forTransforms(\n+            pipeline.getRootTransformIdsList(), pipeline.getComponents());\n+    for (PipelineNode.PTransformNode transformNode : p.getTopologicallyOrderedTransforms()) {\n+      urnToTransformTranslator\n+          .getOrDefault(\n+              transformNode.getTransform().getSpec().getUrn(),\n+              SparkStreamingPortablePipelineTranslator::urnNotFound)\n+          .translate(transformNode, pipeline, context);\n+    }\n+  }\n+\n+  private static void urnNotFound(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    throw new IllegalArgumentException(\n+        String.format(\n+            \"Transform %s has unknown URN %s\",\n+            transformNode.getId(), transformNode.getTransform().getSpec().getUrn()));\n+  }\n+\n+  private static void translateImpulse(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+\n+    TimestampedValue<byte[]> tsValue = TimestampedValue.atMinimumTimestamp(new byte[0]);\n+    Iterable<TimestampedValue<byte[]>> timestampedValues = Collections.singletonList(tsValue);\n+    Iterable<WindowedValue<byte[]>> windowedValues =\n+        StreamSupport.stream(timestampedValues.spliterator(), false)\n+            .map(\n+                timestampedValue ->\n+                    WindowedValue.of(\n+                        timestampedValue.getValue(),\n+                        timestampedValue.getTimestamp(),\n+                        GlobalWindow.INSTANCE,\n+                        PaneInfo.NO_FIRING))\n+            .collect(Collectors.toList());\n+\n+    ByteArrayCoder coder = ByteArrayCoder.of();\n+\n+    WindowedValue.FullWindowedValueCoder<byte[]> windowCoder =\n+        WindowedValue.FullWindowedValueCoder.of(coder, GlobalWindow.Coder.INSTANCE);\n+    JavaRDD<WindowedValue<byte[]>> emptyRDD =\n+        context\n+            .getSparkContext()\n+            .parallelize(CoderHelpers.toByteArrays(windowedValues, windowCoder))\n+            .map(CoderHelpers.fromByteFunction(windowCoder));\n+\n+    // create input DStream from RDD queue", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a"}, "originalPosition": 170}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkwNDIyNA==", "bodyText": "Nit: should be named rddQueue.", "url": "https://github.com/apache/beam/pull/12157#discussion_r470904224", "createdAt": "2020-08-14T23:30:30Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkStreamingPortablePipelineTranslator.java", "diffHunk": "@@ -0,0 +1,393 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark.translation;\n+\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.createOutputMap;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowingStrategy;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.PTransformTranslation;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode.PTransformNode;\n+import org.apache.beam.runners.core.construction.graph.QueryablePipeline;\n+import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n+import org.apache.beam.runners.fnexecution.wire.WireCoders;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n+import org.apache.beam.runners.spark.coders.CoderHelpers;\n+import org.apache.beam.runners.spark.metrics.MetricsAccumulator;\n+import org.apache.beam.runners.spark.stateful.SparkGroupAlsoByWindowViaWindowSet;\n+import org.apache.beam.runners.spark.translation.streaming.UnboundedDataset;\n+import org.apache.beam.runners.spark.util.GlobalWatermarkHolder;\n+import org.apache.beam.runners.spark.util.SparkCompat;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.transforms.join.RawUnionValue;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.TimestampedValue;\n+import org.apache.beam.sdk.values.WindowingStrategy;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.BiMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Iterables;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaInputDStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Tuple2;\n+\n+/** Translates a bounded portable pipeline into a Spark job. */\n+public class SparkStreamingPortablePipelineTranslator\n+    implements SparkPortablePipelineTranslator<SparkStreamingTranslationContext> {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SparkStreamingPortablePipelineTranslator.class);\n+\n+  private final ImmutableMap<String, PTransformTranslator> urnToTransformTranslator;\n+\n+  interface PTransformTranslator {\n+\n+    /** Translates transformNode from Beam into the Spark context. */\n+    void translate(\n+        PTransformNode transformNode,\n+        RunnerApi.Pipeline pipeline,\n+        SparkStreamingTranslationContext context);\n+  }\n+\n+  @Override\n+  public Set<String> knownUrns() {\n+    return urnToTransformTranslator.keySet();\n+  }\n+\n+  public SparkStreamingPortablePipelineTranslator() {\n+    ImmutableMap.Builder<String, PTransformTranslator> translatorMap = ImmutableMap.builder();\n+    translatorMap.put(\n+        PTransformTranslation.IMPULSE_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateImpulse);\n+    translatorMap.put(\n+        PTransformTranslation.GROUP_BY_KEY_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateGroupByKey);\n+    translatorMap.put(\n+        ExecutableStage.URN, SparkStreamingPortablePipelineTranslator::translateExecutableStage);\n+    translatorMap.put(\n+        PTransformTranslation.FLATTEN_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateFlatten);\n+    translatorMap.put(\n+        PTransformTranslation.RESHUFFLE_URN,\n+        SparkStreamingPortablePipelineTranslator::translateReshuffle);\n+    this.urnToTransformTranslator = translatorMap.build();\n+  }\n+\n+  /** Translates pipeline from Beam into the Spark context. */\n+  @Override\n+  public void translate(\n+      final RunnerApi.Pipeline pipeline, SparkStreamingTranslationContext context) {\n+    QueryablePipeline p =\n+        QueryablePipeline.forTransforms(\n+            pipeline.getRootTransformIdsList(), pipeline.getComponents());\n+    for (PipelineNode.PTransformNode transformNode : p.getTopologicallyOrderedTransforms()) {\n+      urnToTransformTranslator\n+          .getOrDefault(\n+              transformNode.getTransform().getSpec().getUrn(),\n+              SparkStreamingPortablePipelineTranslator::urnNotFound)\n+          .translate(transformNode, pipeline, context);\n+    }\n+  }\n+\n+  private static void urnNotFound(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    throw new IllegalArgumentException(\n+        String.format(\n+            \"Transform %s has unknown URN %s\",\n+            transformNode.getId(), transformNode.getTransform().getSpec().getUrn()));\n+  }\n+\n+  private static void translateImpulse(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+\n+    TimestampedValue<byte[]> tsValue = TimestampedValue.atMinimumTimestamp(new byte[0]);\n+    Iterable<TimestampedValue<byte[]>> timestampedValues = Collections.singletonList(tsValue);\n+    Iterable<WindowedValue<byte[]>> windowedValues =\n+        StreamSupport.stream(timestampedValues.spliterator(), false)\n+            .map(\n+                timestampedValue ->\n+                    WindowedValue.of(\n+                        timestampedValue.getValue(),\n+                        timestampedValue.getTimestamp(),\n+                        GlobalWindow.INSTANCE,\n+                        PaneInfo.NO_FIRING))\n+            .collect(Collectors.toList());\n+\n+    ByteArrayCoder coder = ByteArrayCoder.of();\n+\n+    WindowedValue.FullWindowedValueCoder<byte[]> windowCoder =\n+        WindowedValue.FullWindowedValueCoder.of(coder, GlobalWindow.Coder.INSTANCE);\n+    JavaRDD<WindowedValue<byte[]>> emptyRDD =\n+        context\n+            .getSparkContext()\n+            .parallelize(CoderHelpers.toByteArrays(windowedValues, windowCoder))\n+            .map(CoderHelpers.fromByteFunction(windowCoder));\n+\n+    // create input DStream from RDD queue\n+    Queue<JavaRDD<WindowedValue<byte[]>>> queueRDD = new LinkedBlockingQueue<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a"}, "originalPosition": 171}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkwNTI1OQ==", "bodyText": "Nit: it isn't immediately clear what is true here (without the help of an IDE). Use an inline comment:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                    context.getStreamingContext().queueStream(queueRDD, true);\n          \n          \n            \n                    context.getStreamingContext().queueStream(queueRDD, true /* oneAtATime */);", "url": "https://github.com/apache/beam/pull/12157#discussion_r470905259", "createdAt": "2020-08-14T23:36:12Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkStreamingPortablePipelineTranslator.java", "diffHunk": "@@ -0,0 +1,393 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark.translation;\n+\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.createOutputMap;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowingStrategy;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.PTransformTranslation;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode.PTransformNode;\n+import org.apache.beam.runners.core.construction.graph.QueryablePipeline;\n+import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n+import org.apache.beam.runners.fnexecution.wire.WireCoders;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n+import org.apache.beam.runners.spark.coders.CoderHelpers;\n+import org.apache.beam.runners.spark.metrics.MetricsAccumulator;\n+import org.apache.beam.runners.spark.stateful.SparkGroupAlsoByWindowViaWindowSet;\n+import org.apache.beam.runners.spark.translation.streaming.UnboundedDataset;\n+import org.apache.beam.runners.spark.util.GlobalWatermarkHolder;\n+import org.apache.beam.runners.spark.util.SparkCompat;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.transforms.join.RawUnionValue;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.TimestampedValue;\n+import org.apache.beam.sdk.values.WindowingStrategy;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.BiMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Iterables;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaInputDStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Tuple2;\n+\n+/** Translates a bounded portable pipeline into a Spark job. */\n+public class SparkStreamingPortablePipelineTranslator\n+    implements SparkPortablePipelineTranslator<SparkStreamingTranslationContext> {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SparkStreamingPortablePipelineTranslator.class);\n+\n+  private final ImmutableMap<String, PTransformTranslator> urnToTransformTranslator;\n+\n+  interface PTransformTranslator {\n+\n+    /** Translates transformNode from Beam into the Spark context. */\n+    void translate(\n+        PTransformNode transformNode,\n+        RunnerApi.Pipeline pipeline,\n+        SparkStreamingTranslationContext context);\n+  }\n+\n+  @Override\n+  public Set<String> knownUrns() {\n+    return urnToTransformTranslator.keySet();\n+  }\n+\n+  public SparkStreamingPortablePipelineTranslator() {\n+    ImmutableMap.Builder<String, PTransformTranslator> translatorMap = ImmutableMap.builder();\n+    translatorMap.put(\n+        PTransformTranslation.IMPULSE_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateImpulse);\n+    translatorMap.put(\n+        PTransformTranslation.GROUP_BY_KEY_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateGroupByKey);\n+    translatorMap.put(\n+        ExecutableStage.URN, SparkStreamingPortablePipelineTranslator::translateExecutableStage);\n+    translatorMap.put(\n+        PTransformTranslation.FLATTEN_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateFlatten);\n+    translatorMap.put(\n+        PTransformTranslation.RESHUFFLE_URN,\n+        SparkStreamingPortablePipelineTranslator::translateReshuffle);\n+    this.urnToTransformTranslator = translatorMap.build();\n+  }\n+\n+  /** Translates pipeline from Beam into the Spark context. */\n+  @Override\n+  public void translate(\n+      final RunnerApi.Pipeline pipeline, SparkStreamingTranslationContext context) {\n+    QueryablePipeline p =\n+        QueryablePipeline.forTransforms(\n+            pipeline.getRootTransformIdsList(), pipeline.getComponents());\n+    for (PipelineNode.PTransformNode transformNode : p.getTopologicallyOrderedTransforms()) {\n+      urnToTransformTranslator\n+          .getOrDefault(\n+              transformNode.getTransform().getSpec().getUrn(),\n+              SparkStreamingPortablePipelineTranslator::urnNotFound)\n+          .translate(transformNode, pipeline, context);\n+    }\n+  }\n+\n+  private static void urnNotFound(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    throw new IllegalArgumentException(\n+        String.format(\n+            \"Transform %s has unknown URN %s\",\n+            transformNode.getId(), transformNode.getTransform().getSpec().getUrn()));\n+  }\n+\n+  private static void translateImpulse(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+\n+    TimestampedValue<byte[]> tsValue = TimestampedValue.atMinimumTimestamp(new byte[0]);\n+    Iterable<TimestampedValue<byte[]>> timestampedValues = Collections.singletonList(tsValue);\n+    Iterable<WindowedValue<byte[]>> windowedValues =\n+        StreamSupport.stream(timestampedValues.spliterator(), false)\n+            .map(\n+                timestampedValue ->\n+                    WindowedValue.of(\n+                        timestampedValue.getValue(),\n+                        timestampedValue.getTimestamp(),\n+                        GlobalWindow.INSTANCE,\n+                        PaneInfo.NO_FIRING))\n+            .collect(Collectors.toList());\n+\n+    ByteArrayCoder coder = ByteArrayCoder.of();\n+\n+    WindowedValue.FullWindowedValueCoder<byte[]> windowCoder =\n+        WindowedValue.FullWindowedValueCoder.of(coder, GlobalWindow.Coder.INSTANCE);\n+    JavaRDD<WindowedValue<byte[]>> emptyRDD =\n+        context\n+            .getSparkContext()\n+            .parallelize(CoderHelpers.toByteArrays(windowedValues, windowCoder))\n+            .map(CoderHelpers.fromByteFunction(windowCoder));\n+\n+    // create input DStream from RDD queue\n+    Queue<JavaRDD<WindowedValue<byte[]>>> queueRDD = new LinkedBlockingQueue<>();\n+    queueRDD.offer(emptyRDD);\n+    JavaInputDStream<WindowedValue<byte[]>> emptyStream =\n+        context.getStreamingContext().queueStream(queueRDD, true);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a"}, "originalPosition": 174}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkwNTg5Mw==", "bodyText": "This block needs a comment (though maybe this should wait until we figure out what is going on in failing tests).", "url": "https://github.com/apache/beam/pull/12157#discussion_r470905893", "createdAt": "2020-08-14T23:39:27Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkStreamingPortablePipelineTranslator.java", "diffHunk": "@@ -0,0 +1,393 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark.translation;\n+\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.createOutputMap;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowingStrategy;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.PTransformTranslation;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode.PTransformNode;\n+import org.apache.beam.runners.core.construction.graph.QueryablePipeline;\n+import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n+import org.apache.beam.runners.fnexecution.wire.WireCoders;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n+import org.apache.beam.runners.spark.coders.CoderHelpers;\n+import org.apache.beam.runners.spark.metrics.MetricsAccumulator;\n+import org.apache.beam.runners.spark.stateful.SparkGroupAlsoByWindowViaWindowSet;\n+import org.apache.beam.runners.spark.translation.streaming.UnboundedDataset;\n+import org.apache.beam.runners.spark.util.GlobalWatermarkHolder;\n+import org.apache.beam.runners.spark.util.SparkCompat;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.transforms.join.RawUnionValue;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.TimestampedValue;\n+import org.apache.beam.sdk.values.WindowingStrategy;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.BiMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Iterables;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaInputDStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Tuple2;\n+\n+/** Translates a bounded portable pipeline into a Spark job. */\n+public class SparkStreamingPortablePipelineTranslator\n+    implements SparkPortablePipelineTranslator<SparkStreamingTranslationContext> {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SparkStreamingPortablePipelineTranslator.class);\n+\n+  private final ImmutableMap<String, PTransformTranslator> urnToTransformTranslator;\n+\n+  interface PTransformTranslator {\n+\n+    /** Translates transformNode from Beam into the Spark context. */\n+    void translate(\n+        PTransformNode transformNode,\n+        RunnerApi.Pipeline pipeline,\n+        SparkStreamingTranslationContext context);\n+  }\n+\n+  @Override\n+  public Set<String> knownUrns() {\n+    return urnToTransformTranslator.keySet();\n+  }\n+\n+  public SparkStreamingPortablePipelineTranslator() {\n+    ImmutableMap.Builder<String, PTransformTranslator> translatorMap = ImmutableMap.builder();\n+    translatorMap.put(\n+        PTransformTranslation.IMPULSE_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateImpulse);\n+    translatorMap.put(\n+        PTransformTranslation.GROUP_BY_KEY_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateGroupByKey);\n+    translatorMap.put(\n+        ExecutableStage.URN, SparkStreamingPortablePipelineTranslator::translateExecutableStage);\n+    translatorMap.put(\n+        PTransformTranslation.FLATTEN_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateFlatten);\n+    translatorMap.put(\n+        PTransformTranslation.RESHUFFLE_URN,\n+        SparkStreamingPortablePipelineTranslator::translateReshuffle);\n+    this.urnToTransformTranslator = translatorMap.build();\n+  }\n+\n+  /** Translates pipeline from Beam into the Spark context. */\n+  @Override\n+  public void translate(\n+      final RunnerApi.Pipeline pipeline, SparkStreamingTranslationContext context) {\n+    QueryablePipeline p =\n+        QueryablePipeline.forTransforms(\n+            pipeline.getRootTransformIdsList(), pipeline.getComponents());\n+    for (PipelineNode.PTransformNode transformNode : p.getTopologicallyOrderedTransforms()) {\n+      urnToTransformTranslator\n+          .getOrDefault(\n+              transformNode.getTransform().getSpec().getUrn(),\n+              SparkStreamingPortablePipelineTranslator::urnNotFound)\n+          .translate(transformNode, pipeline, context);\n+    }\n+  }\n+\n+  private static void urnNotFound(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    throw new IllegalArgumentException(\n+        String.format(\n+            \"Transform %s has unknown URN %s\",\n+            transformNode.getId(), transformNode.getTransform().getSpec().getUrn()));\n+  }\n+\n+  private static void translateImpulse(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+\n+    TimestampedValue<byte[]> tsValue = TimestampedValue.atMinimumTimestamp(new byte[0]);\n+    Iterable<TimestampedValue<byte[]>> timestampedValues = Collections.singletonList(tsValue);\n+    Iterable<WindowedValue<byte[]>> windowedValues =\n+        StreamSupport.stream(timestampedValues.spliterator(), false)\n+            .map(\n+                timestampedValue ->\n+                    WindowedValue.of(\n+                        timestampedValue.getValue(),\n+                        timestampedValue.getTimestamp(),\n+                        GlobalWindow.INSTANCE,\n+                        PaneInfo.NO_FIRING))\n+            .collect(Collectors.toList());\n+\n+    ByteArrayCoder coder = ByteArrayCoder.of();\n+\n+    WindowedValue.FullWindowedValueCoder<byte[]> windowCoder =\n+        WindowedValue.FullWindowedValueCoder.of(coder, GlobalWindow.Coder.INSTANCE);\n+    JavaRDD<WindowedValue<byte[]>> emptyRDD =\n+        context\n+            .getSparkContext()\n+            .parallelize(CoderHelpers.toByteArrays(windowedValues, windowCoder))\n+            .map(CoderHelpers.fromByteFunction(windowCoder));\n+\n+    // create input DStream from RDD queue\n+    Queue<JavaRDD<WindowedValue<byte[]>>> queueRDD = new LinkedBlockingQueue<>();\n+    queueRDD.offer(emptyRDD);\n+    JavaInputDStream<WindowedValue<byte[]>> emptyStream =\n+        context.getStreamingContext().queueStream(queueRDD, true);\n+\n+    UnboundedDataset<byte[]> output =\n+        new UnboundedDataset<>(\n+            emptyStream, Collections.singletonList(emptyStream.inputDStream().id()));\n+\n+    GlobalWatermarkHolder.SparkWatermarks sparkWatermark =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a"}, "originalPosition": 180}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkwNjkxNA==", "bodyText": "Create a JIRA and point to it here:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                // TODO: handle side inputs?\n          \n          \n            \n                // TODO(BEAM-XXXXX): handle side inputs.", "url": "https://github.com/apache/beam/pull/12157#discussion_r470906914", "createdAt": "2020-08-14T23:45:07Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkStreamingPortablePipelineTranslator.java", "diffHunk": "@@ -0,0 +1,393 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark.translation;\n+\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.createOutputMap;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowingStrategy;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.PTransformTranslation;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode.PTransformNode;\n+import org.apache.beam.runners.core.construction.graph.QueryablePipeline;\n+import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n+import org.apache.beam.runners.fnexecution.wire.WireCoders;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n+import org.apache.beam.runners.spark.coders.CoderHelpers;\n+import org.apache.beam.runners.spark.metrics.MetricsAccumulator;\n+import org.apache.beam.runners.spark.stateful.SparkGroupAlsoByWindowViaWindowSet;\n+import org.apache.beam.runners.spark.translation.streaming.UnboundedDataset;\n+import org.apache.beam.runners.spark.util.GlobalWatermarkHolder;\n+import org.apache.beam.runners.spark.util.SparkCompat;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.transforms.join.RawUnionValue;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.TimestampedValue;\n+import org.apache.beam.sdk.values.WindowingStrategy;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.BiMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Iterables;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaInputDStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Tuple2;\n+\n+/** Translates a bounded portable pipeline into a Spark job. */\n+public class SparkStreamingPortablePipelineTranslator\n+    implements SparkPortablePipelineTranslator<SparkStreamingTranslationContext> {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SparkStreamingPortablePipelineTranslator.class);\n+\n+  private final ImmutableMap<String, PTransformTranslator> urnToTransformTranslator;\n+\n+  interface PTransformTranslator {\n+\n+    /** Translates transformNode from Beam into the Spark context. */\n+    void translate(\n+        PTransformNode transformNode,\n+        RunnerApi.Pipeline pipeline,\n+        SparkStreamingTranslationContext context);\n+  }\n+\n+  @Override\n+  public Set<String> knownUrns() {\n+    return urnToTransformTranslator.keySet();\n+  }\n+\n+  public SparkStreamingPortablePipelineTranslator() {\n+    ImmutableMap.Builder<String, PTransformTranslator> translatorMap = ImmutableMap.builder();\n+    translatorMap.put(\n+        PTransformTranslation.IMPULSE_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateImpulse);\n+    translatorMap.put(\n+        PTransformTranslation.GROUP_BY_KEY_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateGroupByKey);\n+    translatorMap.put(\n+        ExecutableStage.URN, SparkStreamingPortablePipelineTranslator::translateExecutableStage);\n+    translatorMap.put(\n+        PTransformTranslation.FLATTEN_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateFlatten);\n+    translatorMap.put(\n+        PTransformTranslation.RESHUFFLE_URN,\n+        SparkStreamingPortablePipelineTranslator::translateReshuffle);\n+    this.urnToTransformTranslator = translatorMap.build();\n+  }\n+\n+  /** Translates pipeline from Beam into the Spark context. */\n+  @Override\n+  public void translate(\n+      final RunnerApi.Pipeline pipeline, SparkStreamingTranslationContext context) {\n+    QueryablePipeline p =\n+        QueryablePipeline.forTransforms(\n+            pipeline.getRootTransformIdsList(), pipeline.getComponents());\n+    for (PipelineNode.PTransformNode transformNode : p.getTopologicallyOrderedTransforms()) {\n+      urnToTransformTranslator\n+          .getOrDefault(\n+              transformNode.getTransform().getSpec().getUrn(),\n+              SparkStreamingPortablePipelineTranslator::urnNotFound)\n+          .translate(transformNode, pipeline, context);\n+    }\n+  }\n+\n+  private static void urnNotFound(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    throw new IllegalArgumentException(\n+        String.format(\n+            \"Transform %s has unknown URN %s\",\n+            transformNode.getId(), transformNode.getTransform().getSpec().getUrn()));\n+  }\n+\n+  private static void translateImpulse(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+\n+    TimestampedValue<byte[]> tsValue = TimestampedValue.atMinimumTimestamp(new byte[0]);\n+    Iterable<TimestampedValue<byte[]>> timestampedValues = Collections.singletonList(tsValue);\n+    Iterable<WindowedValue<byte[]>> windowedValues =\n+        StreamSupport.stream(timestampedValues.spliterator(), false)\n+            .map(\n+                timestampedValue ->\n+                    WindowedValue.of(\n+                        timestampedValue.getValue(),\n+                        timestampedValue.getTimestamp(),\n+                        GlobalWindow.INSTANCE,\n+                        PaneInfo.NO_FIRING))\n+            .collect(Collectors.toList());\n+\n+    ByteArrayCoder coder = ByteArrayCoder.of();\n+\n+    WindowedValue.FullWindowedValueCoder<byte[]> windowCoder =\n+        WindowedValue.FullWindowedValueCoder.of(coder, GlobalWindow.Coder.INSTANCE);\n+    JavaRDD<WindowedValue<byte[]>> emptyRDD =\n+        context\n+            .getSparkContext()\n+            .parallelize(CoderHelpers.toByteArrays(windowedValues, windowCoder))\n+            .map(CoderHelpers.fromByteFunction(windowCoder));\n+\n+    // create input DStream from RDD queue\n+    Queue<JavaRDD<WindowedValue<byte[]>>> queueRDD = new LinkedBlockingQueue<>();\n+    queueRDD.offer(emptyRDD);\n+    JavaInputDStream<WindowedValue<byte[]>> emptyStream =\n+        context.getStreamingContext().queueStream(queueRDD, true);\n+\n+    UnboundedDataset<byte[]> output =\n+        new UnboundedDataset<>(\n+            emptyStream, Collections.singletonList(emptyStream.inputDStream().id()));\n+\n+    GlobalWatermarkHolder.SparkWatermarks sparkWatermark =\n+        new GlobalWatermarkHolder.SparkWatermarks(\n+            GlobalWindow.INSTANCE.maxTimestamp(),\n+            BoundedWindow.TIMESTAMP_MAX_VALUE,\n+            context.getFirstTimestamp());\n+    GlobalWatermarkHolder.add(output.getStreamSources().get(0), sparkWatermark);\n+\n+    context.pushDataset(getOutputId(transformNode), output);\n+  }\n+\n+  private static <K, V> void translateGroupByKey(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    RunnerApi.Components components = pipeline.getComponents();\n+    String inputId = getInputId(transformNode);\n+    UnboundedDataset<KV<K, V>> inputDataset =\n+        (UnboundedDataset<KV<K, V>>) context.popDataset(inputId);\n+    List<Integer> streamSources = inputDataset.getStreamSources();\n+    JavaDStream<WindowedValue<KV<K, V>>> dStream = inputDataset.getDStream();\n+    WindowedValue.WindowedValueCoder<KV<K, V>> inputCoder =\n+        getWindowedValueCoder(inputId, components);\n+    KvCoder<K, V> inputKvCoder = (KvCoder<K, V>) inputCoder.getValueCoder();\n+    Coder<K> inputKeyCoder = inputKvCoder.getKeyCoder();\n+    Coder<V> inputValueCoder = inputKvCoder.getValueCoder();\n+    final WindowingStrategy windowingStrategy = getWindowingStrategy(inputId, components);\n+    final WindowFn<Object, BoundedWindow> windowFn = windowingStrategy.getWindowFn();\n+    final WindowedValue.WindowedValueCoder<V> wvCoder =\n+        WindowedValue.FullWindowedValueCoder.of(inputValueCoder, windowFn.windowCoder());\n+\n+    JavaDStream<WindowedValue<KV<K, Iterable<V>>>> outStream =\n+        SparkGroupAlsoByWindowViaWindowSet.groupByKeyAndWindow(\n+            dStream,\n+            inputKeyCoder,\n+            wvCoder,\n+            windowingStrategy,\n+            context.getSerializableOptions(),\n+            streamSources,\n+            transformNode.getId());\n+\n+    context.pushDataset(\n+        getOutputId(transformNode), new UnboundedDataset<>(outStream, streamSources));\n+  }\n+\n+  private static <InputT, OutputT, SideInputT> void translateExecutableStage(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    RunnerApi.ExecutableStagePayload stagePayload;\n+    try {\n+      stagePayload =\n+          RunnerApi.ExecutableStagePayload.parseFrom(\n+              transformNode.getTransform().getSpec().getPayload());\n+    } catch (IOException e) {\n+      throw new RuntimeException(e);\n+    }\n+\n+    String inputPCollectionId = stagePayload.getInput();\n+    UnboundedDataset<InputT> inputDataset =\n+        (UnboundedDataset<InputT>) context.popDataset(inputPCollectionId);\n+    List<Integer> streamSources = inputDataset.getStreamSources();\n+    JavaDStream<WindowedValue<InputT>> inputDStream = inputDataset.getDStream();\n+    Map<String, String> outputs = transformNode.getTransform().getOutputsMap();\n+    BiMap<String, Integer> outputMap = createOutputMap(outputs.values());\n+\n+    RunnerApi.Components components = pipeline.getComponents();\n+    Coder windowCoder =\n+        getWindowingStrategy(inputPCollectionId, components).getWindowFn().windowCoder();\n+\n+    // TODO: handle side inputs?", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a"}, "originalPosition": 249}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkwNzUyNg==", "bodyText": "Nit:\n\n  \n    \n      \n        Suggested change\n        \n          \n    \n\n        \n      \n    \n    \n      \n          \n            \n                // pop dataset to mark RDD as used\n          \n          \n            \n                // pop dataset to mark DStream as used", "url": "https://github.com/apache/beam/pull/12157#discussion_r470907526", "createdAt": "2020-08-14T23:47:18Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkStreamingPortablePipelineTranslator.java", "diffHunk": "@@ -0,0 +1,393 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark.translation;\n+\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.createOutputMap;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowingStrategy;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.PTransformTranslation;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode.PTransformNode;\n+import org.apache.beam.runners.core.construction.graph.QueryablePipeline;\n+import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n+import org.apache.beam.runners.fnexecution.wire.WireCoders;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n+import org.apache.beam.runners.spark.coders.CoderHelpers;\n+import org.apache.beam.runners.spark.metrics.MetricsAccumulator;\n+import org.apache.beam.runners.spark.stateful.SparkGroupAlsoByWindowViaWindowSet;\n+import org.apache.beam.runners.spark.translation.streaming.UnboundedDataset;\n+import org.apache.beam.runners.spark.util.GlobalWatermarkHolder;\n+import org.apache.beam.runners.spark.util.SparkCompat;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.transforms.join.RawUnionValue;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.TimestampedValue;\n+import org.apache.beam.sdk.values.WindowingStrategy;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.BiMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Iterables;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaInputDStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Tuple2;\n+\n+/** Translates a bounded portable pipeline into a Spark job. */\n+public class SparkStreamingPortablePipelineTranslator\n+    implements SparkPortablePipelineTranslator<SparkStreamingTranslationContext> {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SparkStreamingPortablePipelineTranslator.class);\n+\n+  private final ImmutableMap<String, PTransformTranslator> urnToTransformTranslator;\n+\n+  interface PTransformTranslator {\n+\n+    /** Translates transformNode from Beam into the Spark context. */\n+    void translate(\n+        PTransformNode transformNode,\n+        RunnerApi.Pipeline pipeline,\n+        SparkStreamingTranslationContext context);\n+  }\n+\n+  @Override\n+  public Set<String> knownUrns() {\n+    return urnToTransformTranslator.keySet();\n+  }\n+\n+  public SparkStreamingPortablePipelineTranslator() {\n+    ImmutableMap.Builder<String, PTransformTranslator> translatorMap = ImmutableMap.builder();\n+    translatorMap.put(\n+        PTransformTranslation.IMPULSE_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateImpulse);\n+    translatorMap.put(\n+        PTransformTranslation.GROUP_BY_KEY_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateGroupByKey);\n+    translatorMap.put(\n+        ExecutableStage.URN, SparkStreamingPortablePipelineTranslator::translateExecutableStage);\n+    translatorMap.put(\n+        PTransformTranslation.FLATTEN_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateFlatten);\n+    translatorMap.put(\n+        PTransformTranslation.RESHUFFLE_URN,\n+        SparkStreamingPortablePipelineTranslator::translateReshuffle);\n+    this.urnToTransformTranslator = translatorMap.build();\n+  }\n+\n+  /** Translates pipeline from Beam into the Spark context. */\n+  @Override\n+  public void translate(\n+      final RunnerApi.Pipeline pipeline, SparkStreamingTranslationContext context) {\n+    QueryablePipeline p =\n+        QueryablePipeline.forTransforms(\n+            pipeline.getRootTransformIdsList(), pipeline.getComponents());\n+    for (PipelineNode.PTransformNode transformNode : p.getTopologicallyOrderedTransforms()) {\n+      urnToTransformTranslator\n+          .getOrDefault(\n+              transformNode.getTransform().getSpec().getUrn(),\n+              SparkStreamingPortablePipelineTranslator::urnNotFound)\n+          .translate(transformNode, pipeline, context);\n+    }\n+  }\n+\n+  private static void urnNotFound(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    throw new IllegalArgumentException(\n+        String.format(\n+            \"Transform %s has unknown URN %s\",\n+            transformNode.getId(), transformNode.getTransform().getSpec().getUrn()));\n+  }\n+\n+  private static void translateImpulse(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+\n+    TimestampedValue<byte[]> tsValue = TimestampedValue.atMinimumTimestamp(new byte[0]);\n+    Iterable<TimestampedValue<byte[]>> timestampedValues = Collections.singletonList(tsValue);\n+    Iterable<WindowedValue<byte[]>> windowedValues =\n+        StreamSupport.stream(timestampedValues.spliterator(), false)\n+            .map(\n+                timestampedValue ->\n+                    WindowedValue.of(\n+                        timestampedValue.getValue(),\n+                        timestampedValue.getTimestamp(),\n+                        GlobalWindow.INSTANCE,\n+                        PaneInfo.NO_FIRING))\n+            .collect(Collectors.toList());\n+\n+    ByteArrayCoder coder = ByteArrayCoder.of();\n+\n+    WindowedValue.FullWindowedValueCoder<byte[]> windowCoder =\n+        WindowedValue.FullWindowedValueCoder.of(coder, GlobalWindow.Coder.INSTANCE);\n+    JavaRDD<WindowedValue<byte[]>> emptyRDD =\n+        context\n+            .getSparkContext()\n+            .parallelize(CoderHelpers.toByteArrays(windowedValues, windowCoder))\n+            .map(CoderHelpers.fromByteFunction(windowCoder));\n+\n+    // create input DStream from RDD queue\n+    Queue<JavaRDD<WindowedValue<byte[]>>> queueRDD = new LinkedBlockingQueue<>();\n+    queueRDD.offer(emptyRDD);\n+    JavaInputDStream<WindowedValue<byte[]>> emptyStream =\n+        context.getStreamingContext().queueStream(queueRDD, true);\n+\n+    UnboundedDataset<byte[]> output =\n+        new UnboundedDataset<>(\n+            emptyStream, Collections.singletonList(emptyStream.inputDStream().id()));\n+\n+    GlobalWatermarkHolder.SparkWatermarks sparkWatermark =\n+        new GlobalWatermarkHolder.SparkWatermarks(\n+            GlobalWindow.INSTANCE.maxTimestamp(),\n+            BoundedWindow.TIMESTAMP_MAX_VALUE,\n+            context.getFirstTimestamp());\n+    GlobalWatermarkHolder.add(output.getStreamSources().get(0), sparkWatermark);\n+\n+    context.pushDataset(getOutputId(transformNode), output);\n+  }\n+\n+  private static <K, V> void translateGroupByKey(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    RunnerApi.Components components = pipeline.getComponents();\n+    String inputId = getInputId(transformNode);\n+    UnboundedDataset<KV<K, V>> inputDataset =\n+        (UnboundedDataset<KV<K, V>>) context.popDataset(inputId);\n+    List<Integer> streamSources = inputDataset.getStreamSources();\n+    JavaDStream<WindowedValue<KV<K, V>>> dStream = inputDataset.getDStream();\n+    WindowedValue.WindowedValueCoder<KV<K, V>> inputCoder =\n+        getWindowedValueCoder(inputId, components);\n+    KvCoder<K, V> inputKvCoder = (KvCoder<K, V>) inputCoder.getValueCoder();\n+    Coder<K> inputKeyCoder = inputKvCoder.getKeyCoder();\n+    Coder<V> inputValueCoder = inputKvCoder.getValueCoder();\n+    final WindowingStrategy windowingStrategy = getWindowingStrategy(inputId, components);\n+    final WindowFn<Object, BoundedWindow> windowFn = windowingStrategy.getWindowFn();\n+    final WindowedValue.WindowedValueCoder<V> wvCoder =\n+        WindowedValue.FullWindowedValueCoder.of(inputValueCoder, windowFn.windowCoder());\n+\n+    JavaDStream<WindowedValue<KV<K, Iterable<V>>>> outStream =\n+        SparkGroupAlsoByWindowViaWindowSet.groupByKeyAndWindow(\n+            dStream,\n+            inputKeyCoder,\n+            wvCoder,\n+            windowingStrategy,\n+            context.getSerializableOptions(),\n+            streamSources,\n+            transformNode.getId());\n+\n+    context.pushDataset(\n+        getOutputId(transformNode), new UnboundedDataset<>(outStream, streamSources));\n+  }\n+\n+  private static <InputT, OutputT, SideInputT> void translateExecutableStage(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    RunnerApi.ExecutableStagePayload stagePayload;\n+    try {\n+      stagePayload =\n+          RunnerApi.ExecutableStagePayload.parseFrom(\n+              transformNode.getTransform().getSpec().getPayload());\n+    } catch (IOException e) {\n+      throw new RuntimeException(e);\n+    }\n+\n+    String inputPCollectionId = stagePayload.getInput();\n+    UnboundedDataset<InputT> inputDataset =\n+        (UnboundedDataset<InputT>) context.popDataset(inputPCollectionId);\n+    List<Integer> streamSources = inputDataset.getStreamSources();\n+    JavaDStream<WindowedValue<InputT>> inputDStream = inputDataset.getDStream();\n+    Map<String, String> outputs = transformNode.getTransform().getOutputsMap();\n+    BiMap<String, Integer> outputMap = createOutputMap(outputs.values());\n+\n+    RunnerApi.Components components = pipeline.getComponents();\n+    Coder windowCoder =\n+        getWindowingStrategy(inputPCollectionId, components).getWindowFn().windowCoder();\n+\n+    // TODO: handle side inputs?\n+    ImmutableMap<\n+            String, Tuple2<Broadcast<List<byte[]>>, WindowedValue.WindowedValueCoder<SideInputT>>>\n+        broadcastVariables = ImmutableMap.copyOf(new HashMap<>());\n+\n+    SparkExecutableStageFunction<InputT, SideInputT> function =\n+        new SparkExecutableStageFunction<>(\n+            stagePayload,\n+            context.jobInfo,\n+            outputMap,\n+            SparkExecutableStageContextFactory.getInstance(),\n+            broadcastVariables,\n+            MetricsAccumulator.getInstance(),\n+            windowCoder);\n+    JavaDStream<RawUnionValue> staged = inputDStream.mapPartitions(function);\n+\n+    String intermediateId = getExecutableStageIntermediateId(transformNode);\n+    context.pushDataset(\n+        intermediateId,\n+        new Dataset() {\n+          @Override\n+          public void cache(String storageLevel, Coder<?> coder) {\n+            StorageLevel level = StorageLevel.fromString(storageLevel);\n+            staged.persist(level);\n+          }\n+\n+          @Override\n+          public void action() {\n+            // Empty function to force computation of RDD.\n+            staged.foreachRDD(TranslationUtils.emptyVoidFunction());\n+          }\n+\n+          @Override\n+          public void setName(String name) {\n+            // ignore\n+          }\n+        });\n+    // pop dataset to mark RDD as used", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a"}, "originalPosition": 286}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkxMTM3Mg==", "bodyText": "We should move these common methods into a shared class like PipelineTranslatorUtils.", "url": "https://github.com/apache/beam/pull/12157#discussion_r470911372", "createdAt": "2020-08-15T00:09:47Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkStreamingPortablePipelineTranslator.java", "diffHunk": "@@ -0,0 +1,393 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark.translation;\n+\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.createOutputMap;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowingStrategy;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.PTransformTranslation;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode.PTransformNode;\n+import org.apache.beam.runners.core.construction.graph.QueryablePipeline;\n+import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n+import org.apache.beam.runners.fnexecution.wire.WireCoders;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n+import org.apache.beam.runners.spark.coders.CoderHelpers;\n+import org.apache.beam.runners.spark.metrics.MetricsAccumulator;\n+import org.apache.beam.runners.spark.stateful.SparkGroupAlsoByWindowViaWindowSet;\n+import org.apache.beam.runners.spark.translation.streaming.UnboundedDataset;\n+import org.apache.beam.runners.spark.util.GlobalWatermarkHolder;\n+import org.apache.beam.runners.spark.util.SparkCompat;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.transforms.join.RawUnionValue;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.TimestampedValue;\n+import org.apache.beam.sdk.values.WindowingStrategy;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.BiMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.Iterables;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaInputDStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Tuple2;\n+\n+/** Translates a bounded portable pipeline into a Spark job. */\n+public class SparkStreamingPortablePipelineTranslator\n+    implements SparkPortablePipelineTranslator<SparkStreamingTranslationContext> {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SparkStreamingPortablePipelineTranslator.class);\n+\n+  private final ImmutableMap<String, PTransformTranslator> urnToTransformTranslator;\n+\n+  interface PTransformTranslator {\n+\n+    /** Translates transformNode from Beam into the Spark context. */\n+    void translate(\n+        PTransformNode transformNode,\n+        RunnerApi.Pipeline pipeline,\n+        SparkStreamingTranslationContext context);\n+  }\n+\n+  @Override\n+  public Set<String> knownUrns() {\n+    return urnToTransformTranslator.keySet();\n+  }\n+\n+  public SparkStreamingPortablePipelineTranslator() {\n+    ImmutableMap.Builder<String, PTransformTranslator> translatorMap = ImmutableMap.builder();\n+    translatorMap.put(\n+        PTransformTranslation.IMPULSE_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateImpulse);\n+    translatorMap.put(\n+        PTransformTranslation.GROUP_BY_KEY_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateGroupByKey);\n+    translatorMap.put(\n+        ExecutableStage.URN, SparkStreamingPortablePipelineTranslator::translateExecutableStage);\n+    translatorMap.put(\n+        PTransformTranslation.FLATTEN_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateFlatten);\n+    translatorMap.put(\n+        PTransformTranslation.RESHUFFLE_URN,\n+        SparkStreamingPortablePipelineTranslator::translateReshuffle);\n+    this.urnToTransformTranslator = translatorMap.build();\n+  }\n+\n+  /** Translates pipeline from Beam into the Spark context. */\n+  @Override\n+  public void translate(\n+      final RunnerApi.Pipeline pipeline, SparkStreamingTranslationContext context) {\n+    QueryablePipeline p =\n+        QueryablePipeline.forTransforms(\n+            pipeline.getRootTransformIdsList(), pipeline.getComponents());\n+    for (PipelineNode.PTransformNode transformNode : p.getTopologicallyOrderedTransforms()) {\n+      urnToTransformTranslator\n+          .getOrDefault(\n+              transformNode.getTransform().getSpec().getUrn(),\n+              SparkStreamingPortablePipelineTranslator::urnNotFound)\n+          .translate(transformNode, pipeline, context);\n+    }\n+  }\n+\n+  private static void urnNotFound(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    throw new IllegalArgumentException(\n+        String.format(\n+            \"Transform %s has unknown URN %s\",\n+            transformNode.getId(), transformNode.getTransform().getSpec().getUrn()));\n+  }\n+\n+  private static void translateImpulse(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+\n+    TimestampedValue<byte[]> tsValue = TimestampedValue.atMinimumTimestamp(new byte[0]);\n+    Iterable<TimestampedValue<byte[]>> timestampedValues = Collections.singletonList(tsValue);\n+    Iterable<WindowedValue<byte[]>> windowedValues =\n+        StreamSupport.stream(timestampedValues.spliterator(), false)\n+            .map(\n+                timestampedValue ->\n+                    WindowedValue.of(\n+                        timestampedValue.getValue(),\n+                        timestampedValue.getTimestamp(),\n+                        GlobalWindow.INSTANCE,\n+                        PaneInfo.NO_FIRING))\n+            .collect(Collectors.toList());\n+\n+    ByteArrayCoder coder = ByteArrayCoder.of();\n+\n+    WindowedValue.FullWindowedValueCoder<byte[]> windowCoder =\n+        WindowedValue.FullWindowedValueCoder.of(coder, GlobalWindow.Coder.INSTANCE);\n+    JavaRDD<WindowedValue<byte[]>> emptyRDD =\n+        context\n+            .getSparkContext()\n+            .parallelize(CoderHelpers.toByteArrays(windowedValues, windowCoder))\n+            .map(CoderHelpers.fromByteFunction(windowCoder));\n+\n+    // create input DStream from RDD queue\n+    Queue<JavaRDD<WindowedValue<byte[]>>> queueRDD = new LinkedBlockingQueue<>();\n+    queueRDD.offer(emptyRDD);\n+    JavaInputDStream<WindowedValue<byte[]>> emptyStream =\n+        context.getStreamingContext().queueStream(queueRDD, true);\n+\n+    UnboundedDataset<byte[]> output =\n+        new UnboundedDataset<>(\n+            emptyStream, Collections.singletonList(emptyStream.inputDStream().id()));\n+\n+    GlobalWatermarkHolder.SparkWatermarks sparkWatermark =\n+        new GlobalWatermarkHolder.SparkWatermarks(\n+            GlobalWindow.INSTANCE.maxTimestamp(),\n+            BoundedWindow.TIMESTAMP_MAX_VALUE,\n+            context.getFirstTimestamp());\n+    GlobalWatermarkHolder.add(output.getStreamSources().get(0), sparkWatermark);\n+\n+    context.pushDataset(getOutputId(transformNode), output);\n+  }\n+\n+  private static <K, V> void translateGroupByKey(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    RunnerApi.Components components = pipeline.getComponents();\n+    String inputId = getInputId(transformNode);\n+    UnboundedDataset<KV<K, V>> inputDataset =\n+        (UnboundedDataset<KV<K, V>>) context.popDataset(inputId);\n+    List<Integer> streamSources = inputDataset.getStreamSources();\n+    JavaDStream<WindowedValue<KV<K, V>>> dStream = inputDataset.getDStream();\n+    WindowedValue.WindowedValueCoder<KV<K, V>> inputCoder =\n+        getWindowedValueCoder(inputId, components);\n+    KvCoder<K, V> inputKvCoder = (KvCoder<K, V>) inputCoder.getValueCoder();\n+    Coder<K> inputKeyCoder = inputKvCoder.getKeyCoder();\n+    Coder<V> inputValueCoder = inputKvCoder.getValueCoder();\n+    final WindowingStrategy windowingStrategy = getWindowingStrategy(inputId, components);\n+    final WindowFn<Object, BoundedWindow> windowFn = windowingStrategy.getWindowFn();\n+    final WindowedValue.WindowedValueCoder<V> wvCoder =\n+        WindowedValue.FullWindowedValueCoder.of(inputValueCoder, windowFn.windowCoder());\n+\n+    JavaDStream<WindowedValue<KV<K, Iterable<V>>>> outStream =\n+        SparkGroupAlsoByWindowViaWindowSet.groupByKeyAndWindow(\n+            dStream,\n+            inputKeyCoder,\n+            wvCoder,\n+            windowingStrategy,\n+            context.getSerializableOptions(),\n+            streamSources,\n+            transformNode.getId());\n+\n+    context.pushDataset(\n+        getOutputId(transformNode), new UnboundedDataset<>(outStream, streamSources));\n+  }\n+\n+  private static <InputT, OutputT, SideInputT> void translateExecutableStage(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    RunnerApi.ExecutableStagePayload stagePayload;\n+    try {\n+      stagePayload =\n+          RunnerApi.ExecutableStagePayload.parseFrom(\n+              transformNode.getTransform().getSpec().getPayload());\n+    } catch (IOException e) {\n+      throw new RuntimeException(e);\n+    }\n+\n+    String inputPCollectionId = stagePayload.getInput();\n+    UnboundedDataset<InputT> inputDataset =\n+        (UnboundedDataset<InputT>) context.popDataset(inputPCollectionId);\n+    List<Integer> streamSources = inputDataset.getStreamSources();\n+    JavaDStream<WindowedValue<InputT>> inputDStream = inputDataset.getDStream();\n+    Map<String, String> outputs = transformNode.getTransform().getOutputsMap();\n+    BiMap<String, Integer> outputMap = createOutputMap(outputs.values());\n+\n+    RunnerApi.Components components = pipeline.getComponents();\n+    Coder windowCoder =\n+        getWindowingStrategy(inputPCollectionId, components).getWindowFn().windowCoder();\n+\n+    // TODO: handle side inputs?\n+    ImmutableMap<\n+            String, Tuple2<Broadcast<List<byte[]>>, WindowedValue.WindowedValueCoder<SideInputT>>>\n+        broadcastVariables = ImmutableMap.copyOf(new HashMap<>());\n+\n+    SparkExecutableStageFunction<InputT, SideInputT> function =\n+        new SparkExecutableStageFunction<>(\n+            stagePayload,\n+            context.jobInfo,\n+            outputMap,\n+            SparkExecutableStageContextFactory.getInstance(),\n+            broadcastVariables,\n+            MetricsAccumulator.getInstance(),\n+            windowCoder);\n+    JavaDStream<RawUnionValue> staged = inputDStream.mapPartitions(function);\n+\n+    String intermediateId = getExecutableStageIntermediateId(transformNode);\n+    context.pushDataset(\n+        intermediateId,\n+        new Dataset() {\n+          @Override\n+          public void cache(String storageLevel, Coder<?> coder) {\n+            StorageLevel level = StorageLevel.fromString(storageLevel);\n+            staged.persist(level);\n+          }\n+\n+          @Override\n+          public void action() {\n+            // Empty function to force computation of RDD.\n+            staged.foreachRDD(TranslationUtils.emptyVoidFunction());\n+          }\n+\n+          @Override\n+          public void setName(String name) {\n+            // ignore\n+          }\n+        });\n+    // pop dataset to mark RDD as used\n+    context.popDataset(intermediateId);\n+\n+    for (String outputId : outputs.values()) {\n+      JavaDStream<WindowedValue<OutputT>> outStream =\n+          staged.flatMap(new SparkExecutableStageExtractionFunction<>(outputMap.get(outputId)));\n+      context.pushDataset(outputId, new UnboundedDataset<>(outStream, streamSources));\n+    }\n+\n+    if (outputs.isEmpty()) {\n+      // Add sink to ensure all outputs are computed\n+      JavaDStream<WindowedValue<OutputT>> outStream =\n+          staged.flatMap((rawUnionValue) -> Collections.emptyIterator());\n+      context.pushDataset(\n+          String.format(\"EmptyOutputSink_%d\", context.nextSinkId()),\n+          new UnboundedDataset<>(outStream, streamSources));\n+    }\n+  }\n+\n+  private static <T> void translateFlatten(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    Map<String, String> inputsMap = transformNode.getTransform().getInputsMap();\n+    JavaDStream<WindowedValue<T>> unifiedStreams;\n+    final List<Integer> streamSources = new ArrayList<>();\n+\n+    if (inputsMap.isEmpty()) {\n+      Queue<JavaRDD<WindowedValue<T>>> q = new LinkedBlockingQueue<>();\n+      q.offer(context.getSparkContext().emptyRDD());\n+      unifiedStreams = context.getStreamingContext().queueStream(q);\n+    } else {\n+      final List<JavaDStream<WindowedValue<T>>> dStreams = new ArrayList<>();\n+      for (String inputId : inputsMap.values()) {\n+        Dataset dataset = context.popDataset(inputId);\n+        if (dataset instanceof UnboundedDataset) {\n+          UnboundedDataset<T> unboundedDataset = (UnboundedDataset<T>) dataset;\n+          streamSources.addAll(unboundedDataset.getStreamSources());\n+          dStreams.add(unboundedDataset.getDStream());\n+        } else {\n+          // create a single RDD stream.\n+          Queue<JavaRDD<WindowedValue<T>>> q = new LinkedBlockingQueue<>();\n+          q.offer(((BoundedDataset) dataset).getRDD());\n+          // TODO: this is not recoverable from checkpoint!\n+          JavaDStream<WindowedValue<T>> dStream = context.getStreamingContext().queueStream(q);\n+          dStreams.add(dStream);\n+        }\n+      }\n+      // Unify streams into a single stream.\n+      unifiedStreams = SparkCompat.joinStreams(context.getStreamingContext(), dStreams);\n+    }\n+\n+    context.pushDataset(\n+        getOutputId(transformNode), new UnboundedDataset<>(unifiedStreams, streamSources));\n+  }\n+\n+  private static <T> void translateReshuffle(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    String inputId = getInputId(transformNode);\n+    UnboundedDataset<T> inputDataset = (UnboundedDataset<T>) context.popDataset(inputId);\n+    List<Integer> streamSources = inputDataset.getStreamSources();\n+    JavaDStream<WindowedValue<T>> dStream = inputDataset.getDStream();\n+    WindowedValue.WindowedValueCoder<T> coder =\n+        getWindowedValueCoder(inputId, pipeline.getComponents());\n+\n+    JavaDStream<WindowedValue<T>> reshuffledStream =\n+        dStream.transform(rdd -> GroupCombineFunctions.reshuffle(rdd, coder));\n+\n+    context.pushDataset(\n+        getOutputId(transformNode), new UnboundedDataset<>(reshuffledStream, streamSources));\n+  }\n+\n+  private static String getInputId(PTransformNode transformNode) {\n+    return Iterables.getOnlyElement(transformNode.getTransform().getInputsMap().values());\n+  }\n+\n+  private static String getOutputId(PTransformNode transformNode) {\n+    return Iterables.getOnlyElement(transformNode.getTransform().getOutputsMap().values());\n+  }\n+\n+  private static String getExecutableStageIntermediateId(PTransformNode transformNode) {\n+    return transformNode.getId();\n+  }\n+\n+  private static <T> WindowedValue.WindowedValueCoder<T> getWindowedValueCoder(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a"}, "originalPosition": 372}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkxMTY0Mw==", "bodyText": "Nit: this comment is technically accurate, but it should mention streaming.", "url": "https://github.com/apache/beam/pull/12157#discussion_r470911643", "createdAt": "2020-08-15T00:11:24Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkStreamingTranslationContext.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark.translation;\n+\n+import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.streaming.Duration;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.joda.time.Instant;\n+\n+/**\n+ * Translation context used to lazily store Spark data sets during portable pipeline translation and", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkxMTgzMw==", "bodyText": "Make these methods package-private.", "url": "https://github.com/apache/beam/pull/12157#discussion_r470911833", "createdAt": "2020-08-15T00:12:28Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkStreamingTranslationContext.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark.translation;\n+\n+import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.streaming.Duration;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.joda.time.Instant;\n+\n+/**\n+ * Translation context used to lazily store Spark data sets during portable pipeline translation and\n+ * compute them after translation.\n+ */\n+public class SparkStreamingTranslationContext extends SparkTranslationContext {\n+  private final JavaStreamingContext streamingContext;\n+  private final Instant firstTimestamp;\n+\n+  public SparkStreamingTranslationContext(\n+      JavaSparkContext jsc, SparkPipelineOptions options, JobInfo jobInfo) {\n+    super(jsc, options, jobInfo);\n+    Duration batchDuration = new Duration(options.getBatchIntervalMillis());\n+    this.streamingContext = new JavaStreamingContext(jsc, batchDuration);\n+    this.firstTimestamp = new Instant();\n+  }\n+\n+  public JavaStreamingContext getStreamingContext() {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkxMTk5Ng==", "bodyText": "Nit: firstTimestamp sounds a little strange here, since it's the only timestamp around. Maybe initialTimestamp or something?", "url": "https://github.com/apache/beam/pull/12157#discussion_r470911996", "createdAt": "2020-08-15T00:13:36Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkStreamingTranslationContext.java", "diffHunk": "@@ -0,0 +1,50 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark.translation;\n+\n+import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.streaming.Duration;\n+import org.apache.spark.streaming.api.java.JavaStreamingContext;\n+import org.joda.time.Instant;\n+\n+/**\n+ * Translation context used to lazily store Spark data sets during portable pipeline translation and\n+ * compute them after translation.\n+ */\n+public class SparkStreamingTranslationContext extends SparkTranslationContext {\n+  private final JavaStreamingContext streamingContext;\n+  private final Instant firstTimestamp;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a"}, "originalPosition": 33}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkxMjU1Mw==", "bodyText": "Maybe we don't have to do it in this PR, but I'd be in favor of removing this test. It's kind of pointless to test a constant.", "url": "https://github.com/apache/beam/pull/12157#discussion_r470912553", "createdAt": "2020-08-15T00:17:11Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/test/java/org/apache/beam/runners/spark/SparkRunnerRegistrarTest.java", "diffHunk": "@@ -37,7 +37,10 @@\n   @Test\n   public void testOptions() {\n     assertEquals(\n-        ImmutableList.of(SparkPipelineOptions.class, SparkStructuredStreamingPipelineOptions.class),\n+        ImmutableList.of(", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a"}, "originalPosition": 5}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MDkxMzI5MA==", "bodyText": "If possible, exclude test categories instead of test classes.", "url": "https://github.com/apache/beam/pull/12157#discussion_r470913290", "createdAt": "2020-08-15T00:22:30Z", "author": {"login": "ibzib"}, "path": "runners/spark/job-server/build.gradle", "diffHunk": "@@ -82,53 +82,116 @@ runShadow {\n     jvmArgs += [\"-Dorg.slf4j.simpleLogger.defaultLogLevel=${project.property('logLevel')}\"]\n }\n \n-def portableValidatesRunnerTask(String name) {\n-  createPortableValidatesRunnerTask(\n-    name: \"validatesPortableRunner${name}\",\n-    jobServerDriver: \"org.apache.beam.runners.spark.SparkJobServerDriver\",\n-    jobServerConfig: \"--job-host=localhost,--job-port=0,--artifact-port=0,--expansion-port=0\",\n-    testClasspathConfiguration: configurations.validatesPortableRunner,\n-    numParallelTests: 4,\n-    environment: BeamModulePlugin.PortableValidatesRunnerConfiguration.Environment.EMBEDDED,\n-    systemProperties: [\n-      \"beam.spark.test.reuseSparkContext\": \"false\",\n-      \"spark.ui.enabled\": \"false\",\n-      \"spark.ui.showConsoleProgress\": \"false\",\n-    ],\n-    testCategories: {\n-      includeCategories 'org.apache.beam.sdk.testing.ValidatesRunner'\n-      excludeCategories 'org.apache.beam.sdk.testing.FlattenWithHeterogeneousCoders'\n-      excludeCategories 'org.apache.beam.sdk.testing.LargeKeys$Above100MB'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesCommittedMetrics'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesCustomWindowMerging'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesFailureMessage'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesGaugeMetrics'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesParDoLifecycle'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesMapState'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesSetState'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesOrderedListState'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesTimerMap'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesKeyInParDo'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesOnWindowExpiration'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesTestStream'\n-      //SplitableDoFnTests\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesBoundedSplittableParDo'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesSplittableParDoWithWindowedSideInputs'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesUnboundedSplittableParDo'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesStrictTimerOrdering'\n-      excludeCategories 'org.apache.beam.sdk.testing.UsesBundleFinalizer'\n-    },\n-    testFilter: {\n-      // TODO(BEAM-10094)\n-      excludeTestsMatching 'org.apache.beam.sdk.transforms.FlattenTest.testFlattenWithDifferentInputAndOutputCoders2'\n-    },\n-  )\n+def portableValidatesRunnerTask(String name, Boolean streaming) {\n+  def pipelineOptions = []\n+  if (streaming) {\n+    pipelineOptions += \"--streaming\"\n+    pipelineOptions += \"--timeout=20000\"\n+    // exclude unsupported tests\n+    createPortableValidatesRunnerTask(\n+            name: \"validatesPortableRunner${name}\",\n+            jobServerDriver: \"org.apache.beam.runners.spark.SparkJobServerDriver\",\n+            jobServerConfig: \"--job-host=localhost,--job-port=0,--artifact-port=0,--expansion-port=0\",\n+            testClasspathConfiguration: configurations.validatesPortableRunner,\n+            numParallelTests: 4,\n+            pipelineOpts: pipelineOptions,\n+            environment: BeamModulePlugin.PortableValidatesRunnerConfiguration.Environment.EMBEDDED,\n+            systemProperties: [\n+                    \"beam.spark.test.reuseSparkContext\": \"false\",\n+                    \"spark.ui.enabled\": \"false\",\n+                    \"spark.ui.showConsoleProgress\": \"false\",\n+            ],\n+            testCategories: {\n+              includeCategories 'org.apache.beam.sdk.testing.ValidatesRunner'\n+              excludeCategories 'org.apache.beam.sdk.testing.FlattenWithHeterogeneousCoders'\n+              excludeCategories 'org.apache.beam.sdk.testing.LargeKeys$Above100MB'\n+              excludeCategories 'org.apache.beam.sdk.testing.UsesCommittedMetrics'\n+              excludeCategories 'org.apache.beam.sdk.testing.UsesCustomWindowMerging'\n+              excludeCategories 'org.apache.beam.sdk.testing.UsesFailureMessage'\n+              excludeCategories 'org.apache.beam.sdk.testing.UsesGaugeMetrics'\n+              excludeCategories 'org.apache.beam.sdk.testing.UsesParDoLifecycle'\n+              excludeCategories 'org.apache.beam.sdk.testing.UsesMapState'\n+              excludeCategories 'org.apache.beam.sdk.testing.UsesSetState'\n+              excludeCategories 'org.apache.beam.sdk.testing.UsesOrderedListState'\n+              excludeCategories 'org.apache.beam.sdk.testing.UsesTimerMap'\n+              excludeCategories 'org.apache.beam.sdk.testing.UsesKeyInParDo'\n+              excludeCategories 'org.apache.beam.sdk.testing.UsesOnWindowExpiration'\n+              excludeCategories 'org.apache.beam.sdk.testing.UsesTestStream'\n+              //SplitableDoFnTests\n+              excludeCategories 'org.apache.beam.sdk.testing.UsesBoundedSplittableParDo'\n+              excludeCategories 'org.apache.beam.sdk.testing.UsesSplittableParDoWithWindowedSideInputs'\n+              excludeCategories 'org.apache.beam.sdk.testing.UsesUnboundedSplittableParDo'\n+              excludeCategories 'org.apache.beam.sdk.testing.UsesStrictTimerOrdering'\n+              excludeCategories 'org.apache.beam.sdk.testing.UsesBundleFinalizer'\n+              // Tests to exclude when streaming\n+              excludeCategories 'org.apache.beam.sdk.testing.UsesSideInputs'\n+            },\n+            testFilter: {\n+              // TODO(BEAM-10094)\n+              excludeTestsMatching 'org.apache.beam.sdk.transforms.FlattenTest.testFlattenWithDifferentInputAndOutputCoders2'\n+              // Tests to exclude when streaming\n+              excludeTestsMatching 'org.apache.beam.sdk.transforms.CombineTest'\n+              excludeTestsMatching 'org.apache.beam.sdk.transforms.CombineFnsTest'\n+              excludeTestsMatching 'org.apache.beam.sdk.transforms.ParDoTest$MultipleInputsAndOutputTests'\n+              excludeTestsMatching 'org.apache.beam.sdk.transforms.ParDoTest$StateCoderInferenceTests'\n+              excludeTestsMatching 'org.apache.beam.sdk.transforms.ParDoTest$StateTests'\n+              excludeTestsMatching 'org.apache.beam.sdk.transforms.ParDoTest$TimerTests'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a"}, "originalPosition": 98}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "b3fba9cb53d5c0e1e5950d5065736966af25b97a", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/b3fba9cb53d5c0e1e5950d5065736966af25b97a", "committedDate": "2020-08-14T18:43:44Z", "message": "separate build task for streaming, updated registrar test"}, "afterCommit": {"oid": "e13a378c1d372b3099f4cc13a6984e16245462f4", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/e13a378c1d372b3099f4cc13a6984e16245462f4", "committedDate": "2020-08-17T22:32:47Z", "message": "spotlessApply"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDY4OTA3MTEy", "url": "https://github.com/apache/beam/pull/12157#pullrequestreview-468907112", "createdAt": "2020-08-18T00:02:43Z", "commit": {"oid": "e13a378c1d372b3099f4cc13a6984e16245462f4"}, "state": "COMMENTED", "comments": {"totalCount": 1, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMDowMjo0M1rOHB-wTQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0xOFQwMDowMjo0M1rOHB-wTQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3MTgzODc5Nw==", "bodyText": "Link JIRAs for side input, state, and timer support.", "url": "https://github.com/apache/beam/pull/12157#discussion_r471838797", "createdAt": "2020-08-18T00:02:43Z", "author": {"login": "ibzib"}, "path": "runners/spark/job-server/build.gradle", "diffHunk": "@@ -82,20 +82,60 @@ runShadow {\n     jvmArgs += [\"-Dorg.slf4j.simpleLogger.defaultLogLevel=${project.property('logLevel')}\"]\n }\n \n-def portableValidatesRunnerTask(String name) {\n-  createPortableValidatesRunnerTask(\n-    name: \"validatesPortableRunner${name}\",\n-    jobServerDriver: \"org.apache.beam.runners.spark.SparkJobServerDriver\",\n-    jobServerConfig: \"--job-host=localhost,--job-port=0,--artifact-port=0,--expansion-port=0\",\n-    testClasspathConfiguration: configurations.validatesPortableRunner,\n-    numParallelTests: 4,\n-    environment: BeamModulePlugin.PortableValidatesRunnerConfiguration.Environment.EMBEDDED,\n-    systemProperties: [\n-      \"beam.spark.test.reuseSparkContext\": \"false\",\n-      \"spark.ui.enabled\": \"false\",\n-      \"spark.ui.showConsoleProgress\": \"false\",\n-    ],\n-    testCategories: {\n+def portableValidatesRunnerTask(String name, Boolean streaming) {\n+  def pipelineOptions = []\n+  def testCategories\n+  def testFilter\n+\n+  if (streaming) {\n+    pipelineOptions += \"--streaming\"\n+    pipelineOptions += \"--streamingTimeoutMs=20000\"\n+\n+    testCategories = {\n+      includeCategories 'org.apache.beam.sdk.testing.ValidatesRunner'\n+      excludeCategories 'org.apache.beam.sdk.testing.FlattenWithHeterogeneousCoders'\n+      excludeCategories 'org.apache.beam.sdk.testing.LargeKeys$Above100MB'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesCommittedMetrics'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesCustomWindowMerging'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesFailureMessage'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesGaugeMetrics'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesParDoLifecycle'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesMapState'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesSetState'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesOrderedListState'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesTimerMap'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesKeyInParDo'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesOnWindowExpiration'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesTestStream'\n+      // SplittableDoFnTests\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesBoundedSplittableParDo'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesSplittableParDoWithWindowedSideInputs'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesUnboundedSplittableParDo'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesStrictTimerOrdering'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesBundleFinalizer'\n+      // Currently unsupported in portable streaming\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesSideInputs'", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "e13a378c1d372b3099f4cc13a6984e16245462f4"}, "originalPosition": 50}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "e13a378c1d372b3099f4cc13a6984e16245462f4", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/e13a378c1d372b3099f4cc13a6984e16245462f4", "committedDate": "2020-08-17T22:32:47Z", "message": "spotlessApply"}, "afterCommit": {"oid": "de3819877b2f80c4b138f503377802966566c20d", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/de3819877b2f80c4b138f503377802966566c20d", "committedDate": "2020-08-20T22:31:54Z", "message": "checkstyle"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "de3819877b2f80c4b138f503377802966566c20d", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/de3819877b2f80c4b138f503377802966566c20d", "committedDate": "2020-08-20T22:31:54Z", "message": "checkstyle"}, "afterCommit": {"oid": "c08ae930843714cf4ea17edf502d4b08f11915d3", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/c08ae930843714cf4ea17edf502d4b08f11915d3", "committedDate": "2020-08-20T22:35:09Z", "message": "checkstyle\n\nspotless"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcyMDM5NzIz", "url": "https://github.com/apache/beam/pull/12157#pullrequestreview-472039723", "createdAt": "2020-08-20T22:59:05Z", "commit": {"oid": "c08ae930843714cf4ea17edf502d4b08f11915d3"}, "state": "COMMENTED", "comments": {"totalCount": 5, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMFQyMjo1OTowNVrOHEWAIQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMFQyMzoyMDowNVrOHEWY-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDMxNjgzMw==", "bodyText": "We should file a JIRA for all these (since they are conspicuously all windowing related).", "url": "https://github.com/apache/beam/pull/12157#discussion_r474316833", "createdAt": "2020-08-20T22:59:05Z", "author": {"login": "ibzib"}, "path": "runners/spark/job-server/build.gradle", "diffHunk": "@@ -82,20 +82,68 @@ runShadow {\n     jvmArgs += [\"-Dorg.slf4j.simpleLogger.defaultLogLevel=${project.property('logLevel')}\"]\n }\n \n-def portableValidatesRunnerTask(String name) {\n-  createPortableValidatesRunnerTask(\n-    name: \"validatesPortableRunner${name}\",\n-    jobServerDriver: \"org.apache.beam.runners.spark.SparkJobServerDriver\",\n-    jobServerConfig: \"--job-host=localhost,--job-port=0,--artifact-port=0,--expansion-port=0\",\n-    testClasspathConfiguration: configurations.validatesPortableRunner,\n-    numParallelTests: 4,\n-    environment: BeamModulePlugin.PortableValidatesRunnerConfiguration.Environment.EMBEDDED,\n-    systemProperties: [\n-      \"beam.spark.test.reuseSparkContext\": \"false\",\n-      \"spark.ui.enabled\": \"false\",\n-      \"spark.ui.showConsoleProgress\": \"false\",\n-    ],\n-    testCategories: {\n+def portableValidatesRunnerTask(String name, Boolean streaming) {\n+  def pipelineOptions = []\n+  def testCategories\n+  def testFilter\n+\n+  if (streaming) {\n+    pipelineOptions += \"--streaming\"\n+    pipelineOptions += \"--streamingTimeoutMs=30000\"\n+\n+    testCategories = {\n+      includeCategories 'org.apache.beam.sdk.testing.ValidatesRunner'\n+      excludeCategories 'org.apache.beam.sdk.testing.FlattenWithHeterogeneousCoders'\n+      excludeCategories 'org.apache.beam.sdk.testing.LargeKeys$Above100MB'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesCommittedMetrics'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesCustomWindowMerging'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesFailureMessage'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesGaugeMetrics'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesParDoLifecycle'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesMapState'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesSetState'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesOrderedListState'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesTimerMap'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesKeyInParDo'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesOnWindowExpiration'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesTestStream'\n+      // SplittableDoFnTests\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesBoundedSplittableParDo'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesSplittableParDoWithWindowedSideInputs'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesUnboundedSplittableParDo'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesStrictTimerOrdering'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesBundleFinalizer'\n+      // Currently unsupported in portable streaming:\n+      // TODO (BEAM-10712)\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesSideInputs'\n+      // TODO (BEAM-10754)\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesStatefulParDo'\n+      // TODO (BEAM-10755)\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesTimersInParDo'\n+    }\n+\n+    testFilter = {\n+      // TODO(BEAM-10094)\n+      excludeTestsMatching 'org.apache.beam.sdk.transforms.FlattenTest.testFlattenWithDifferentInputAndOutputCoders2'\n+      // Currently unsupported in portable streaming:", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c08ae930843714cf4ea17edf502d4b08f11915d3"}, "originalPosition": 61}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDMxODA1Nw==", "bodyText": "This should've been there before, but the jira for this (BEAM-7222) should be linked.", "url": "https://github.com/apache/beam/pull/12157#discussion_r474318057", "createdAt": "2020-08-20T23:02:52Z", "author": {"login": "ibzib"}, "path": "runners/spark/job-server/build.gradle", "diffHunk": "@@ -82,20 +82,68 @@ runShadow {\n     jvmArgs += [\"-Dorg.slf4j.simpleLogger.defaultLogLevel=${project.property('logLevel')}\"]\n }\n \n-def portableValidatesRunnerTask(String name) {\n-  createPortableValidatesRunnerTask(\n-    name: \"validatesPortableRunner${name}\",\n-    jobServerDriver: \"org.apache.beam.runners.spark.SparkJobServerDriver\",\n-    jobServerConfig: \"--job-host=localhost,--job-port=0,--artifact-port=0,--expansion-port=0\",\n-    testClasspathConfiguration: configurations.validatesPortableRunner,\n-    numParallelTests: 4,\n-    environment: BeamModulePlugin.PortableValidatesRunnerConfiguration.Environment.EMBEDDED,\n-    systemProperties: [\n-      \"beam.spark.test.reuseSparkContext\": \"false\",\n-      \"spark.ui.enabled\": \"false\",\n-      \"spark.ui.showConsoleProgress\": \"false\",\n-    ],\n-    testCategories: {\n+def portableValidatesRunnerTask(String name, Boolean streaming) {\n+  def pipelineOptions = []\n+  def testCategories\n+  def testFilter\n+\n+  if (streaming) {\n+    pipelineOptions += \"--streaming\"\n+    pipelineOptions += \"--streamingTimeoutMs=30000\"\n+\n+    testCategories = {\n+      includeCategories 'org.apache.beam.sdk.testing.ValidatesRunner'\n+      excludeCategories 'org.apache.beam.sdk.testing.FlattenWithHeterogeneousCoders'\n+      excludeCategories 'org.apache.beam.sdk.testing.LargeKeys$Above100MB'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesCommittedMetrics'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesCustomWindowMerging'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesFailureMessage'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesGaugeMetrics'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesParDoLifecycle'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesMapState'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesSetState'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesOrderedListState'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesTimerMap'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesKeyInParDo'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesOnWindowExpiration'\n+      excludeCategories 'org.apache.beam.sdk.testing.UsesTestStream'\n+      // SplittableDoFnTests", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c08ae930843714cf4ea17edf502d4b08f11915d3"}, "originalPosition": 43}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDMyMDM3NQ==", "bodyText": "Collections.emptyIterator() has the same behavior, but is more concise.", "url": "https://github.com/apache/beam/pull/12157#discussion_r474320375", "createdAt": "2020-08-20T23:10:22Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkExecutableStageFunction.java", "diffHunk": "@@ -118,6 +118,12 @@\n \n   @Override\n   public Iterator<RawUnionValue> call(Iterator<WindowedValue<InputT>> inputs) throws Exception {\n+    // Do not call processElements if there are no inputs\n+    // Otherwise, this may cause validation errors (e.g. ParDoTest)\n+    if (!inputs.hasNext()) {\n+      return new ConcurrentLinkedQueue<RawUnionValue>().iterator();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c08ae930843714cf4ea17edf502d4b08f11915d3"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDMyMTMzNg==", "bodyText": "\"streaming and portable\" should be \"streaming and batch.\"", "url": "https://github.com/apache/beam/pull/12157#discussion_r474321336", "createdAt": "2020-08-20T23:13:49Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkPortablePipelineTranslator.java", "diffHunk": "@@ -0,0 +1,38 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark.translation;\n+\n+import java.util.Set;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n+import org.apache.spark.api.java.JavaSparkContext;\n+\n+/**\n+ * Interface for portable Spark translators. This allows for a uniform invocation pattern for\n+ * pipeline translation between streaming and portable runners.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c08ae930843714cf4ea17edf502d4b08f11915d3"}, "originalPosition": 28}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDMyMzE5NA==", "bodyText": "Nit: \"dummy\" sounds a lot like mocks or stubs, which are technical terms that describe the implementation of test objects. This list isn't a \"dummy\" in that sense; it's a real list, albeit one constructed for test purposes.\ntl;dr rename this inputs.", "url": "https://github.com/apache/beam/pull/12157#discussion_r474323194", "createdAt": "2020-08-20T23:20:05Z", "author": {"login": "ibzib"}, "path": "runners/spark/src/test/java/org/apache/beam/runners/spark/translation/SparkExecutableStageFunctionTest.java", "diffHunk": "@@ -101,7 +104,9 @@ public void setUpMocks() throws Exception {\n   public void sdkErrorsSurfaceOnClose() throws Exception {\n     SparkExecutableStageFunction<Integer, ?> function = getFunction(Collections.emptyMap());\n     doThrow(new Exception()).when(remoteBundle).close();\n-    function.call(Collections.emptyIterator());\n+    List<WindowedValue<Integer>> dummyList = new ArrayList<>();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "c08ae930843714cf4ea17edf502d4b08f11915d3"}, "originalPosition": 20}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "c08ae930843714cf4ea17edf502d4b08f11915d3", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/c08ae930843714cf4ea17edf502d4b08f11915d3", "committedDate": "2020-08-20T22:35:09Z", "message": "checkstyle\n\nspotless"}, "afterCommit": {"oid": "ca707137600be0b54bc95e378546620a7ce77887", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/ca707137600be0b54bc95e378546620a7ce77887", "committedDate": "2020-08-21T02:20:51Z", "message": "addressing comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcyMTIxODQ2", "url": "https://github.com/apache/beam/pull/12157#pullrequestreview-472121846", "createdAt": "2020-08-21T03:27:12Z", "commit": {"oid": "ca707137600be0b54bc95e378546620a7ce77887"}, "state": "COMMENTED", "comments": {"totalCount": 9, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQwMzoyNzoxM1rOHEaVyA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOC0yMVQwMzo0NTowMlrOHEalWQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDM4NzkxMg==", "bodyText": "It seems it'd be easier construct this with\nCollections.singletonList(\n    WindowedValue.of(...));\n\nrather than use an intermediate timestamped value list.", "url": "https://github.com/apache/beam/pull/12157#discussion_r474387912", "createdAt": "2020-08-21T03:27:13Z", "author": {"login": "robertwb"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkStreamingPortablePipelineTranslator.java", "diffHunk": "@@ -0,0 +1,366 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark.translation;\n+\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.createOutputMap;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getExecutableStageIntermediateId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getInputId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getOutputId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowedValueCoder;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowingStrategy;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.PTransformTranslation;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode.PTransformNode;\n+import org.apache.beam.runners.core.construction.graph.QueryablePipeline;\n+import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n+import org.apache.beam.runners.spark.coders.CoderHelpers;\n+import org.apache.beam.runners.spark.metrics.MetricsAccumulator;\n+import org.apache.beam.runners.spark.stateful.SparkGroupAlsoByWindowViaWindowSet;\n+import org.apache.beam.runners.spark.translation.streaming.UnboundedDataset;\n+import org.apache.beam.runners.spark.util.GlobalWatermarkHolder;\n+import org.apache.beam.runners.spark.util.SparkCompat;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.transforms.join.RawUnionValue;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.TimestampedValue;\n+import org.apache.beam.sdk.values.WindowingStrategy;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.BiMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaInputDStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Tuple2;\n+\n+/** Translates an unbounded portable pipeline into a Spark job. */\n+public class SparkStreamingPortablePipelineTranslator\n+    implements SparkPortablePipelineTranslator<SparkStreamingTranslationContext> {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SparkStreamingPortablePipelineTranslator.class);\n+\n+  private final ImmutableMap<String, PTransformTranslator> urnToTransformTranslator;\n+\n+  interface PTransformTranslator {\n+\n+    /** Translates transformNode from Beam into the Spark context. */\n+    void translate(\n+        PTransformNode transformNode,\n+        RunnerApi.Pipeline pipeline,\n+        SparkStreamingTranslationContext context);\n+  }\n+\n+  @Override\n+  public Set<String> knownUrns() {\n+    return urnToTransformTranslator.keySet();\n+  }\n+\n+  public SparkStreamingPortablePipelineTranslator() {\n+    ImmutableMap.Builder<String, PTransformTranslator> translatorMap = ImmutableMap.builder();\n+    translatorMap.put(\n+        PTransformTranslation.IMPULSE_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateImpulse);\n+    translatorMap.put(\n+        PTransformTranslation.GROUP_BY_KEY_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateGroupByKey);\n+    translatorMap.put(\n+        ExecutableStage.URN, SparkStreamingPortablePipelineTranslator::translateExecutableStage);\n+    translatorMap.put(\n+        PTransformTranslation.FLATTEN_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateFlatten);\n+    translatorMap.put(\n+        PTransformTranslation.RESHUFFLE_URN,\n+        SparkStreamingPortablePipelineTranslator::translateReshuffle);\n+    this.urnToTransformTranslator = translatorMap.build();\n+  }\n+\n+  /** Translates pipeline from Beam into the Spark context. */\n+  @Override\n+  public void translate(\n+      final RunnerApi.Pipeline pipeline, SparkStreamingTranslationContext context) {\n+    QueryablePipeline p =\n+        QueryablePipeline.forTransforms(\n+            pipeline.getRootTransformIdsList(), pipeline.getComponents());\n+    for (PipelineNode.PTransformNode transformNode : p.getTopologicallyOrderedTransforms()) {\n+      urnToTransformTranslator\n+          .getOrDefault(\n+              transformNode.getTransform().getSpec().getUrn(),\n+              SparkStreamingPortablePipelineTranslator::urnNotFound)\n+          .translate(transformNode, pipeline, context);\n+    }\n+  }\n+\n+  private static void urnNotFound(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    throw new IllegalArgumentException(\n+        String.format(\n+            \"Transform %s has unknown URN %s\",\n+            transformNode.getId(), transformNode.getTransform().getSpec().getUrn()));\n+  }\n+\n+  private static void translateImpulse(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+\n+    TimestampedValue<byte[]> tsValue = TimestampedValue.atMinimumTimestamp(new byte[0]);\n+    Iterable<TimestampedValue<byte[]>> timestampedValues = Collections.singletonList(tsValue);\n+    Iterable<WindowedValue<byte[]>> windowedValues =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca707137600be0b54bc95e378546620a7ce77887"}, "originalPosition": 151}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDM4ODM1Ng==", "bodyText": "You could even defer the singleton iterable wrapping to here and use windowCoder.encode(...) rather than use CoderHelpers.toByteArrays.", "url": "https://github.com/apache/beam/pull/12157#discussion_r474388356", "createdAt": "2020-08-21T03:29:15Z", "author": {"login": "robertwb"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkStreamingPortablePipelineTranslator.java", "diffHunk": "@@ -0,0 +1,366 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark.translation;\n+\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.createOutputMap;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getExecutableStageIntermediateId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getInputId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getOutputId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowedValueCoder;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowingStrategy;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.PTransformTranslation;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode.PTransformNode;\n+import org.apache.beam.runners.core.construction.graph.QueryablePipeline;\n+import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n+import org.apache.beam.runners.spark.coders.CoderHelpers;\n+import org.apache.beam.runners.spark.metrics.MetricsAccumulator;\n+import org.apache.beam.runners.spark.stateful.SparkGroupAlsoByWindowViaWindowSet;\n+import org.apache.beam.runners.spark.translation.streaming.UnboundedDataset;\n+import org.apache.beam.runners.spark.util.GlobalWatermarkHolder;\n+import org.apache.beam.runners.spark.util.SparkCompat;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.transforms.join.RawUnionValue;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.TimestampedValue;\n+import org.apache.beam.sdk.values.WindowingStrategy;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.BiMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaInputDStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Tuple2;\n+\n+/** Translates an unbounded portable pipeline into a Spark job. */\n+public class SparkStreamingPortablePipelineTranslator\n+    implements SparkPortablePipelineTranslator<SparkStreamingTranslationContext> {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SparkStreamingPortablePipelineTranslator.class);\n+\n+  private final ImmutableMap<String, PTransformTranslator> urnToTransformTranslator;\n+\n+  interface PTransformTranslator {\n+\n+    /** Translates transformNode from Beam into the Spark context. */\n+    void translate(\n+        PTransformNode transformNode,\n+        RunnerApi.Pipeline pipeline,\n+        SparkStreamingTranslationContext context);\n+  }\n+\n+  @Override\n+  public Set<String> knownUrns() {\n+    return urnToTransformTranslator.keySet();\n+  }\n+\n+  public SparkStreamingPortablePipelineTranslator() {\n+    ImmutableMap.Builder<String, PTransformTranslator> translatorMap = ImmutableMap.builder();\n+    translatorMap.put(\n+        PTransformTranslation.IMPULSE_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateImpulse);\n+    translatorMap.put(\n+        PTransformTranslation.GROUP_BY_KEY_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateGroupByKey);\n+    translatorMap.put(\n+        ExecutableStage.URN, SparkStreamingPortablePipelineTranslator::translateExecutableStage);\n+    translatorMap.put(\n+        PTransformTranslation.FLATTEN_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateFlatten);\n+    translatorMap.put(\n+        PTransformTranslation.RESHUFFLE_URN,\n+        SparkStreamingPortablePipelineTranslator::translateReshuffle);\n+    this.urnToTransformTranslator = translatorMap.build();\n+  }\n+\n+  /** Translates pipeline from Beam into the Spark context. */\n+  @Override\n+  public void translate(\n+      final RunnerApi.Pipeline pipeline, SparkStreamingTranslationContext context) {\n+    QueryablePipeline p =\n+        QueryablePipeline.forTransforms(\n+            pipeline.getRootTransformIdsList(), pipeline.getComponents());\n+    for (PipelineNode.PTransformNode transformNode : p.getTopologicallyOrderedTransforms()) {\n+      urnToTransformTranslator\n+          .getOrDefault(\n+              transformNode.getTransform().getSpec().getUrn(),\n+              SparkStreamingPortablePipelineTranslator::urnNotFound)\n+          .translate(transformNode, pipeline, context);\n+    }\n+  }\n+\n+  private static void urnNotFound(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    throw new IllegalArgumentException(\n+        String.format(\n+            \"Transform %s has unknown URN %s\",\n+            transformNode.getId(), transformNode.getTransform().getSpec().getUrn()));\n+  }\n+\n+  private static void translateImpulse(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+\n+    TimestampedValue<byte[]> tsValue = TimestampedValue.atMinimumTimestamp(new byte[0]);\n+    Iterable<TimestampedValue<byte[]>> timestampedValues = Collections.singletonList(tsValue);\n+    Iterable<WindowedValue<byte[]>> windowedValues =\n+        StreamSupport.stream(timestampedValues.spliterator(), false)\n+            .map(\n+                timestampedValue ->\n+                    WindowedValue.of(\n+                        timestampedValue.getValue(),\n+                        timestampedValue.getTimestamp(),\n+                        GlobalWindow.INSTANCE,\n+                        PaneInfo.NO_FIRING))\n+            .collect(Collectors.toList());\n+\n+    ByteArrayCoder coder = ByteArrayCoder.of();\n+    WindowedValue.FullWindowedValueCoder<byte[]> windowCoder =\n+        WindowedValue.FullWindowedValueCoder.of(coder, GlobalWindow.Coder.INSTANCE);\n+    JavaRDD<WindowedValue<byte[]>> emptyByteArrayRDD =\n+        context\n+            .getSparkContext()\n+            .parallelize(CoderHelpers.toByteArrays(windowedValues, windowCoder))", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca707137600be0b54bc95e378546620a7ce77887"}, "originalPosition": 168}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDM4ODY4OQ==", "bodyText": "This stream isn't empty, is it?", "url": "https://github.com/apache/beam/pull/12157#discussion_r474388689", "createdAt": "2020-08-21T03:30:44Z", "author": {"login": "robertwb"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkStreamingPortablePipelineTranslator.java", "diffHunk": "@@ -0,0 +1,366 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark.translation;\n+\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.createOutputMap;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getExecutableStageIntermediateId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getInputId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getOutputId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowedValueCoder;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowingStrategy;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.PTransformTranslation;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode.PTransformNode;\n+import org.apache.beam.runners.core.construction.graph.QueryablePipeline;\n+import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n+import org.apache.beam.runners.spark.coders.CoderHelpers;\n+import org.apache.beam.runners.spark.metrics.MetricsAccumulator;\n+import org.apache.beam.runners.spark.stateful.SparkGroupAlsoByWindowViaWindowSet;\n+import org.apache.beam.runners.spark.translation.streaming.UnboundedDataset;\n+import org.apache.beam.runners.spark.util.GlobalWatermarkHolder;\n+import org.apache.beam.runners.spark.util.SparkCompat;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.transforms.join.RawUnionValue;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.TimestampedValue;\n+import org.apache.beam.sdk.values.WindowingStrategy;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.BiMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaInputDStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Tuple2;\n+\n+/** Translates an unbounded portable pipeline into a Spark job. */\n+public class SparkStreamingPortablePipelineTranslator\n+    implements SparkPortablePipelineTranslator<SparkStreamingTranslationContext> {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SparkStreamingPortablePipelineTranslator.class);\n+\n+  private final ImmutableMap<String, PTransformTranslator> urnToTransformTranslator;\n+\n+  interface PTransformTranslator {\n+\n+    /** Translates transformNode from Beam into the Spark context. */\n+    void translate(\n+        PTransformNode transformNode,\n+        RunnerApi.Pipeline pipeline,\n+        SparkStreamingTranslationContext context);\n+  }\n+\n+  @Override\n+  public Set<String> knownUrns() {\n+    return urnToTransformTranslator.keySet();\n+  }\n+\n+  public SparkStreamingPortablePipelineTranslator() {\n+    ImmutableMap.Builder<String, PTransformTranslator> translatorMap = ImmutableMap.builder();\n+    translatorMap.put(\n+        PTransformTranslation.IMPULSE_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateImpulse);\n+    translatorMap.put(\n+        PTransformTranslation.GROUP_BY_KEY_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateGroupByKey);\n+    translatorMap.put(\n+        ExecutableStage.URN, SparkStreamingPortablePipelineTranslator::translateExecutableStage);\n+    translatorMap.put(\n+        PTransformTranslation.FLATTEN_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateFlatten);\n+    translatorMap.put(\n+        PTransformTranslation.RESHUFFLE_URN,\n+        SparkStreamingPortablePipelineTranslator::translateReshuffle);\n+    this.urnToTransformTranslator = translatorMap.build();\n+  }\n+\n+  /** Translates pipeline from Beam into the Spark context. */\n+  @Override\n+  public void translate(\n+      final RunnerApi.Pipeline pipeline, SparkStreamingTranslationContext context) {\n+    QueryablePipeline p =\n+        QueryablePipeline.forTransforms(\n+            pipeline.getRootTransformIdsList(), pipeline.getComponents());\n+    for (PipelineNode.PTransformNode transformNode : p.getTopologicallyOrderedTransforms()) {\n+      urnToTransformTranslator\n+          .getOrDefault(\n+              transformNode.getTransform().getSpec().getUrn(),\n+              SparkStreamingPortablePipelineTranslator::urnNotFound)\n+          .translate(transformNode, pipeline, context);\n+    }\n+  }\n+\n+  private static void urnNotFound(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    throw new IllegalArgumentException(\n+        String.format(\n+            \"Transform %s has unknown URN %s\",\n+            transformNode.getId(), transformNode.getTransform().getSpec().getUrn()));\n+  }\n+\n+  private static void translateImpulse(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+\n+    TimestampedValue<byte[]> tsValue = TimestampedValue.atMinimumTimestamp(new byte[0]);\n+    Iterable<TimestampedValue<byte[]>> timestampedValues = Collections.singletonList(tsValue);\n+    Iterable<WindowedValue<byte[]>> windowedValues =\n+        StreamSupport.stream(timestampedValues.spliterator(), false)\n+            .map(\n+                timestampedValue ->\n+                    WindowedValue.of(\n+                        timestampedValue.getValue(),\n+                        timestampedValue.getTimestamp(),\n+                        GlobalWindow.INSTANCE,\n+                        PaneInfo.NO_FIRING))\n+            .collect(Collectors.toList());\n+\n+    ByteArrayCoder coder = ByteArrayCoder.of();\n+    WindowedValue.FullWindowedValueCoder<byte[]> windowCoder =\n+        WindowedValue.FullWindowedValueCoder.of(coder, GlobalWindow.Coder.INSTANCE);\n+    JavaRDD<WindowedValue<byte[]>> emptyByteArrayRDD =\n+        context\n+            .getSparkContext()\n+            .parallelize(CoderHelpers.toByteArrays(windowedValues, windowCoder))\n+            .map(CoderHelpers.fromByteFunction(windowCoder));\n+\n+    Queue<JavaRDD<WindowedValue<byte[]>>> rddQueue = new LinkedBlockingQueue<>();\n+    rddQueue.offer(emptyByteArrayRDD);\n+    JavaInputDStream<WindowedValue<byte[]>> emptyStream =", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca707137600be0b54bc95e378546620a7ce77887"}, "originalPosition": 173}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDM4OTMwOQ==", "bodyText": "I have to admit I'm a bit fuzzy on how this holds back (or doesn't) the watermark.", "url": "https://github.com/apache/beam/pull/12157#discussion_r474389309", "createdAt": "2020-08-21T03:33:39Z", "author": {"login": "robertwb"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkStreamingPortablePipelineTranslator.java", "diffHunk": "@@ -0,0 +1,366 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark.translation;\n+\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.createOutputMap;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getExecutableStageIntermediateId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getInputId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getOutputId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowedValueCoder;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowingStrategy;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.PTransformTranslation;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode.PTransformNode;\n+import org.apache.beam.runners.core.construction.graph.QueryablePipeline;\n+import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n+import org.apache.beam.runners.spark.coders.CoderHelpers;\n+import org.apache.beam.runners.spark.metrics.MetricsAccumulator;\n+import org.apache.beam.runners.spark.stateful.SparkGroupAlsoByWindowViaWindowSet;\n+import org.apache.beam.runners.spark.translation.streaming.UnboundedDataset;\n+import org.apache.beam.runners.spark.util.GlobalWatermarkHolder;\n+import org.apache.beam.runners.spark.util.SparkCompat;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.transforms.join.RawUnionValue;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.TimestampedValue;\n+import org.apache.beam.sdk.values.WindowingStrategy;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.BiMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaInputDStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Tuple2;\n+\n+/** Translates an unbounded portable pipeline into a Spark job. */\n+public class SparkStreamingPortablePipelineTranslator\n+    implements SparkPortablePipelineTranslator<SparkStreamingTranslationContext> {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SparkStreamingPortablePipelineTranslator.class);\n+\n+  private final ImmutableMap<String, PTransformTranslator> urnToTransformTranslator;\n+\n+  interface PTransformTranslator {\n+\n+    /** Translates transformNode from Beam into the Spark context. */\n+    void translate(\n+        PTransformNode transformNode,\n+        RunnerApi.Pipeline pipeline,\n+        SparkStreamingTranslationContext context);\n+  }\n+\n+  @Override\n+  public Set<String> knownUrns() {\n+    return urnToTransformTranslator.keySet();\n+  }\n+\n+  public SparkStreamingPortablePipelineTranslator() {\n+    ImmutableMap.Builder<String, PTransformTranslator> translatorMap = ImmutableMap.builder();\n+    translatorMap.put(\n+        PTransformTranslation.IMPULSE_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateImpulse);\n+    translatorMap.put(\n+        PTransformTranslation.GROUP_BY_KEY_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateGroupByKey);\n+    translatorMap.put(\n+        ExecutableStage.URN, SparkStreamingPortablePipelineTranslator::translateExecutableStage);\n+    translatorMap.put(\n+        PTransformTranslation.FLATTEN_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateFlatten);\n+    translatorMap.put(\n+        PTransformTranslation.RESHUFFLE_URN,\n+        SparkStreamingPortablePipelineTranslator::translateReshuffle);\n+    this.urnToTransformTranslator = translatorMap.build();\n+  }\n+\n+  /** Translates pipeline from Beam into the Spark context. */\n+  @Override\n+  public void translate(\n+      final RunnerApi.Pipeline pipeline, SparkStreamingTranslationContext context) {\n+    QueryablePipeline p =\n+        QueryablePipeline.forTransforms(\n+            pipeline.getRootTransformIdsList(), pipeline.getComponents());\n+    for (PipelineNode.PTransformNode transformNode : p.getTopologicallyOrderedTransforms()) {\n+      urnToTransformTranslator\n+          .getOrDefault(\n+              transformNode.getTransform().getSpec().getUrn(),\n+              SparkStreamingPortablePipelineTranslator::urnNotFound)\n+          .translate(transformNode, pipeline, context);\n+    }\n+  }\n+\n+  private static void urnNotFound(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    throw new IllegalArgumentException(\n+        String.format(\n+            \"Transform %s has unknown URN %s\",\n+            transformNode.getId(), transformNode.getTransform().getSpec().getUrn()));\n+  }\n+\n+  private static void translateImpulse(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+\n+    TimestampedValue<byte[]> tsValue = TimestampedValue.atMinimumTimestamp(new byte[0]);\n+    Iterable<TimestampedValue<byte[]>> timestampedValues = Collections.singletonList(tsValue);\n+    Iterable<WindowedValue<byte[]>> windowedValues =\n+        StreamSupport.stream(timestampedValues.spliterator(), false)\n+            .map(\n+                timestampedValue ->\n+                    WindowedValue.of(\n+                        timestampedValue.getValue(),\n+                        timestampedValue.getTimestamp(),\n+                        GlobalWindow.INSTANCE,\n+                        PaneInfo.NO_FIRING))\n+            .collect(Collectors.toList());\n+\n+    ByteArrayCoder coder = ByteArrayCoder.of();\n+    WindowedValue.FullWindowedValueCoder<byte[]> windowCoder =\n+        WindowedValue.FullWindowedValueCoder.of(coder, GlobalWindow.Coder.INSTANCE);\n+    JavaRDD<WindowedValue<byte[]>> emptyByteArrayRDD =\n+        context\n+            .getSparkContext()\n+            .parallelize(CoderHelpers.toByteArrays(windowedValues, windowCoder))\n+            .map(CoderHelpers.fromByteFunction(windowCoder));\n+\n+    Queue<JavaRDD<WindowedValue<byte[]>>> rddQueue = new LinkedBlockingQueue<>();\n+    rddQueue.offer(emptyByteArrayRDD);\n+    JavaInputDStream<WindowedValue<byte[]>> emptyStream =\n+        context.getStreamingContext().queueStream(rddQueue, true /* oneAtATime */);\n+\n+    UnboundedDataset<byte[]> output =\n+        new UnboundedDataset<>(\n+            emptyStream, Collections.singletonList(emptyStream.inputDStream().id()));\n+\n+    // Add watermark to holder and advance to infinity to ensure future watermarks can be updated\n+    GlobalWatermarkHolder.SparkWatermarks sparkWatermark =\n+        new GlobalWatermarkHolder.SparkWatermarks(\n+            GlobalWindow.INSTANCE.maxTimestamp(),\n+            BoundedWindow.TIMESTAMP_MAX_VALUE,\n+            context.getFirstTimestamp());\n+    GlobalWatermarkHolder.add(output.getStreamSources().get(0), sparkWatermark);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca707137600be0b54bc95e378546620a7ce77887"}, "originalPosition": 186}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDM4OTQ4NA==", "bodyText": "Be consistent on what's declared final vs. what's not.", "url": "https://github.com/apache/beam/pull/12157#discussion_r474389484", "createdAt": "2020-08-21T03:34:27Z", "author": {"login": "robertwb"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkStreamingPortablePipelineTranslator.java", "diffHunk": "@@ -0,0 +1,366 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark.translation;\n+\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.createOutputMap;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getExecutableStageIntermediateId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getInputId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getOutputId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowedValueCoder;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowingStrategy;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.PTransformTranslation;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode.PTransformNode;\n+import org.apache.beam.runners.core.construction.graph.QueryablePipeline;\n+import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n+import org.apache.beam.runners.spark.coders.CoderHelpers;\n+import org.apache.beam.runners.spark.metrics.MetricsAccumulator;\n+import org.apache.beam.runners.spark.stateful.SparkGroupAlsoByWindowViaWindowSet;\n+import org.apache.beam.runners.spark.translation.streaming.UnboundedDataset;\n+import org.apache.beam.runners.spark.util.GlobalWatermarkHolder;\n+import org.apache.beam.runners.spark.util.SparkCompat;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.transforms.join.RawUnionValue;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.TimestampedValue;\n+import org.apache.beam.sdk.values.WindowingStrategy;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.BiMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaInputDStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Tuple2;\n+\n+/** Translates an unbounded portable pipeline into a Spark job. */\n+public class SparkStreamingPortablePipelineTranslator\n+    implements SparkPortablePipelineTranslator<SparkStreamingTranslationContext> {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SparkStreamingPortablePipelineTranslator.class);\n+\n+  private final ImmutableMap<String, PTransformTranslator> urnToTransformTranslator;\n+\n+  interface PTransformTranslator {\n+\n+    /** Translates transformNode from Beam into the Spark context. */\n+    void translate(\n+        PTransformNode transformNode,\n+        RunnerApi.Pipeline pipeline,\n+        SparkStreamingTranslationContext context);\n+  }\n+\n+  @Override\n+  public Set<String> knownUrns() {\n+    return urnToTransformTranslator.keySet();\n+  }\n+\n+  public SparkStreamingPortablePipelineTranslator() {\n+    ImmutableMap.Builder<String, PTransformTranslator> translatorMap = ImmutableMap.builder();\n+    translatorMap.put(\n+        PTransformTranslation.IMPULSE_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateImpulse);\n+    translatorMap.put(\n+        PTransformTranslation.GROUP_BY_KEY_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateGroupByKey);\n+    translatorMap.put(\n+        ExecutableStage.URN, SparkStreamingPortablePipelineTranslator::translateExecutableStage);\n+    translatorMap.put(\n+        PTransformTranslation.FLATTEN_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateFlatten);\n+    translatorMap.put(\n+        PTransformTranslation.RESHUFFLE_URN,\n+        SparkStreamingPortablePipelineTranslator::translateReshuffle);\n+    this.urnToTransformTranslator = translatorMap.build();\n+  }\n+\n+  /** Translates pipeline from Beam into the Spark context. */\n+  @Override\n+  public void translate(\n+      final RunnerApi.Pipeline pipeline, SparkStreamingTranslationContext context) {\n+    QueryablePipeline p =\n+        QueryablePipeline.forTransforms(\n+            pipeline.getRootTransformIdsList(), pipeline.getComponents());\n+    for (PipelineNode.PTransformNode transformNode : p.getTopologicallyOrderedTransforms()) {\n+      urnToTransformTranslator\n+          .getOrDefault(\n+              transformNode.getTransform().getSpec().getUrn(),\n+              SparkStreamingPortablePipelineTranslator::urnNotFound)\n+          .translate(transformNode, pipeline, context);\n+    }\n+  }\n+\n+  private static void urnNotFound(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    throw new IllegalArgumentException(\n+        String.format(\n+            \"Transform %s has unknown URN %s\",\n+            transformNode.getId(), transformNode.getTransform().getSpec().getUrn()));\n+  }\n+\n+  private static void translateImpulse(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+\n+    TimestampedValue<byte[]> tsValue = TimestampedValue.atMinimumTimestamp(new byte[0]);\n+    Iterable<TimestampedValue<byte[]>> timestampedValues = Collections.singletonList(tsValue);\n+    Iterable<WindowedValue<byte[]>> windowedValues =\n+        StreamSupport.stream(timestampedValues.spliterator(), false)\n+            .map(\n+                timestampedValue ->\n+                    WindowedValue.of(\n+                        timestampedValue.getValue(),\n+                        timestampedValue.getTimestamp(),\n+                        GlobalWindow.INSTANCE,\n+                        PaneInfo.NO_FIRING))\n+            .collect(Collectors.toList());\n+\n+    ByteArrayCoder coder = ByteArrayCoder.of();\n+    WindowedValue.FullWindowedValueCoder<byte[]> windowCoder =\n+        WindowedValue.FullWindowedValueCoder.of(coder, GlobalWindow.Coder.INSTANCE);\n+    JavaRDD<WindowedValue<byte[]>> emptyByteArrayRDD =\n+        context\n+            .getSparkContext()\n+            .parallelize(CoderHelpers.toByteArrays(windowedValues, windowCoder))\n+            .map(CoderHelpers.fromByteFunction(windowCoder));\n+\n+    Queue<JavaRDD<WindowedValue<byte[]>>> rddQueue = new LinkedBlockingQueue<>();\n+    rddQueue.offer(emptyByteArrayRDD);\n+    JavaInputDStream<WindowedValue<byte[]>> emptyStream =\n+        context.getStreamingContext().queueStream(rddQueue, true /* oneAtATime */);\n+\n+    UnboundedDataset<byte[]> output =\n+        new UnboundedDataset<>(\n+            emptyStream, Collections.singletonList(emptyStream.inputDStream().id()));\n+\n+    // Add watermark to holder and advance to infinity to ensure future watermarks can be updated\n+    GlobalWatermarkHolder.SparkWatermarks sparkWatermark =\n+        new GlobalWatermarkHolder.SparkWatermarks(\n+            GlobalWindow.INSTANCE.maxTimestamp(),\n+            BoundedWindow.TIMESTAMP_MAX_VALUE,\n+            context.getFirstTimestamp());\n+    GlobalWatermarkHolder.add(output.getStreamSources().get(0), sparkWatermark);\n+\n+    context.pushDataset(getOutputId(transformNode), output);\n+  }\n+\n+  private static <K, V> void translateGroupByKey(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    RunnerApi.Components components = pipeline.getComponents();\n+    String inputId = getInputId(transformNode);\n+    UnboundedDataset<KV<K, V>> inputDataset =\n+        (UnboundedDataset<KV<K, V>>) context.popDataset(inputId);\n+    List<Integer> streamSources = inputDataset.getStreamSources();\n+    JavaDStream<WindowedValue<KV<K, V>>> dStream = inputDataset.getDStream();\n+    WindowedValue.WindowedValueCoder<KV<K, V>> inputCoder =\n+        getWindowedValueCoder(inputId, components);\n+    KvCoder<K, V> inputKvCoder = (KvCoder<K, V>) inputCoder.getValueCoder();\n+    Coder<K> inputKeyCoder = inputKvCoder.getKeyCoder();\n+    Coder<V> inputValueCoder = inputKvCoder.getValueCoder();\n+    final WindowingStrategy windowingStrategy = getWindowingStrategy(inputId, components);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca707137600be0b54bc95e378546620a7ce77887"}, "originalPosition": 206}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDM5MDgzMg==", "bodyText": "Nit: I'm noticing the pattern of assigning a huge number of locals at the top of a function and then only using them once below. Often (not always) the code would be more concise if the declarations were used inline (e.g. use inputDataset.getDStream() rather than assign to an intermediate inputDStream).", "url": "https://github.com/apache/beam/pull/12157#discussion_r474390832", "createdAt": "2020-08-21T03:40:06Z", "author": {"login": "robertwb"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkStreamingPortablePipelineTranslator.java", "diffHunk": "@@ -0,0 +1,366 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark.translation;\n+\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.createOutputMap;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getExecutableStageIntermediateId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getInputId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getOutputId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowedValueCoder;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowingStrategy;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.PTransformTranslation;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode.PTransformNode;\n+import org.apache.beam.runners.core.construction.graph.QueryablePipeline;\n+import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n+import org.apache.beam.runners.spark.coders.CoderHelpers;\n+import org.apache.beam.runners.spark.metrics.MetricsAccumulator;\n+import org.apache.beam.runners.spark.stateful.SparkGroupAlsoByWindowViaWindowSet;\n+import org.apache.beam.runners.spark.translation.streaming.UnboundedDataset;\n+import org.apache.beam.runners.spark.util.GlobalWatermarkHolder;\n+import org.apache.beam.runners.spark.util.SparkCompat;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.transforms.join.RawUnionValue;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.TimestampedValue;\n+import org.apache.beam.sdk.values.WindowingStrategy;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.BiMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaInputDStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Tuple2;\n+\n+/** Translates an unbounded portable pipeline into a Spark job. */\n+public class SparkStreamingPortablePipelineTranslator\n+    implements SparkPortablePipelineTranslator<SparkStreamingTranslationContext> {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SparkStreamingPortablePipelineTranslator.class);\n+\n+  private final ImmutableMap<String, PTransformTranslator> urnToTransformTranslator;\n+\n+  interface PTransformTranslator {\n+\n+    /** Translates transformNode from Beam into the Spark context. */\n+    void translate(\n+        PTransformNode transformNode,\n+        RunnerApi.Pipeline pipeline,\n+        SparkStreamingTranslationContext context);\n+  }\n+\n+  @Override\n+  public Set<String> knownUrns() {\n+    return urnToTransformTranslator.keySet();\n+  }\n+\n+  public SparkStreamingPortablePipelineTranslator() {\n+    ImmutableMap.Builder<String, PTransformTranslator> translatorMap = ImmutableMap.builder();\n+    translatorMap.put(\n+        PTransformTranslation.IMPULSE_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateImpulse);\n+    translatorMap.put(\n+        PTransformTranslation.GROUP_BY_KEY_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateGroupByKey);\n+    translatorMap.put(\n+        ExecutableStage.URN, SparkStreamingPortablePipelineTranslator::translateExecutableStage);\n+    translatorMap.put(\n+        PTransformTranslation.FLATTEN_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateFlatten);\n+    translatorMap.put(\n+        PTransformTranslation.RESHUFFLE_URN,\n+        SparkStreamingPortablePipelineTranslator::translateReshuffle);\n+    this.urnToTransformTranslator = translatorMap.build();\n+  }\n+\n+  /** Translates pipeline from Beam into the Spark context. */\n+  @Override\n+  public void translate(\n+      final RunnerApi.Pipeline pipeline, SparkStreamingTranslationContext context) {\n+    QueryablePipeline p =\n+        QueryablePipeline.forTransforms(\n+            pipeline.getRootTransformIdsList(), pipeline.getComponents());\n+    for (PipelineNode.PTransformNode transformNode : p.getTopologicallyOrderedTransforms()) {\n+      urnToTransformTranslator\n+          .getOrDefault(\n+              transformNode.getTransform().getSpec().getUrn(),\n+              SparkStreamingPortablePipelineTranslator::urnNotFound)\n+          .translate(transformNode, pipeline, context);\n+    }\n+  }\n+\n+  private static void urnNotFound(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    throw new IllegalArgumentException(\n+        String.format(\n+            \"Transform %s has unknown URN %s\",\n+            transformNode.getId(), transformNode.getTransform().getSpec().getUrn()));\n+  }\n+\n+  private static void translateImpulse(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+\n+    TimestampedValue<byte[]> tsValue = TimestampedValue.atMinimumTimestamp(new byte[0]);\n+    Iterable<TimestampedValue<byte[]>> timestampedValues = Collections.singletonList(tsValue);\n+    Iterable<WindowedValue<byte[]>> windowedValues =\n+        StreamSupport.stream(timestampedValues.spliterator(), false)\n+            .map(\n+                timestampedValue ->\n+                    WindowedValue.of(\n+                        timestampedValue.getValue(),\n+                        timestampedValue.getTimestamp(),\n+                        GlobalWindow.INSTANCE,\n+                        PaneInfo.NO_FIRING))\n+            .collect(Collectors.toList());\n+\n+    ByteArrayCoder coder = ByteArrayCoder.of();\n+    WindowedValue.FullWindowedValueCoder<byte[]> windowCoder =\n+        WindowedValue.FullWindowedValueCoder.of(coder, GlobalWindow.Coder.INSTANCE);\n+    JavaRDD<WindowedValue<byte[]>> emptyByteArrayRDD =\n+        context\n+            .getSparkContext()\n+            .parallelize(CoderHelpers.toByteArrays(windowedValues, windowCoder))\n+            .map(CoderHelpers.fromByteFunction(windowCoder));\n+\n+    Queue<JavaRDD<WindowedValue<byte[]>>> rddQueue = new LinkedBlockingQueue<>();\n+    rddQueue.offer(emptyByteArrayRDD);\n+    JavaInputDStream<WindowedValue<byte[]>> emptyStream =\n+        context.getStreamingContext().queueStream(rddQueue, true /* oneAtATime */);\n+\n+    UnboundedDataset<byte[]> output =\n+        new UnboundedDataset<>(\n+            emptyStream, Collections.singletonList(emptyStream.inputDStream().id()));\n+\n+    // Add watermark to holder and advance to infinity to ensure future watermarks can be updated\n+    GlobalWatermarkHolder.SparkWatermarks sparkWatermark =\n+        new GlobalWatermarkHolder.SparkWatermarks(\n+            GlobalWindow.INSTANCE.maxTimestamp(),\n+            BoundedWindow.TIMESTAMP_MAX_VALUE,\n+            context.getFirstTimestamp());\n+    GlobalWatermarkHolder.add(output.getStreamSources().get(0), sparkWatermark);\n+\n+    context.pushDataset(getOutputId(transformNode), output);\n+  }\n+\n+  private static <K, V> void translateGroupByKey(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    RunnerApi.Components components = pipeline.getComponents();\n+    String inputId = getInputId(transformNode);\n+    UnboundedDataset<KV<K, V>> inputDataset =\n+        (UnboundedDataset<KV<K, V>>) context.popDataset(inputId);\n+    List<Integer> streamSources = inputDataset.getStreamSources();\n+    JavaDStream<WindowedValue<KV<K, V>>> dStream = inputDataset.getDStream();\n+    WindowedValue.WindowedValueCoder<KV<K, V>> inputCoder =\n+        getWindowedValueCoder(inputId, components);\n+    KvCoder<K, V> inputKvCoder = (KvCoder<K, V>) inputCoder.getValueCoder();\n+    Coder<K> inputKeyCoder = inputKvCoder.getKeyCoder();\n+    Coder<V> inputValueCoder = inputKvCoder.getValueCoder();\n+    final WindowingStrategy windowingStrategy = getWindowingStrategy(inputId, components);\n+    final WindowFn<Object, BoundedWindow> windowFn = windowingStrategy.getWindowFn();\n+    final WindowedValue.WindowedValueCoder<V> wvCoder =\n+        WindowedValue.FullWindowedValueCoder.of(inputValueCoder, windowFn.windowCoder());\n+\n+    JavaDStream<WindowedValue<KV<K, Iterable<V>>>> outStream =\n+        SparkGroupAlsoByWindowViaWindowSet.groupByKeyAndWindow(\n+            dStream,\n+            inputKeyCoder,\n+            wvCoder,\n+            windowingStrategy,\n+            context.getSerializableOptions(),\n+            streamSources,\n+            transformNode.getId());\n+\n+    context.pushDataset(\n+        getOutputId(transformNode), new UnboundedDataset<>(outStream, streamSources));\n+  }\n+\n+  private static <InputT, OutputT, SideInputT> void translateExecutableStage(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    RunnerApi.ExecutableStagePayload stagePayload;\n+    try {\n+      stagePayload =\n+          RunnerApi.ExecutableStagePayload.parseFrom(\n+              transformNode.getTransform().getSpec().getPayload());\n+    } catch (IOException e) {\n+      throw new RuntimeException(e);\n+    }\n+\n+    String inputPCollectionId = stagePayload.getInput();\n+    UnboundedDataset<InputT> inputDataset =\n+        (UnboundedDataset<InputT>) context.popDataset(inputPCollectionId);\n+    List<Integer> streamSources = inputDataset.getStreamSources();\n+    JavaDStream<WindowedValue<InputT>> inputDStream = inputDataset.getDStream();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca707137600be0b54bc95e378546620a7ce77887"}, "originalPosition": 242}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDM5MTAwMg==", "bodyText": "Trow a clear exception here if the executable stage declares itself as having side inputs.", "url": "https://github.com/apache/beam/pull/12157#discussion_r474391002", "createdAt": "2020-08-21T03:40:49Z", "author": {"login": "robertwb"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkStreamingPortablePipelineTranslator.java", "diffHunk": "@@ -0,0 +1,366 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark.translation;\n+\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.createOutputMap;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getExecutableStageIntermediateId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getInputId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getOutputId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowedValueCoder;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowingStrategy;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.PTransformTranslation;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode.PTransformNode;\n+import org.apache.beam.runners.core.construction.graph.QueryablePipeline;\n+import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n+import org.apache.beam.runners.spark.coders.CoderHelpers;\n+import org.apache.beam.runners.spark.metrics.MetricsAccumulator;\n+import org.apache.beam.runners.spark.stateful.SparkGroupAlsoByWindowViaWindowSet;\n+import org.apache.beam.runners.spark.translation.streaming.UnboundedDataset;\n+import org.apache.beam.runners.spark.util.GlobalWatermarkHolder;\n+import org.apache.beam.runners.spark.util.SparkCompat;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.transforms.join.RawUnionValue;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.TimestampedValue;\n+import org.apache.beam.sdk.values.WindowingStrategy;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.BiMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaInputDStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Tuple2;\n+\n+/** Translates an unbounded portable pipeline into a Spark job. */\n+public class SparkStreamingPortablePipelineTranslator\n+    implements SparkPortablePipelineTranslator<SparkStreamingTranslationContext> {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SparkStreamingPortablePipelineTranslator.class);\n+\n+  private final ImmutableMap<String, PTransformTranslator> urnToTransformTranslator;\n+\n+  interface PTransformTranslator {\n+\n+    /** Translates transformNode from Beam into the Spark context. */\n+    void translate(\n+        PTransformNode transformNode,\n+        RunnerApi.Pipeline pipeline,\n+        SparkStreamingTranslationContext context);\n+  }\n+\n+  @Override\n+  public Set<String> knownUrns() {\n+    return urnToTransformTranslator.keySet();\n+  }\n+\n+  public SparkStreamingPortablePipelineTranslator() {\n+    ImmutableMap.Builder<String, PTransformTranslator> translatorMap = ImmutableMap.builder();\n+    translatorMap.put(\n+        PTransformTranslation.IMPULSE_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateImpulse);\n+    translatorMap.put(\n+        PTransformTranslation.GROUP_BY_KEY_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateGroupByKey);\n+    translatorMap.put(\n+        ExecutableStage.URN, SparkStreamingPortablePipelineTranslator::translateExecutableStage);\n+    translatorMap.put(\n+        PTransformTranslation.FLATTEN_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateFlatten);\n+    translatorMap.put(\n+        PTransformTranslation.RESHUFFLE_URN,\n+        SparkStreamingPortablePipelineTranslator::translateReshuffle);\n+    this.urnToTransformTranslator = translatorMap.build();\n+  }\n+\n+  /** Translates pipeline from Beam into the Spark context. */\n+  @Override\n+  public void translate(\n+      final RunnerApi.Pipeline pipeline, SparkStreamingTranslationContext context) {\n+    QueryablePipeline p =\n+        QueryablePipeline.forTransforms(\n+            pipeline.getRootTransformIdsList(), pipeline.getComponents());\n+    for (PipelineNode.PTransformNode transformNode : p.getTopologicallyOrderedTransforms()) {\n+      urnToTransformTranslator\n+          .getOrDefault(\n+              transformNode.getTransform().getSpec().getUrn(),\n+              SparkStreamingPortablePipelineTranslator::urnNotFound)\n+          .translate(transformNode, pipeline, context);\n+    }\n+  }\n+\n+  private static void urnNotFound(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    throw new IllegalArgumentException(\n+        String.format(\n+            \"Transform %s has unknown URN %s\",\n+            transformNode.getId(), transformNode.getTransform().getSpec().getUrn()));\n+  }\n+\n+  private static void translateImpulse(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+\n+    TimestampedValue<byte[]> tsValue = TimestampedValue.atMinimumTimestamp(new byte[0]);\n+    Iterable<TimestampedValue<byte[]>> timestampedValues = Collections.singletonList(tsValue);\n+    Iterable<WindowedValue<byte[]>> windowedValues =\n+        StreamSupport.stream(timestampedValues.spliterator(), false)\n+            .map(\n+                timestampedValue ->\n+                    WindowedValue.of(\n+                        timestampedValue.getValue(),\n+                        timestampedValue.getTimestamp(),\n+                        GlobalWindow.INSTANCE,\n+                        PaneInfo.NO_FIRING))\n+            .collect(Collectors.toList());\n+\n+    ByteArrayCoder coder = ByteArrayCoder.of();\n+    WindowedValue.FullWindowedValueCoder<byte[]> windowCoder =\n+        WindowedValue.FullWindowedValueCoder.of(coder, GlobalWindow.Coder.INSTANCE);\n+    JavaRDD<WindowedValue<byte[]>> emptyByteArrayRDD =\n+        context\n+            .getSparkContext()\n+            .parallelize(CoderHelpers.toByteArrays(windowedValues, windowCoder))\n+            .map(CoderHelpers.fromByteFunction(windowCoder));\n+\n+    Queue<JavaRDD<WindowedValue<byte[]>>> rddQueue = new LinkedBlockingQueue<>();\n+    rddQueue.offer(emptyByteArrayRDD);\n+    JavaInputDStream<WindowedValue<byte[]>> emptyStream =\n+        context.getStreamingContext().queueStream(rddQueue, true /* oneAtATime */);\n+\n+    UnboundedDataset<byte[]> output =\n+        new UnboundedDataset<>(\n+            emptyStream, Collections.singletonList(emptyStream.inputDStream().id()));\n+\n+    // Add watermark to holder and advance to infinity to ensure future watermarks can be updated\n+    GlobalWatermarkHolder.SparkWatermarks sparkWatermark =\n+        new GlobalWatermarkHolder.SparkWatermarks(\n+            GlobalWindow.INSTANCE.maxTimestamp(),\n+            BoundedWindow.TIMESTAMP_MAX_VALUE,\n+            context.getFirstTimestamp());\n+    GlobalWatermarkHolder.add(output.getStreamSources().get(0), sparkWatermark);\n+\n+    context.pushDataset(getOutputId(transformNode), output);\n+  }\n+\n+  private static <K, V> void translateGroupByKey(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    RunnerApi.Components components = pipeline.getComponents();\n+    String inputId = getInputId(transformNode);\n+    UnboundedDataset<KV<K, V>> inputDataset =\n+        (UnboundedDataset<KV<K, V>>) context.popDataset(inputId);\n+    List<Integer> streamSources = inputDataset.getStreamSources();\n+    JavaDStream<WindowedValue<KV<K, V>>> dStream = inputDataset.getDStream();\n+    WindowedValue.WindowedValueCoder<KV<K, V>> inputCoder =\n+        getWindowedValueCoder(inputId, components);\n+    KvCoder<K, V> inputKvCoder = (KvCoder<K, V>) inputCoder.getValueCoder();\n+    Coder<K> inputKeyCoder = inputKvCoder.getKeyCoder();\n+    Coder<V> inputValueCoder = inputKvCoder.getValueCoder();\n+    final WindowingStrategy windowingStrategy = getWindowingStrategy(inputId, components);\n+    final WindowFn<Object, BoundedWindow> windowFn = windowingStrategy.getWindowFn();\n+    final WindowedValue.WindowedValueCoder<V> wvCoder =\n+        WindowedValue.FullWindowedValueCoder.of(inputValueCoder, windowFn.windowCoder());\n+\n+    JavaDStream<WindowedValue<KV<K, Iterable<V>>>> outStream =\n+        SparkGroupAlsoByWindowViaWindowSet.groupByKeyAndWindow(\n+            dStream,\n+            inputKeyCoder,\n+            wvCoder,\n+            windowingStrategy,\n+            context.getSerializableOptions(),\n+            streamSources,\n+            transformNode.getId());\n+\n+    context.pushDataset(\n+        getOutputId(transformNode), new UnboundedDataset<>(outStream, streamSources));\n+  }\n+\n+  private static <InputT, OutputT, SideInputT> void translateExecutableStage(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    RunnerApi.ExecutableStagePayload stagePayload;\n+    try {\n+      stagePayload =\n+          RunnerApi.ExecutableStagePayload.parseFrom(\n+              transformNode.getTransform().getSpec().getPayload());\n+    } catch (IOException e) {\n+      throw new RuntimeException(e);\n+    }\n+\n+    String inputPCollectionId = stagePayload.getInput();\n+    UnboundedDataset<InputT> inputDataset =\n+        (UnboundedDataset<InputT>) context.popDataset(inputPCollectionId);\n+    List<Integer> streamSources = inputDataset.getStreamSources();\n+    JavaDStream<WindowedValue<InputT>> inputDStream = inputDataset.getDStream();\n+    Map<String, String> outputs = transformNode.getTransform().getOutputsMap();\n+    BiMap<String, Integer> outputMap = createOutputMap(outputs.values());\n+\n+    RunnerApi.Components components = pipeline.getComponents();\n+    Coder windowCoder =\n+        getWindowingStrategy(inputPCollectionId, components).getWindowFn().windowCoder();\n+\n+    // TODO (BEAM-10712): handle side inputs.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca707137600be0b54bc95e378546620a7ce77887"}, "originalPosition": 250}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDM5MTcyMQ==", "bodyText": "In this case there are no outputs (per the above line). Perhaps just say something to the effect that we want to ensure the stage is executed.", "url": "https://github.com/apache/beam/pull/12157#discussion_r474391721", "createdAt": "2020-08-21T03:44:17Z", "author": {"login": "robertwb"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkStreamingPortablePipelineTranslator.java", "diffHunk": "@@ -0,0 +1,366 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark.translation;\n+\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.createOutputMap;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getExecutableStageIntermediateId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getInputId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getOutputId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowedValueCoder;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowingStrategy;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.PTransformTranslation;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode.PTransformNode;\n+import org.apache.beam.runners.core.construction.graph.QueryablePipeline;\n+import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n+import org.apache.beam.runners.spark.coders.CoderHelpers;\n+import org.apache.beam.runners.spark.metrics.MetricsAccumulator;\n+import org.apache.beam.runners.spark.stateful.SparkGroupAlsoByWindowViaWindowSet;\n+import org.apache.beam.runners.spark.translation.streaming.UnboundedDataset;\n+import org.apache.beam.runners.spark.util.GlobalWatermarkHolder;\n+import org.apache.beam.runners.spark.util.SparkCompat;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.transforms.join.RawUnionValue;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.TimestampedValue;\n+import org.apache.beam.sdk.values.WindowingStrategy;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.BiMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaInputDStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Tuple2;\n+\n+/** Translates an unbounded portable pipeline into a Spark job. */\n+public class SparkStreamingPortablePipelineTranslator\n+    implements SparkPortablePipelineTranslator<SparkStreamingTranslationContext> {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SparkStreamingPortablePipelineTranslator.class);\n+\n+  private final ImmutableMap<String, PTransformTranslator> urnToTransformTranslator;\n+\n+  interface PTransformTranslator {\n+\n+    /** Translates transformNode from Beam into the Spark context. */\n+    void translate(\n+        PTransformNode transformNode,\n+        RunnerApi.Pipeline pipeline,\n+        SparkStreamingTranslationContext context);\n+  }\n+\n+  @Override\n+  public Set<String> knownUrns() {\n+    return urnToTransformTranslator.keySet();\n+  }\n+\n+  public SparkStreamingPortablePipelineTranslator() {\n+    ImmutableMap.Builder<String, PTransformTranslator> translatorMap = ImmutableMap.builder();\n+    translatorMap.put(\n+        PTransformTranslation.IMPULSE_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateImpulse);\n+    translatorMap.put(\n+        PTransformTranslation.GROUP_BY_KEY_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateGroupByKey);\n+    translatorMap.put(\n+        ExecutableStage.URN, SparkStreamingPortablePipelineTranslator::translateExecutableStage);\n+    translatorMap.put(\n+        PTransformTranslation.FLATTEN_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateFlatten);\n+    translatorMap.put(\n+        PTransformTranslation.RESHUFFLE_URN,\n+        SparkStreamingPortablePipelineTranslator::translateReshuffle);\n+    this.urnToTransformTranslator = translatorMap.build();\n+  }\n+\n+  /** Translates pipeline from Beam into the Spark context. */\n+  @Override\n+  public void translate(\n+      final RunnerApi.Pipeline pipeline, SparkStreamingTranslationContext context) {\n+    QueryablePipeline p =\n+        QueryablePipeline.forTransforms(\n+            pipeline.getRootTransformIdsList(), pipeline.getComponents());\n+    for (PipelineNode.PTransformNode transformNode : p.getTopologicallyOrderedTransforms()) {\n+      urnToTransformTranslator\n+          .getOrDefault(\n+              transformNode.getTransform().getSpec().getUrn(),\n+              SparkStreamingPortablePipelineTranslator::urnNotFound)\n+          .translate(transformNode, pipeline, context);\n+    }\n+  }\n+\n+  private static void urnNotFound(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    throw new IllegalArgumentException(\n+        String.format(\n+            \"Transform %s has unknown URN %s\",\n+            transformNode.getId(), transformNode.getTransform().getSpec().getUrn()));\n+  }\n+\n+  private static void translateImpulse(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+\n+    TimestampedValue<byte[]> tsValue = TimestampedValue.atMinimumTimestamp(new byte[0]);\n+    Iterable<TimestampedValue<byte[]>> timestampedValues = Collections.singletonList(tsValue);\n+    Iterable<WindowedValue<byte[]>> windowedValues =\n+        StreamSupport.stream(timestampedValues.spliterator(), false)\n+            .map(\n+                timestampedValue ->\n+                    WindowedValue.of(\n+                        timestampedValue.getValue(),\n+                        timestampedValue.getTimestamp(),\n+                        GlobalWindow.INSTANCE,\n+                        PaneInfo.NO_FIRING))\n+            .collect(Collectors.toList());\n+\n+    ByteArrayCoder coder = ByteArrayCoder.of();\n+    WindowedValue.FullWindowedValueCoder<byte[]> windowCoder =\n+        WindowedValue.FullWindowedValueCoder.of(coder, GlobalWindow.Coder.INSTANCE);\n+    JavaRDD<WindowedValue<byte[]>> emptyByteArrayRDD =\n+        context\n+            .getSparkContext()\n+            .parallelize(CoderHelpers.toByteArrays(windowedValues, windowCoder))\n+            .map(CoderHelpers.fromByteFunction(windowCoder));\n+\n+    Queue<JavaRDD<WindowedValue<byte[]>>> rddQueue = new LinkedBlockingQueue<>();\n+    rddQueue.offer(emptyByteArrayRDD);\n+    JavaInputDStream<WindowedValue<byte[]>> emptyStream =\n+        context.getStreamingContext().queueStream(rddQueue, true /* oneAtATime */);\n+\n+    UnboundedDataset<byte[]> output =\n+        new UnboundedDataset<>(\n+            emptyStream, Collections.singletonList(emptyStream.inputDStream().id()));\n+\n+    // Add watermark to holder and advance to infinity to ensure future watermarks can be updated\n+    GlobalWatermarkHolder.SparkWatermarks sparkWatermark =\n+        new GlobalWatermarkHolder.SparkWatermarks(\n+            GlobalWindow.INSTANCE.maxTimestamp(),\n+            BoundedWindow.TIMESTAMP_MAX_VALUE,\n+            context.getFirstTimestamp());\n+    GlobalWatermarkHolder.add(output.getStreamSources().get(0), sparkWatermark);\n+\n+    context.pushDataset(getOutputId(transformNode), output);\n+  }\n+\n+  private static <K, V> void translateGroupByKey(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    RunnerApi.Components components = pipeline.getComponents();\n+    String inputId = getInputId(transformNode);\n+    UnboundedDataset<KV<K, V>> inputDataset =\n+        (UnboundedDataset<KV<K, V>>) context.popDataset(inputId);\n+    List<Integer> streamSources = inputDataset.getStreamSources();\n+    JavaDStream<WindowedValue<KV<K, V>>> dStream = inputDataset.getDStream();\n+    WindowedValue.WindowedValueCoder<KV<K, V>> inputCoder =\n+        getWindowedValueCoder(inputId, components);\n+    KvCoder<K, V> inputKvCoder = (KvCoder<K, V>) inputCoder.getValueCoder();\n+    Coder<K> inputKeyCoder = inputKvCoder.getKeyCoder();\n+    Coder<V> inputValueCoder = inputKvCoder.getValueCoder();\n+    final WindowingStrategy windowingStrategy = getWindowingStrategy(inputId, components);\n+    final WindowFn<Object, BoundedWindow> windowFn = windowingStrategy.getWindowFn();\n+    final WindowedValue.WindowedValueCoder<V> wvCoder =\n+        WindowedValue.FullWindowedValueCoder.of(inputValueCoder, windowFn.windowCoder());\n+\n+    JavaDStream<WindowedValue<KV<K, Iterable<V>>>> outStream =\n+        SparkGroupAlsoByWindowViaWindowSet.groupByKeyAndWindow(\n+            dStream,\n+            inputKeyCoder,\n+            wvCoder,\n+            windowingStrategy,\n+            context.getSerializableOptions(),\n+            streamSources,\n+            transformNode.getId());\n+\n+    context.pushDataset(\n+        getOutputId(transformNode), new UnboundedDataset<>(outStream, streamSources));\n+  }\n+\n+  private static <InputT, OutputT, SideInputT> void translateExecutableStage(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    RunnerApi.ExecutableStagePayload stagePayload;\n+    try {\n+      stagePayload =\n+          RunnerApi.ExecutableStagePayload.parseFrom(\n+              transformNode.getTransform().getSpec().getPayload());\n+    } catch (IOException e) {\n+      throw new RuntimeException(e);\n+    }\n+\n+    String inputPCollectionId = stagePayload.getInput();\n+    UnboundedDataset<InputT> inputDataset =\n+        (UnboundedDataset<InputT>) context.popDataset(inputPCollectionId);\n+    List<Integer> streamSources = inputDataset.getStreamSources();\n+    JavaDStream<WindowedValue<InputT>> inputDStream = inputDataset.getDStream();\n+    Map<String, String> outputs = transformNode.getTransform().getOutputsMap();\n+    BiMap<String, Integer> outputMap = createOutputMap(outputs.values());\n+\n+    RunnerApi.Components components = pipeline.getComponents();\n+    Coder windowCoder =\n+        getWindowingStrategy(inputPCollectionId, components).getWindowFn().windowCoder();\n+\n+    // TODO (BEAM-10712): handle side inputs.\n+    ImmutableMap<\n+            String, Tuple2<Broadcast<List<byte[]>>, WindowedValue.WindowedValueCoder<SideInputT>>>\n+        broadcastVariables = ImmutableMap.copyOf(new HashMap<>());\n+\n+    SparkExecutableStageFunction<InputT, SideInputT> function =\n+        new SparkExecutableStageFunction<>(\n+            stagePayload,\n+            context.jobInfo,\n+            outputMap,\n+            SparkExecutableStageContextFactory.getInstance(),\n+            broadcastVariables,\n+            MetricsAccumulator.getInstance(),\n+            windowCoder);\n+    JavaDStream<RawUnionValue> staged = inputDStream.mapPartitions(function);\n+\n+    String intermediateId = getExecutableStageIntermediateId(transformNode);\n+    context.pushDataset(\n+        intermediateId,\n+        new Dataset() {\n+          @Override\n+          public void cache(String storageLevel, Coder<?> coder) {\n+            StorageLevel level = StorageLevel.fromString(storageLevel);\n+            staged.persist(level);\n+          }\n+\n+          @Override\n+          public void action() {\n+            // Empty function to force computation of RDD.\n+            staged.foreachRDD(TranslationUtils.emptyVoidFunction());\n+          }\n+\n+          @Override\n+          public void setName(String name) {\n+            // ignore\n+          }\n+        });\n+    // pop dataset to mark DStream as used\n+    context.popDataset(intermediateId);\n+\n+    for (String outputId : outputs.values()) {\n+      JavaDStream<WindowedValue<OutputT>> outStream =\n+          staged.flatMap(new SparkExecutableStageExtractionFunction<>(outputMap.get(outputId)));\n+      context.pushDataset(outputId, new UnboundedDataset<>(outStream, streamSources));\n+    }\n+\n+    if (outputs.isEmpty()) {\n+      // Add sink to ensure all outputs are computed", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca707137600be0b54bc95e378546620a7ce77887"}, "originalPosition": 297}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ3NDM5MTg5Nw==", "bodyText": "File and reference a JIRA.", "url": "https://github.com/apache/beam/pull/12157#discussion_r474391897", "createdAt": "2020-08-21T03:45:02Z", "author": {"login": "robertwb"}, "path": "runners/spark/src/main/java/org/apache/beam/runners/spark/translation/SparkStreamingPortablePipelineTranslator.java", "diffHunk": "@@ -0,0 +1,366 @@\n+/*\n+ * Licensed to the Apache Software Foundation (ASF) under one\n+ * or more contributor license agreements.  See the NOTICE file\n+ * distributed with this work for additional information\n+ * regarding copyright ownership.  The ASF licenses this file\n+ * to you under the Apache License, Version 2.0 (the\n+ * \"License\"); you may not use this file except in compliance\n+ * with the License.  You may obtain a copy of the License at\n+ *\n+ *     http://www.apache.org/licenses/LICENSE-2.0\n+ *\n+ * Unless required by applicable law or agreed to in writing, software\n+ * distributed under the License is distributed on an \"AS IS\" BASIS,\n+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+ * See the License for the specific language governing permissions and\n+ * limitations under the License.\n+ */\n+package org.apache.beam.runners.spark.translation;\n+\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.createOutputMap;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getExecutableStageIntermediateId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getInputId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getOutputId;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowedValueCoder;\n+import static org.apache.beam.runners.fnexecution.translation.PipelineTranslatorUtils.getWindowingStrategy;\n+\n+import java.io.IOException;\n+import java.util.ArrayList;\n+import java.util.Collections;\n+import java.util.HashMap;\n+import java.util.List;\n+import java.util.Map;\n+import java.util.Queue;\n+import java.util.Set;\n+import java.util.concurrent.LinkedBlockingQueue;\n+import java.util.stream.Collectors;\n+import java.util.stream.StreamSupport;\n+import org.apache.beam.model.pipeline.v1.RunnerApi;\n+import org.apache.beam.runners.core.construction.PTransformTranslation;\n+import org.apache.beam.runners.core.construction.graph.ExecutableStage;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode;\n+import org.apache.beam.runners.core.construction.graph.PipelineNode.PTransformNode;\n+import org.apache.beam.runners.core.construction.graph.QueryablePipeline;\n+import org.apache.beam.runners.fnexecution.provisioning.JobInfo;\n+import org.apache.beam.runners.spark.SparkPipelineOptions;\n+import org.apache.beam.runners.spark.coders.CoderHelpers;\n+import org.apache.beam.runners.spark.metrics.MetricsAccumulator;\n+import org.apache.beam.runners.spark.stateful.SparkGroupAlsoByWindowViaWindowSet;\n+import org.apache.beam.runners.spark.translation.streaming.UnboundedDataset;\n+import org.apache.beam.runners.spark.util.GlobalWatermarkHolder;\n+import org.apache.beam.runners.spark.util.SparkCompat;\n+import org.apache.beam.sdk.coders.ByteArrayCoder;\n+import org.apache.beam.sdk.coders.Coder;\n+import org.apache.beam.sdk.coders.KvCoder;\n+import org.apache.beam.sdk.transforms.join.RawUnionValue;\n+import org.apache.beam.sdk.transforms.windowing.BoundedWindow;\n+import org.apache.beam.sdk.transforms.windowing.GlobalWindow;\n+import org.apache.beam.sdk.transforms.windowing.PaneInfo;\n+import org.apache.beam.sdk.transforms.windowing.WindowFn;\n+import org.apache.beam.sdk.util.WindowedValue;\n+import org.apache.beam.sdk.values.KV;\n+import org.apache.beam.sdk.values.TimestampedValue;\n+import org.apache.beam.sdk.values.WindowingStrategy;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.BiMap;\n+import org.apache.beam.vendor.guava.v26_0_jre.com.google.common.collect.ImmutableMap;\n+import org.apache.spark.api.java.JavaRDD;\n+import org.apache.spark.api.java.JavaSparkContext;\n+import org.apache.spark.broadcast.Broadcast;\n+import org.apache.spark.storage.StorageLevel;\n+import org.apache.spark.streaming.api.java.JavaDStream;\n+import org.apache.spark.streaming.api.java.JavaInputDStream;\n+import org.slf4j.Logger;\n+import org.slf4j.LoggerFactory;\n+import scala.Tuple2;\n+\n+/** Translates an unbounded portable pipeline into a Spark job. */\n+public class SparkStreamingPortablePipelineTranslator\n+    implements SparkPortablePipelineTranslator<SparkStreamingTranslationContext> {\n+\n+  private static final Logger LOG =\n+      LoggerFactory.getLogger(SparkStreamingPortablePipelineTranslator.class);\n+\n+  private final ImmutableMap<String, PTransformTranslator> urnToTransformTranslator;\n+\n+  interface PTransformTranslator {\n+\n+    /** Translates transformNode from Beam into the Spark context. */\n+    void translate(\n+        PTransformNode transformNode,\n+        RunnerApi.Pipeline pipeline,\n+        SparkStreamingTranslationContext context);\n+  }\n+\n+  @Override\n+  public Set<String> knownUrns() {\n+    return urnToTransformTranslator.keySet();\n+  }\n+\n+  public SparkStreamingPortablePipelineTranslator() {\n+    ImmutableMap.Builder<String, PTransformTranslator> translatorMap = ImmutableMap.builder();\n+    translatorMap.put(\n+        PTransformTranslation.IMPULSE_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateImpulse);\n+    translatorMap.put(\n+        PTransformTranslation.GROUP_BY_KEY_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateGroupByKey);\n+    translatorMap.put(\n+        ExecutableStage.URN, SparkStreamingPortablePipelineTranslator::translateExecutableStage);\n+    translatorMap.put(\n+        PTransformTranslation.FLATTEN_TRANSFORM_URN,\n+        SparkStreamingPortablePipelineTranslator::translateFlatten);\n+    translatorMap.put(\n+        PTransformTranslation.RESHUFFLE_URN,\n+        SparkStreamingPortablePipelineTranslator::translateReshuffle);\n+    this.urnToTransformTranslator = translatorMap.build();\n+  }\n+\n+  /** Translates pipeline from Beam into the Spark context. */\n+  @Override\n+  public void translate(\n+      final RunnerApi.Pipeline pipeline, SparkStreamingTranslationContext context) {\n+    QueryablePipeline p =\n+        QueryablePipeline.forTransforms(\n+            pipeline.getRootTransformIdsList(), pipeline.getComponents());\n+    for (PipelineNode.PTransformNode transformNode : p.getTopologicallyOrderedTransforms()) {\n+      urnToTransformTranslator\n+          .getOrDefault(\n+              transformNode.getTransform().getSpec().getUrn(),\n+              SparkStreamingPortablePipelineTranslator::urnNotFound)\n+          .translate(transformNode, pipeline, context);\n+    }\n+  }\n+\n+  private static void urnNotFound(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    throw new IllegalArgumentException(\n+        String.format(\n+            \"Transform %s has unknown URN %s\",\n+            transformNode.getId(), transformNode.getTransform().getSpec().getUrn()));\n+  }\n+\n+  private static void translateImpulse(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+\n+    TimestampedValue<byte[]> tsValue = TimestampedValue.atMinimumTimestamp(new byte[0]);\n+    Iterable<TimestampedValue<byte[]>> timestampedValues = Collections.singletonList(tsValue);\n+    Iterable<WindowedValue<byte[]>> windowedValues =\n+        StreamSupport.stream(timestampedValues.spliterator(), false)\n+            .map(\n+                timestampedValue ->\n+                    WindowedValue.of(\n+                        timestampedValue.getValue(),\n+                        timestampedValue.getTimestamp(),\n+                        GlobalWindow.INSTANCE,\n+                        PaneInfo.NO_FIRING))\n+            .collect(Collectors.toList());\n+\n+    ByteArrayCoder coder = ByteArrayCoder.of();\n+    WindowedValue.FullWindowedValueCoder<byte[]> windowCoder =\n+        WindowedValue.FullWindowedValueCoder.of(coder, GlobalWindow.Coder.INSTANCE);\n+    JavaRDD<WindowedValue<byte[]>> emptyByteArrayRDD =\n+        context\n+            .getSparkContext()\n+            .parallelize(CoderHelpers.toByteArrays(windowedValues, windowCoder))\n+            .map(CoderHelpers.fromByteFunction(windowCoder));\n+\n+    Queue<JavaRDD<WindowedValue<byte[]>>> rddQueue = new LinkedBlockingQueue<>();\n+    rddQueue.offer(emptyByteArrayRDD);\n+    JavaInputDStream<WindowedValue<byte[]>> emptyStream =\n+        context.getStreamingContext().queueStream(rddQueue, true /* oneAtATime */);\n+\n+    UnboundedDataset<byte[]> output =\n+        new UnboundedDataset<>(\n+            emptyStream, Collections.singletonList(emptyStream.inputDStream().id()));\n+\n+    // Add watermark to holder and advance to infinity to ensure future watermarks can be updated\n+    GlobalWatermarkHolder.SparkWatermarks sparkWatermark =\n+        new GlobalWatermarkHolder.SparkWatermarks(\n+            GlobalWindow.INSTANCE.maxTimestamp(),\n+            BoundedWindow.TIMESTAMP_MAX_VALUE,\n+            context.getFirstTimestamp());\n+    GlobalWatermarkHolder.add(output.getStreamSources().get(0), sparkWatermark);\n+\n+    context.pushDataset(getOutputId(transformNode), output);\n+  }\n+\n+  private static <K, V> void translateGroupByKey(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    RunnerApi.Components components = pipeline.getComponents();\n+    String inputId = getInputId(transformNode);\n+    UnboundedDataset<KV<K, V>> inputDataset =\n+        (UnboundedDataset<KV<K, V>>) context.popDataset(inputId);\n+    List<Integer> streamSources = inputDataset.getStreamSources();\n+    JavaDStream<WindowedValue<KV<K, V>>> dStream = inputDataset.getDStream();\n+    WindowedValue.WindowedValueCoder<KV<K, V>> inputCoder =\n+        getWindowedValueCoder(inputId, components);\n+    KvCoder<K, V> inputKvCoder = (KvCoder<K, V>) inputCoder.getValueCoder();\n+    Coder<K> inputKeyCoder = inputKvCoder.getKeyCoder();\n+    Coder<V> inputValueCoder = inputKvCoder.getValueCoder();\n+    final WindowingStrategy windowingStrategy = getWindowingStrategy(inputId, components);\n+    final WindowFn<Object, BoundedWindow> windowFn = windowingStrategy.getWindowFn();\n+    final WindowedValue.WindowedValueCoder<V> wvCoder =\n+        WindowedValue.FullWindowedValueCoder.of(inputValueCoder, windowFn.windowCoder());\n+\n+    JavaDStream<WindowedValue<KV<K, Iterable<V>>>> outStream =\n+        SparkGroupAlsoByWindowViaWindowSet.groupByKeyAndWindow(\n+            dStream,\n+            inputKeyCoder,\n+            wvCoder,\n+            windowingStrategy,\n+            context.getSerializableOptions(),\n+            streamSources,\n+            transformNode.getId());\n+\n+    context.pushDataset(\n+        getOutputId(transformNode), new UnboundedDataset<>(outStream, streamSources));\n+  }\n+\n+  private static <InputT, OutputT, SideInputT> void translateExecutableStage(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    RunnerApi.ExecutableStagePayload stagePayload;\n+    try {\n+      stagePayload =\n+          RunnerApi.ExecutableStagePayload.parseFrom(\n+              transformNode.getTransform().getSpec().getPayload());\n+    } catch (IOException e) {\n+      throw new RuntimeException(e);\n+    }\n+\n+    String inputPCollectionId = stagePayload.getInput();\n+    UnboundedDataset<InputT> inputDataset =\n+        (UnboundedDataset<InputT>) context.popDataset(inputPCollectionId);\n+    List<Integer> streamSources = inputDataset.getStreamSources();\n+    JavaDStream<WindowedValue<InputT>> inputDStream = inputDataset.getDStream();\n+    Map<String, String> outputs = transformNode.getTransform().getOutputsMap();\n+    BiMap<String, Integer> outputMap = createOutputMap(outputs.values());\n+\n+    RunnerApi.Components components = pipeline.getComponents();\n+    Coder windowCoder =\n+        getWindowingStrategy(inputPCollectionId, components).getWindowFn().windowCoder();\n+\n+    // TODO (BEAM-10712): handle side inputs.\n+    ImmutableMap<\n+            String, Tuple2<Broadcast<List<byte[]>>, WindowedValue.WindowedValueCoder<SideInputT>>>\n+        broadcastVariables = ImmutableMap.copyOf(new HashMap<>());\n+\n+    SparkExecutableStageFunction<InputT, SideInputT> function =\n+        new SparkExecutableStageFunction<>(\n+            stagePayload,\n+            context.jobInfo,\n+            outputMap,\n+            SparkExecutableStageContextFactory.getInstance(),\n+            broadcastVariables,\n+            MetricsAccumulator.getInstance(),\n+            windowCoder);\n+    JavaDStream<RawUnionValue> staged = inputDStream.mapPartitions(function);\n+\n+    String intermediateId = getExecutableStageIntermediateId(transformNode);\n+    context.pushDataset(\n+        intermediateId,\n+        new Dataset() {\n+          @Override\n+          public void cache(String storageLevel, Coder<?> coder) {\n+            StorageLevel level = StorageLevel.fromString(storageLevel);\n+            staged.persist(level);\n+          }\n+\n+          @Override\n+          public void action() {\n+            // Empty function to force computation of RDD.\n+            staged.foreachRDD(TranslationUtils.emptyVoidFunction());\n+          }\n+\n+          @Override\n+          public void setName(String name) {\n+            // ignore\n+          }\n+        });\n+    // pop dataset to mark DStream as used\n+    context.popDataset(intermediateId);\n+\n+    for (String outputId : outputs.values()) {\n+      JavaDStream<WindowedValue<OutputT>> outStream =\n+          staged.flatMap(new SparkExecutableStageExtractionFunction<>(outputMap.get(outputId)));\n+      context.pushDataset(outputId, new UnboundedDataset<>(outStream, streamSources));\n+    }\n+\n+    if (outputs.isEmpty()) {\n+      // Add sink to ensure all outputs are computed\n+      JavaDStream<WindowedValue<OutputT>> outStream =\n+          staged.flatMap((rawUnionValue) -> Collections.emptyIterator());\n+      context.pushDataset(\n+          String.format(\"EmptyOutputSink_%d\", context.nextSinkId()),\n+          new UnboundedDataset<>(outStream, streamSources));\n+    }\n+  }\n+\n+  private static <T> void translateFlatten(\n+      PTransformNode transformNode,\n+      RunnerApi.Pipeline pipeline,\n+      SparkStreamingTranslationContext context) {\n+    Map<String, String> inputsMap = transformNode.getTransform().getInputsMap();\n+    JavaDStream<WindowedValue<T>> unifiedStreams;\n+    final List<Integer> streamSources = new ArrayList<>();\n+\n+    if (inputsMap.isEmpty()) {\n+      Queue<JavaRDD<WindowedValue<T>>> q = new LinkedBlockingQueue<>();\n+      q.offer(context.getSparkContext().emptyRDD());\n+      unifiedStreams = context.getStreamingContext().queueStream(q);\n+    } else {\n+      final List<JavaDStream<WindowedValue<T>>> dStreams = new ArrayList<>();\n+      for (String inputId : inputsMap.values()) {\n+        Dataset dataset = context.popDataset(inputId);\n+        if (dataset instanceof UnboundedDataset) {\n+          UnboundedDataset<T> unboundedDataset = (UnboundedDataset<T>) dataset;\n+          streamSources.addAll(unboundedDataset.getStreamSources());\n+          dStreams.add(unboundedDataset.getDStream());\n+        } else {\n+          // create a single RDD stream.\n+          Queue<JavaRDD<WindowedValue<T>>> q = new LinkedBlockingQueue<>();\n+          q.offer(((BoundedDataset) dataset).getRDD());\n+          // TODO: this is not recoverable from checkpoint!", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "ca707137600be0b54bc95e378546620a7ce77887"}, "originalPosition": 330}]}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "ca707137600be0b54bc95e378546620a7ce77887", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/ca707137600be0b54bc95e378546620a7ce77887", "committedDate": "2020-08-21T02:20:51Z", "message": "addressing comments"}, "afterCommit": {"oid": "42bcf99006e27f5b9638f82e6df92cd37213385a", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/42bcf99006e27f5b9638f82e6df92cd37213385a", "committedDate": "2020-08-21T19:02:38Z", "message": "addressing translator comments"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcyODY1ODY0", "url": "https://github.com/apache/beam/pull/12157#pullrequestreview-472865864", "createdAt": "2020-08-21T23:31:07Z", "commit": {"oid": "42bcf99006e27f5b9638f82e6df92cd37213385a"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}, {"__typename": "PullRequestCommit", "commit": {"oid": "67f771ca62701a2af5f555c09c52926f2dd0ccad", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/67f771ca62701a2af5f555c09c52926f2dd0ccad", "committedDate": "2020-08-22T04:04:11Z", "message": "[BEAM-7587] preliminary interface changes"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "498498f9e848c24ce83b2e119e2b71c0860018e6", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/498498f9e848c24ce83b2e119e2b71c0860018e6", "committedDate": "2020-08-22T04:04:11Z", "message": "spotlessApply formatting"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "85608fe0cec922777c5076650cde84a58790aa57", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/85608fe0cec922777c5076650cde84a58790aa57", "committedDate": "2020-08-22T04:04:11Z", "message": "fixes from feedback"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a1583f60125133da94463aaa980e1d9f6edc0533", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/a1583f60125133da94463aaa980e1d9f6edc0533", "committedDate": "2020-08-22T04:04:11Z", "message": "impulse v0"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8b3f39fb27a3215e32d76d0304d7448dc964cd28", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/8b3f39fb27a3215e32d76d0304d7448dc964cd28", "committedDate": "2020-08-22T04:04:11Z", "message": "formatting"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0a871c3409161f43703ac4c08ffdc7146162c61b", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/0a871c3409161f43703ac4c08ffdc7146162c61b", "committedDate": "2020-08-22T04:04:11Z", "message": "add streaming option in build.gradle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "2edb934c835e64d56f5b68580bd9197f8f51c044", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/2edb934c835e64d56f5b68580bd9197f8f51c044", "committedDate": "2020-08-22T04:04:11Z", "message": "transform group by key v0"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "c220242a8e26bb90d170cae4777cbe02158047c6", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/c220242a8e26bb90d170cae4777cbe02158047c6", "committedDate": "2020-08-22T04:04:12Z", "message": "streaming pipeline result"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "34dbd5b4fcf6271013ec8a3186669a4fad59bb0a", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/34dbd5b4fcf6271013ec8a3186669a4fad59bb0a", "committedDate": "2020-08-22T04:04:12Z", "message": "changed RDD creation in impulse translation"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "159b7f66de925d56d84aecfa8663be372ea167e3", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/159b7f66de925d56d84aecfa8663be372ea167e3", "committedDate": "2020-08-22T04:04:12Z", "message": "executable stage v0"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "b5fe54028fa757fd26736f56cd2816fb95ab8a1c", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/b5fe54028fa757fd26736f56cd2816fb95ab8a1c", "committedDate": "2020-08-22T04:04:12Z", "message": "flatten v0"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "ba50d6c13dd0778bc925b495d015213eb1da071d", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/ba50d6c13dd0778bc925b495d015213eb1da071d", "committedDate": "2020-08-22T04:04:12Z", "message": "no-output sink"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d59c015b18de06abe5c4f73e2beaa8d6c8d5e01a", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/d59c015b18de06abe5c4f73e2beaa8d6c8d5e01a", "committedDate": "2020-08-22T04:04:12Z", "message": "start streaming context in portable runner; modified impulse and exec stage"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "fc57c1ed6a7688bc5217cd799157b077af916ee6", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/fc57c1ed6a7688bc5217cd799157b077af916ee6", "committedDate": "2020-08-22T04:04:12Z", "message": "add watermark in impulse, max timestamp"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "a97a147e58a3adec95c626e83f80a24050bb6e23", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/a97a147e58a3adec95c626e83f80a24050bb6e23", "committedDate": "2020-08-22T04:04:13Z", "message": "reshuffle v0"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5b4ee13dc49f210303838205e3e1d7cdc8d69af7", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/5b4ee13dc49f210303838205e3e1d7cdc8d69af7", "committedDate": "2020-08-22T04:04:13Z", "message": "added jenkins job, changed watermark timestamp"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e5675a08994f9c64ed1f7d6efe3a817223076862", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/e5675a08994f9c64ed1f7d6efe3a817223076862", "committedDate": "2020-08-22T04:04:13Z", "message": "timeout option for streaming"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d57a0dfafd451eefd6d8ca77489180cf002dfa06", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/d57a0dfafd451eefd6d8ca77489180cf002dfa06", "committedDate": "2020-08-22T04:04:13Z", "message": "spotless, checkstyle, spotbugs"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "bc352ae9043f4695f89ebebf7876586893d8307a", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/bc352ae9043f4695f89ebebf7876586893d8307a", "committedDate": "2020-08-22T04:04:13Z", "message": "flatten v1, handle empty PCollections"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "829d32b5825cceb309cc8a7d62b4dce402a8baf4", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/829d32b5825cceb309cc8a7d62b4dce402a8baf4", "committedDate": "2020-08-22T04:04:13Z", "message": "separate build task for streaming, updated registrar test"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "55039cb6d9f712b1c866542d5c645e55c3c16454", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/55039cb6d9f712b1c866542d5c645e55c3c16454", "committedDate": "2020-08-22T04:04:14Z", "message": "address minor review comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "8793ed19e6ca8b6f403820ea487341570f6510e0", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/8793ed19e6ca8b6f403820ea487341570f6510e0", "committedDate": "2020-08-22T04:04:14Z", "message": "moved translator util functions to shared file"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "eef8bcf976a619c625991048270a4106fa0e8f1e", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/eef8bcf976a619c625991048270a4106fa0e8f1e", "committedDate": "2020-08-22T04:04:14Z", "message": "refactor build.gradle"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "0cdeaf2eb9f953b2ba067ab5b295c48c2b599d0d", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/0cdeaf2eb9f953b2ba067ab5b295c48c2b599d0d", "committedDate": "2020-08-22T04:04:14Z", "message": "rename timeout option\n\nrename timeout option"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "22d0c3f057f885a69dc040e45cae63fc6f7b2e5a", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/22d0c3f057f885a69dc040e45cae63fc6f7b2e5a", "committedDate": "2020-08-22T04:04:14Z", "message": "spotlessApply"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "e1fcbeef8d994a383224ad96ebbe939293a772c3", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/e1fcbeef8d994a383224ad96ebbe939293a772c3", "committedDate": "2020-08-22T04:04:14Z", "message": "add jira links"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "18a35f1ae464696e456b94a23a8b1a81ee1dbd42", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/18a35f1ae464696e456b94a23a8b1a81ee1dbd42", "committedDate": "2020-08-22T04:04:15Z", "message": "do not processElements if inputs is empty in executable stage function"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "42c97eb9e1cf4f9004eb00d3dbefbb197bbc3cf4", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/42c97eb9e1cf4f9004eb00d3dbefbb197bbc3cf4", "committedDate": "2020-08-22T04:04:15Z", "message": "modified SESF test to check that empty input iterators are not processed"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "dcbd09689ceea25ef9b3ec665bb6eab9175dbd75", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/dcbd09689ceea25ef9b3ec665bb6eab9175dbd75", "committedDate": "2020-08-22T04:04:15Z", "message": "filter tests in gradle build file"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "f057c944eef27ae5cb2a22219ae0801a577b380d", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/f057c944eef27ae5cb2a22219ae0801a577b380d", "committedDate": "2020-08-22T04:04:15Z", "message": "checkstyle\n\nspotless"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "d7518fcb6c70588bf609f087c79b20504ccdb042", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/d7518fcb6c70588bf609f087c79b20504ccdb042", "committedDate": "2020-08-22T04:04:15Z", "message": "addressing comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "5601d4cd39944669f084511325da1fac3e6f26b5", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/5601d4cd39944669f084511325da1fac3e6f26b5", "committedDate": "2020-08-22T04:04:15Z", "message": "addressing translator comments"}}, {"__typename": "PullRequestCommit", "commit": {"oid": "202e5c494ee14043123de13146f13589ed4a6743", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/202e5c494ee14043123de13146f13589ed4a6743", "committedDate": "2020-08-22T04:04:15Z", "message": "exclude unbounded pcollection tests in batch mode"}}, {"__typename": "HeadRefForcePushedEvent", "beforeCommit": {"oid": "42bcf99006e27f5b9638f82e6df92cd37213385a", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/42bcf99006e27f5b9638f82e6df92cd37213385a", "committedDate": "2020-08-21T19:02:38Z", "message": "addressing translator comments"}, "afterCommit": {"oid": "202e5c494ee14043123de13146f13589ed4a6743", "author": {"user": {"login": "annaqin418", "name": null}}, "url": "https://github.com/apache/beam/commit/202e5c494ee14043123de13146f13589ed4a6743", "committedDate": "2020-08-22T04:04:15Z", "message": "exclude unbounded pcollection tests in batch mode"}}, {"__typename": "PullRequestReview", "id": "MDE3OlB1bGxSZXF1ZXN0UmV2aWV3NDcyOTM4Mzc0", "url": "https://github.com/apache/beam/pull/12157#pullrequestreview-472938374", "createdAt": "2020-08-22T16:58:20Z", "commit": {"oid": "202e5c494ee14043123de13146f13589ed4a6743"}, "state": "APPROVED", "comments": {"totalCount": 0, "pageInfo": {"startCursor": null, "endCursor": null, "hasNextPage": false, "hasPreviousPage": false}, "nodes": []}}]}}}, "rateLimit": {"limit": 5000, "remaining": 3329, "cost": 1, "resetAt": "2021-10-28T16:48:13Z"}}}