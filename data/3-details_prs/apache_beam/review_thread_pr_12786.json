{"data": {"repository": {"pullRequest": {"id": "MDExOlB1bGxSZXF1ZXN0NDgyMjMyOTM2", "number": 12786, "reviewThreads": {"totalCount": 11, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMjo1MzoxN1rOEhcRpA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQxOTo1NzoxOVrOEh1iBg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTAxNzMyOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMjo1MzoxN1rOHOwoAQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDoxODowNVrOHOyLyA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTIzODc4NQ==", "bodyText": "Please also mention that withSplit() will be enabled automatically.", "url": "https://github.com/apache/beam/pull/12786#discussion_r485238785", "createdAt": "2020-09-08T22:53:17Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -135,6 +135,16 @@\n  * ...\n  * }</pre>\n  *\n+ * <p>Reading with projection can be enabled with the projection schema as following. The\n+ * projection_schema contains only the column that we would like to read and encoder_schema contains\n+ * all field but with the unwanted columns changed to nullable.", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a334bac48f808380386e1a7bb15a68d159045ef2"}, "originalPosition": 6}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2NDMyOA==", "bodyText": "Added.", "url": "https://github.com/apache/beam/pull/12786#discussion_r485264328", "createdAt": "2020-09-09T00:18:05Z", "author": {"login": "danielxjd"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -135,6 +135,16 @@\n  * ...\n  * }</pre>\n  *\n+ * <p>Reading with projection can be enabled with the projection schema as following. The\n+ * projection_schema contains only the column that we would like to read and encoder_schema contains\n+ * all field but with the unwanted columns changed to nullable.", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTIzODc4NQ=="}, "originalCommit": {"oid": "a334bac48f808380386e1a7bb15a68d159045ef2"}, "originalPosition": 6}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTAyNDMyOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMjo1NjoxOVrOHOwr8g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDoxODowMVrOHOyLtQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTIzOTc5NA==", "bodyText": "I think getProjectionSchema represents the field better.", "url": "https://github.com/apache/beam/pull/12786#discussion_r485239794", "createdAt": "2020-09-08T22:56:19Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -194,6 +204,10 @@ public static ReadFiles readFiles(Schema schema) {\n \n     abstract @Nullable Schema getSchema();\n \n+    abstract @Nullable Schema getProjection();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a334bac48f808380386e1a7bb15a68d159045ef2"}, "originalPosition": 21}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2NDMwOQ==", "bodyText": "Changed.", "url": "https://github.com/apache/beam/pull/12786#discussion_r485264309", "createdAt": "2020-09-09T00:18:01Z", "author": {"login": "danielxjd"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -194,6 +204,10 @@ public static ReadFiles readFiles(Schema schema) {\n \n     abstract @Nullable Schema getSchema();\n \n+    abstract @Nullable Schema getProjection();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTIzOTc5NA=="}, "originalCommit": {"oid": "a334bac48f808380386e1a7bb15a68d159045ef2"}, "originalPosition": 21}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTAyNDczOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMjo1NjozMFrOHOwsNQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDoxNzo1MVrOHOyLew==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTIzOTg2MQ==", "bodyText": "ditto.", "url": "https://github.com/apache/beam/pull/12786#discussion_r485239861", "createdAt": "2020-09-08T22:56:30Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -209,6 +223,10 @@ public static ReadFiles readFiles(Schema schema) {\n \n       abstract Builder setSchema(Schema schema);\n \n+      abstract Builder setProjectionEncoder(Schema schema);\n+\n+      abstract Builder setProjection(Schema schema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a334bac48f808380386e1a7bb15a68d159045ef2"}, "originalPosition": 34}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2NDI1MQ==", "bodyText": "Changed.", "url": "https://github.com/apache/beam/pull/12786#discussion_r485264251", "createdAt": "2020-09-09T00:17:51Z", "author": {"login": "danielxjd"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -209,6 +223,10 @@ public static ReadFiles readFiles(Schema schema) {\n \n       abstract Builder setSchema(Schema schema);\n \n+      abstract Builder setProjectionEncoder(Schema schema);\n+\n+      abstract Builder setProjection(Schema schema);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTIzOTg2MQ=="}, "originalCommit": {"oid": "a334bac48f808380386e1a7bb15a68d159045ef2"}, "originalPosition": 34}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTAyNzgyOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMjo1Nzo1NlrOHOwt7g==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDoxNzo0NlrOHOyLZw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI0MDMwMg==", "bodyText": "ditto.", "url": "https://github.com/apache/beam/pull/12786#discussion_r485240302", "createdAt": "2020-09-08T22:57:56Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -269,6 +298,10 @@ public void populateDisplayData(DisplayData.Builder builder) {\n \n     abstract @Nullable GenericData getAvroDataModel();\n \n+    abstract @Nullable Schema getProjectionEncoder();\n+\n+    abstract @Nullable Schema getProjection();", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a334bac48f808380386e1a7bb15a68d159045ef2"}, "originalPosition": 72}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2NDIzMQ==", "bodyText": "Changed.", "url": "https://github.com/apache/beam/pull/12786#discussion_r485264231", "createdAt": "2020-09-09T00:17:46Z", "author": {"login": "danielxjd"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -269,6 +298,10 @@ public void populateDisplayData(DisplayData.Builder builder) {\n \n     abstract @Nullable GenericData getAvroDataModel();\n \n+    abstract @Nullable Schema getProjectionEncoder();\n+\n+    abstract @Nullable Schema getProjection();", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI0MDMwMg=="}, "originalCommit": {"oid": "a334bac48f808380386e1a7bb15a68d159045ef2"}, "originalPosition": 72}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTAyODA3OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMjo1ODowM1rOHOwuEA==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDoxNzozOFrOHOyLUA==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI0MDMzNg==", "bodyText": "ditto.", "url": "https://github.com/apache/beam/pull/12786#discussion_r485240336", "createdAt": "2020-09-08T22:58:03Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -279,6 +312,10 @@ public void populateDisplayData(DisplayData.Builder builder) {\n \n       abstract Builder setAvroDataModel(GenericData model);\n \n+      abstract Builder setProjectionEncoder(Schema schema);\n+\n+      abstract Builder setProjection(Schema schema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a334bac48f808380386e1a7bb15a68d159045ef2"}, "originalPosition": 83}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2NDIwOA==", "bodyText": "Changed", "url": "https://github.com/apache/beam/pull/12786#discussion_r485264208", "createdAt": "2020-09-09T00:17:38Z", "author": {"login": "danielxjd"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -279,6 +312,10 @@ public void populateDisplayData(DisplayData.Builder builder) {\n \n       abstract Builder setAvroDataModel(GenericData model);\n \n+      abstract Builder setProjectionEncoder(Schema schema);\n+\n+      abstract Builder setProjection(Schema schema);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI0MDMzNg=="}, "originalCommit": {"oid": "a334bac48f808380386e1a7bb15a68d159045ef2"}, "originalPosition": 83}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTEwNjQ2OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMzozNTo1M1rOHOxb5A==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDoxNzozMVrOHOyLLg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI1MjA2OA==", "bodyText": "To minimize duplication:\nSchema coderSchema = getProjection() == null ? getSchema() : getProjectionEncoder(); \nreturn input\n              .apply(ParDo.of(new SplitReadFn(getAvroDataModel(), getProjection())))\n              .setCoder(AvroCoder.of(coderSchema));", "url": "https://github.com/apache/beam/pull/12786#discussion_r485252068", "createdAt": "2020-09-08T23:35:53Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -299,9 +344,14 @@ public ReadFiles withSplit() {\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n       if (isSplittable()) {\n+        if (getProjection() == null) {", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a334bac48f808380386e1a7bb15a68d159045ef2"}, "originalPosition": 107}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2NDE3NA==", "bodyText": "Changed.", "url": "https://github.com/apache/beam/pull/12786#discussion_r485264174", "createdAt": "2020-09-09T00:17:31Z", "author": {"login": "danielxjd"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -299,9 +344,14 @@ public ReadFiles withSplit() {\n     public PCollection<GenericRecord> expand(PCollection<FileIO.ReadableFile> input) {\n       checkNotNull(getSchema(), \"Schema can not be null\");\n       if (isSplittable()) {\n+        if (getProjection() == null) {", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI1MjA2OA=="}, "originalCommit": {"oid": "a334bac48f808380386e1a7bb15a68d159045ef2"}, "originalPosition": 107}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTExNjkzOnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMzo0MDo0N1rOHOxhxg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDowMjozNFrOHOx7CQ==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI1MzU3NA==", "bodyText": "Is there any reason to use String instead of Schema? Looks like this is referred only once.", "url": "https://github.com/apache/beam/pull/12786#discussion_r485253574", "createdAt": "2020-09-08T23:40:47Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -312,12 +362,14 @@ public ReadFiles withSplit() {\n     static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n       private Class<? extends GenericData> modelClass;\n       private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private String requestSchemaString;", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a334bac48f808380386e1a7bb15a68d159045ef2"}, "originalPosition": 124}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2MDA0MQ==", "bodyText": "Because the Schema class is not serializable so can only pass with string.", "url": "https://github.com/apache/beam/pull/12786#discussion_r485260041", "createdAt": "2020-09-09T00:02:34Z", "author": {"login": "danielxjd"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -312,12 +362,14 @@ public ReadFiles withSplit() {\n     static class SplitReadFn extends DoFn<FileIO.ReadableFile, GenericRecord> {\n       private Class<? extends GenericData> modelClass;\n       private static final Logger LOG = LoggerFactory.getLogger(SplitReadFn.class);\n+      private String requestSchemaString;", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI1MzU3NA=="}, "originalCommit": {"oid": "a334bac48f808380386e1a7bb15a68d159045ef2"}, "originalPosition": 124}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTEzMDk2OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMzo0Nzo0NlrOHOxpvQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDowMzozN1rOHOx8Eg==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI1NTYxMw==", "bodyText": "Just out of curiosity: hadoopConf is from options, is this necessary to set properties from options again?", "url": "https://github.com/apache/beam/pull/12786#discussion_r485255613", "createdAt": "2020-09-08T23:47:46Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -336,36 +388,41 @@ public void processElement(\n                 + tracker.currentRestriction().getFrom()\n                 + \" to \"\n                 + tracker.currentRestriction().getTo());\n-        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n-        ParquetFileReader reader =\n-            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        Configuration conf = getConfWithModelClass();\n         GenericData model = null;\n         if (modelClass != null) {\n           model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n         }\n-        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n-\n+        AvroReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+        if (requestSchemaString != null) {\n+          AvroReadSupport.setRequestedProjection(\n+              conf, new Schema.Parser().parse(requestSchemaString));\n+        }\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n         Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n         Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          hadoopConf.set(property, options.getProperty(property));", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a334bac48f808380386e1a7bb15a68d159045ef2"}, "originalPosition": 161}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2MDMwNg==", "bodyText": "You are right ,I should delete this part.", "url": "https://github.com/apache/beam/pull/12786#discussion_r485260306", "createdAt": "2020-09-09T00:03:37Z", "author": {"login": "danielxjd"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -336,36 +388,41 @@ public void processElement(\n                 + tracker.currentRestriction().getFrom()\n                 + \" to \"\n                 + tracker.currentRestriction().getTo());\n-        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n-        ParquetFileReader reader =\n-            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        Configuration conf = getConfWithModelClass();\n         GenericData model = null;\n         if (modelClass != null) {\n           model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n         }\n-        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n-\n+        AvroReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+        if (requestSchemaString != null) {\n+          AvroReadSupport.setRequestedProjection(\n+              conf, new Schema.Parser().parse(requestSchemaString));\n+        }\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n         Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n         Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          hadoopConf.set(property, options.getProperty(property));", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI1NTYxMw=="}, "originalCommit": {"oid": "a334bac48f808380386e1a7bb15a68d159045ef2"}, "originalPosition": 161}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTEzNjcyOnYy", "diffSide": "LEFT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMzo1MDozMVrOHOxtDg==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQwMDoxMjoyN1rOHOyF2w==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI1NjQ2Mg==", "bodyText": "Is this okay to be skipped?", "url": "https://github.com/apache/beam/pull/12786#discussion_r485256462", "createdAt": "2020-09-08T23:50:31Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -336,36 +388,41 @@ public void processElement(\n                 + tracker.currentRestriction().getFrom()\n                 + \" to \"\n                 + tracker.currentRestriction().getTo());\n-        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n-        ParquetFileReader reader =\n-            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        Configuration conf = getConfWithModelClass();\n         GenericData model = null;\n         if (modelClass != null) {\n           model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n         }\n-        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n-\n+        AvroReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+        if (requestSchemaString != null) {\n+          AvroReadSupport.setRequestedProjection(\n+              conf, new Schema.Parser().parse(requestSchemaString));\n+        }\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n         Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n         Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          hadoopConf.set(property, options.getProperty(property));\n+        }\n         FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n         MessageType fileSchema = parquetFileMetadata.getSchema();\n         Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n-\n         ReadSupport.ReadContext readContext =\n             readSupport.init(\n                 new InitContext(\n                     hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n         ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n-        MessageType requestedSchema = readContext.getRequestedSchema();\n+\n         RecordMaterializer<GenericRecord> recordConverter =\n             readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n-        reader.setRequestedSchema(requestedSchema);", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a334bac48f808380386e1a7bb15a68d159045ef2"}, "originalPosition": 176}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2MjgxMQ==", "bodyText": "Maybe I should leave this line", "url": "https://github.com/apache/beam/pull/12786#discussion_r485262811", "createdAt": "2020-09-09T00:12:27Z", "author": {"login": "danielxjd"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -336,36 +388,41 @@ public void processElement(\n                 + tracker.currentRestriction().getFrom()\n                 + \" to \"\n                 + tracker.currentRestriction().getTo());\n-        ParquetReadOptions options = HadoopReadOptions.builder(getConfWithModelClass()).build();\n-        ParquetFileReader reader =\n-            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n+        Configuration conf = getConfWithModelClass();\n         GenericData model = null;\n         if (modelClass != null) {\n           model = (GenericData) modelClass.getMethod(\"get\").invoke(null);\n         }\n-        ReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n-\n+        AvroReadSupport<GenericRecord> readSupport = new AvroReadSupport<GenericRecord>(model);\n+        if (requestSchemaString != null) {\n+          AvroReadSupport.setRequestedProjection(\n+              conf, new Schema.Parser().parse(requestSchemaString));\n+        }\n+        ParquetReadOptions options = HadoopReadOptions.builder(conf).build();\n+        ParquetFileReader reader =\n+            ParquetFileReader.open(new BeamParquetInputFile(file.openSeekable()), options);\n         Filter filter = checkNotNull(options.getRecordFilter(), \"filter\");\n         Configuration hadoopConf = ((HadoopReadOptions) options).getConf();\n+        for (String property : options.getPropertyNames()) {\n+          hadoopConf.set(property, options.getProperty(property));\n+        }\n         FileMetaData parquetFileMetadata = reader.getFooter().getFileMetaData();\n         MessageType fileSchema = parquetFileMetadata.getSchema();\n         Map<String, String> fileMetadata = parquetFileMetadata.getKeyValueMetaData();\n-\n         ReadSupport.ReadContext readContext =\n             readSupport.init(\n                 new InitContext(\n                     hadoopConf, Maps.transformValues(fileMetadata, ImmutableSet::of), fileSchema));\n         ColumnIOFactory columnIOFactory = new ColumnIOFactory(parquetFileMetadata.getCreatedBy());\n-        MessageType requestedSchema = readContext.getRequestedSchema();\n+\n         RecordMaterializer<GenericRecord> recordConverter =\n             readSupport.prepareForRead(hadoopConf, fileMetadata, fileSchema, readContext);\n-        reader.setRequestedSchema(requestedSchema);", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI1NjQ2Mg=="}, "originalCommit": {"oid": "a334bac48f808380386e1a7bb15a68d159045ef2"}, "originalPosition": 176}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzNTE1MjY2OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "isResolved": false, "comments": {"totalCount": 6, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOFQyMzo1ODo0NFrOHOx2XQ==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMDowNjozMFrOHPYeKw==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI1ODg0NQ==", "bodyText": "It would be also great if we could mention what is the expected improvement by projecting columns such as better memory usage or faster reading time.", "url": "https://github.com/apache/beam/pull/12786#discussion_r485258845", "createdAt": "2020-09-08T23:58:44Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -135,6 +135,16 @@\n  * ...\n  * }</pre>\n  *\n+ * <p>Reading with projection can be enabled with the projection schema as following. The\n+ * projection_schema contains only the column that we would like to read and encoder_schema contains\n+ * all field but with the unwanted columns changed to nullable.\n+ *", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "a334bac48f808380386e1a7bb15a68d159045ef2"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI2NDEzNQ==", "bodyText": "Added", "url": "https://github.com/apache/beam/pull/12786#discussion_r485264135", "createdAt": "2020-09-09T00:17:21Z", "author": {"login": "danielxjd"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -135,6 +135,16 @@\n  * ...\n  * }</pre>\n  *\n+ * <p>Reading with projection can be enabled with the projection schema as following. The\n+ * projection_schema contains only the column that we would like to read and encoder_schema contains\n+ * all field but with the unwanted columns changed to nullable.\n+ *", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI1ODg0NQ=="}, "originalCommit": {"oid": "a334bac48f808380386e1a7bb15a68d159045ef2"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTM4Mzg1OA==", "bodyText": "The description is still a bit vague. Can you please elaborate more on how much improvement users could expect from the column projection? For example, if the transform with the column projection only reads a half of the columns, does it also use only half of the memory and finish two times faster than the transform without the column projection? (If not, why is that?)", "url": "https://github.com/apache/beam/pull/12786#discussion_r485383858", "createdAt": "2020-09-09T07:04:01Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -135,6 +135,16 @@\n  * ...\n  * }</pre>\n  *\n+ * <p>Reading with projection can be enabled with the projection schema as following. The\n+ * projection_schema contains only the column that we would like to read and encoder_schema contains\n+ * all field but with the unwanted columns changed to nullable.\n+ *", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI1ODg0NQ=="}, "originalCommit": {"oid": "a334bac48f808380386e1a7bb15a68d159045ef2"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTc3OTcyNQ==", "bodyText": "The improvement is not as significant, since the processing time saved is only the time to read the unwanted columns, the reader will still go over the entire data set since data for each column in a row is stored interleaved. So the test showed that there is only a slight reduction in the reading time. For the memory, it is hard for me to track with the Dataflow server.", "url": "https://github.com/apache/beam/pull/12786#discussion_r485779725", "createdAt": "2020-09-09T17:03:13Z", "author": {"login": "danielxjd"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -135,6 +135,16 @@\n  * ...\n  * }</pre>\n  *\n+ * <p>Reading with projection can be enabled with the projection schema as following. The\n+ * projection_schema contains only the column that we would like to read and encoder_schema contains\n+ * all field but with the unwanted columns changed to nullable.\n+ *", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI1ODg0NQ=="}, "originalCommit": {"oid": "a334bac48f808380386e1a7bb15a68d159045ef2"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg4Mzg2OA==", "bodyText": "Please also adding\nNote that the improvement is not as significant though, since the processing\ntime saved is only the time to read the unwanted columns, the reader will still\ngo over the entire data set since data for each column in a row is stored interleaved.\n\nto the comments. It will help users a lot to understand how the column projection works.", "url": "https://github.com/apache/beam/pull/12786#discussion_r485883868", "createdAt": "2020-09-09T19:56:19Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -135,6 +135,16 @@\n  * ...\n  * }</pre>\n  *\n+ * <p>Reading with projection can be enabled with the projection schema as following. The\n+ * projection_schema contains only the column that we would like to read and encoder_schema contains\n+ * all field but with the unwanted columns changed to nullable.\n+ *", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI1ODg0NQ=="}, "originalCommit": {"oid": "a334bac48f808380386e1a7bb15a68d159045ef2"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg5MTYyNw==", "bodyText": "Added", "url": "https://github.com/apache/beam/pull/12786#discussion_r485891627", "createdAt": "2020-09-09T20:06:30Z", "author": {"login": "danielxjd"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -135,6 +135,16 @@\n  * ...\n  * }</pre>\n  *\n+ * <p>Reading with projection can be enabled with the projection schema as following. The\n+ * projection_schema contains only the column that we would like to read and encoder_schema contains\n+ * all field but with the unwanted columns changed to nullable.\n+ *", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTI1ODg0NQ=="}, "originalCommit": {"oid": "a334bac48f808380386e1a7bb15a68d159045ef2"}, "originalPosition": 7}]}}, {"id": "MDIzOlB1bGxSZXF1ZXN0UmV2aWV3VGhyZWFkMzAzOTE1NTI2OnYy", "diffSide": "RIGHT", "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "isResolved": false, "comments": {"totalCount": 2, "pageInfo": {"startCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQxOTo1NzoxOVrOHPYC6w==", "endCursor": "Y3Vyc29yOnYyOpK0MjAyMC0wOS0wOVQyMDowMTozN1rOHPYP-g==", "hasNextPage": false, "hasPreviousPage": false}, "nodes": [{"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg4NDY1MQ==", "bodyText": "increase or decrease?", "url": "https://github.com/apache/beam/pull/12786#discussion_r485884651", "createdAt": "2020-09-09T19:57:19Z", "author": {"login": "ihji"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -135,6 +135,20 @@\n  * ...\n  * }</pre>\n  *\n+ * <p>Reading with projection can be enabled with the projection schema as following. Splittable\n+ * reading is enabled when reading with projection. The projection_schema contains only the column\n+ * that we would like to read and encoder_schema contains the schema to encode the output with the\n+ * unwanted columns changed to nullable. Partial reading provide increase of reading time due to", "state": "SUBMITTED", "replyTo": null, "originalCommit": {"oid": "49114534fa52f057595a28d754357f168c8b0ecc"}, "originalPosition": 7}, {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg4Nzk5NA==", "bodyText": "should be decrease", "url": "https://github.com/apache/beam/pull/12786#discussion_r485887994", "createdAt": "2020-09-09T20:01:37Z", "author": {"login": "danielxjd"}, "path": "sdks/java/io/parquet/src/main/java/org/apache/beam/sdk/io/parquet/ParquetIO.java", "diffHunk": "@@ -135,6 +135,20 @@\n  * ...\n  * }</pre>\n  *\n+ * <p>Reading with projection can be enabled with the projection schema as following. Splittable\n+ * reading is enabled when reading with projection. The projection_schema contains only the column\n+ * that we would like to read and encoder_schema contains the schema to encode the output with the\n+ * unwanted columns changed to nullable. Partial reading provide increase of reading time due to", "state": "SUBMITTED", "replyTo": {"id": "MDI0OlB1bGxSZXF1ZXN0UmV2aWV3Q29tbWVudDQ4NTg4NDY1MQ=="}, "originalCommit": {"oid": "49114534fa52f057595a28d754357f168c8b0ecc"}, "originalPosition": 7}]}}]}}}, "rateLimit": {"limit": 5000, "remaining": 476, "cost": 1, "resetAt": "2021-11-12T09:44:50Z"}}}